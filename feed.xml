<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="ko"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://sweetlittlebird.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://sweetlittlebird.github.io/" rel="alternate" type="text/html" hreflang="ko" /><updated>2024-10-20T02:55:39+09:00</updated><id>https://sweetlittlebird.github.io/feed.xml</id><title type="html">Sweet Little Bird</title><subtitle>공부 기록과 개발 이야기를 담은 블로그입니다.</subtitle><entry><title type="html">[KANS 3기] Service Mesh : Istio - Mode (Sidecar, Ambient)</title><link href="https://sweetlittlebird.github.io/posts/2024-10-19-KANS-Study-Week7/" rel="alternate" type="text/html" title="[KANS 3기] Service Mesh : Istio - Mode (Sidecar, Ambient)" /><published>2024-10-19T01:00:18+09:00</published><updated>2024-10-19T01:00:18+09:00</updated><id>https://sweetlittlebird.github.io/posts/KANS%20Study%20-%20Week7</id><content type="html" xml:base="https://sweetlittlebird.github.io/posts/2024-10-19-KANS-Study-Week7/"><![CDATA[<h2 id="들어가며">들어가며</h2>

<p>이번주에는 Service Mesh와 대표적인 오픈소스 프로젝트인 Istio에 대해 알아 보겠습니다.
KANS 3기 7주차 스터디를 시작하겠습니다.</p>

<hr />

<h2 id="istio-소개">Istio 소개</h2>

<p>Istio는 서비스 메시를 구축하고 관리하기 위한 오픈소스 플랫폼입니다. Istio를 알아보기에 앞서 서비스 메시에 대해 알아보겠습니다.</p>

<h3 id="서비스-메시-service-mesh란">서비스 메시 (Service Mesh)란?</h3>

<ul>
  <li><strong>서비스 메시</strong>는 서비스 간 <strong>통신을 제어</strong>하고 <strong>모니터링</strong>하는 레이어를 제공하는 인프라스트럭처 계층입니다.</li>
  <li><strong>등장배경</strong> : MSA 환경에서 서비스가 많아지다 보니 서비스 간 통신이 복잡해지고, 이로 인해 서비스 간 통신을 관리하고 모니터링하는 것이 어려워졌습니다.
이로인해 장애가 발생하거나 병목 현상이 발생했을때 원인과 발생하는 구간을 찾기가 어려워졌습니다. 이것을 해결 하기 위해 등장한 것이 서비스 메시입니다.
<img src="/assets/2024/kans-3th/w7/20241019_kans_w7_1.png" alt="img.png" class="image-center w-80" /></li>
  <li><strong>개념</strong> : 마이크로 서비스 간에 통신이나 경로를 제어 - 예) istio, linkerd, consul, envoy, …</li>
  <li><strong>기본 동작</strong> : 파드간 통신경로에 프록시를 두고 트래픽을 모니터링하거나 컨트롤 합니다. 따라서 기존 어플리케이션을 수정하지 않고도 적용할 수 있습니다.
<img src="/assets/2024/kans-3th/w7/20241019_kans_w7_2.png" alt="img.png" class="image-center w-80" />
위의 그림 처럼 서비스 메시는 각 파드에 프록시를 두고 프록시를 통해 통신을 하도록한 다음 프록시를 통해 트래픽을 모니터링하거나 컨트롤 합니다.
    <ul>
      <li>이때 프록시는 <strong>Sidecar</strong> 모드로 동작하거나 <strong>Ambient</strong> 모드로 동작하며 대표적인 프록시로는 Envoys가 있습니다.</li>
      <li>Envoy는 구글, IBM, Lyft가 중심이 되어 개발하고 있는 오픈소스 프록시입니다.</li>
      <li>네트워크 투명성을 목표로 다양한 필터 체인 지원(L3, L4, L7), 동적 Configuration API를 제공하고, hot reload를 지원합니다.</li>
    </ul>
  </li>
  <li><strong>주요기능</strong>
    <ul>
      <li><strong>트래픽 모니터링</strong> : 요청의 에러율, 지연시간, 컨넥션 개수, 요청개수 등의 메트릭을 수집하여 모니터링하고, 서비스간 혹은 특정 요청 경로를 필터링 할 수 있습니다.
=&gt; 원인 파악 용이</li>
      <li><strong>트래픽 컨트롤</strong>
        <ul>
          <li>트래픽 시프팅(traffic shifting) : 트래픽을 서비스간에 분산시키는 기능으로, 특정 단말/사용자는 신규 어플리케이션에 연결하도록 하는 카나리 배포등에 활용할 수도 있습니다.</li>
          <li>서킷 브레이커(circuit breaker) : 특정 서비스에 문제가 있을때 접속을 차단하고, 출발지 서비스에 에러를 반환하도록 하는 기능입니다. (연쇄장애, 시스템 전체 장애 방지)</li>
          <li>플트 인젝션(fault injection) : 의도적으로 요청을 지연시키거나 실패하도록 할 수 있습니다. (비정상 상황 테스트)</li>
          <li>속도 제한(rate limiting) : 특정 서비스에 대한 요청 개수를 제한하는 기능입니다.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="envoy">Envoy</h3>

<ul>
  <li>지난주에 살짝 언급되었던 내용인데 이번 주에 좀 더 자세히 알아보겠습니다.</li>
  <li>Envoy는 구글, IBM, Lyft가 중심이 되어 개발하고 있는 오픈소스 프록시입니다.</li>
  <li>Istio의 핵심 기능들은 Envoy를 감싼 istio proxy를 통해 이루어지므로 Envoy에 대한 이해가 필요합니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_4.svg" alt="20241019_kans_w7_4.svg" class="image-center w-80" /></p>

<ul>
  <li>Envoy에서 사용하는 용어 들을 정리해 보았습니다.</li>
</ul>

<table>
  <thead>
    <tr>
      <th>용어</th>
      <th>설명</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Cluster</td>
      <td>envoy가 트래픽을 포워딩할 수 있는 논리적 서비스 (엔드포인트 셋트)</td>
    </tr>
    <tr>
      <td>Endpoint</td>
      <td>IP 주소와 포트 번호로 구성된 서비스의 실제 인스턴스. 엔드포인트가 모여서 하나의 Cluster를 이룸</td>
    </tr>
    <tr>
      <td>Listener</td>
      <td>클라이언트가 접속하는 포트, 유닉스 도메인 소켓 등을 노출하고, 다운스트림으로 부터 받은 요청을 처리</td>
    </tr>
    <tr>
      <td>Route</td>
      <td>Listener로 들어온 요청을 어떤 클러스터로 보낼지 정의</td>
    </tr>
    <tr>
      <td>Filter</td>
      <td>Listener로 부터 서비스에 트래픽 전달하기 전에 트래픽을 가공하거나 차단하는 역할을 하는 컴포넌트</td>
    </tr>
    <tr>
      <td>UpStream</td>
      <td>envoy 요청을 포워딩해서 연결하는 백엔드 네트워크 노드 - 사이드카일때는 application app, 아닐때는 원격 백엔드</td>
    </tr>
    <tr>
      <td>DownStream</td>
      <td>envoy로 연결하여 요청을 보내는 개체. 사이드카가 아닐때는 원격지의 클라이언트</td>
    </tr>
    <tr>
      <td>Host</td>
      <td>네트워크 통신이 가능한 개체 (PC, 서버, 휴대폰, 네트워크 어플리케이션 등)</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>많은 Service Mesh 솔루션이나, Gateway API 구현체들이 내부적으로 Envoy를 사용하고 있으며, Envoy가 제공하는 동적 구성을 위한 API(xDS Sync API)를 
이용하여 다양한 네트워크 정책을 구성하게 됩니다.</li>
  <li>Envoy의 xDS Sync API는 아래와 같은 레이어에서 동작합니다.
    <ul>
      <li>LDS - Listener Discovery Service</li>
      <li>RDS - Route Discovery Service</li>
      <li>CDS - Cluster Discovery Service</li>
      <li>EDS - Endpoint Discovery Service</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_5.png" alt="img.png" class="image-center w-80" /></p>

<p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_6.png" alt="img.png" class="image-center w-80" /></p>

<h4 id="envoy-실습">Envoy 실습</h4>

<ul>
  <li>test pc에 Envoy 설치</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 설치</span>
<span class="c"># echo "deb [signed-by=/etc/apt/keyrings/envoy-keyring.gpg] https://apt.envoyproxy.io focal main" | sudo tee /etc/apt/sources.list.d/envoy.list</span>
<span class="nv">$ </span>wget <span class="nt">-O-</span> https://apt.envoyproxy.io/signing.key | <span class="nb">sudo </span>gpg <span class="nt">--dearmor</span> <span class="nt">-o</span> /etc/apt/keyrings/envoy-keyring.gpg
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"deb [signed-by=/etc/apt/keyrings/envoy-keyring.gpg] https://apt.envoyproxy.io jammy main"</span> | <span class="nb">sudo tee</span> /etc/apt/sources.list.d/envoy.list
<span class="nv">$ </span><span class="nb">sudo </span>apt-get update <span class="o">&amp;&amp;</span> <span class="nb">sudo </span>apt-get <span class="nb">install </span>envoy <span class="nt">-y</span>

<span class="c"># 확인</span>
<span class="nv">$ </span>envoy <span class="nt">--version</span>
<span class="c"># =&gt; envoy  version: e3b4a6e9570da15ac1caffdded17a8bebdc7dfc9/1.32.0/Clean/RELEASE/BoringSSL</span>

<span class="c"># 도움말</span>
<span class="nv">$ </span>envoy <span class="nt">--help</span>
</code></pre></div></div>

<ul>
  <li>Envoy proxy 실습 - <a href="https://www.envoyproxy.io/docs/envoy/latest/start/quick-start/">Link</a>
    <ul>
      <li>envoy-demo.yml 작성
        <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># envoy-demo.yml</span>
<span class="na">static_resources</span><span class="pi">:</span>
    
  <span class="na">listeners</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">listener_0</span>
    <span class="na">address</span><span class="pi">:</span>
      <span class="na">socket_address</span><span class="pi">:</span>
        <span class="na">address</span><span class="pi">:</span> <span class="s">0.0.0.0</span>
        <span class="na">port_value</span><span class="pi">:</span> <span class="m">10000</span>
    <span class="na">filter_chains</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">filters</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">envoy.filters.network.http_connection_manager</span>
        <span class="na">typed_config</span><span class="pi">:</span>
          <span class="s2">"</span><span class="s">@type"</span><span class="err">:</span> <span class="s">type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager</span>
          <span class="s">stat_prefix</span><span class="err">:</span> <span class="s">ingress_http</span>
          <span class="s">access_log</span><span class="err">:</span>
          <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">envoy.access_loggers.stdout</span>
            <span class="na">typed_config</span><span class="pi">:</span>
              <span class="s2">"</span><span class="s">@type"</span><span class="err">:</span> <span class="s">type.googleapis.com/envoy.extensions.access_loggers.stream.v3.StdoutAccessLog</span>
          <span class="na">http_filters</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">envoy.filters.http.router</span>
          <span class="na">route_config</span><span class="pi">:</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">local_route</span>
            <span class="na">virtual_hosts</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">local_service</span>
              <span class="na">domains</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">*"</span><span class="pi">]</span>
              <span class="na">routes</span><span class="pi">:</span>
              <span class="pi">-</span> <span class="na">match</span><span class="pi">:</span>
                  <span class="na">prefix</span><span class="pi">:</span> <span class="s2">"</span><span class="s">/"</span>
                <span class="na">route</span><span class="pi">:</span>
                  <span class="na">host_rewrite_literal</span><span class="pi">:</span> <span class="s">www.envoyproxy.io</span>
                  <span class="na">cluster</span><span class="pi">:</span> <span class="s">service_envoyproxy_io</span>
    
  <span class="na">clusters</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">service_envoyproxy_io</span>
    <span class="na">type</span><span class="pi">:</span> <span class="s">LOGICAL_DNS</span>
    <span class="c1"># Comment out the following line to test on v6 networks</span>
    <span class="na">dns_lookup_family</span><span class="pi">:</span> <span class="s">V4_ONLY</span>
    <span class="na">connect_timeout</span><span class="pi">:</span> <span class="s">5s</span>
    <span class="na">load_assignment</span><span class="pi">:</span>
      <span class="na">cluster_name</span><span class="pi">:</span> <span class="s">service_envoyproxy_io</span>
      <span class="na">endpoints</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">lb_endpoints</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">endpoint</span><span class="pi">:</span>
            <span class="na">address</span><span class="pi">:</span>
              <span class="na">socket_address</span><span class="pi">:</span>
                <span class="na">address</span><span class="pi">:</span> <span class="s">www.envoyproxy.io</span>
                <span class="na">port_value</span><span class="pi">:</span> <span class="m">443</span>
    <span class="na">transport_socket</span><span class="pi">:</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">envoy.transport_sockets.tls</span>
      <span class="na">typed_config</span><span class="pi">:</span>
        <span class="s2">"</span><span class="s">@type"</span><span class="err">:</span> <span class="s">type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext</span>
        <span class="s">sni</span><span class="err">:</span> <span class="s">www.envoyproxy.io</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>실행</p>

        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c"># (터미널1) 데모 config 적용하여 실행</span>
  <span class="nv">$ </span>curl <span class="nt">-O</span> https://www.envoyproxy.io/docs/envoy/latest/_downloads/92dcb9714fb6bc288d042029b34c0de4/envoy-demo.yaml
  <span class="nv">$ </span>envoy <span class="nt">-c</span> envoy-demo.yaml
  <span class="c"># =&gt; [2024-10-01 16:41:51.547][4479][info][main] [source/server/server.cc:426] initializing epoch 0 (base id=0, hot restart version=11.104)</span>
  <span class="c">#    ...</span>
    
  <span class="c"># (터미널2) 정보 확인</span>
  <span class="nv">$ </span>ss <span class="nt">-tnlp</span>
  <span class="c"># =&gt; State           Recv-Q           Send-Q                     Local Address:Port                      Peer Address:Port          Process</span>
  <span class="c">#    LISTEN          0                4096                             0.0.0.0:10000                          0.0.0.0:*              users:((&amp;quot;envoy&amp;quot;,pid=4479,fd=35))</span>
  <span class="c">#    LISTEN          0                4096                             0.0.0.0:10000                          0.0.0.0:*              users:((&amp;quot;envoy&amp;quot;,pid=4479,fd=34))</span>
  <span class="c">#    LISTEN          0                4096                             0.0.0.0:10000                          0.0.0.0:*              users:((&amp;quot;envoy&amp;quot;,pid=4479,fd=33))</span>
  <span class="c">#    LISTEN          0                4096                             0.0.0.0:10000                          0.0.0.0:*              users:((&amp;quot;envoy&amp;quot;,pid=4479,fd=32))</span>
  <span class="c">#    ...</span>
    
  <span class="c"># 접속 테스트</span>
  <span class="nv">$ </span>curl <span class="nt">-s</span> http://127.0.0.1:10000 | <span class="nb">grep</span> <span class="nt">-o</span> <span class="s2">"&lt;title&gt;.*&lt;/title&gt;"</span>
  <span class="c"># =&gt; &amp;lt;title&amp;gt;Envoy proxy - home&amp;lt;/title&amp;gt;</span>
    
  <span class="c"># 외부 접속 정보 출력</span>
  <span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"http://</span><span class="si">$(</span>curl <span class="nt">-s</span> ipinfo.io/ip<span class="si">)</span><span class="s2">:10000"</span>
  <span class="c"># =&gt; http://54.123.42.212:10000</span>
    
  <span class="nt">--------------------</span>
  <span class="c"># 자신의 PC 웹브라우저에서 외부 접속 정보 접속 확인!</span>
    
  <span class="c"># k3s-m 에서 접속 테스트</span>
  <span class="nv">$ </span>curl <span class="nt">-s</span> http://192.168.56.104:10000 | <span class="nb">grep</span> <span class="nt">-o</span> <span class="s2">"&lt;title&gt;.*&lt;/title&gt;"</span>
  <span class="c"># =&gt; &amp;lt;title&amp;gt;Envoy proxy - home&amp;lt;/title&amp;gt; </span>
  <span class="nt">--------------------</span>
    
  <span class="c"># 연결 정보 확인</span>
  <span class="nv">$ </span>ss <span class="nt">-tnp</span>
    
  <span class="c"># (터미널1) envoy 실행 취소(CTRL+C) 후 (관리자페이지) 설정 덮어쓰기 - 링크</span>
  <span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOT</span><span class="sh">&gt; envoy-override.yaml
  admin:
    address:
      socket_address:
        address: 0.0.0.0
        port_value: 9902
</span><span class="no">  EOT
</span>  <span class="nv">$ </span>envoy <span class="nt">-c</span> envoy-demo.yaml <span class="nt">--config-yaml</span> <span class="s2">"</span><span class="si">$(</span><span class="nb">cat </span>envoy-override.yaml<span class="si">)</span><span class="s2">"</span>
    
  <span class="c"># envoy 관리페이지 외부 접속 정보 출력</span>
  <span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"http://</span><span class="si">$(</span>curl <span class="nt">-s</span> ipinfo.io/ip<span class="si">)</span><span class="s2">:9902"</span>
  <span class="c"># =&gt; http://54.123.42.212:9902</span>
    
  <span class="nt">--------------------</span>
  <span class="c"># 자신의 PC 웹브라우저에서 관리 페이지 외부 접속 정보 접속 확인!</span>
</code></pre></div>        </div>

        <p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_7.png" alt="img.png" class="image-center" />
  <em class="image-caption">PC에서 접속한 화면</em></p>
      </li>
    </ul>
  </li>
</ul>

<h3 id="istio-소개-1">Istio 소개</h3>

<ul>
  <li><strong>Istio</strong>는 서비스 메시를 구축하고 관리하기 위한 오픈소스 플랫폼입니다.</li>
  <li><strong>Istio의 구성</strong>
<img src="/assets/2024/kans-3th/w7/20241019_kans_w7_3.svg" alt="20241019_kans_w7_3.svg" class="image-center w-80" />
    <ul>
      <li>파일럿(Pilot) : 모든 Envoy 사이드카에서 프록시 라우팅 규칙을 관리하며, 서비스 디스커버리, 로드밸런싱 설정을 제공합니다.</li>
      <li>겔리(Galley) : Istio와 쿠버네티스를 연결하는 역할을 합니다. 서비스 메시 구성 데이터를 검증하고 변환합니다.</li>
      <li>시타델(Citadel) : 서비스 간의 인증과 보안을 관리합니다. 서비스 간의 TLS 통신을 제공하고, 서비스 간의 인증을 관리합니다.</li>
    </ul>
  </li>
  <li>Istio의 구성요소
    <ul>
      <li>istiod : Istio의 중앙 제어 플레인으로, Pilot, Citadel, Galley를 포함합니다.</li>
      <li>istio proxy : Envoy 기반의 프록시로, istiod와 통신하며, 서비스 트래픽을 통제하고 옵저빌리티를 위한 메트릭을 제공합니다.</li>
    </ul>
  </li>
  <li>특징
    <ul>
      <li>Istio는 각 파드안에서 사이드카로 동작하는 Envoy가 트래픽을 제어하고 모니터링합니다.</li>
      <li>모든 마이크로 서비스간 통신은 Envoy를 통해 이루어지며, 이를 통해 메트릭을 수집하거나 컨트롤 할 수 있습니다.</li>
      <li>트래픽을 컨트롤 하기 위해서 Envoy 프록시에 전송룰을 정의 합니다.</li>
      <li>마이크로 서비스간의 통신을 mutual TLS 인증(mTLS)을 통해 보안합니다.</li>
      <li>각 어플리케이션은 파드 내의 엔보이 프록시에 접속하기 위해 localhost에 TCP 접속을 합니다.</li>
    </ul>
  </li>
</ul>

<h3 id="istio-설치-sidecar-모드">Istio 설치 (Sidecar 모드)</h3>

<ul>
  <li>Istio 공식 문서 : <a href="https://istio.io/latest/docs/">Link</a>
    <ul>
      <li>Istio Sidecar mode 설치 : v1.23.2 - 
<a href="https://istio.io/latest/docs/releases/supported-releases/#support-status-of-istio-releases">버전</a>
<a href="https://istio.io/latest/docs/setup/getting-started/">설치</a>,</li>
      <li>without GwApi - <a href="https://istio.io/latest/docs/setup/additional-setup/getting-started-istio-apis/">Docs</a></li>
      <li>Operator 방식 설치 : https://istio.io/latest/docs/setup/install/operator/ (Istio Operator는 <strong>Deprecated</strong> 되었습니다.)</li>
    </ul>
  </li>
  <li>Istio 설치
```bash</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># istioctl 설치</span>
<span class="nv">$ </span><span class="nb">export </span><span class="nv">ISTIOV</span><span class="o">=</span>1.23.2
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"export ISTIOV=1.23.2"</span> <span class="o">&gt;&gt;</span> /etc/profile
<span class="nv">$ </span>curl <span class="nt">-s</span> <span class="nt">-L</span> https://istio.io/downloadIstio | <span class="nv">ISTIO_VERSION</span><span class="o">=</span><span class="nv">$ISTIOV</span> <span class="nv">TARGET_ARCH</span><span class="o">=</span>x86_64 sh -
<span class="nv">$ </span>tree istio-<span class="nv">$ISTIOV</span> <span class="nt">-L</span> 2 <span class="c"># sample yaml 포함</span>
<span class="c"># =&gt; istio-1.23.2</span>
<span class="c">#    ├── LICENSE</span>
<span class="c">#    ├── README.md</span>
<span class="c">#    ├── bin</span>
<span class="c">#    │   └── istioctl</span>
<span class="c">#    ├── manifest.yaml</span>
<span class="c">#    ├── manifests</span>
<span class="c">#    │   ├── charts</span>
<span class="c">#    │   └── profiles</span>
<span class="c">#    ├── samples</span>
<span class="c">#    │   ...</span>
<span class="c">#    └── tools</span>
<span class="c">#        ├── _istioctl</span>
<span class="c">#        ├── certs</span>
<span class="c">#        └── istioctl.bash</span>
<span class="nv">$ </span><span class="nb">cp </span>istio-<span class="nv">$ISTIOV</span>/bin/istioctl /usr/local/bin/istioctl
<span class="nv">$ </span>istioctl version <span class="nt">--remote</span><span class="o">=</span><span class="nb">false</span>
<span class="c"># =&gt; client version: 1.23.2</span>

<span class="c"># (demo 프로파일) 컨트롤 플레인 배포 - 링크 Customizing</span>
<span class="c"># The istioctl command supports the full IstioOperator API via command-line options for individual settings or for passing a yaml file containing an IstioOperator custom resource (CR).</span>
<span class="nv">$ </span>istioctl profile list
<span class="c"># =&gt; Istio configuration profiles:</span>
<span class="c">#        ambient</span>
<span class="c">#        default</span>
<span class="c">#        demo</span>
<span class="c">#        empty</span>
<span class="c">#        minimal</span>
<span class="c">#        openshift</span>
<span class="c">#        openshift-ambient</span>
<span class="c">#        preview</span>
<span class="c">#        remote</span>
<span class="c">#        stable</span>
<span class="nv">$ </span>istioctl profile dump default
<span class="nv">$ </span>istioctl profile dump <span class="nt">--config-path</span> components.ingressGateways
<span class="c"># =&gt; - enabled: true</span>
<span class="c">#      name: istio-ingressgateway</span>
<span class="nv">$ </span>istioctl profile dump <span class="nt">--config-path</span> values.gateways.istio-ingressgateway
<span class="c"># =&gt; {}</span>
<span class="nv">$ </span>istioctl profile dump demo
<span class="c"># =&gt; apiVersion: install.istio.io/v1alpha1</span>
<span class="c">#    kind: IstioOperator</span>
<span class="c">#    spec:</span>
<span class="c">#      components:</span>
<span class="c">#        base:</span>
<span class="c">#          enabled: true</span>
<span class="c">#        egressGateways:</span>
<span class="c">#        - enabled: true</span>
<span class="c">#          name: istio-egressgateway</span>
<span class="c">#        ingressGateways:</span>
<span class="c">#        - enabled: true</span>
<span class="c">#          name: istio-ingressgateway</span>
<span class="c">#        pilot:</span>
<span class="c">#          enabled: true</span>
<span class="c">#      hub: docker.io/istio</span>
<span class="c">#      profile: demo</span>
<span class="c">#      tag: 1.23.2</span>
<span class="c">#      values:</span>
<span class="c">#        defaultRevision: &amp;quot;&amp;quot;</span>
<span class="c">#        gateways:</span>
<span class="c">#          istio-egressgateway: {}</span>
<span class="c">#          istio-ingressgateway: {}</span>
<span class="c">#        global:</span>
<span class="c">#          configValidation: true</span>
<span class="c">#          istioNamespace: istio-system</span>
<span class="c">#        profile: demo</span>

<span class="nv">$ </span>istioctl profile dump demo <span class="o">&gt;</span> demo-profile.yaml
<span class="nv">$ </span>vi demo-profile.yaml <span class="c"># 복잡성을 줄이게 실습 시나리오 환경 맞춤</span>
<span class="nt">--------------------</span>
    egressGateways:
    - enabled: <span class="nb">false</span>
<span class="nt">--------------------</span>    

<span class="nv">$ </span>istioctl <span class="nb">install</span> <span class="nt">-f</span> demo-profile.yaml <span class="nt">-y</span>
<span class="c"># =&gt; ✔ Istio core installed ⛵️</span>
<span class="c">#    ✔ Istiod installed 🧠</span>
<span class="c">#    ✔ Ingress gateways installed 🛬</span>
<span class="c">#    ✔ Installation complete</span>
<span class="c">#    Made this installation the default for cluster-wide operations.</span>

<span class="c"># 설치 확인 : istiod, istio-ingressgateway</span>
<span class="nv">$ </span>kubectl get all,svc,ep,sa,cm,secret,pdb <span class="nt">-n</span> istio-system
<span class="c"># =&gt; NAME                                        READY   STATUS    RESTARTS      AGE</span>
<span class="c">#    pod/istio-ingressgateway-5f9f654d46-l7mqp   1/1     Running   0             14m</span>
<span class="c">#    pod/istiod-7f8b586864-8mc4c                 1/1     Running   1 (82s ago)   14m</span>
<span class="c">#    </span>
<span class="c">#    NAME                           TYPE           CLUSTER-IP      EXTERNAL-IP                                    PORT(S)                                                                      AGE</span>
<span class="c">#    service/istio-ingressgateway   LoadBalancer   10.10.200.171   192.168.10.101,192.168.10.102,192.168.10.103   15021:30953/TCP,80:31677/TCP,443:30737/TCP,31400:30617/TCP,15443:31668/TCP   14m</span>
<span class="c">#    service/istiod                 ClusterIP      10.10.200.215   &amp;lt;none&amp;gt;                                         15010/TCP,15012/TCP,443/TCP,15014/TCP                                        14m</span>
<span class="c">#    </span>
<span class="c">#    NAME                                   READY   UP-TO-DATE   AVAILABLE   AGE</span>
<span class="c">#    deployment.apps/istio-ingressgateway   1/1     1            1           14m</span>
<span class="c">#    deployment.apps/istiod                 1/1     1            1           14m</span>
<span class="c">#    </span>
<span class="c">#    NAME                                              DESIRED   CURRENT   READY   AGE</span>
<span class="c">#    replicaset.apps/istio-ingressgateway-5f9f654d46   1         1         1       14m</span>
<span class="c">#    replicaset.apps/istiod-7f8b586864                 1         1         1       14m</span>
<span class="c">#    </span>
<span class="c">#    NAME                             ENDPOINTS                                                           AGE</span>
<span class="c">#    endpoints/istio-ingressgateway   172.16.2.14:15443,172.16.2.14:15021,172.16.2.14:31400 + 2 more...   14m</span>
<span class="c">#    endpoints/istiod                 172.16.3.16:15012,172.16.3.16:15010,172.16.3.16:15017 + 1 more...   14m</span>
<span class="c">#    </span>
<span class="c">#    NAME                                                  SECRETS   AGE</span>
<span class="c">#    serviceaccount/istio-ingressgateway-service-account   0         14m</span>
<span class="c">#    serviceaccount/istio-reader-service-account           0         14m</span>
<span class="c">#    serviceaccount/istiod                                 0         14m</span>
<span class="c">#    ...</span>
<span class="c">#    </span>
<span class="c">#    NAME                                            DATA   AGE</span>
<span class="c">#    configmap/istio                                 2      14m</span>
<span class="c">#    configmap/istio-sidecar-injector                2      14m</span>
<span class="c">#    ...</span>
<span class="c">#    </span>
<span class="c">#    NAME                     TYPE               DATA   AGE</span>
<span class="c">#    secret/istio-ca-secret   istio.io/ca-root   5      14m</span>
<span class="c">#    </span>
<span class="c">#    NAME                                              MIN AVAILABLE   MAX UNAVAILABLE   ALLOWED DISRUPTIONS   AGE</span>
<span class="c">#    poddisruptionbudget.policy/istio-ingressgateway   1               N/A               0                     14m</span>
<span class="c">#    poddisruptionbudget.policy/istiod                 1               N/A               0                     14m</span>
<span class="nv">$ </span>kubectl get crd | <span class="nb">grep </span>istio.io | <span class="nb">sort</span>
<span class="c"># =&gt; authorizationpolicies.security.istio.io      2024-10-01T05:26:47Z</span>
<span class="c">#    destinationrules.networking.istio.io         2024-10-01T05:26:47Z</span>
<span class="c">#    envoyfilters.networking.istio.io             2024-10-01T05:26:47Z</span>
<span class="c">#    gateways.networking.istio.io                 2024-10-01T05:26:47Z</span>
<span class="c">#    peerauthentications.security.istio.io        2024-10-01T05:26:47Z</span>
<span class="c">#    proxyconfigs.networking.istio.io             2024-10-01T05:26:47Z</span>
<span class="c">#    requestauthentications.security.istio.io     2024-10-01T05:26:47Z</span>
<span class="c">#    serviceentries.networking.istio.io           2024-10-01T05:26:47Z</span>
<span class="c">#    sidecars.networking.istio.io                 2024-10-01T05:26:47Z</span>
<span class="c">#    telemetries.telemetry.istio.io               2024-10-01T05:26:48Z</span>
<span class="c">#    virtualservices.networking.istio.io          2024-10-01T05:26:48Z</span>
<span class="c">#    wasmplugins.extensions.istio.io              2024-10-01T05:26:48Z</span>
<span class="c">#    workloadentries.networking.istio.io          2024-10-01T05:26:48Z</span>
<span class="c">#    workloadgroups.networking.istio.io           2024-10-01T05:26:48Z</span>

<span class="c"># istio-ingressgateway 의 envoy 버전 확인</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deploy/istio-ingressgateway <span class="nt">-n</span> istio-system <span class="nt">-c</span> istio-proxy <span class="nt">--</span> envoy <span class="nt">--version</span>
<span class="c"># =&gt; envoy  version: 6c72b2179f5a58988b920a55b0be8346de3f7b35/1.31.2-dev/Clean/RELEASE/BoringSSL</span>

<span class="c"># istio-ingressgateway 서비스 NodePort로 변경</span>
<span class="nv">$ </span>kubectl patch svc <span class="nt">-n</span> istio-system istio-ingressgateway <span class="nt">-p</span> <span class="s1">'{"spec":{"type":"NodePort"}}'</span>
<span class="c"># =&gt; service/istio-ingressgateway patched</span>

<span class="c"># istio-ingressgateway 서비스 확인</span>
<span class="nv">$ </span>kubectl get svc,ep <span class="nt">-n</span> istio-system istio-ingressgateway
<span class="c"># =&gt; NAME                           TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                                      AGE</span>
<span class="c">#    service/istio-ingressgateway   NodePort   10.10.200.171   &amp;lt;none&amp;gt;        15021:30953/TCP,80:31677/TCP,443:30737/TCP,31400:30617/TCP,15443:31668/TCP   16m</span>
<span class="c">#    </span>
<span class="c">#    NAME                             ENDPOINTS                                                           AGE</span>
<span class="c">#    endpoints/istio-ingressgateway   172.16.2.14:15443,172.16.2.14:15021,172.16.2.14:31400 + 2 more...   16m</span>

<span class="c">## istio-ingressgateway 서비스 포트 정보 확인</span>
<span class="nv">$ </span>kubectl get svc <span class="nt">-n</span> istio-system istio-ingressgateway <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.spec.ports[<span class="k">*</span><span class="o">]}</span> | jq
<span class="c"># =&gt; {</span>
<span class="c">#      &amp;quot;name&amp;quot;: &amp;quot;https&amp;quot;,</span>
<span class="c">#      &amp;quot;nodePort&amp;quot;: 30737,</span>
<span class="c">#      &amp;quot;port&amp;quot;: 443,</span>
<span class="c">#      &amp;quot;protocol&amp;quot;: &amp;quot;TCP&amp;quot;,</span>
<span class="c">#      &amp;quot;targetPort&amp;quot;: 8443</span>
<span class="c">#    }</span>
<span class="c">#    {</span>
<span class="c">#      &amp;quot;name&amp;quot;: &amp;quot;tcp&amp;quot;,</span>
<span class="c">#      &amp;quot;nodePort&amp;quot;: 30617,</span>
<span class="c">#      &amp;quot;port&amp;quot;: 31400,</span>
<span class="c">#      &amp;quot;protocol&amp;quot;: &amp;quot;TCP&amp;quot;,</span>
<span class="c">#      &amp;quot;targetPort&amp;quot;: 31400</span>
<span class="c">#    }</span>
<span class="c">#    ...</span>

<span class="c">## istio-ingressgateway 디플로이먼트 파드의 포트 정보 확인 </span>
<span class="nv">$ </span>kubectl get deploy/istio-ingressgateway <span class="nt">-n</span> istio-system <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.spec.template.spec.containers[0].ports[<span class="k">*</span><span class="o">]}</span> | jq
<span class="c"># =&gt; ...</span>
<span class="c">#    {</span>
<span class="c">#      &amp;quot;containerPort&amp;quot;: 8443,</span>
<span class="c">#      &amp;quot;protocol&amp;quot;: &amp;quot;TCP&amp;quot;</span>
<span class="c">#    }</span>
<span class="c">#    {</span>
<span class="c">#      &amp;quot;containerPort&amp;quot;: 31400,</span>
<span class="c">#      &amp;quot;protocol&amp;quot;: &amp;quot;TCP&amp;quot;</span>
<span class="c">#    }</span>
<span class="c">#    ...</span>
<span class="nv">$ </span>kubectl get deploy/istio-ingressgateway <span class="nt">-n</span> istio-system <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.spec.template.spec.containers[0].readinessProbe<span class="o">}</span> | jq
<span class="c"># =&gt; {</span>
<span class="c">#      &amp;quot;failureThreshold&amp;quot;: 30,</span>
<span class="c">#      &amp;quot;httpGet&amp;quot;: {</span>
<span class="c">#        &amp;quot;path&amp;quot;: &amp;quot;/healthz/ready&amp;quot;,</span>
<span class="c">#        &amp;quot;port&amp;quot;: 15021,</span>
<span class="c">#        &amp;quot;scheme&amp;quot;: &amp;quot;HTTP&amp;quot;</span>
<span class="c">#      },</span>
<span class="c">#      &amp;quot;initialDelaySeconds&amp;quot;: 1,</span>
<span class="c">#      &amp;quot;periodSeconds&amp;quot;: 2,</span>
<span class="c">#      &amp;quot;successThreshold&amp;quot;: 1,</span>
<span class="c">#      &amp;quot;timeoutSeconds&amp;quot;: 1</span>
<span class="c">#    }</span>

<span class="c"># istiod(컨트롤플레인) 디플로이먼트 정보 확인</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deployment.apps/istiod <span class="nt">-n</span> istio-system <span class="nt">--</span> ss <span class="nt">-tnlp</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deployment.apps/istiod <span class="nt">-n</span> istio-system <span class="nt">--</span> ss <span class="nt">-tnp</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deployment.apps/istiod <span class="nt">-n</span> istio-system <span class="nt">--</span> ps <span class="nt">-ef</span>
<span class="c"># =&gt; UID          PID    PPID  C STIME TTY          TIME CMD</span>
<span class="c">#    istio-p+       1       0  0 05:39 ?        00:00:01 /usr/local/bin/pilot-discovery discovery --monitoringAddr=:15014 --log_output_level=default:info --domain cluster.local --ke</span>

<span class="c"># istio-ingressgateway 디플로이먼트 정보 확인</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deployment.apps/istio-ingressgateway <span class="nt">-n</span> istio-system <span class="nt">--</span> ss <span class="nt">-tnlp</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deployment.apps/istio-ingressgateway <span class="nt">-n</span> istio-system <span class="nt">--</span> ss <span class="nt">-tnp</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deployment.apps/istio-ingressgateway <span class="nt">-n</span> istio-system <span class="nt">--</span> ps <span class="nt">-ef</span>
<span class="c"># =&gt; UID          PID    PPID  C STIME TTY          TIME CMD</span>
<span class="c">#    istio-p+       1       0  0 05:27 ?        00:00:08 /usr/local/bin/pilot-agent proxy router --domain istio-system.svc.cluster.local</span>
<span class="c">#    istio-p+      16       1  0 05:27 ?        00:00:04 /usr/local/bin/envoy -c etc/istio/proxy/envoy-rev.json --drain-time-s 45 --drai</span>
<span class="c"># # &lt;span style="color: green;"&gt;👉 pilot-agent와 envoy가 동작 중입니다.&lt;/span&gt;</span>

<span class="c"># envoy 설정 확인</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deployment.apps/istio-ingressgateway <span class="nt">-n</span> istio-system <span class="nt">--</span> <span class="nb">cat</span> /etc/istio/proxy/envoy-rev.json
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deployment.apps/istio-ingressgateway <span class="nt">-n</span> istio-system <span class="nt">--</span> ss <span class="nt">-xnlp</span>
<span class="c"># =&gt; Netid      State       Recv-Q      Send-Q                                          Local Address:Port              Peer Address:Port      Process</span>
<span class="c">#    u_str      LISTEN      0           4096               var/run/secrets/workload-spiffe-uds/socket 32430                        * 0          users:((&amp;quot;pilot-agent&amp;quot;,pid=1,fd=9))</span>
<span class="c">#    u_str      LISTEN      0           4096                                      etc/istio/proxy/XDS 32431                        * 0          users:((&amp;quot;pilot-agent&amp;quot;,pid=1,fd=10))</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deployment.apps/istio-ingressgateway <span class="nt">-n</span> istio-system <span class="nt">--</span> ss <span class="nt">-xnp</span>
<span class="c"># =&gt; Netid State Recv-Q Send-Q                              Local Address:Port  Peer Address:Port Process</span>
<span class="c">#    u_str ESTAB 0      0                                               * 39978            * 37977 users:((&amp;quot;envoy&amp;quot;,pid=16,fd=19))</span>
<span class="c">#    u_str ESTAB 0      0                                               * 37501            * 37981 users:((&amp;quot;envoy&amp;quot;,pid=16,fd=32))</span>
<span class="c">#    u_str ESTAB 0      0                             etc/istio/proxy/XDS 37977            * 39978 users:((&amp;quot;pilot-agent&amp;quot;,pid=1,fd=11))</span>
<span class="c">#    u_str ESTAB 0      0      var/run/secrets/workload-spiffe-uds/socket 37981            * 37501 users:((&amp;quot;pilot-agent&amp;quot;,pid=1,fd=15))</span>
</code></pre></div></div>

<ul>
  <li>Auto Injection with namespace label</li>
  <li>
    <p>해당 네임스페이스에 생성되는 모든 파드들은 istio 사이드카가 자동으로 injection 됩니다.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># mutating Webhook admisstion controller 사용</span>
<span class="nv">$ </span>kubectl label namespace default istio-injection<span class="o">=</span>enabled
<span class="c"># =&gt; namespace/default labeled</span>
<span class="nv">$ </span>kubectl get ns <span class="nt">-L</span> istio-injection
<span class="c"># =&gt; NAME              STATUS   AGE     ISTIO-INJECTION</span>
<span class="c">#    default           Active   7d      enabled</span>
<span class="c">#    ...</span>
</code></pre></div>    </div>

    <p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_8.png" alt="img.png" class="image-center" /></p>
  </li>
  <li>Istio 접속 테스트를 위한 변수 지정 및 k3s-m에서 접속 테스트</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># k3s-m)</span>
<span class="c"># istio ingress gw NodePort(HTTP 접속용) 변수 지정 </span>
<span class="nv">$ </span><span class="nb">export </span><span class="nv">IGWHTTP</span><span class="o">=</span><span class="si">$(</span>kubectl get service <span class="nt">-n</span> istio-system istio-ingressgateway <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.spec.ports[1].nodePort}'</span><span class="si">)</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$IGWHTTP</span>
<span class="c"># =&gt; 31677</span>

<span class="c"># /etc/hosts 파일 수정</span>
<span class="c"># $ MYDOMAIN=&lt;각자 자신의 www 도메인&gt; # 단, 사용하고 있지 않는 공인 도메인을 사용 할 것</span>
<span class="c"># $ echo "&lt;istio-ingressgateway 파드가 있는 워커 노드&gt; $MYDOMAIN" &gt;&gt; /etc/hosts</span>

<span class="nv">$ </span><span class="nb">export </span><span class="nv">MYDOMAIN</span><span class="o">=</span>sweetlittlebird.com
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"192.168.10.10 </span><span class="nv">$MYDOMAIN</span><span class="s2">"</span> <span class="o">&gt;&gt;</span> /etc/hosts
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"export MYDOMAIN=</span><span class="nv">$MYDOMAIN</span><span class="s2">"</span> <span class="o">&gt;&gt;</span> /etc/profile

<span class="c"># istio ingress gw 접속 테스트 : 아직은 설정이 없어서 접속 실패가 됩니다.</span>
<span class="nv">$ </span>curl <span class="nt">-v</span> <span class="nt">-s</span> <span class="nv">$MYDOMAIN</span>:<span class="nv">$IGWHTTP</span>
<span class="c"># =&gt; *   Trying 192.168.10.10:31677...</span>
<span class="c">#    * connect to 192.168.10.10 port 31677 failed: Connection refused</span>
<span class="c">#    * Failed to connect to sweetlittlebird.com port 31677 after 3 ms: Connection refused</span>
<span class="c">#    * Closing connection 0</span>
</code></pre></div></div>

<ul>
  <li>testpc에서 접속 테스트</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 아래 변수는 각자 자신의 값을 직접 입력 할 것</span>
<span class="c"># $ IGWHTTP=&lt;각자 출력된 NodePort&gt;</span>
<span class="nv">$ IGWHTTP</span><span class="o">=</span>31677
<span class="nv">$ </span><span class="nb">export </span><span class="nv">MYDOMAIN</span><span class="o">=</span>sweetlittlebird.com
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"192.168.10.10 </span><span class="nv">$MYDOMAIN</span><span class="s2">"</span> <span class="o">&gt;&gt;</span> /etc/hosts
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"export MYDOMAIN=</span><span class="nv">$MYDOMAIN</span><span class="s2">"</span> <span class="o">&gt;&gt;</span> /etc/profile

<span class="c"># istio ingress gw 접속 테스트 : 아직은 설정이 없어서 접속 실패가 된다</span>
<span class="nv">$ </span>curl <span class="nt">-v</span> <span class="nt">-s</span> <span class="nv">$MYDOMAIN</span>:<span class="nv">$IGWHTTP</span>
<span class="c"># =&gt; * Host sweetlittlebird.com:31677 was resolved.</span>
<span class="c">#    * ...</span>
<span class="c">#    * Failed to connect to sweetlittlebird.com port 31677 after 2 ms: Couldn't connect to server</span>
<span class="c">#    * ...</span>
</code></pre></div></div>

<ul>
  <li>자신의 pc에서 접속 테스트</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 아래 변수는 각자 자신의 값을 직접 입력 할 것</span>
<span class="c"># $ IGWHTTP=&lt;각자 출력된 NodePort&gt;</span>
<span class="nv">$ IGWHTTP</span><span class="o">=</span>31677
<span class="c"># $ ISTIONODEIP=&lt;k3s-m 의 유동 공인 IP&gt;</span>
<span class="nv">$ ISTIONODEIP</span><span class="o">=</span>54.123.42.212

<span class="nv">$ </span><span class="nb">export </span><span class="nv">MYDOMAIN</span><span class="o">=</span>sweetlittlebird.com
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"192.168.10.10 </span><span class="nv">$MYDOMAIN</span><span class="s2">"</span> <span class="o">&gt;&gt;</span> /etc/hosts
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"export MYDOMAIN=</span><span class="nv">$MYDOMAIN</span><span class="s2">"</span> <span class="o">&gt;&gt;</span> /etc/profile

<span class="c"># istio ingress gw 접속 테스트 : 아직은 설정이 없어서 접속 실패가 된다</span>
<span class="nv">$ </span>curl <span class="nt">-v</span> <span class="nt">-s</span> <span class="nv">$MYDOMAIN</span>:<span class="nv">$IGWHTTP</span>
<span class="c"># =&gt; * Host sweetlittlebird.com:31677 was resolved.</span>
<span class="c">#    * ...</span>
<span class="c">#    * Failed to connect to sweetlittlebird.com port 31677 after 2 ms: Couldn't connect to server</span>
<span class="c">#    * ...</span>
</code></pre></div></div>

<h3 id="istio를-통한-외부-노출">Istio를 통한 외부 노출</h3>

<ul>
  <li>Nginx 디플로이먼트와 서비스 배포</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 로그 모니터링</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-n</span> istio-system <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>istiod
<span class="c"># =&gt; NAME                      READY   STATUS    RESTARTS      AGE</span>
<span class="c">#    istiod-7f8b586864-8mc4c   1/1     Running   1 (60m ago)   73m</span>
<span class="nv">$ </span>kubetail <span class="nt">-n</span> istio-system <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>istiod <span class="nt">-f</span>

<span class="nv">$ </span>kubectl get pod <span class="nt">-n</span> istio-system <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>istio-ingressgateway
<span class="c"># =&gt; NAME                                    READY   STATUS    RESTARTS   AGE</span>
<span class="c">#    istio-ingressgateway-5f9f654d46-l7mqp   1/1     Running   0          74m</span>
<span class="nv">$ </span>kubetail <span class="nt">-n</span> istio-system <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>istio-ingressgateway <span class="nt">-f</span>
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl create -f -
apiVersion: v1
kind: ServiceAccount
metadata:
  name: kans-nginx
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deploy-websrv
spec:
  replicas: 1
  selector:
    matchLabels:
      app: deploy-websrv
  template:
    metadata:
      labels:
        app: deploy-websrv
    spec:
      serviceAccountName: kans-nginx
      terminationGracePeriodSeconds: 0
      containers:
      - name: deploy-websrv
        image: nginx:alpine
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: svc-clusterip
spec:
  ports:
    - name: svc-webport
      port: 80
      targetPort: 80
  selector:
    app: deploy-websrv
  type: ClusterIP
</span><span class="no">EOF
</span><span class="c"># =&gt; serviceaccount/kans-nginx created</span>
<span class="c">#    deployment.apps/deploy-websrv created</span>
<span class="c">#    service/svc-clusterip created</span>

<span class="c"># 사이드카 컨테이너 배포 확인</span>
<span class="nv">$ </span>kubectl get pod,svc,ep,sa <span class="nt">-o</span> wide
<span class="c"># =&gt; NAME                                 READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES</span>
<span class="c">#    pod/deploy-websrv-778ffd6947-cxf5k   2/2     Running   0          50s   172.16.1.13   k3s-w2   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    </span>
<span class="c">#    NAME                    TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE     SELECTOR</span>
<span class="c">#    service/kubernetes      ClusterIP   10.10.200.1     &amp;lt;none&amp;gt;        443/TCP   6d16h   &amp;lt;none&amp;gt;</span>
<span class="c">#    service/svc-clusterip   ClusterIP   10.10.200.243   &amp;lt;none&amp;gt;        80/TCP    50s     app=deploy-websrv</span>
<span class="c">#    </span>
<span class="c">#    NAME                      ENDPOINTS            AGE</span>
<span class="c">#    endpoints/kubernetes      192.168.10.10:6443   6d16h</span>
<span class="c">#    endpoints/svc-clusterip   172.16.1.13:80       50s</span>
<span class="c">#    </span>
<span class="c">#    NAME                        SECRETS   AGE</span>
<span class="c">#    serviceaccount/default      0         7d1h</span>
<span class="c">#    serviceaccount/kans-nginx   0         50s</span>

<span class="nv">$ </span>kubectl describe pod
<span class="c"># =&gt; Name:             deploy-websrv-778ffd6947-cxf5k</span>
<span class="c">#    Namespace:        default</span>
<span class="c">#    Priority:         0</span>
<span class="c">#    Service Account:  kans-nginx</span>
<span class="c">#    Node:             k3s-w2/192.168.10.102</span>
<span class="c">#    Labels:           app=deploy-websrv</span>
<span class="c">#                      security.istio.io/tlsMode=istio</span>
<span class="c">#                      ...</span>
<span class="c">#    Annotations:      istio.io/rev: default</span>
<span class="c">#                      sidecar.istio.io/status:</span>
<span class="c">#                        {&amp;quot;initContainers&amp;quot;:[&amp;quot;istio-init&amp;quot;],&amp;quot;containers&amp;quot;:[&amp;quot;istio-proxy&amp;quot;],&amp;quot;volumes&amp;quot;:[&amp;quot;workload-socket&amp;quot;,&amp;quot;credential-socket&amp;quot;,&amp;quot;workload-certs&amp;quot;,&amp;quot;istio-env...</span>
<span class="c">#    Status:           Running</span>
<span class="c">#    ...</span>
<span class="c">#    Controlled By:  ReplicaSet/deploy-websrv-778ffd6947</span>
<span class="c">#    &lt;span style="color: red;"&gt;Init Containers:&lt;/span&gt;  # &lt;span style="color: green;"&gt;👉 init container가 파드내 iptables 셋팅&lt;/span&gt;</span>
<span class="c">#      istio-init:</span>
<span class="c">#        Container ID:  containerd://2a114fe0624581b35bda9ca257c6d3c831138e8a44900a6130e988bb51eb05da</span>
<span class="c">#        Image:         docker.io/istio/proxyv2:1.23.2</span>
<span class="c">#        Image ID:      docker.io/istio/proxyv2@sha256:2876cfc2fdf47e4b9665390ccc9ccf2bf913b71379325b8438135c9f35578e1a</span>
<span class="c">#        Port:          &amp;lt;none&amp;gt;</span>
<span class="c">#        Host Port:     &amp;lt;none&amp;gt;</span>
<span class="c">#        Args:</span>
<span class="c">#          &lt;span style="color: red;"&gt;istio-iptables&lt;/span&gt;</span>
<span class="c">#          -p</span>
<span class="c">#          15001</span>
<span class="c">#          -z</span>
<span class="c">#          15006</span>
<span class="c">#          -u</span>
<span class="c">#          1337</span>
<span class="c">#          -m</span>
<span class="c">#          REDIRECT</span>
<span class="c">#          -i</span>
<span class="c">#          *</span>
<span class="c">#          -x</span>
<span class="c">#    </span>
<span class="c">#          -b</span>
<span class="c">#          *</span>
<span class="c">#          -d</span>
<span class="c">#          15090,15021,15020</span>
<span class="c">#          --log_output_level=default:info</span>
<span class="c">#        State:          Terminated</span>
<span class="c">#          Reason:       Completed</span>
<span class="c">#        ...</span>
<span class="c">#    Containers:</span>
<span class="c">#      deploy-websrv:</span>
<span class="c">#        Container ID:   containerd://8918e0bb760bce8d090e84818bc189ae3ababdf9e74eb7dd3fb9709b356891f9</span>
<span class="c">#        Image:          nginx:alpine</span>
<span class="c">#        ...</span>
<span class="c">#      &lt;span style="color: red;"&gt;istio-proxy:&lt;/span&gt;  # &lt;span style="color: green;"&gt;👉 istio-proxy라는 컨테이너가 sidecar로 동작 중&lt;/span&gt;</span>
<span class="c">#        Container ID:  containerd://71d9e07a530dfce2ec34810d60a28dc3f9445b8eab714c2a7e204c459c59bcd3</span>
<span class="c">#        Image:         docker.io/istio/proxyv2:1.23.2</span>
<span class="c">#        Image ID:      docker.io/istio/proxyv2@sha256:2876cfc2fdf47e4b9665390ccc9ccf2bf913b71379325b8438135c9f35578e1a</span>
<span class="c">#        Port:          15090/TCP</span>
<span class="c">#        Host Port:     0/TCP</span>
<span class="c">#        Args:</span>
<span class="c">#          proxy</span>
<span class="c">#          sidecar</span>
<span class="c">#          --domain</span>
<span class="c">#          $(POD_NAMESPACE).svc.cluster.local</span>
<span class="c">#          --proxyLogLevel=warning</span>
<span class="c">#          --proxyComponentLogLevel=misc:error</span>
<span class="c">#          --log_output_level=default:info</span>
<span class="c">#        State:          Running</span>
<span class="c">#        ...</span>
</code></pre></div></div>

<ul>
  <li>Istio Gateway/VirtualService 설정 - Host 기반 트래픽 라우팅 설정 <a href="https://istio.io/latest/docs/setup/additional-setup/gateway/">링크</a>
<img src="/assets/2024/kans-3th/w7/20241019_kans_w7_9.png" alt="img.png" class="image-center w-80" />
    <ul>
      <li>클라이언트 PC → (Service:NodePort) Istio ingressgateway 파드 → (Gateway, VirtualService, Service 는 Bypass) → Endpoint(파드 : 사이드카 - Application 컨테이너)</li>
      <li>Gateway : 지정한 인그레스 게이트웨이로부터 트래픽이 인입, 프로토콜 및 포트, HOSTS, Proxy 등 설정 가능 <a href="https://istio.io/latest/docs/setup/additional-setup/gateway/">링크</a></li>
      <li>VirtualService : 인입 처리할 hosts 설정, L7 PATH 별 라우팅, 목적지에 대한 정책 설정 가능 (envoy route config)</li>
      <li>(참고) Introducing Istio v1 APIs - <a href="https://istio.io/latest/blog/2024/v1-apis/">Blog</a></li>
    </ul>
  </li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl create -f -
apiVersion: networking.istio.io/v1
kind: Gateway
metadata:
  name: test-gateway
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 80
      name: http
      protocol: HTTP
    hosts:
    - "*"
---
apiVersion: networking.istio.io/v1
kind: VirtualService
metadata:
  name: nginx-service
spec:
  hosts:
  - "</span><span class="nv">$MYDOMAIN</span><span class="sh">"
  gateways:
  - test-gateway
  http:
  - route:
    - destination:
        host: svc-clusterip
        port:
          number: 80
</span><span class="no">EOF
</span><span class="c"># =&gt; gateway.networking.istio.io/test-gateway created</span>
<span class="c">#    virtualservice.networking.istio.io/nginx-service created</span>

<span class="c"># Istio Gateway(=gw)/VirtualService(=vs) 설정 정보를 확인</span>
<span class="nv">$ </span>kubectl explain gateways.networking.istio.io
<span class="nv">$ </span>kubectl explain virtualservices.networking.istio.io
<span class="nv">$ </span>kubectl api-resources  | <span class="nb">grep </span>istio
<span class="c"># =&gt; wasmplugins                                      extensions.istio.io/v1alpha1      true         WasmPlugin</span>
<span class="c">#    destinationrules                    dr           networking.istio.io/v1            true         DestinationRule</span>
<span class="c">#    envoyfilters                                     networking.istio.io/v1alpha3      true         EnvoyFilter</span>
<span class="c">#    gateways                            gw           networking.istio.io/v1            true         Gateway</span>
<span class="c">#    proxyconfigs                                     networking.istio.io/v1beta1       true         ProxyConfig</span>
<span class="c">#    serviceentries                      se           networking.istio.io/v1            true         ServiceEntry</span>
<span class="c">#    sidecars                                         networking.istio.io/v1            true         Sidecar</span>
<span class="c">#    virtualservices                     vs           networking.istio.io/v1            true         VirtualService</span>
<span class="c">#    workloadentries                     we           networking.istio.io/v1            true         WorkloadEntry</span>
<span class="c">#    workloadgroups                      wg           networking.istio.io/v1            true         WorkloadGroup</span>
<span class="c">#    authorizationpolicies               ap           security.istio.io/v1              true         AuthorizationPolicy</span>
<span class="c">#    peerauthentications                 pa           security.istio.io/v1              true         PeerAuthentication</span>
<span class="c">#    requestauthentications              ra           security.istio.io/v1              true         RequestAuthentication</span>
<span class="c">#    telemetries                         telemetry    telemetry.istio.io/v1             true         Telemetry</span>

<span class="c"># virtual service 는 다른 네임스페이스의 서비스(ex. svc-nn.&lt;ns&gt;)도 참조할 수 있다</span>
<span class="nv">$ </span>kubectl get gw,vs
<span class="c"># =&gt; NAME                                       AGE</span>
<span class="c">#    gateway.networking.istio.io/test-gateway   105s</span>
<span class="c">#    </span>
<span class="c">#    NAME                                               GATEWAYS           HOSTS                     AGE</span>
<span class="c">#    virtualservice.networking.istio.io/nginx-service   [&amp;quot;test-gateway&amp;quot;]   [&amp;quot;sweetlittlebird.com&amp;quot;]   105s</span>

<span class="c"># Retrieves last sent and last acknowledged xDS sync from Istiod to each Envoy in the mesh</span>
<span class="c"># istioctl proxy-status command was improved to include the time since last change, and more relevant status values.</span>
<span class="nv">$ </span>istioctl proxy-status <span class="c"># 단축어 ps</span>
<span class="nv">$ </span>istioctl ps
<span class="c"># =&gt; NAME                                                   CLUSTER        CDS                LDS                EDS              RDS                ECDS        ISTIOD                      VERSION</span>
<span class="c">#    deploy-websrv-778ffd6947-cxf5k.default                 Kubernetes     SYNCED (22m)       SYNCED (22m)       SYNCED (22m)     SYNCED (22m)       IGNORED     istiod-7f8b586864-8mc4c     1.23.2</span>
<span class="c">#    istio-ingressgateway-5f9f654d46-l7mqp.istio-system     Kubernetes     SYNCED (2m14s)     SYNCED (2m14s)     SYNCED (22m)     SYNCED (2m14s)     IGNORED     istiod-7f8b586864-8mc4c     1.23.2</span>
</code></pre></div></div>

<ul>
  <li>Istio를 통한 Nginx 파드 접속 테스트
    <ul>
      <li>외부 (자신의 PC, test pc)에서 접속 테스트</li>
    </ul>
  </li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># istio ingress gw 를 통한 접속 테스트</span>
<span class="nv">$ </span>curl <span class="nt">-s</span> <span class="nv">$MYDOMAIN</span>:<span class="nv">$IGWHTTP</span> | <span class="nb">grep</span> <span class="nt">-o</span> <span class="s2">"&lt;title&gt;.*&lt;/title&gt;"</span>
<span class="c"># =&gt; &lt;title&gt;Welcome to nginx!&lt;/title&gt;</span>
<span class="nv">$ </span>curl <span class="nt">-v</span> <span class="nt">-s</span> <span class="nv">$MYDOMAIN</span>:<span class="nv">$IGWHTTP</span>
<span class="c"># =&gt; * Host sweetlittlebird.com:31677 was resolved.</span>
<span class="c">#    * IPv6: (none)</span>
<span class="c">#    * IPv4: 192.168.10.10, 192.168.10.10</span>
<span class="c">#    *   Trying 192.168.10.10:31677...</span>
<span class="c">#    * Connected to sweetlittlebird.com (192.168.10.10) port 31677</span>
<span class="c">#    &amp;gt; GET / HTTP/1.1</span>
<span class="c">#    &amp;gt; Host: sweetlittlebird.com:31677</span>
<span class="c">#    &amp;gt; User-Agent: curl/8.5.0</span>
<span class="c">#    &amp;gt; Accept: */*</span>
<span class="c">#    &amp;gt;</span>
<span class="c">#    &amp;lt; HTTP/1.1 200 OK</span>
<span class="c">#    &amp;lt; server: istio-envoy</span>
<span class="c">#    &amp;lt; date: Sat, 19 Oct 2024 07:14:25 GMT</span>
<span class="c">#    &amp;lt; content-type: text/html</span>
<span class="c">#    &amp;lt; content-length: 615</span>
<span class="c">#    &amp;lt; last-modified: Wed, 02 Oct 2024 16:07:39 GMT</span>
<span class="c">#    &amp;lt; etag: &amp;quot;66fd6fcb-267&amp;quot;</span>
<span class="c">#    &amp;lt; accept-ranges: bytes</span>
<span class="c">#    &amp;lt; x-envoy-upstream-service-time: 1</span>
<span class="c">#    ...</span>
<span class="c"># $ curl -v -s &lt;유동공인이IP&gt;:$IGWHTTP</span>
<span class="nv">$ </span>curl <span class="nt">-v</span> <span class="nt">-s</span> 54.123.42.212:<span class="nv">$IGWHTTP</span>
</code></pre></div></div>

<ul>
  <li>출력 로그 정보 확인</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubetail <span class="nt">-n</span> istio-system <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>istio-ingressgateway <span class="nt">-f</span>
<span class="c"># =&gt; [istio-ingressgateway-5f9f654d46-l7mqp] [2024-10-01T07:49:20.833Z] &amp;quot;GET / HTTP/1.1&amp;quot; 200 - via_upstream - &amp;quot;-&amp;quot; 0 615 6 5 &amp;quot;172.16.0.0&amp;quot; &amp;quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Safari/605.1.15&amp;quot; &amp;quot;6f246b4e-d675-971e-9a9d-dd809f560c6f&amp;quot; &amp;quot;sweetlittlebird.com:31677&amp;quot; &amp;quot;172.16.1.13:80&amp;quot; </span>
<span class="c">#    &lt;span style="color: red;"&gt;outbound|80||svc-clusterip.default.svc.cluster.local&lt;/span&gt; 172.16.2.14:60786 172.16.2.14:8080 172.16.0.0:8773 - -</span>
<span class="nv">$ </span>kubetail <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>deploy-websrv
<span class="c"># =&gt; [deploy-websrv-778ffd6947-cxf5k istio-proxy] [2024-10-01T07:49:20.866Z] &amp;quot;GET / HTTP/1.1&amp;quot; 200 - via_upstream - &amp;quot;-&amp;quot; 0 615 2 1 &amp;quot;172.16.0.0&amp;quot; &amp;quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Safari/605.1.15&amp;quot; &amp;quot;6f246b4e-d675-971e-9a9d-dd809f560c6f&amp;quot; &amp;quot;sweetlittlebird.com:31677&amp;quot; &amp;quot;172.16.1.13:80&amp;quot; </span>
<span class="c">#    &lt;span style="color: red;"&gt;inbound|80||&lt;/span&gt; 127.0.0.6:40337 172.16.1.13:80 172.16.0.0:0 invalid:outbound_.80_._.svc-clusterip.default.svc.cluster.local default</span>
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_10.png" alt="img.png" class="image-center" /></p>

<ul>
  <li>istioctl 정보 확인</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#</span>
<span class="nv">$ </span>istioctl proxy-status
<span class="c"># =&gt; NAME                                                   CLUSTER        CDS              LDS              EDS              RDS              ECDS        ISTIOD                      VERSION</span>
<span class="c">#    deploy-websrv-778ffd6947-cxf5k.default                 Kubernetes     SYNCED (19m)     SYNCED (19m)     SYNCED (19m)     SYNCED (19m)     IGNORED     istiod-7f8b586864-8mc4c     1.23.2</span>
<span class="c">#    istio-ingressgateway-5f9f654d46-l7mqp.istio-system     Kubernetes     SYNCED (24m)     SYNCED (24m)     SYNCED (24m)     SYNCED (24m)     IGNORED     istiod-7f8b586864-8mc4c     1.23.2</span>

<span class="c"># Envoy config dump : all, cluster, endpoint, listener 등</span>
<span class="nv">$ </span>istioctl proxy-config <span class="nt">--help</span> 
<span class="nv">$ </span>istioctl proxy-config all deploy-websrv-778ffd6947-cxf5k
<span class="nv">$ </span>istioctl proxy-config all deploy-websrv-778ffd6947-cxf5k <span class="nt">-o</span> json | jq
<span class="nv">$ </span>istioctl proxy-config route deploy-websrv-778ffd6947-cxf5k <span class="nt">-o</span> json | jq
</code></pre></div></div>

<ul>
  <li>pilot : istio-proxy내 uds로 envoy와 grpc통신, istiod에서 받아온 dynamic config를 envoy에 전달</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># istio-proxy 사용자 정보 확인 : uid(1337):gid(1337) 확인 -&gt; iptables rule 에서 사용됨</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deploy/deploy-websrv <span class="nt">-c</span> istio-proxy <span class="nt">--</span> <span class="nb">tail</span> <span class="nt">-n</span> 3 /etc/passwd
<span class="c"># =&gt; ubuntu:x:1000:1000:Ubuntu:/home/ubuntu:/bin/bash</span>
<span class="c">#    tcpdump:x:100:102::/nonexistent:/usr/sbin/nologin</span>
<span class="c">#    istio-proxy:x:1337:1337::/home/istio-proxy:/bin/sh</span>

<span class="c"># envoy 설정 정보 확인 : dynamic_resources , static_resources - listeners  </span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deploy/deploy-websrv <span class="nt">-c</span> istio-proxy <span class="nt">--</span> <span class="nb">cat</span> /etc/istio/proxy/envoy-rev.json
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deploy/deploy-websrv <span class="nt">-c</span> istio-proxy <span class="nt">--</span> ss <span class="nt">-nlp</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deploy/deploy-websrv <span class="nt">-c</span> istio-proxy <span class="nt">--</span> ss <span class="nt">-np</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deploy/deploy-websrv <span class="nt">-c</span> istio-proxy <span class="nt">--</span> netstat <span class="nt">-np</span>
<span class="c"># =&gt; Active Internet connections (w/o servers)</span>
<span class="c">#    Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name</span>
<span class="c">#    tcp        0      0 127.0.0.1:33168         127.0.0.1:15020         ESTABLISHED 13/envoy</span>
<span class="c">#    tcp        0      0 127.0.0.1:33156         127.0.0.1:15020         ESTABLISHED 13/envoy</span>
<span class="c">#    tcp        0      0 172.16.1.13:15006       172.16.2.14:33006       ESTABLISHED 13/envoy</span>
<span class="c">#    tcp        0      0 172.16.1.13:59774       10.10.200.215:15012     ESTABLISHED 1/pilot-agent</span>
<span class="c">#    tcp        0      0 172.16.1.13:15006       172.16.2.14:60786       ESTABLISHED 13/envoy</span>
<span class="c"># &lt;span style="color: green;"&gt;👉 172.16.1.13 : deploy-websrv 파드 ip&lt;/span&gt;</span>
<span class="c"># &lt;span style="color: green;"&gt;    172.16.2.14 : istio-ingressgateway 파드 ip&lt;/span&gt;</span>
<span class="c"># &lt;span style="color: green;"&gt;    10.10.200.215 : istiod 서비스의 Cluster-IP&lt;/span&gt;</span>
<span class="c">#    ...</span>
<span class="c">#    Active UNIX domain sockets (w/o servers)</span>
<span class="c">#    Proto RefCnt Flags       Type       State         I-Node   PID/Program name     Path</span>
<span class="c">#    unix  3      [ ]         STREAM     CONNECTED     54162    13/envoy</span>
<span class="c">#    unix  3      [ ]         STREAM     CONNECTED     54709    1/pilot-agent        var/run/secrets/workload-spiffe-uds/socket</span>
<span class="c">#    unix  3      [ ]         STREAM     CONNECTED     71034    1/pilot-agent        etc/istio/proxy/XDS</span>
<span class="c">#    unix  3      [ ]         STREAM     CONNECTED     72979    13/envoy</span>
<span class="c">#    ...</span>

<span class="c">#</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deploy/deploy-websrv <span class="nt">-c</span> istio-proxy <span class="nt">--</span> ps <span class="nt">-ef</span>
<span class="c"># =&gt; UID          PID    PPID  C STIME TTY          TIME CMD</span>
<span class="c">#    istio-p+       1       0  0 06:43 ?        00:00:01 /usr/local/bin/pilot-agent proxy sidecar --domain default.svc.cluster.local --proxyLogLevel=warning --proxyComponentLogLevel=mi</span>
<span class="c">#    istio-p+      13       1  0 06:43 ?        00:00:34 /usr/local/bin/envoy -c etc/istio/proxy/envoy-rev.json --drain-time-s 45 --drain-strategy immediate --local-address-ip-version</span>

<span class="c"># </span>
<span class="nv">$ </span>kubectl get pod,svc <span class="nt">-A</span> <span class="nt">-owide</span>
<span class="c"># =&gt; NAMESPACE      NAME                                            READY   STATUS    RESTARTS        AGE     IP            NODE     NOMINATED NODE   READINESS GATES</span>
<span class="c">#    default        pod/deploy-websrv-778ffd6947-cxf5k              2/2     Running   0               91m     172.16.1.13   k3s-w2   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    istio-system   pod/istio-ingressgateway-5f9f654d46-l7mqp       1/1     Running   0               167m    172.16.2.14   k3s-w3   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    istio-system   pod/istiod-7f8b586864-8mc4c                     1/1     Running   1 (154m ago)    167m    172.16.3.16   k3s-w1   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    ...</span>
<span class="c">#    </span>
<span class="c">#    NAMESPACE      NAME                                         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                                      AGE     SELECTOR</span>
<span class="c">#    default        service/kubernetes                           ClusterIP   10.10.200.1     &amp;lt;none&amp;gt;        443/TCP                                                                      6d17h   &amp;lt;none&amp;gt;</span>
<span class="c">#    default        service/svc-clusterip                        ClusterIP   10.10.200.243   &amp;lt;none&amp;gt;        80/TCP                                                                       91m     app=deploy-websrv</span>
<span class="c">#    istio-system   service/istio-ingressgateway                 NodePort    10.10.200.171   &amp;lt;none&amp;gt;        15021:30953/TCP,80:31677/TCP,443:30737/TCP,31400:30617/TCP,15443:31668/TCP   167m    app=istio-ingressgateway,istio=ingressgateway</span>
<span class="c">#    istio-system   service/istiod                               ClusterIP   10.10.200.215   &amp;lt;none&amp;gt;        15010/TCP,15012/TCP,443/TCP,15014/TCP                                        167m    app=istiod,istio=pilot</span>
<span class="c">#    ...</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deploy/deploy-websrv <span class="nt">-c</span> istio-proxy <span class="nt">--</span> netstat <span class="nt">-antp</span>
<span class="c"># =&gt; Active Internet connections (servers and established)</span>
<span class="c">#    Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name</span>
<span class="c">#    tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      -</span>
<span class="c">#    ...</span>
<span class="c">#    tcp        0      0 127.0.0.1:33168         127.0.0.1:15020         ESTABLISHED 13/envoy</span>
<span class="c">#    tcp        0      0 127.0.0.1:33156         127.0.0.1:15020         ESTABLISHED 13/envoy</span>
<span class="c">#    tcp        0      0 172.16.1.13:15006       172.16.2.14:33006       ESTABLISHED 13/envoy</span>
<span class="c">#    tcp        0      0 172.16.1.13:59774       10.10.200.215:15012     ESTABLISHED 1/pilot-agent</span>
<span class="c">#    tcp        0      0 172.16.1.13:15006       172.16.2.14:60786       ESTABLISHED 13/envoy</span>
<span class="c">#    tcp6       0      0 127.0.0.1:15020         127.0.0.1:33168         ESTABLISHED 1/pilot-agent</span>
<span class="c">#    tcp6       0      0 127.0.0.1:15020         127.0.0.1:33156         ESTABLISHED 1/pilot-agent</span>

<span class="c"># istiod 정보 같이 확인 : 출력되는 IP가 누구인지 확인 해보자</span>
<span class="nv">$ </span>kubectl get pod,svc <span class="nt">-A</span> <span class="nt">-owide</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deploy/istiod <span class="nt">-n</span> istio-system <span class="nt">--</span> ps <span class="nt">-ef</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deploy/istiod <span class="nt">-n</span> istio-system <span class="nt">--</span> netstat <span class="nt">-antp</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deploy/istiod <span class="nt">-n</span> istio-system <span class="nt">--</span> ss <span class="nt">-nlp</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deploy/istiod <span class="nt">-n</span> istio-system <span class="nt">--</span> ss <span class="nt">-np</span>
<span class="c"># =&gt; Netid  State  Recv-Q  Send-Q         Local Address:Port           Peer Address:Port   Process</span>
<span class="c">#    tcp    ESTAB  0       0                172.16.3.16:33552           10.10.200.1:443     users:((&amp;quot;pilot-discovery&amp;quot;,pid=1,fd=7))</span>
<span class="c">#    tcp    ESTAB  0       0       [::ffff:172.16.3.16]:15012  [::ffff:172.16.2.14]:37102   users:((&amp;quot;pilot-discovery&amp;quot;,pid=1,fd=13))</span>
<span class="c">#    tcp    ESTAB  0       0       [::ffff:172.16.3.16]:15012  [::ffff:172.16.1.13]:56560   users:((&amp;quot;pilot-discovery&amp;quot;,pid=1,fd=14))</span>
</code></pre></div></div>

<ul>
  <li>istio-proxy, istiod가 각각 사용하는 포트 정보 <a href="https://istio.io/latest/docs/ops/deployment/application-requirements/">링크</a>
    <ul>
      <li>
        <p>istio-proxy</p>

        <table>
          <thead>
            <tr>
              <th>Port</th>
              <th>Protocol</th>
              <th>Description</th>
              <th>Pod-internal only</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>15000</td>
              <td>TCP</td>
              <td>Envoy admin port (commands/diagnostics)</td>
              <td>Yes</td>
            </tr>
            <tr>
              <td>15001</td>
              <td>TCP</td>
              <td>Envoy outbound</td>
              <td>No</td>
            </tr>
            <tr>
              <td>15004</td>
              <td>HTTP</td>
              <td>Debug port</td>
              <td>Yes</td>
            </tr>
            <tr>
              <td>15006</td>
              <td>TCP</td>
              <td>Envoy inbound</td>
              <td>No</td>
            </tr>
            <tr>
              <td>15008</td>
              <td>HTTP2</td>
              <td>HBONE mTLS tunnel port</td>
              <td>No</td>
            </tr>
            <tr>
              <td>15020</td>
              <td>HTTP</td>
              <td>Merged Prometheus telemetry from Istio agent, Envoy, and application No</td>
              <td>No</td>
            </tr>
            <tr>
              <td>15021</td>
              <td>HTTP</td>
              <td>Health checks</td>
              <td>No</td>
            </tr>
            <tr>
              <td>15053</td>
              <td>DNS</td>
              <td>DNS port, if capture is enabled</td>
              <td>Yes</td>
            </tr>
            <tr>
              <td>15090</td>
              <td>HTTP</td>
              <td>Envoy Prometheus telemetry</td>
              <td>No</td>
            </tr>
          </tbody>
        </table>
      </li>
      <li>
        <p>istiod (컨트롤플레인)</p>

        <table>
          <thead>
            <tr>
              <th>Port</th>
              <th>Protocol</th>
              <th>Description</th>
              <th>Pod-internal only</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>443</td>
              <td>HTTPS</td>
              <td>Webhooks service port</td>
              <td>No</td>
            </tr>
            <tr>
              <td>8080</td>
              <td>HTTP</td>
              <td>Debug interface (deprecated, container port only)</td>
              <td>No</td>
            </tr>
            <tr>
              <td>15010</td>
              <td>GRPC</td>
              <td>XDS and CA services (Plaintext, only for secure networks)</td>
              <td>No</td>
            </tr>
            <tr>
              <td>15012</td>
              <td>GRPC</td>
              <td>XDS and CA services (TLS and mTLS, recommended for production use)</td>
              <td>No</td>
            </tr>
            <tr>
              <td>15014</td>
              <td>HTTP</td>
              <td>Control plane monitoring</td>
              <td>No</td>
            </tr>
            <tr>
              <td>15017</td>
              <td>HTTPS</td>
              <td>Webhook container port, forwarded from 443</td>
              <td>No</td>
            </tr>
          </tbody>
        </table>
      </li>
    </ul>
  </li>
  <li>Istio - Istio proxy와 Envoy 프로세스간 유닉스 도메인 소켓 통신 확인</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 파드 확인</span>
<span class="nv">$ </span>kubectl get pod
<span class="c"># =&gt; NAME                             READY   STATUS    RESTARTS   AGE</span>
<span class="c">#    deploy-websrv-778ffd6947-cxf5k   2/2     Running   0          106m</span>

<span class="c"># istio 컨테이너 접속</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deploy/deploy-websrv <span class="nt">-c</span> istio-proxy <span class="nt">--</span> bash
<span class="nt">---------------------------------------------------------------</span>
<span class="c"># SDS, XDS 는 소켓 타입</span>
<span class="nv">$ </span><span class="nb">ls</span> <span class="nt">-al</span> /etc/istio/proxy
<span class="c"># =&gt; total 24</span>
<span class="c">#    drwxrwxrwt 2 root        root          100 Oct 19 06:43 .</span>
<span class="c">#    drwxr-xr-x 4 root        root         4096 Oct 19 06:43 ..</span>
<span class="c">#    srw-rw-rw- 1 istio-proxy istio-proxy     0 Oct 19 06:43 XDS</span>
<span class="c">#    -rw-r--r-- 1 istio-proxy istio-proxy 13644 Oct 19 06:43 envoy-rev.json</span>
<span class="c">#    -rw-r--r-- 1 istio-proxy istio-proxy  2747 Oct 19 06:43 grpc-bootstrap.json</span>

<span class="c"># .json 파일 확인</span>
<span class="nv">$ </span>more /etc/istio/proxy/envoy-rev.json
<span class="c"># =&gt; {</span>
<span class="c">#      &amp;quot;node&amp;quot;: {</span>
<span class="c">#        &amp;quot;id&amp;quot;: &amp;quot;sidecar~172.16.1.13~deploy-websrv-778ffd6947-cxf5k.default~default.svc.cluster.local&amp;quot;,</span>
<span class="c">#        &amp;quot;cluster&amp;quot;: &amp;quot;deploy-websrv.default&amp;quot;,</span>
<span class="c">#    ...</span>
<span class="c">#      &amp;quot;admin&amp;quot;: {</span>
<span class="c">#        &amp;quot;access_log&amp;quot;: [</span>
<span class="c">#          {</span>
<span class="c">#            &amp;quot;name&amp;quot;: &amp;quot;envoy.access_loggers.file&amp;quot;,</span>
<span class="c">#            &amp;quot;typed_config&amp;quot;: {</span>
<span class="c">#              &amp;quot;@type&amp;quot;: &amp;quot;type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog&amp;quot;,</span>
<span class="c">#              &amp;quot;path&amp;quot;: &amp;quot;/dev/null&amp;quot;</span>
<span class="c">#            }</span>
<span class="c">#          }</span>
<span class="c">#        ],</span>
<span class="c">#        &amp;quot;profile_path&amp;quot;: &amp;quot;/var/lib/istio/data/envoy.prof&amp;quot;,</span>
<span class="c">#        &amp;quot;address&amp;quot;: {</span>
<span class="c">#          &amp;quot;socket_address&amp;quot;: {</span>
<span class="c">#            &amp;quot;address&amp;quot;: &amp;quot;127.0.0.1&amp;quot;,</span>
<span class="c">#            &amp;quot;port_value&amp;quot;: 15000</span>
<span class="c">#          }</span>
<span class="c">#        }</span>
<span class="c">#      },</span>
<span class="c">#    ...</span>
<span class="c">#    &amp;quot;dynamic_resources&amp;quot;: {</span>
<span class="c">#        &amp;quot;lds_config&amp;quot;: {</span>
<span class="c">#          &amp;quot;ads&amp;quot;: {},</span>
<span class="c">#          &amp;quot;initial_fetch_timeout&amp;quot;: &amp;quot;0s&amp;quot;,</span>
<span class="c">#          &amp;quot;resource_api_version&amp;quot;: &amp;quot;V3&amp;quot;</span>
<span class="c">#        },</span>
<span class="c">#        &amp;quot;cds_config&amp;quot;: {</span>
<span class="c">#          &amp;quot;ads&amp;quot;: {},</span>
<span class="c">#          &amp;quot;initial_fetch_timeout&amp;quot;: &amp;quot;0s&amp;quot;,</span>
<span class="c">#          &amp;quot;resource_api_version&amp;quot;: &amp;quot;V3&amp;quot;</span>
<span class="c">#        },</span>
<span class="c">#        &amp;quot;ads_config&amp;quot;: {</span>
<span class="c">#          &amp;quot;api_type&amp;quot;: &amp;quot;GRPC&amp;quot;,</span>
<span class="c">#          &amp;quot;set_node_on_first_message_only&amp;quot;: true,</span>
<span class="c">#          &amp;quot;transport_api_version&amp;quot;: &amp;quot;V3&amp;quot;,</span>
<span class="c">#          &amp;quot;grpc_services&amp;quot;: [</span>
<span class="c">#            {</span>
<span class="c">#              &amp;quot;envoy_grpc&amp;quot;: {</span>
<span class="c">#                &amp;quot;cluster_name&amp;quot;: &amp;quot;xds-grpc&amp;quot;</span>
<span class="c">#    ...</span>
<span class="c">#    &amp;quot;static_resources&amp;quot;: {</span>
<span class="c">#        &amp;quot;clusters&amp;quot;: [</span>
<span class="c">#          {</span>
<span class="c">#            ...</span>
<span class="c">#            &amp;quot;name&amp;quot;: &amp;quot;xds-grpc&amp;quot;,</span>
<span class="c">#            &amp;quot;alt_stat_name&amp;quot;: &amp;quot;xds-grpc;&amp;quot;,</span>
<span class="c">#            &amp;quot;type&amp;quot; : &amp;quot;STATIC&amp;quot;,</span>
<span class="c">#            &amp;quot;connect_timeout&amp;quot;: &amp;quot;1s&amp;quot;,</span>
<span class="c">#            &amp;quot;lb_policy&amp;quot;: &amp;quot;ROUND_ROBIN&amp;quot;,</span>
<span class="c">#            &amp;quot;load_assignment&amp;quot;: {</span>
<span class="c">#              &amp;quot;cluster_name&amp;quot;: &amp;quot;xds-grpc&amp;quot;,</span>
<span class="c">#              &amp;quot;endpoints&amp;quot;: [{</span>
<span class="c">#                &amp;quot;lb_endpoints&amp;quot;: [{</span>
<span class="c">#                  &amp;quot;endpoint&amp;quot;: {</span>
<span class="c">#                    &amp;quot;address&amp;quot;:{</span>
<span class="c">#                      &amp;quot;pipe&amp;quot;: {</span>
<span class="c">#                        &amp;quot;path&amp;quot;: &amp;quot;./etc/istio/proxy/XDS&amp;quot;</span>
<span class="c">#                      }</span>
<span class="c">#    ...</span>
<span class="c">#    &amp;quot;listeners&amp;quot;:[</span>
<span class="c">#      ...</span>
<span class="c">#      &amp;quot;address&amp;quot;: {</span>
<span class="c">#        &amp;quot;socket_address&amp;quot;: {</span>
<span class="c">#          &amp;quot;protocol&amp;quot;: &amp;quot;TCP&amp;quot;,</span>
<span class="c">#          &amp;quot;address&amp;quot;: &amp;quot;0.0.0.0&amp;quot;,</span>
<span class="c">#          &amp;quot;port_value&amp;quot;: 15090</span>
<span class="c">#        }</span>
<span class="c">#      },</span>
<span class="c">#     &amp;quot;filter_chains&amp;quot;: [</span>
<span class="c">#                &amp;quot;filters&amp;quot;: [</span>
<span class="c">#                  {</span>
<span class="c">#                    &amp;quot;name&amp;quot;: &amp;quot;envoy.filters.network.http_connection_manager&amp;quot;,</span>
<span class="c">#                    &amp;quot;typed_config&amp;quot;: {</span>
<span class="c">#                      &amp;quot;@type&amp;quot;: &amp;quot;type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager&amp;quot;,</span>
<span class="c">#                      &amp;quot;codec_type&amp;quot;: &amp;quot;AUTO&amp;quot;,</span>
<span class="c">#                      &amp;quot;stat_prefix&amp;quot;: &amp;quot;agent&amp;quot;,</span>
<span class="c">#                      &amp;quot;route_config&amp;quot;: {</span>
<span class="c">#                        &amp;quot;virtual_hosts&amp;quot;: [</span>
<span class="c">#                          {</span>
<span class="c">#                            &amp;quot;name&amp;quot;: &amp;quot;backend&amp;quot;,</span>
<span class="c">#                            &amp;quot;domains&amp;quot;: [</span>
<span class="c">#                              &amp;quot;*&amp;quot;</span>
<span class="c">#                            ],</span>
<span class="c">#                            &amp;quot;routes&amp;quot;: [</span>
<span class="c">#                              {</span>
<span class="c">#                                &amp;quot;match&amp;quot;: {</span>
<span class="c">#                                  &amp;quot;prefix&amp;quot;: &amp;quot;/healthz/ready&amp;quot;</span>
<span class="c">#                                },</span>
<span class="c">#                                &amp;quot;route&amp;quot;: {</span>
<span class="c">#                                  &amp;quot;cluster&amp;quot;: &amp;quot;agent&amp;quot;</span>
<span class="c">#    ...</span>

<span class="nv">$ </span>more /etc/istio/proxy/grpc-bootstrap.json
<span class="c"># =&gt; {</span>
<span class="c">#      &amp;quot;xds_servers&amp;quot;: [</span>
<span class="c">#        {</span>
<span class="c">#          &amp;quot;server_uri&amp;quot;: &lt;span style="color: red;"&gt;&amp;quot;unix:///etc/istio/proxy/XDS&amp;quot;,&lt;/span&gt;</span>
<span class="c">#          ...</span>
<span class="c">#        }</span>
<span class="c">#      ],</span>
<span class="c">#      &amp;quot;node&amp;quot;: {</span>
<span class="c">#        &amp;quot;id&amp;quot;: &amp;quot;sidecar~172.16.1.13~deploy-websrv-778ffd6947-cxf5k.default~default.svc.cluster.local&amp;quot;,</span>
<span class="c">#        &amp;quot;metadata&amp;quot;: {</span>
<span class="c">#          &amp;quot;ANNOTATIONS&amp;quot;: {</span>
<span class="c">#            &amp;quot;istio.io/rev&amp;quot;: &amp;quot;default&amp;quot;,</span>
<span class="c">#            &amp;quot;kubectl.kubernetes.io/default-container&amp;quot;: &amp;quot;deploy-websrv&amp;quot;,</span>
<span class="c">#            &amp;quot;kubectl.kubernetes.io/default-logs-container&amp;quot;: &amp;quot;deploy-websrv&amp;quot;,</span>
<span class="c">#            ...</span>
<span class="c">#            &amp;quot;sidecar.istio.io/status&amp;quot;: &amp;quot;{\&amp;quot;initContainers\&amp;quot;:[\&amp;quot;istio-init\&amp;quot;],\&amp;quot;containers\&amp;quot;:[\&amp;quot;istio-proxy\&amp;quot;],\&amp;quot;volumes\&amp;quot;:[\&amp;quot;workload-socket\&amp;quot;,\&amp;quot;credential-socket\&amp;quot;,\&amp;quot;workload-certs\&amp;quot;</span>
<span class="c">#    ,\&amp;quot;istio-envoy\&amp;quot;,\&amp;quot;istio-data\&amp;quot;,\&amp;quot;istio-podinfo\&amp;quot;,\&amp;quot;istio-token\&amp;quot;,\&amp;quot;istiod-ca-cert\&amp;quot;],\&amp;quot;imagePullSecrets\&amp;quot;:null,\&amp;quot;revision\&amp;quot;:\&amp;quot;default\&amp;quot;}&amp;quot;</span>
<span class="c">#          },</span>
<span class="c">#          ...</span>
<span class="c">#          &amp;quot;SERVICE_ACCOUNT&amp;quot;: &amp;quot;kans-nginx&amp;quot;,</span>
<span class="c">#          &amp;quot;WORKLOAD_NAME&amp;quot;: &amp;quot;deploy-websrv&amp;quot;</span>
<span class="c">#        },</span>
<span class="c">#        &amp;quot;locality&amp;quot;: {},</span>
<span class="c">#        &amp;quot;UserAgentVersionType&amp;quot;: null</span>
<span class="c">#      },</span>
<span class="c">#      &amp;quot;server_listener_resource_name_template&amp;quot;: &amp;quot;xds.istio.io/grpc/lds/inbound/%s&amp;quot;</span>
<span class="c">#    }</span>

<span class="c"># display only Unix domain sockets : Listener 과 ESTAB 상태 정보 확인</span>
<span class="nv">$ </span>ss <span class="nt">-xpl</span>
<span class="c"># =&gt; Netid State  Recv-Q Send-Q                              Local Address:Port  Peer Address:PortProcess</span>
<span class="c">#    u_str LISTEN 0      4096                          etc/istio/proxy/XDS 54694            * 0    users:((&amp;quot;pilot-agent&amp;quot;,pid=1,fd=11))</span>
<span class="c">#    u_str LISTEN 0      4096   var/run/secrets/workload-spiffe-uds/socket 54693            * 0    users:((&amp;quot;pilot-agent&amp;quot;,pid=1,fd=9))</span>
<span class="nv">$ </span>ss <span class="nt">-xp</span>
<span class="c"># =&gt; Netid State Recv-Q Send-Q                              Local Address:Port  Peer Address:Port Process</span>
<span class="c">#    u_str ESTAB 0      0                             etc/istio/proxy/XDS 79304            * 82228 users:((&amp;quot;pilot-agent&amp;quot;,pid=1,fd=10))</span>
<span class="c">#    u_str ESTAB 0      0      var/run/secrets/workload-spiffe-uds/socket 54709            * 54162 users:((&amp;quot;pilot-agent&amp;quot;,pid=1,fd=16))</span>
<span class="c">#    u_str ESTAB 0      0                                               * 82228            * 79304 users:((&amp;quot;envoy&amp;quot;,pid=13,fd=19))</span>
<span class="c">#    u_str ESTAB 0      0                                               * 54162            * 54709 users:((&amp;quot;envoy&amp;quot;,pid=13,fd=32))</span>

<span class="c"># display only TCP sockets and display only IP version 4 sockets : TCP 상태 정보 확인</span>
<span class="nv">$ </span>ss <span class="nt">-4tpl</span>
<span class="c"># =&gt; LISTEN 0      4096         0.0.0.0:15090      0.0.0.0:*    users:((&amp;quot;envoy&amp;quot;,pid=13,fd=21))</span>
<span class="c">#    LISTEN 0      4096         0.0.0.0:15090      0.0.0.0:*    users:((&amp;quot;envoy&amp;quot;,pid=13,fd=20))</span>
<span class="c">#    LISTEN 0      4096         0.0.0.0:15021      0.0.0.0:*    users:((&amp;quot;envoy&amp;quot;,pid=13,fd=23))</span>
<span class="c">#    LISTEN 0      4096         0.0.0.0:15021      0.0.0.0:*    users:((&amp;quot;envoy&amp;quot;,pid=13,fd=22))</span>
<span class="c">#    LISTEN 0      4096         0.0.0.0:15001      0.0.0.0:*    users:((&amp;quot;envoy&amp;quot;,pid=13,fd=35))</span>
<span class="c">#    LISTEN 0      4096         0.0.0.0:15001      0.0.0.0:*    users:((&amp;quot;envoy&amp;quot;,pid=13,fd=34))</span>
<span class="c">#    LISTEN 0      4096         0.0.0.0:15006      0.0.0.0:*    users:((&amp;quot;envoy&amp;quot;,pid=13,fd=37))</span>
<span class="c">#    LISTEN 0      4096         0.0.0.0:15006      0.0.0.0:*    users:((&amp;quot;envoy&amp;quot;,pid=13,fd=36))</span>
<span class="c">#    LISTEN 0      4096       127.0.0.1:15000      0.0.0.0:*    users:((&amp;quot;envoy&amp;quot;,pid=13,fd=18))</span>
<span class="c">#    LISTEN 0      4096       127.0.0.1:15004      0.0.0.0:*    users:((&amp;quot;pilot-agent&amp;quot;,pid=1,fd=13))</span>
<span class="c">#    LISTEN 0      511          0.0.0.0:http       0.0.0.0:*</span>
</code></pre></div></div>

<h3 id="bookinfo-실습-및-istio-기능-확인">Bookinfo 실습 및 Istio 기능 확인</h3>

<h4 id="bookinfo">Bookinfo</h4>

<ul>
  <li>Bookinfo는 istio의 기능을 설명하기위한 MSA(Microservices Architecture)  기반의 예제 어플리케이션입니다.
<img src="/assets/2024/kans-3th/w7/20241019_kans_w7_11.png" alt="img.png" class="image-center" />
<em class="image-caption">Bookinfo 어플리케이션 구성</em>
    <ul>
      <li>productpage, details, reviews, ratings 서비스로 구성됩니다. <a href="https://istio.io/latest/docs/examples/bookinfo/">소개 링크</a></li>
      <li><strong>ProductPage</strong> 페이지에서 요청을 받으면, 도서 리뷰를 보여주는 <strong>Reviews</strong> 서비스와 도서 상세 정보를 보여주는 <strong>Details</strong> 서비스에 접속하고,</li>
      <li>ProductPage 는 <strong>Reviews</strong> 와 <strong>Details</strong> 결과를 사용자에게 응답합니다.</li>
      <li><strong>Reviews</strong> 서비스는 v1, v2, v3 세 개의 버전이 있고 v2, v3 버전의 경우 <strong>Ratings</strong> 서비스에 접소갛여 도서에 대한 5단계 평가를 가져옵니다.</li>
      <li>Reviews 서비스의 차이는, v1은 Rating 이 <strong>없고</strong>, v2는 <strong>검은색</strong> 별로 Ratings 가 표시되며, v3는 <strong>색깔이</strong> 있는 별로 Ratings 가 표시됩니다.</li>
    </ul>
  </li>
  <li>설치 및 확인</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 모니터링</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s1">'kubectl get pod -owide;echo;kubectl get svc'</span>

<span class="c"># Bookinfo 애플리케이션 배포</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$ISTIOV</span>
<span class="c"># =&gt; 1.23.2</span>
<span class="nv">$ </span><span class="nb">cat</span> ~/istio-<span class="nv">$ISTIOV</span>/samples/bookinfo/platform/kube/bookinfo.yaml
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> ~/istio-<span class="nv">$ISTIOV</span>/samples/bookinfo/platform/kube/bookinfo.yaml
<span class="c"># =&gt; service/details created</span>
<span class="c">#    serviceaccount/bookinfo-details created</span>
<span class="c">#    deployment.apps/details-v1 created</span>
<span class="c">#    service/ratings created</span>
<span class="c">#    serviceaccount/bookinfo-ratings created</span>
<span class="c">#    deployment.apps/ratings-v1 created</span>
<span class="c">#    service/reviews created</span>
<span class="c">#    serviceaccount/bookinfo-reviews created</span>
<span class="c">#    deployment.apps/reviews-v1 created</span>
<span class="c">#    deployment.apps/reviews-v2 created</span>
<span class="c">#    deployment.apps/reviews-v3 created</span>
<span class="c">#    service/productpage created</span>
<span class="c">#    serviceaccount/bookinfo-productpage created</span>
<span class="c">#    deployment.apps/productpage-v1 created</span>

<span class="c"># 확인</span>
<span class="nv">$ </span>kubectl get all,sa
<span class="c"># =&gt; NAME                                 READY   STATUS    RESTARTS   AGE</span>
<span class="c">#    pod/details-v1-65cfcf56f9-8465x      2/2     Running   0          87s</span>
<span class="c">#    pod/productpage-v1-d5789fdfb-f8gdf   2/2     Running   0          86s</span>
<span class="c">#    pod/ratings-v1-7c9bd4b87f-s9gxs      2/2     Running   0          87s</span>
<span class="c">#    pod/reviews-v1-6584ddcf65-gnc9j      2/2     Running   0          87s</span>
<span class="c">#    pod/reviews-v2-6f85cb9b7c-cfc68      2/2     Running   0          87s</span>
<span class="c">#    pod/reviews-v3-6f5b775685-cw7tc      2/2     Running   0          87s</span>
<span class="c">#    </span>
<span class="c">#    NAME                    TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE</span>
<span class="c">#    service/details         ClusterIP   10.10.200.54    &amp;lt;none&amp;gt;        9080/TCP   87s</span>
<span class="c">#    service/productpage     ClusterIP   10.10.200.184   &amp;lt;none&amp;gt;        9080/TCP   87s</span>
<span class="c">#    service/ratings         ClusterIP   10.10.200.163   &amp;lt;none&amp;gt;        9080/TCP   87s</span>
<span class="c">#    service/reviews         ClusterIP   10.10.200.214   &amp;lt;none&amp;gt;        9080/TCP   87s</span>
<span class="c">#    </span>
<span class="c">#    NAME                             READY   UP-TO-DATE   AVAILABLE   AGE</span>
<span class="c">#    deployment.apps/details-v1       1/1     1            1           87s</span>
<span class="c">#    deployment.apps/productpage-v1   1/1     1            1           86s</span>
<span class="c">#    deployment.apps/ratings-v1       1/1     1            1           87s</span>
<span class="c">#    deployment.apps/reviews-v1       1/1     1            1           87s</span>
<span class="c">#    deployment.apps/reviews-v2       1/1     1            1           87s</span>
<span class="c">#    deployment.apps/reviews-v3       1/1     1            1           87s</span>
<span class="c">#    </span>
<span class="c">#    NAME                                       DESIRED   CURRENT   READY   AGE</span>
<span class="c">#    replicaset.apps/details-v1-65cfcf56f9      1         1         1       87s</span>
<span class="c">#    replicaset.apps/productpage-v1-d5789fdfb   1         1         1       86s</span>
<span class="c">#    replicaset.apps/ratings-v1-7c9bd4b87f      1         1         1       87s</span>
<span class="c">#    replicaset.apps/reviews-v1-6584ddcf65      1         1         1       87s</span>
<span class="c">#    replicaset.apps/reviews-v2-6f85cb9b7c      1         1         1       87s</span>
<span class="c">#    replicaset.apps/reviews-v3-6f5b775685      1         1         1       87s</span>
<span class="c">#    </span>
<span class="c">#    NAME                                  SECRETS   AGE</span>
<span class="c">#    serviceaccount/bookinfo-details       0         87s</span>
<span class="c">#    serviceaccount/bookinfo-productpage   0         86s</span>
<span class="c">#    serviceaccount/bookinfo-ratings       0         87s</span>
<span class="c">#    serviceaccount/bookinfo-reviews       0         87s</span>

<span class="c"># product 웹 접속 확인</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="s2">"</span><span class="si">$(</span>kubectl get pod <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>ratings <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.items[0].metadata.name}'</span><span class="si">)</span><span class="s2">"</span> <span class="nt">-c</span> ratings <span class="nt">--</span> curl <span class="nt">-sS</span> productpage:9080/productpage | <span class="nb">grep</span> <span class="nt">-o</span> <span class="s2">"&lt;title&gt;.*&lt;/title&gt;"</span>
<span class="c"># =&gt; &amp;lt;title&amp;gt;Simple Bookstore App&amp;lt;/title&amp;gt;</span>

<span class="c"># 로그</span>
<span class="nv">$ </span>kubetail <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>productpage <span class="nt">-f</span>
</code></pre></div></div>

<h4 id="istio를-통한-인입-기본-설정">Istio를 통한 인입 기본 설정</h4>

<h5 id="istio-gatewayvirtualservice-설정">Istio Gateway/VirtualService 설정</h5>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Istio Gateway/VirtualService 설정</span>
<span class="nv">$ </span><span class="nb">cat</span> ~/istio-<span class="nv">$ISTIOV</span>/samples/bookinfo/networking/bookinfo-gateway.yaml
<span class="c"># =&gt; apiVersion: networking.istio.io/v1alpha3</span>
<span class="c">#    kind: Gateway</span>
<span class="c">#    metadata:</span>
<span class="c">#      name: bookinfo-gateway</span>
<span class="c">#    spec:</span>
<span class="c">#      # The selector matches the ingress gateway pod labels.</span>
<span class="c">#      # If you installed Istio using Helm following the standard documentation, this would be &amp;quot;istio=ingress&amp;quot;</span>
<span class="c">#      selector:</span>
<span class="c">#        istio: ingressgateway # use istio default controller</span>
<span class="c">#      servers:</span>
<span class="c">#      - port:</span>
<span class="c">#          number: 8080</span>
<span class="c">#          name: http</span>
<span class="c">#          protocol: HTTP</span>
<span class="c">#        hosts:</span>
<span class="c">#        - &amp;quot;*&amp;quot;</span>
<span class="c">#    ---</span>
<span class="c">#    apiVersion: networking.istio.io/v1alpha3</span>
<span class="c">#    kind: VirtualService</span>
<span class="c">#    metadata:</span>
<span class="c">#      name: bookinfo</span>
<span class="c">#    spec:</span>
<span class="c">#      hosts:</span>
<span class="c">#      - &amp;quot;*&amp;quot;</span>
<span class="c">#      gateways:</span>
<span class="c">#      - bookinfo-gateway</span>
<span class="c">#      http:</span>
<span class="c">#      - match:</span>
<span class="c">#        - uri:</span>
<span class="c">#            exact: /productpage</span>
<span class="c">#        - uri:</span>
<span class="c">#            prefix: /static</span>
<span class="c">#        - uri:</span>
<span class="c">#            exact: /login</span>
<span class="c">#        - uri:</span>
<span class="c">#            exact: /logout</span>
<span class="c">#        - uri:</span>
<span class="c">#            prefix: /api/v1/products</span>
<span class="c">#        route:</span>
<span class="c">#        - destination:</span>
<span class="c">#            host: productpage</span>
<span class="c">#            port:</span>
<span class="c">#              number: 9080</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> ~/istio-<span class="nv">$ISTIOV</span>/samples/bookinfo/networking/bookinfo-gateway.yaml
<span class="c"># =&gt; gateway.networking.istio.io/bookinfo-gateway created</span>
<span class="c">#    virtualservice.networking.istio.io/bookinfo created</span>

<span class="c"># 확인</span>
<span class="nv">$ </span>kubectl get gw,vs
<span class="nv">$ </span>istioctl proxy-status
<span class="c"># =&gt; NAME                                                   CLUSTER        CDS                LDS                EDS                RDS                ECDS        ISTIOD                      VERSION</span>
<span class="c">#    deploy-websrv-778ffd6947-cxf5k.default                 Kubernetes     SYNCED (5m9s)      SYNCED (5m9s)      SYNCED (4m24s)     SYNCED (5m9s)      IGNORED     istiod-7f8b586864-8mc4c     1.23.2</span>
<span class="c">#    details-v1-65cfcf56f9-8465x.default                    Kubernetes     SYNCED (4m43s)     SYNCED (4m43s)     SYNCED (4m24s)     SYNCED (4m43s)     IGNORED     istiod-7f8b586864-8mc4c     1.23.2</span>
<span class="c">#    istio-ingressgateway-5f9f654d46-l7mqp.istio-system     Kubernetes     SYNCED (10s)       SYNCED (10s)       SYNCED (4m24s)     SYNCED (10s)       IGNORED     istiod-7f8b586864-8mc4c     1.23.2</span>
<span class="c">#    productpage-v1-d5789fdfb-f8gdf.default                 Kubernetes     SYNCED (4m53s)     SYNCED (4m53s)     SYNCED (4m24s)     SYNCED (4m53s)     IGNORED     istiod-7f8b586864-8mc4c     1.23.2</span>
<span class="c">#    ratings-v1-7c9bd4b87f-s9gxs.default                    Kubernetes     SYNCED (4m24s)     SYNCED (4m24s)     SYNCED (4m24s)     SYNCED (4m24s)     IGNORED     istiod-7f8b586864-8mc4c     1.23.2</span>
<span class="c">#    reviews-v1-6584ddcf65-gnc9j.default                    Kubernetes     SYNCED (4m47s)     SYNCED (4m47s)     SYNCED (4m24s)     SYNCED (4m47s)     IGNORED     istiod-7f8b586864-8mc4c     1.23.2</span>
<span class="c">#    reviews-v2-6f85cb9b7c-cfc68.default                    Kubernetes     SYNCED (4m41s)     SYNCED (4m41s)     SYNCED (4m24s)     SYNCED (4m41s)     IGNORED     istiod-7f8b586864-8mc4c     1.23.2</span>
<span class="c">#    reviews-v3-6f5b775685-cw7tc.default                    Kubernetes     SYNCED (4m43s)     SYNCED (4m43s)     SYNCED (4m24s)     SYNCED (4m43s)     IGNORED     istiod-7f8b586864-8mc4c     1.23.2</span>

<span class="c"># productpage 파드의 istio-proxy 로그 확인 Access log 가 출력 - Default access log format : 링크</span>
<span class="nv">$ </span>kubetail <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>productpage <span class="nt">-c</span> istio-proxy <span class="nt">-f</span>
</code></pre></div></div>

<ul>
  <li><strong>k3s-m</strong> NodePort 접속 확인</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#</span>
<span class="nv">$ </span><span class="nb">export </span><span class="nv">IGWHTTP</span><span class="o">=</span><span class="si">$(</span>kubectl get service <span class="nt">-n</span> istio-system istio-ingressgateway <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.spec.ports[1].nodePort}'</span><span class="si">)</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$IGWHTTP</span>
<span class="c"># =&gt; 31677</span>

<span class="c"># 접속 확인</span>
<span class="nv">$ </span>kubectl get svc <span class="nt">-n</span> istio-system istio-ingressgateway
<span class="c"># =&gt; NAME                   TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                                      AGE</span>
<span class="c">#    istio-ingressgateway   NodePort   10.10.200.171   &amp;lt;none&amp;gt;        15021:30953/TCP,80:31677/TCP,443:30737/TCP,31400:30617/TCP,15443:31668/TCP   3h31m</span>
<span class="nv">$ </span>curl <span class="nt">-s</span> http://localhost:<span class="nv">$IGWHTTP</span>/productpage
<span class="nv">$ </span>curl <span class="nt">-s</span> http://192.168.10.101:<span class="nv">$IGWHTTP</span>/productpage
<span class="nv">$ </span>curl <span class="nt">-s</span> http://192.168.10.102:<span class="nv">$IGWHTTP</span>/productpage
<span class="c"># =&gt; &amp;lt;meta charset=&amp;quot;utf-8&amp;quot;&amp;gt;</span>
<span class="c">#    &amp;lt;meta http-equiv=&amp;quot;X-UA-Compatible&amp;quot; content=&amp;quot;IE=edge&amp;quot;&amp;gt;</span>
<span class="c">#    &amp;lt;meta name=&amp;quot;viewport&amp;quot; content=&amp;quot;width=device-width, initial-scale=1.0&amp;quot;&amp;gt;</span>
<span class="c">#    </span>
<span class="c">#    &amp;lt;title&amp;gt;Simple Bookstore App&amp;lt;/title&amp;gt;</span>
<span class="c">#    ...</span>

<span class="c"># 정보 확인</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$MYDOMAIN</span>
<span class="c"># =&gt; sweetlittlebird.com</span>
<span class="nv">$ </span><span class="nb">cat</span> /etc/hosts
<span class="c"># =&gt; ...</span>
<span class="c">#    127.0.2.1 k3s-m k3s-m</span>
<span class="c">#    192.168.10.10 k3s-m</span>
<span class="c">#    192.168.10.101 k3s-w1</span>
<span class="c">#    192.168.10.102 k3s-w2</span>
<span class="c">#    192.168.10.103 k3s-w3</span>
<span class="c">#    192.168.10.10 sweetlittlebird.com</span>

<span class="c">#</span>
<span class="nv">$ </span>curl <span class="nt">-s</span> http://<span class="nv">$MYDOMAIN</span>:<span class="nv">$IGWHTTP</span>/productpage
<span class="c"># =&gt; &amp;lt;meta charset=&amp;quot;utf-8&amp;quot;&amp;gt;</span>
<span class="c">#    &amp;lt;meta http-equiv=&amp;quot;X-UA-Compatible&amp;quot; content=&amp;quot;IE=edge&amp;quot;&amp;gt;</span>
<span class="c">#    &amp;lt;meta name=&amp;quot;viewport&amp;quot; content=&amp;quot;width=device-width, initial-scale=1.0&amp;quot;&amp;gt;</span>
<span class="c">#    </span>
<span class="c">#    &amp;lt;title&amp;gt;Simple Bookstore App&amp;lt;/title&amp;gt;</span>
<span class="c">#    ...</span>
</code></pre></div></div>

<ul>
  <li>자신의 PC에서 접속 확인</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$MYDOMAIN</span> <span class="nv">$IGWHTTP</span>
<span class="c"># =&gt; sweetlittlebird.com 31677</span>
<span class="nv">$ </span><span class="nb">cat</span> /etc/hosts

<span class="c">#</span>
<span class="nv">$ </span>curl <span class="nt">-v</span> <span class="nt">-s</span> <span class="nv">$MYDOMAIN</span>:<span class="nv">$IGWHTTP</span>/productpage
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_12.png" alt="img.png" class="image-center" />
<em class="image-caption">Bookinfo 접속 결과 - 새로고침 할 때 마다 다른 파드에 접속되면서 리뷰가 달라짐</em></p>

<ul>
  <li><strong>testpc 에서 접속 실행</strong></li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># istio ingress gw 를 통한 접속 테스트</span>
<span class="nv">$ </span>curl <span class="nt">-s</span> <span class="nv">$MYDOMAIN</span>:<span class="nv">$IGWHTTP</span>/productpage | <span class="nb">grep</span> <span class="nt">-o</span> <span class="s2">"&lt;title&gt;.*&lt;/title&gt;"</span>
<span class="c"># =&gt; &amp;lt;title&amp;gt;Simple Bookstore App&amp;lt;/title&amp;gt;</span>
<span class="nv">$ </span><span class="k">while </span><span class="nb">true</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$MYDOMAIN</span>:<span class="nv">$IGWHTTP</span>/productpage | <span class="nb">grep</span> <span class="nt">-o</span> <span class="s2">"&lt;title&gt;.*&lt;/title&gt;"</span> <span class="p">;</span> <span class="nb">echo</span> <span class="s2">"--------------"</span> <span class="p">;</span> <span class="nb">sleep </span>1<span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; &amp;lt;title&amp;gt;Simple Bookstore App&amp;lt;/title&amp;gt;</span>
<span class="c">#    --------------</span>
<span class="c">#    &amp;lt;title&amp;gt;Simple Bookstore App&amp;lt;/title&amp;gt;</span>
<span class="c">#    --------------</span>
<span class="c">#    ...</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span>  <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$MYDOMAIN</span>:<span class="nv">$IGWHTTP</span>/productpage | <span class="nb">grep</span> <span class="nt">-o</span> <span class="s2">"&lt;title&gt;.*&lt;/title&gt;"</span> <span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; &amp;lt;title&amp;gt;Simple Bookstore App&amp;lt;/title&amp;gt;</span>
<span class="c">#    &amp;lt;title&amp;gt;Simple Bookstore App&amp;lt;/title&amp;gt;</span>
<span class="c">#    ...</span>
</code></pre></div></div>

<h4 id="모니터링">모니터링</h4>

<ul>
  <li>옵저빌리티 연동 문서 : <a href="https://istio.io/latest/docs/ops/integrations/">링크</a></li>
</ul>

<h5 id="kiali-키알리-소개">Kiali (키알리) 소개</h5>

<ul>
  <li>Kiali는 Istio 서비스 메시의 모니터링 및 시각화 도구입니다.</li>
  <li>주 데이터 소스는 Prometheus와 Jaeger 등입니다.</li>
  <li>특히 Jaeger와 연동하여 서비스 간의 호출 관계를 시각화하여 볼 수 있습니다.</li>
  <li>Istiod의 health 상태를 확인하기 위해 istiod 파드를 직접 접속합니다. (기본 15014포트)</li>
</ul>

<h5 id="kiali-설치-및-확인">Kiali 설치 및 확인</h5>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Install Kiali and the other addons and wait for them to be deployed. : Kiali dashboard, along with Prometheus, Grafana, and Jaeger.</span>
<span class="nv">$ </span>tree ~/istio-<span class="nv">$ISTIOV</span>/samples/addons/
<span class="c"># =&gt; /root/istio-1.23.2/samples/addons/</span>
<span class="c">#    ├── README.md</span>
<span class="c">#    ├── extras</span>
<span class="c">#    │   ├── prometheus-operator.yaml</span>
<span class="c">#    │   ├── skywalking.yaml</span>
<span class="c">#    │   └── zipkin.yaml</span>
<span class="c">#    ├── grafana.yaml</span>
<span class="c">#    ├── jaeger.yaml</span>
<span class="c">#    ├── kiali.yaml</span>
<span class="c">#    ├── loki.yaml</span>
<span class="c">#    └── prometheus.yaml</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> ~/istio-<span class="nv">$ISTIOV</span>/samples/addons <span class="c"># 디렉터리에 있는 모든 yaml 자원을 생성</span>
<span class="c"># =&gt; deployment.apps/grafana created</span>
<span class="c">#    deployment.apps/jaeger created</span>
<span class="c">#    deployment.apps/kiali created</span>
<span class="c">#    deployment.apps/prometheus created</span>
<span class="c">#    ...</span>
<span class="nv">$ </span>kubectl rollout status deployment/kiali <span class="nt">-n</span> istio-system
<span class="c"># =&gt; deployment &amp;quot;kiali&amp;quot; successfully rolled out</span>

<span class="c"># 확인</span>
<span class="nv">$ </span>kubectl get all,sa,cm <span class="nt">-n</span> istio-system
<span class="nv">$ </span>kubectl get svc,ep <span class="nt">-n</span> istio-system
<span class="c"># =&gt; NAME                           TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                                      AGE</span>
<span class="c">#    service/grafana                ClusterIP   10.10.200.178   &amp;lt;none&amp;gt;        3000/TCP                                                                     69s</span>
<span class="c">#    service/istio-ingressgateway   NodePort    10.10.200.171   &amp;lt;none&amp;gt;        15021:30953/TCP,80:31677/TCP,443:30737/TCP,31400:30617/TCP,15443:31668/TCP   5h48m</span>
<span class="c">#    service/istiod                 ClusterIP   10.10.200.215   &amp;lt;none&amp;gt;        15010/TCP,15012/TCP,443/TCP,15014/TCP                                        5h48m</span>
<span class="c">#    service/jaeger-collector       ClusterIP   10.10.200.121   &amp;lt;none&amp;gt;        14268/TCP,14250/TCP,9411/TCP,4317/TCP,4318/TCP                               69s</span>
<span class="c">#    service/kiali                  ClusterIP   10.10.200.19    &amp;lt;none&amp;gt;        20001/TCP,9090/TCP                                                           68s</span>
<span class="c">#    service/loki                   ClusterIP   10.10.200.227   &amp;lt;none&amp;gt;        3100/TCP,9095/TCP                                                            68s</span>
<span class="c">#    service/loki-headless          ClusterIP   None            &amp;lt;none&amp;gt;        3100/TCP                                                                     68s</span>
<span class="c">#    service/loki-memberlist        ClusterIP   None            &amp;lt;none&amp;gt;        7946/TCP                                                                     68s</span>
<span class="c">#    service/prometheus             ClusterIP   10.10.200.148   &amp;lt;none&amp;gt;        9090/TCP                                                                     68s</span>
<span class="c">#    service/tracing                ClusterIP   10.10.200.133   &amp;lt;none&amp;gt;        80/TCP,16685/TCP                                                             69s</span>
<span class="c">#    service/zipkin                 ClusterIP   10.10.200.29    &amp;lt;none&amp;gt;        9411/TCP                                                                     69s</span>
<span class="c">#    </span>
<span class="c">#    NAME                             ENDPOINTS                                                           AGE</span>
<span class="c">#    endpoints/grafana                172.16.2.17:3000                                                    69s</span>
<span class="c">#    endpoints/istio-ingressgateway   172.16.2.14:15443,172.16.2.14:15021,172.16.2.14:31400 + 2 more...   5h48m</span>
<span class="c">#    endpoints/istiod                 172.16.3.16:15012,172.16.3.16:15010,172.16.3.16:15017 + 1 more...   5h48m</span>
<span class="c">#    endpoints/jaeger-collector       172.16.3.19:9411,172.16.3.19:14250,172.16.3.19:4317 + 2 more...     69s</span>
<span class="c">#    endpoints/kiali                  172.16.1.16:9090,172.16.1.16:20001                                  68s</span>
<span class="c">#    endpoints/loki                                                                                       68s</span>
<span class="c">#    endpoints/loki-headless                                                                              68s</span>
<span class="c">#    endpoints/loki-memberlist                                                                            68s</span>
<span class="c">#    endpoints/prometheus             172.16.3.20:9090                                                    67s</span>
<span class="c">#    endpoints/tracing                172.16.3.19:16685,172.16.3.19:16686                                 69s</span>
<span class="c">#    endpoints/zipkin                 172.16.3.19:9411                                                    69s</span>

<span class="c"># kiali 서비스 변경</span>
<span class="nv">$ </span>kubectl patch svc <span class="nt">-n</span> istio-system kiali <span class="nt">-p</span> <span class="s1">'{"spec":{"type":"NodePort"}}'</span>
<span class="c"># =&gt; service/kiali patched</span>

<span class="c"># kiali 웹 접속 주소 확인</span>
<span class="nv">$ KIALINodePort</span><span class="o">=</span><span class="si">$(</span>kubectl get svc <span class="nt">-n</span> istio-system kiali <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.spec.ports[0].nodePort<span class="o">}</span><span class="si">)</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"KIALI UI URL = http://</span><span class="si">$(</span>curl <span class="nt">-s</span> ipinfo.io/ip<span class="si">)</span><span class="s2">:</span><span class="nv">$KIALINodePort</span><span class="s2">"</span>
<span class="c"># =&gt; KIALI UI URL = http://54.123.42.212:31274</span>

<span class="c"># Grafana 서비스 변경</span>
<span class="nv">$ </span>kubectl patch svc <span class="nt">-n</span> istio-system grafana <span class="nt">-p</span> <span class="s1">'{"spec":{"type":"NodePort"}}'</span>
<span class="c"># =&gt; service/grafana patched</span>

<span class="c"># Grafana 웹 접속 주소 확인 : 7개의 대시보드</span>
<span class="nv">$ GRAFANANodePort</span><span class="o">=</span><span class="si">$(</span>kubectl get svc <span class="nt">-n</span> istio-system grafana <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.spec.ports[0].nodePort<span class="o">}</span><span class="si">)</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"Grafana URL = http://</span><span class="si">$(</span>curl <span class="nt">-s</span> ipinfo.io/ip<span class="si">)</span><span class="s2">:</span><span class="nv">$GRAFANANodePort</span><span class="s2">"</span>
<span class="c"># =&gt; Grafana URL = http://54.123.42.212:30266</span>

<span class="c"># Prometheus 서비스 변경</span>
<span class="nv">$ </span>kubectl patch svc <span class="nt">-n</span> istio-system prometheus <span class="nt">-p</span> <span class="s1">'{"spec":{"type":"NodePort"}}'</span>
<span class="c"># =&gt; service/prometheus patched</span>

<span class="c"># Prometheus 웹 접속 주소 확인</span>
<span class="nv">$ PROMENodePort</span><span class="o">=</span><span class="si">$(</span>kubectl get svc <span class="nt">-n</span> istio-system prometheus <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.spec.ports[0].nodePort<span class="o">}</span><span class="si">)</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"Prometheus URL = http://</span><span class="si">$(</span>curl <span class="nt">-s</span> ipinfo.io/ip<span class="si">)</span><span class="s2">:</span><span class="nv">$PROMENodePort</span><span class="s2">"</span>
<span class="c"># =&gt; Prometheus URL = http://54.123.42.212:30506</span>
</code></pre></div></div>

<ul>
  <li>Prometheus : Targets - 파드별로 tcp/15020의 /stats/prometheus를 통해 수집</li>
</ul>

<p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_13.png" alt="img.png" class="image-center" /></p>

<p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_14.png" alt="img.png" class="image-center" /></p>

<p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_15.png" alt="img.png" class="image-center" /></p>

<ul>
  <li>Grafana : 7개의 대시보드</li>
</ul>

<p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_17.png" alt="img.png" class="image-center" /></p>

<p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_16.png" alt="img.png" class="image-center" /></p>

<ul>
  <li>Kiali : 서비스간의 호출 관계를 시각화</li>
</ul>

<p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_18.png" alt="img.png" class="image-center" /></p>

<h5 id="kiali-키알리-대시보드-둘러보기">Kiali (키알리) 대시보드 둘러보기</h5>

<ul>
  <li>Namespace 를 default 로 선택 후 Graph (Traffic, Versioned app graph) 에서 Display 옵션 중 ‘Traffic Distribution’과
‘Traffic Animation’ 활성화, Security 체크 해서 확인해보겠습니다.</li>
  <li>트래픽을 발생시켜서 Kiali 대시보드를 확인해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># testpc 에서 아래 실행</span>
<span class="c"># 반복 접속 테스트</span>
<span class="nv">$ </span><span class="k">while </span><span class="nb">true</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$MYDOMAIN</span>:<span class="nv">$IGWHTTP</span>/productpage | <span class="nb">grep</span> <span class="nt">-o</span> <span class="s2">"&lt;title&gt;.*&lt;/title&gt;"</span> <span class="p">;</span> <span class="nb">echo</span> <span class="s2">"--------------"</span> <span class="p">;</span> <span class="nb">sleep </span>1<span class="p">;</span> <span class="k">done</span>
<span class="nv">$ </span><span class="k">while </span><span class="nb">true</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$MYDOMAIN</span>:<span class="nv">$IGWHTTP</span>/productpage | <span class="nb">grep</span> <span class="nt">-o</span> <span class="s2">"&lt;title&gt;.*&lt;/title&gt;"</span> <span class="p">;</span> <span class="nb">echo</span> <span class="s2">"--------------"</span> <span class="p">;</span> <span class="nb">sleep </span>0.1<span class="p">;</span> <span class="k">done</span>
<span class="nv">$ </span><span class="k">while </span><span class="nb">true</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$MYDOMAIN</span>:<span class="nv">$IGWHTTP</span>/productpage | <span class="nb">grep</span> <span class="nt">-o</span> <span class="s2">"&lt;title&gt;.*&lt;/title&gt;"</span> <span class="p">;</span> <span class="nb">echo</span> <span class="s2">"--------------"</span> <span class="p">;</span> <span class="nb">sleep </span>0.5<span class="p">;</span> <span class="k">done</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span>  <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$MYDOMAIN</span>:<span class="nv">$IGWHTTP</span>/productpage | <span class="nb">grep</span> <span class="nt">-o</span> <span class="s2">"&lt;title&gt;.*&lt;/title&gt;"</span> <span class="p">;</span> <span class="k">done</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..1000<span class="o">}</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$MYDOMAIN</span>:<span class="nv">$IGWHTTP</span>/productpage | <span class="nb">grep</span> <span class="nt">-o</span> <span class="s2">"&lt;title&gt;.*&lt;/title&gt;"</span> <span class="p">;</span> <span class="k">done</span>
</code></pre></div></div>

<ul>
  <li><strong>Traffic Graph</strong>에서는 트래픽 흐름을 확인할 수 있습니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_19.png" alt="img.png" class="image-center" /></p>

<ul>
  <li><strong>Workloads</strong>에서는 Log 등을 확인할 수 있고, Envoy 관련 설정 정보(Listener, Cluster, Route, Endpoint 등)를 확인할 수 있습니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_20.png" alt="img.png" class="image-center" /></p>

<ul>
  <li><strong>Istio Config</strong>에서 Istio 관련 설정을 볼 수 있고, <strong>Action</strong> 으로 Istio 관련 오브젝트를 설정/삭제 할 수 있습니다.</li>
</ul>

<h4 id="traffic-management">Traffic Management</h4>

<ul>
  <li><strong>동작 소개</strong> : 클라이언트 PC → Istio <strong>ingressgateway</strong> 파드 → (Gateway, <strong>VirtualService</strong> + <strong>DestinationRule</strong>) → Cluster(<strong>Endpoint</strong> - 파드)
    <ul>
      <li><strong>Gateway</strong> : 지정한 인그레스 게이트웨이로부터 트래픽이 인입, 프로토콜 및 포트, HOSTS, Proxy 등 설정이 가능합니다.</li>
      <li><strong>VirtualService</strong> : 인입 처리할 hosts 설정, L7 PATH 별 라우팅, 목적지에 대한 정책 설정 가능합니다. (envoy route config) - <a href="https://istio.io/latest/docs/concepts/traffic-management/#virtual-services">링크</a>
        <ul>
          <li>VirtualService 는 DestinationRule 에서 설정된 <strong>서브셋(subset)</strong>을 사용하여 <strong>트래픽 컨트롤</strong>을 할 수 있습니다.</li>
          <li><strong>hosts 필드</strong> : 목적지 주소 - IP address, a DNS name (FQDN), 혹은 k8s svc 이름, wildcard (”*”) prefixes</li>
          <li><strong>Routing rules</strong> : HTTP 경우 - Match 필드(예) 헤더), Destination(istio/envoy 에 등록된 대상, subnet 에 DestinationRule 활용)
            <ul>
              <li><strong>HTTPRoute</strong> : redirect , rewrite , fault(장애 주입) , mirror(복제, 기본 100%) , corsPolicy(CORS 삽입) , headers(헤더 조작) 등 - <a href="https://istio.io/latest/docs/reference/config/networking/virtual-service/#HTTPRoute">링크</a></li>
            </ul>
          </li>
          <li>Routing rule precedence : Routing rules are evaluated in sequential order from top to bottom - 위에서 순차적 적용</li>
        </ul>
      </li>
      <li>DestinationRule : 실제 도착지(서비스와 1:1 연결)의 정교한 정책(부하분산, 연결 옵션, 서킷 브레이크, TLS 등)을 설정 - <a href="https://istio.io/latest/docs/concepts/traffic-management/#destination-rules">링크</a>
        <ul>
          <li><strong>Load balancing options</strong> : Round robin(기본값) , Random , Weighted , Least requests - <a href="https://www.envoyproxy.io/docs/envoy/v1.5.0/intro/arch_overview/load_balancing">링크</a>
            <ul>
              <li><strong>Destination Rule</strong> : TrafficPolicy , Subset , ConnectionPoolSettings 등 - <a href="https://istio.io/latest/docs/reference/config/networking/destination-rule/">링크</a></li>
              <li>서브셋(subsets)을 정의할 수 있어 마이크로서비스 <strong>버전별로 라우팅</strong>할 때 사용한다</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h5 id="request-routing-실습">Request Routing 실습</h5>

<ul>
  <li>실습전 기본 DestinationRule 적용</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 샘플 파일들 확인</span>
<span class="nv">$ </span>tree ~/istio-<span class="nv">$ISTIOV</span>/samples/bookinfo/networking
<span class="c"># =&gt; /root/istio-1.23.2/samples/bookinfo/networking</span>
<span class="c">#    ├── bookinfo-gateway.yaml</span>
<span class="c">#    ├── certmanager-gateway.yaml</span>
<span class="c">#    ├── destination-rule-all-mtls.yaml</span>
<span class="c">#    ├── destination-rule-all.yaml</span>
<span class="c">#    ├── destination-rule-reviews.yaml</span>
<span class="c">#    ├── egress-rule-google-apis.yaml</span>
<span class="c">#    ├── fault-injection-details-v1.yaml</span>
<span class="c">#    ├── virtual-service-all-v1.yaml</span>
<span class="c">#    ├── virtual-service-details-v2.yaml</span>
<span class="c">#    ├── virtual-service-ratings-db.yaml</span>
<span class="c">#    ├── virtual-service-ratings-mysql-vm.yaml</span>
<span class="c">#    ├── virtual-service-ratings-mysql.yaml</span>
<span class="c">#    ├── virtual-service-ratings-test-abort.yaml</span>
<span class="c">#    ├── virtual-service-ratings-test-delay.yaml</span>
<span class="c">#    ├── virtual-service-reviews-50-v3.yaml</span>
<span class="c">#    ├── virtual-service-reviews-80-20.yaml</span>
<span class="c">#    ├── virtual-service-reviews-90-10.yaml</span>
<span class="c">#    ├── virtual-service-reviews-jason-v2-v3.yaml</span>
<span class="c">#    ├── virtual-service-reviews-test-v2.yaml</span>
<span class="c">#    ├── virtual-service-reviews-v2-v3.yaml</span>
<span class="c">#    └── virtual-service-reviews-v3.yaml</span>

<span class="c"># 기본 DestinationRule 적용</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> ~/istio-<span class="nv">$ISTIOV</span>/samples/bookinfo/networking/destination-rule-all.yaml
<span class="c"># =&gt; destinationrule.networking.istio.io/productpage created</span>
<span class="c">#    destinationrule.networking.istio.io/reviews created</span>
<span class="c">#    destinationrule.networking.istio.io/ratings created</span>
<span class="c">#    destinationrule.networking.istio.io/details created</span>

<span class="c"># DestinationRule 확인 dr(=destinationrules) : KIALI Services 확인 시 GW, VS, DR 확인</span>
<span class="nv">$ </span>kubectl get dr
<span class="c"># =&gt; NAME          HOST          AGE</span>
<span class="c">#    details       details       31s</span>
<span class="c">#    productpage   productpage   31s</span>
<span class="c">#    ratings       ratings       31s</span>
<span class="c">#    reviews       reviews       31s</span>
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_21.png" alt="img.png" /></p>

<ul>
  <li><strong>virtual-service-all-v1.yaml</strong> : 4개 서비스 모두 v1 의 서브셋(subset) 에 전송하는 정책 테스트</li>
</ul>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># virtual-service-all-v1.yaml</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">networking.istio.io/v1alpha3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualService</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">productpage</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">hosts</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">productpage</span>
  <span class="na">http</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">route</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">destination</span><span class="pi">:</span>
        <span class="na">host</span><span class="pi">:</span> <span class="s">productpage</span>
        <span class="na">subset</span><span class="pi">:</span> <span class="s">v1</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">networking.istio.io/v1alpha3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualService</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">reviews</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">hosts</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">reviews</span>
  <span class="na">http</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">route</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">destination</span><span class="pi">:</span>
        <span class="na">host</span><span class="pi">:</span> <span class="s">reviews</span>
        <span class="na">subset</span><span class="pi">:</span> <span class="s">v1</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">networking.istio.io/v1alpha3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualService</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">ratings</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">hosts</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">ratings</span>
  <span class="na">http</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">route</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">destination</span><span class="pi">:</span>
        <span class="na">host</span><span class="pi">:</span> <span class="s">ratings</span>
        <span class="na">subset</span><span class="pi">:</span> <span class="s">v1</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">networking.istio.io/v1alpha3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualService</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">details</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">hosts</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">details</span>
  <span class="na">http</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">route</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">destination</span><span class="pi">:</span>
        <span class="na">host</span><span class="pi">:</span> <span class="s">details</span>
        <span class="na">subset</span><span class="pi">:</span> <span class="s">v1</span>
<span class="nn">---</span>
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># istio vs(virtualservices) 확인</span>
<span class="nv">$ </span>kubectl get vs
<span class="c"># =&gt; NAME       GATEWAYS               HOSTS   AGE</span>
<span class="c">#    bookinfo   [&amp;quot;bookinfo-gateway&amp;quot;]   [&amp;quot;*&amp;quot;]   3h30m</span>

<span class="c"># 모든 마이크로서비스에 대해 v1 의 서브셋(subset) 에 전송되게 virtualservices 적용</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> virtual-service-all-v1.yaml
<span class="c"># =&gt; virtualservice.networking.istio.io/productpage created</span>
<span class="c">#    virtualservice.networking.istio.io/reviews created</span>
<span class="c">#    virtualservice.networking.istio.io/ratings created</span>
<span class="c">#    virtualservice.networking.istio.io/details created</span>

<span class="c"># istio vs(virtualservices) 확인 &gt;&gt; KIALI 에서 reviews v2,v3 향하는 트래픽 경로가 사라진다!</span>
<span class="nv">$ </span>kubectl get virtualservices
<span class="c"># =&gt; NAME          GATEWAYS               HOSTS             AGE</span>
<span class="c">#    bookinfo      [&amp;quot;bookinfo-gateway&amp;quot;]   [&amp;quot;*&amp;quot;]             3h30m</span>
<span class="c">#    details                              [&amp;quot;details&amp;quot;]       10s</span>
<span class="c">#    productpage                          [&amp;quot;productpage&amp;quot;]   10s</span>
<span class="c">#    ratings                              [&amp;quot;ratings&amp;quot;]       10s</span>
<span class="c">#    reviews                              [&amp;quot;reviews&amp;quot;]       10s</span>
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_22.png" alt="img.png" /></p>

<ul>
  <li>
    <p>모든 트래픽이 v1으로 향하게 되어서 브라우저를 새로고침해도 v1만 나오게 됩니다.</p>
  </li>
  <li>
    <p><strong>virtual-service-reviews-test-v2.yaml</strong> : User Identity 기반 라우팅, end-user 커스텀 헤더에 <strong>jason</strong> 매칭 시 <strong>reviews v2</strong> 로 전달</p>
    <ul>
      <li>Match 조건에는 완전 일치(exact) , 전방 일치(prefix) , 정규 표현(regex) - 3가지 패턴을 선택할 수 있다</li>
    </ul>
  </li>
</ul>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># virtual-service-reviews-test-v2.yaml</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">networking.istio.io/v1alpha3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualService</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">reviews</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">hosts</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">reviews</span>
  <span class="na">http</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">match</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">headers</span><span class="pi">:</span>
        <span class="na">end-user</span><span class="pi">:</span>
          <span class="na">exact</span><span class="pi">:</span> <span class="s">jason</span>
    <span class="na">route</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">destination</span><span class="pi">:</span>
        <span class="na">host</span><span class="pi">:</span> <span class="s">reviews</span>
        <span class="na">subset</span><span class="pi">:</span> <span class="s">v2</span>
  <span class="pi">-</span> <span class="na">route</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">destination</span><span class="pi">:</span>
        <span class="na">host</span><span class="pi">:</span> <span class="s">reviews</span>
        <span class="na">subset</span><span class="pi">:</span> <span class="s">v1</span>
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 모든 마이크로서비스에 대해 v1 의 서브셋(subset) 에 전송되게 virtualservices 적용</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> virtual-service-reviews-test-v2.yaml
<span class="c"># =&gt; virtualservice.networking.istio.io/reviews configured</span>

<span class="c"># jason 로그인 시 로그 확인</span>
<span class="nv">$ </span>kubetail <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>productpage <span class="nt">-f</span>
<span class="c"># =&gt; [productpage-v1-d5789fdfb-f8gdf productpage] DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): details:9080</span>
<span class="c">#    [productpage-v1-d5789fdfb-f8gdf productpage] send: b'GET /details/0 HTTP/1.1\r\nHost: details:9080\r\nuser-agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Safari/605.1.15\r\nAccept-Encoding: gzip, deflate\r\nAccept: */*\r\nConnection: keep-alive\r\nx-request-id: 3aa3c711-d014-930f-8c2a-563c3dbbd8b5\r\n\r\n'</span>
<span class="c">#    [productpage-v1-d5789fdfb-f8gdf productpage] reply: 'HTTP/1.1 200 OK\r\n'</span>
<span class="c">#    [productpage-v1-d5789fdfb-f8gdf productpage] header: content-type: application/json</span>
<span class="c">#    [productpage-v1-d5789fdfb-f8gdf productpage] header: server: envoy</span>
<span class="c">#    [productpage-v1-d5789fdfb-f8gdf productpage] header: date: Sat, 19 Oct 2024 14:32:09 GMT</span>
<span class="c">#    [productpage-v1-d5789fdfb-f8gdf productpage] header: content-length: 178</span>
<span class="c">#    [productpage-v1-d5789fdfb-f8gdf productpage] header: x-envoy-upstream-service-time: 6</span>
<span class="c">#    [productpage-v1-d5789fdfb-f8gdf productpage] DEBUG:urllib3.connectionpool:http://details:9080 &amp;quot;GET /details/0 HTTP/1.1&amp;quot; 200 178</span>
<span class="c">#    [productpage-v1-d5789fdfb-f8gdf productpage] DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): reviews:9080</span>
<span class="c">#    [productpage-v1-d5789fdfb-f8gdf productpage] send: b'GET /reviews/0 HTTP/1.1</span>
<span class="c">#    Host: reviews:9080</span>
<span class="c">#    user-agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Safari/605.1.15</span>
<span class="c">#    Accept-Encoding: gzip, deflate</span>
<span class="c">#    Accept: */*</span>
<span class="c">#    Connection: keep-alive</span>
<span class="c">#    x-request-id: 3aa3c711-d014-930f-8c2a-563c3dbbd8b5\r\n\r\n'</span>
<span class="c">#    [productpage-v1-d5789fdfb-f8gdf productpage] reply: 'HTTP/1.1 200 OK\r\n'</span>
<span class="c">#    [productpage-v1-d5789fdfb-f8gdf productpage] header: x-powered-by: Servlet/3.1</span>
<span class="c">#    [productpage-v1-d5789fdfb-f8gdf productpage] header: content-type: application/json</span>
<span class="c">#    [productpage-v1-d5789fdfb-f8gdf productpage] header: date: Sat, 19 Oct 2024 14:32:09 GMT</span>
<span class="c">#    [productpage-v1-d5789fdfb-f8gdf productpage] header: content-language: en-US</span>
<span class="c">#    [productpage-v1-d5789fdfb-f8gdf productpage] header: content-length: 358</span>
<span class="c">#    [productpage-v1-d5789fdfb-f8gdf productpage] header: x-envoy-upstream-service-time: 1</span>
<span class="c">#    [productpage-v1-d5789fdfb-f8gdf productpage] header: server: envoy</span>
<span class="c">#    [productpage-v1-d5789fdfb-f8gdf productpage] DEBUG:urllib3.connectionpool:http://reviews:9080 &amp;quot;GET /reviews/0 HTTP/1.1&amp;quot; 200 358</span>
<span class="c">#    [productpage-v1-d5789fdfb-f8gdf productpage] INFO:werkzeug:::ffff:127.0.0.6 - - [19/Oct/2024 14:32:09] &amp;quot;GET /productpage HTTP/1.1&amp;quot; 200 -</span>
<span class="c">#    ...</span>
</code></pre></div></div>

<ul>
  <li>웹브라우저에서 productpage로 접속 후 새로고침을 해보겠습니다.
    <ul>
      <li>로그인 전에는 v1으로 접속 됩니다.
<img src="/assets/2024/kans-3th/w7/20241019_kans_w7_23.png" alt="img.png" /></li>
    </ul>
  </li>
  <li>오른쪽 상단의 Sign in 클릭 후 jason으로 로그인 후 새로고침을 해보겠습니다.
    <ul>
      <li>로그인 후에는 v2로 접속 됩니다.
<img src="/assets/2024/kans-3th/w7/20241019_kans_w7_24.png" alt="img_1.png" /></li>
      <li>
        <p>헤더에는 end-user:jason 이 추가되어 있습니다.</p>

        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 로그인 후 헤더헤더</span>
<span class="c"># =&gt; [productpage-v1-d5789fdfb-f8gdf productpage] send: b'GET /reviews/0 HTTP/1.1</span>
<span class="c">#    Host: reviews:9080</span>
<span class="c">#    user-agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Safari/605.1.15</span>
<span class="c">#    Accept-Encoding: gzip, deflate</span>
<span class="c">#    Accept: */*</span>
<span class="c">#    Connection: keep-alive</span>
<span class="c">#    &lt;span style="color: red;"&gt;end-user: jason&lt;/span&gt;</span>
<span class="c">#    x-request-id: 03366677-7032-9291-a4b9-7009a6257394</span>
<span class="c">#    cookie: session=eyJ1c2VyIjoiamFzb24ifQ.ZxPUxA.3MJkXTFH8zJtg_YlXlvzq8xArpc'</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
</ul>

<h5 id="fault-injection-실습">Fault Injection 실습</h5>

<ul>
  <li><strong>virtual-service-ratings-test-delay.yaml</strong> : end-user 가 jason 는 ratings v1 에 7초 지연 발생, 그외 사용자는 ratings v1 정상 연결</li>
</ul>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># virtual-service-ratings-test-delay.yaml</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">networking.istio.io/v1alpha3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualService</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">ratings</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">hosts</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">ratings</span>
  <span class="na">http</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">match</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">headers</span><span class="pi">:</span>
            <span class="na">end-user</span><span class="pi">:</span>
              <span class="na">exact</span><span class="pi">:</span> <span class="s">jason</span>
      <span class="na">fault</span><span class="pi">:</span>
        <span class="na">delay</span><span class="pi">:</span>
          <span class="na">percentage</span><span class="pi">:</span>
            <span class="na">value</span><span class="pi">:</span> <span class="m">100.0</span>
          <span class="na">fixedDelay</span><span class="pi">:</span> <span class="s">7s</span>
      <span class="na">route</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">destination</span><span class="pi">:</span>
            <span class="na">host</span><span class="pi">:</span> <span class="s">ratings</span>
            <span class="na">subset</span><span class="pi">:</span> <span class="s">v1</span>
    <span class="pi">-</span> <span class="na">route</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">destination</span><span class="pi">:</span>
            <span class="na">host</span><span class="pi">:</span> <span class="s">ratings</span>
            <span class="na">subset</span><span class="pi">:</span> <span class="s">v1</span>
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># virtualservices 적용</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> virtual-service-ratings-test-delay.yaml
<span class="c"># =&gt; virtualservice.networking.istio.io/ratings configured</span>

<span class="c"># 로그 확인 : product 입장에서 접속 사용자(clinet) 연결을 끊어버림 0 DC downstream_remote_disconnect</span>
<span class="nv">$ </span>kubetail <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>productpage <span class="nt">-f</span>
</code></pre></div></div>

<ul>
  <li>웹브라우저에서 jason으로 로그인된 상태에서 접속시 6~7초 지연이 발생하는것을 확인할 수 있습니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_25.png" alt="img.png" /></p>

<ul>
  <li><strong>virtual-service-ratings-test-abort.yaml</strong> : end-user 가 jason 는 ratings v1 에 500 에러 리턴, 그외 사용자는 ratings v1 정상 연결</li>
</ul>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># virtual-service-ratings-test-abort.yaml</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">networking.istio.io/v1alpha3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualService</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">ratings</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">hosts</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">ratings</span>
  <span class="na">http</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">match</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">headers</span><span class="pi">:</span>
            <span class="na">end-user</span><span class="pi">:</span>
              <span class="na">exact</span><span class="pi">:</span> <span class="s">jason</span>
      <span class="na">fault</span><span class="pi">:</span>
        <span class="na">abort</span><span class="pi">:</span>
          <span class="na">percentage</span><span class="pi">:</span>
            <span class="na">value</span><span class="pi">:</span> <span class="m">100.0</span>
          <span class="na">httpStatus</span><span class="pi">:</span> <span class="m">500</span>
      <span class="na">route</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">destination</span><span class="pi">:</span>
            <span class="na">host</span><span class="pi">:</span> <span class="s">ratings</span>
            <span class="na">subset</span><span class="pi">:</span> <span class="s">v1</span>
    <span class="pi">-</span> <span class="na">route</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">destination</span><span class="pi">:</span>
            <span class="na">host</span><span class="pi">:</span> <span class="s">ratings</span>
            <span class="na">subset</span><span class="pi">:</span> <span class="s">v1</span>
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># virtualservices 적용</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> virtual-service-ratings-test-abort.yaml
<span class="c"># =&gt; virtualservice.networking.istio.io/ratings configured</span>

<span class="c"># 로그 확인</span>
<span class="nv">$ </span>kubetail <span class="nt">-l</span> <span class="nv">version</span><span class="o">=</span>v2 <span class="nt">-f</span>
</code></pre></div></div>

<p>jason으로 로그인 했을때 Rating 서비스에 500 에러가 발생하는것을 확인할 수 있습니다.</p>

<p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_26.png" alt="img.png" /></p>

<p>또한 kiali에서도 어느 구간에서 오류가 발생했는지 확인할 수 있으며, Flags도 확인할 수 있습니다. <a href="https://www.envoyproxy.io/docs/envoy/latest/configuration/observability/access_log/usage#config-access-log-format-response-flags">링크</a><br />
(이경우 FI는 Fault Injection을 의미으로 일부러 오류를 일으킨 것을 확인 할 수 있습니다.)</p>

<p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_27.png" alt="img.png" /></p>

<h5 id="traffic-shifting-실습">Traffic Shifting 실습</h5>

<ul>
  <li>
    <p>카나라 배포 전략 등 활용 - <a href="https://istio.io/latest/docs/tasks/traffic-management/traffic-shifting/">링크</a></p>
  </li>
  <li>
    <p><strong>virtual-service-reviews-50-v3.yaml</strong> : 가중치 (weight-based routing), reviews 에 v1(50%), v3(50%)</p>
  </li>
</ul>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">networking.istio.io/v1alpha3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualService</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">reviews</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">hosts</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">reviews</span>
  <span class="na">http</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">route</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">destination</span><span class="pi">:</span>
        <span class="na">host</span><span class="pi">:</span> <span class="s">reviews</span>
        <span class="na">subset</span><span class="pi">:</span> <span class="s">v1</span>
      <span class="na">weight</span><span class="pi">:</span> <span class="m">50</span>
    <span class="pi">-</span> <span class="na">destination</span><span class="pi">:</span>
        <span class="na">host</span><span class="pi">:</span> <span class="s">reviews</span>
        <span class="na">subset</span><span class="pi">:</span> <span class="s">v3</span>
      <span class="na">weight</span><span class="pi">:</span> <span class="m">50</span>
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># virtualservices 적용</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> virtual-service-reviews-50-v3.yaml
<span class="c"># =&gt; virtualservice.networking.istio.io/reviews configured</span>

<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span>  <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$MYDOMAIN</span>:<span class="nv">$IGWHTTP</span>/productpage | <span class="nb">grep</span> <span class="nt">-m</span> 1 <span class="s2">"reviews-v</span><span class="se">\?</span><span class="s2">"</span> <span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span>
<span class="c"># =&gt;      53                   reviews-v1-6584ddcf65-gnc9j</span>
<span class="c">#         47                   reviews-v3-6f5b775685-cw7tc</span>
</code></pre></div></div>

<p>대략 50%의 확률로 v1과 v3로 접속되는것을 확인할 수 있습니다.</p>

<ul>
  <li><strong>virtual-service-reviews-80-20.yaml</strong> : 가중치 (weight-based routing), reviews 에 v1(80%), v2(20%)</li>
</ul>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">networking.istio.io/v1alpha3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualService</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">reviews</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">hosts</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">reviews</span>
  <span class="na">http</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">route</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">destination</span><span class="pi">:</span>
        <span class="na">host</span><span class="pi">:</span> <span class="s">reviews</span>
        <span class="na">subset</span><span class="pi">:</span> <span class="s">v1</span>
      <span class="na">weight</span><span class="pi">:</span> <span class="m">80</span>
    <span class="pi">-</span> <span class="na">destination</span><span class="pi">:</span>
        <span class="na">host</span><span class="pi">:</span> <span class="s">reviews</span>
        <span class="na">subset</span><span class="pi">:</span> <span class="s">v2</span>
      <span class="na">weight</span><span class="pi">:</span> <span class="m">20</span>
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># virtualservices 적용</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> virtual-service-reviews-80-20.yaml
<span class="c"># =&gt; virtualservice.networking.istio.io/reviews configured</span>

<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span>  <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$MYDOMAIN</span>:<span class="nv">$IGWHTTP</span>/productpage | <span class="nb">grep</span> <span class="nt">-m</span> 1 <span class="s2">"reviews-v</span><span class="se">\?</span><span class="s2">"</span> <span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span>
<span class="c"># =&gt;      79                   reviews-v1-6584ddcf65-gnc9j</span>
<span class="c">#         21                   reviews-v2-6f85cb9b7c-cfc68</span>
</code></pre></div></div>

<p>대략 80%의 확률로 v1과 20%의 확률로 v2로 접속되는것을 확인할 수 있습니다.</p>

<p>kiali에서도 어느 구간에서 어떠한 비중으로 트래픽이 흘러가는지 확인할 수 있습니다.
<img src="/assets/2024/kans-3th/w7/20241019_kans_w7_28.png" alt="img.png" /></p>

<h4 id="security-보안">Security (보안)</h4>

<ul>
  <li>요구사항
    <ul>
      <li>MITM (Man-In-The-Middle) 공격 방지를 위해 모든 트래픽은 mTLS로 암호화 되어야 합니다.</li>
      <li>또한 접근 제어 정책이 필요하며, 감사 로깅을 통해 보안 이슈를 식별이 가능 해야 합니다.</li>
    </ul>
  </li>
  <li>목표
    <ul>
      <li>기본 셋팅을 안전하게 하기 : 별도의 셋팅이 없어도 보안을 유지할 수 있도록 설정</li>
      <li>깊은 방어 : 기존에 존재하는 보안 시스템과 통합되어, 다층 방어를 구성</li>
      <li><strong>Zero-trust network</strong> : 네트워크를 신뢰하지 않음으로써 보안 강화 <a href="https://genians.co.kr/genians-nac/zt/">https://genians.co.kr/genians-nac/zt/</a></li>
    </ul>
  </li>
  <li>구성요소
    <ul>
      <li>Certification Authority (CA) : 인증서 발급, 관리, 갱신</li>
      <li>보안 정책 (인증정책, 인가정책 등) 관련 설정을 각 프록시에 전달하는 API 서버</li>
      <li>사이드카와 프록시를 정책 강제 지점(Policy Enforcement Point-PEPs)으로 사용하여 클라이언트와 서버간의 통신을 보호</li>
      <li>Envoy Proxy 확장 기능을 통해 telemetry와 감사 로깅 수집</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_29.svg" alt="20241019_kans_w7_29.svg" class="image-center" />
<em class="image-caption">istio 보안 아키텍쳐</em></p>

<ul>
  <li>TLS와 mTLS
    <ul>
      <li><strong>TLS (Transport Layer Security)</strong> : 통신 보안을 위한 프로토콜로, 인터넷 상에서 데이터를 암호화하는 표준화된 방법입니다.
기본적으로 서버의 인증서만 확인하는 방식을 사용합니다.</li>
      <li><strong>mTLS (mutual TLS)</strong> : 서버와 클라이언트가 서로 인증을 하고 서로 신뢰할 수 있는지 확인하는 방식입니다.</li>
    </ul>
  </li>
  <li>Authentication (인증), Authorization (인가) (Auto mTLS)
    <ul>
      <li>Istio는 모든 워크로드에 X.509 인증서를 부여하고, 서로 인증을 통해 통신을 보호합니다.</li>
      <li>Envoy proxy와 함께 실행되는 Istio agent는 istiod와 함께 동작하면서 자동으로 인증서를 갱신합니다. 
 <img src="/assets/2024/kans-3th/w7/20241019_kans_w7_30.svg" alt="20241019_kans_w7_30.svg" class="image-center" />
        <ol>
          <li>istiod는 CSR(인증서 서명 요청)을 수행하기 위해 gRPC 서비스를 제공합니다.</li>
          <li>Envoy는 SDS(Secret Discovery Serice) API를 통해 인증서와 키 요청을 보냅니다.</li>
          <li>istio-agent는 SDs 요청을 받으면 Private Key와 CSR을 생성한 후 자격증명
   (credential)과 함께 CSR istiod에 전송하여 서명을 요청합니다.</li>
          <li>CA는 CSR에 포함된 자격증명(credential) 의 유효성을 검사하고 CSR에 서명하여 인증서를 생성합니다.</li>
          <li>istio-agent는 istiod로부터 받은 인증서(certiftcate)와 개인키 private Key)를 Envoy SDS
   API를 통해 Envoy에게 보냅니다.</li>
          <li>위의 CSR 프로세스는 인증서 및 키 순환을 위해 주기적으로 반복됩니다</li>
        </ol>
      </li>
    </ul>
  </li>
  <li>Authentication (인증) : 2가지 타입 제공
<img src="/assets/2024/kans-3th/w7/20241019_kans_w7_31.svg" alt="20241019_kans_w7_31.svg" />
    <ol>
      <li>Peer Authentication :
        <ul>
          <li>서비스간 인증에 사용되며 Client가 연결을 확인하는데 사용. 서비스 코드 변경없이 mTLS 제공</li>
        </ul>
      </li>
      <li>Request Authentication :
        <ul>
          <li>Request에 첨부된 자격증명(Credential)을 통해 최종 사용자 인증에 사용</li>
          <li>istio는 JWT (JSON Web Token)을 지원하여 최종 사용자 인증을 제공</li>
          <li>커스텀 인증 제공자를 비롯하여 OpenID Connect, Keycloak, Auth0 등 다양한 인증 방식을 지원</li>
        </ul>
      </li>
    </ol>
  </li>
  <li>Authorization (인가)
<img src="/assets/2024/kans-3th/w7/20241019_kans_w7_32.svg" alt="20241019_kans_w7_32.svg" />
    <ul>
      <li>istio는 mesh 단위, 네임스페이스 단위, 워크로드 단위로 접근을 제어 할 수 있는 인가 기능을 제공합니다. 다음과 같은 이점이 있습니다.
        <ul>
          <li>워크로드와 워크로드간, 또는 사용자와 워크로드간 인가 제공</li>
          <li>단일한 AuthorizationPolicy CRD를 사용한 단순한 API 제공</li>
          <li>유연한 정책 제공 : 커스텀 조건을 등록할 수 있고, CUSTOM, DENY, ALLOW 액션 지원</li>
          <li>고성능 : Envoy Proxy를 통해 인가 정책을 적용하므로 성능 저하가 없음</li>
          <li>높은 호환성 : gRPC, HTTP, HTTPS 등을 지원하며, 일반적인 TCP 프로토콜도 지원</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h5 id="authentication-auto-mtls-실습">Authentication (Auto mTLS) 실습</h5>

<ul>
  <li>
    <p>기존 파드에 로그에서 인증서 등 보안 관련 내용 확인</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c"># CA Endpoint, CA(/var/run/secret/istio/root-cert,pem), citadelclient, SDS server 등등</span>
  <span class="nv">$ </span>kubectl logs ratings-v1-7c9bd4b87f-s9gxs <span class="nt">-c</span> istio-proxy <span class="nt">-f</span>
  <span class="nv">$ </span>kubetail
  
  <span class="c"># 인증정책 확인</span>
  <span class="nv">$ </span>kubectl get peerauthentications.security.istio.io
  <span class="c"># =&gt; No resources found </span>
    
  <span class="c"># envoy 에 cert 정보 확인 : istio-proxy 에 admin페이지 접속 or kaila 에서 envoy 에서 확인    </span>
</code></pre></div>    </div>
  </li>
  <li>bookinfo → kiali → product 계속 접속</li>
  <li>kiali 에서 Display(Security 체크) 후 자물쇠 클릭하면 오른쪽 창에서 보안설정을 확이할 수 있습니다. : mTLS Enabled, spiffe(Secure name)</li>
</ul>

<p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_33.png" alt="img.png" /></p>

<ul>
  <li>test 네임스페이스 생성 후 파드 생성(sidecar 미적용) 후 ratings 접속</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 네임스페이스 생성</span>
<span class="nv">$ </span>kubectl create ns <span class="nb">test</span>
<span class="c"># =&gt; namespace/test created</span>

<span class="c"># 파드 생성</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl create -f -
apiVersion: v1
kind: Pod
metadata:
  name: netpod
  namespace: test
spec:
  containers:
  - name: netshoot-pod
    image: nicolaka/netshoot
    command: ["tail"]
    args: ["-f", "/dev/null"]
  terminationGracePeriodSeconds: 0
</span><span class="no">EOF

</span><span class="c"># 확인 : sidecar 미적용</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-n</span> <span class="nb">test</span>
<span class="c"># =&gt; NAME     READY   STATUS    RESTARTS   AGE</span>
<span class="c">#    netpod   1/1     Running   0          19s</span>

<span class="c"># ratings 서비스 확인</span>
<span class="nv">$ </span>kubectl get svc ratings
<span class="c"># =&gt; NAME      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE</span>
<span class="c">#    ratings   ClusterIP   10.10.200.163   &amp;lt;none&amp;gt;        9080/TCP   8h</span>

<span class="c"># ratings 접속 시도 : 성공</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> <span class="nt">-n</span> <span class="nb">test </span>netpod <span class="nt">--</span> curl ratings.default:9080/health <span class="p">;</span><span class="nb">echo</span>
<span class="nv">$ </span><span class="o">{</span><span class="s2">"status"</span>:<span class="s2">"Ratings is healthy"</span><span class="o">}</span>

<span class="c"># 로그 확인</span>
<span class="nv">$ </span>kubetail <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>ratings <span class="nt">-f</span>
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_34.png" alt="img.png" class="image-center w-80" />
<em class="image-caption">NS(default, test 체크) netpod 에서 접속 시 unknown 으로 표기되며, 접근 성공(녹색) 확인</em></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Peer authentication 설정 변경 : PERMISSIVE(mTLS 사용/미사용 모두 허용) → STRICT(반드시 mTLS 사용, 미사용 시 거부)</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl create -f -
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: default-strict
spec:
  mtls:
    mode: STRICT
</span><span class="no">EOF

</span><span class="c"># ratings 접속 시도 : 실패!</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> <span class="nt">-n</span> <span class="nb">test </span>netpod <span class="nt">--</span> curl ratings.default:9080/health <span class="p">;</span><span class="nb">echo</span>
<span class="c"># =&gt; curl: (56) Recv failure: Connection reset by peer</span>
<span class="c">#    command terminated with exit code 56</span>

<span class="nv">$ </span>kubetail <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>ratings <span class="nt">-f</span>
<span class="c"># =&gt; [ratings-v1-7c9bd4b87f-s9gxs istio-proxy] [2024-10-01T17:21:12.708Z] &amp;quot;- - -&amp;quot; 0 NR filter_chain_not_found - &amp;quot;-&amp;quot; 0 0 0 - &amp;quot;-&amp;quot; &amp;quot;-&amp;quot; &amp;quot;-&amp;quot; &amp;quot;-&amp;quot; &amp;quot;-&amp;quot; - - 172.16.3.17:9080 172.16.1.17:34938 - -</span>
</code></pre></div></div>

<ul>
  <li>실습 자원 삭제</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl delete PeerAuthentication default-strict
<span class="c"># =&gt; peerauthentication.security.istio.io &amp;quot;default-strict&amp;quot; deleted</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> <span class="nt">-n</span> <span class="nb">test </span>netpod <span class="nt">--</span> curl ratings.default:9080/health <span class="p">;</span><span class="nb">echo</span>
<span class="c"># =&gt; {&amp;quot;status&amp;quot;:&amp;quot;Ratings is healthy&amp;quot;}</span>
<span class="c"># &lt;span style="color: green;"&gt;👉 인증정책 강제 정책 삭제시 다시 통신이 됩니다.&lt;/span&gt;</span>

<span class="nv">$ </span>kubectl delete pod <span class="nt">-n</span> <span class="nb">test </span>netpod
<span class="nv">$ </span>kubectl delete ns <span class="nb">test</span>
</code></pre></div></div>

<h3 id="istio-통신-흐름">Istio 통신 흐름</h3>

<p>istio 사용시 트래픽은 호스트의 tcp/ip 스택과 iptables, 파드내의 iptables와 envoy를 경유하게 됩니다.
istio는 강력하고 다양한 기능들을 제공하지만 비용(지연추가, 프로세서 사용량 추가, 복잡한 구조)이 필요합니다.</p>

<p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_35.png" alt="img.png" class="image-center w-80" /></p>

<p>외부 클라이언트(PC 등)에서 파드로 접속되는 과정은 다음과 같습니다.</p>

<p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_36.png" alt="img_1.png" /></p>

<p>위의 그림에서 처럼 iptables를 여러번 거치고, DNAT등을 통해 포트번호등이 80 (http) =&gt; 15006 (istio-proxy) 로 변경되는 등의 작업을 여러번 거칩니다.
또한 파드와 호스트간 통신 envoy를 요청을 받을때와 응답할때 모두 거쳐가는 것을 확인할 수 있습니다.</p>

<ul>
  <li>파드 내 Iptables 적용 흐름</li>
</ul>

<p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_37.png" alt="img.png" class="image-center" />
<em class="image-caption">출처 : <a href="https://jimmysong.io/en/blog/sidecar-injection-iptables-and-traffic-routing/#understand-outbound-handler">Jimmy song</a> 블로그</em></p>

<p>다음 블로그에서 자세한 내용을 확인할 수 있습니다.
<a href="https://jimmysong.io/en/blog/sidecar-injection-iptables-and-traffic-routing/#understand-outbound-handler">Understanding the Sidecar Injection, Traffic Intercepting &amp; Routing Process in Istio</a></p>

<hr />

<h2 id="마치며">마치며</h2>

<p>1주차 컨테이너 격리 이후에 또 다시 뇌정지가 찾아온 주차였습니다.
이번주에 학습한것도 많은데, 이것이 일부만 살펴본것이라니 놀랍습니다.
정말 만든 분들도, 쓰는 분들도, 스터디를 진행해주시는 분들도 대단합니다. :thumbsup:</p>

<p>인증이나 인가 등 gateway api에서 아쉬웠던 부분들이 나와서 좋았습니다.
찾던 기능인데 마침 이번주에 다루게 되어서 좋았습니다. 
Kiali를 통한 트래픽 시각화도 정말 유용하게 쓰일 것 같습니다.
복잡하긴 하지만 좋은 기능들이 많았습니다.</p>

<p>시간이 모자라서 미처 실습하지 못한 부분들과 Ambient Mesh도 바쁜일이 지나가면 다시 살펴봐야겠습니다.
스터디를 준비해주신 가시다님과 참여하신 분들 감사합니다. :pray:</p>]]></content><author><name></name></author><category term="kans" /><category term="kubernetes," /><category term="network," /><category term="linux" /><summary type="html"><![CDATA[이번주에는 Service Mesh와 대표적인 오픈소스 프로젝트인 Istio에 대해 알아 보겠습니다.]]></summary></entry><entry><title type="html">[KANS 3기] Ingress &amp;amp; Gateway API</title><link href="https://sweetlittlebird.github.io/posts/2024-10-13-KANS-Study-Week6/" rel="alternate" type="text/html" title="[KANS 3기] Ingress &amp;amp; Gateway API" /><published>2024-10-13T01:00:18+09:00</published><updated>2024-10-13T01:00:18+09:00</updated><id>https://sweetlittlebird.github.io/posts/KANS%20Study%20-%20Week6</id><content type="html" xml:base="https://sweetlittlebird.github.io/posts/2024-10-13-KANS-Study-Week6/"><![CDATA[<h2 id="들어가며">들어가며</h2>

<p>이번주에는 ingress와 gateway api 에대해 알아 보겠습니다.
KANS 3기 6주차 스터디를 시작하겠습니다.</p>

<hr />

<h2 id="ingress">Ingress</h2>

<h3 id="ingress란">Ingress란?</h3>

<ul>
  <li>Ingress는 클러스터 외부에서 클러스터 내부로 HTTP 및 HTTPS 트래픽을 라우팅하는 Web Proxy 역할을 수행합니다.</li>
  <li>지난주에 스터디했던 LoadBalancer와 비슷한 역할을 수행하지만, LoadBalancer는 Layer 4에서 동작하는 반면 Ingress는 Layer 7에서 동작한다는 차이가 있습니다.</li>
  <li>Ingress는 HTTP와 HTTPS를 이해하기 때문에 호스트명, 경로 등에 따라 트래픽을 라우팅할 수도 있고, SSL Offloading 등의 기능도 제공합니다.</li>
  <li>이렇게 다양한 기능이 있지만 <strong>Ingress는 동결처리</strong> 되었으며, <strong>신규 기능들은 Gateway API라는 다른 API에 추가되고 있고</strong>, 향후에는 Ingress 대신 Gateway API를 사용하는 것이 권장될것으로 보입니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w6/20241012_kans_w6_1.png" alt="img.png" class="image-center" /></p>

<ul>
  <li>특이한 점은 Ingress 를 통한 트래픽은 서비스를 통하지 않고, 서비스를 통해서 파드의 IP를 확인하고, 위의 그림과 같이 서비스를 거치지 않고 파드와 직접 통신합니다.</li>
</ul>

<h3 id="ingress-controller의-종류">Ingress Controller의 종류</h3>

<ul>
  <li>Ingress는 Kubernetes에 내장된 기능이 아니어서 별도의 Ingress Controller를 설치해야만 사용할 수 있습니다. 
많이 사용되는 Ingress Controller는 다음과 같습니다.</li>
</ul>

<table>
  <thead>
    <tr>
      <th>구분</th>
      <th>특징</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Pomerium</td>
      <td>보안에 특화 된 Ingress로 Identity-Aware 접근이 가능하며 Zero Trust 모델을 지원합니다.</td>
    </tr>
    <tr>
      <td>NGINX Ingress Controller</td>
      <td>신뢰할 수 있는 안정적으로, 라우팅이 유연하고, Lua 스크립트 등으로 기능확장이 가능합니다.</td>
    </tr>
    <tr>
      <td>Traefik</td>
      <td>Auto-discovery를 제공하고, 실시간으로 업데이트 되며, 관리 대시보드를 제공합니다. 동적으로 운영하기 좋습니다.</td>
    </tr>
    <tr>
      <td>HAProxy Ingress</td>
      <td>고성능이며, 다양한 고급 기능을 제공합니다.</td>
    </tr>
    <tr>
      <td>Envoy</td>
      <td>확장성이 있으며 재시도, 서킷 브레이커, 레이트 제한 등 다양한 기능을 제공합니다.</td>
    </tr>
    <tr>
      <td>Istio Ingress Gateway</td>
      <td>트래픽 관리에 강점이 있으며, Istio 서비스 메시와 연동하기 좋습니다.</td>
    </tr>
    <tr>
      <td>Contour</td>
      <td>HTTP/2와 gRPC를 지원하는 경량의 고성능을 제공합니다.</td>
    </tr>
    <tr>
      <td>Kong Ingress Controller</td>
      <td>Kong은 API Gateway로 널리 알려져있지만 Ingress Controller 기능도 제공합니다. NGINX Ingress Controller의 기능에 추가적인 기능을 제공하지만 학습 곡선이 높은 편입니다.</td>
    </tr>
  </tbody>
</table>

<p>이외에도 다양한 Ingress Controller가 존재하며 다음의 링크에서 확인 할 수 있습니다. <a href="https://docs.google.com/spreadsheets/d/191WWNpjJ2za6-nbG4ZoUMXMpUK8KlCIosvQB0f-oq3k/">Kubernetes Ingress Controllers 비교</a></p>

<h3 id="실습-환경-준비">실습 환경 준비</h3>

<ul>
  <li>이번 실습에는 k3s라는 경량 Kubernetes 클러스터를 사용하겠습니다. k3s는 Rancher에서 개발한 경량 Kubernetes 클러스터로, 쉽게 설치가 가능하고, 
전체가 100MB보다 적을 정도로 적은 자원으로도 Kubernetes를 사용할 수 있습니다.</li>
  <li>하지만 K8S와는 기능 차이가 있기 때문에, 이러한 부분을 감안하고 사용하시면 됩니다.</li>
</ul>

<h4 id="k3s-특징">k3s 특징</h4>

<ul>
  <li>k3s의 특징은 다음과 같습니다
    <ul>
      <li>단일 바이너리 또는 최소 컨테이너 이미지로 배포됩니다.</li>
      <li>기본 저장소 백엔드로 sqlite3를 기반으로 한 경량 데이터 저장소가 사용됩니다. etcd, MySQL 및 Postgres도 사용할 수 있습니다.</li>
      <li>TLS 및 옵션의 복잡성을 처리하는 런처에 포함되어 있습니다.</li>
      <li>경량 환경에 적합한 합리적인 기본값으로 보안에 신경을 썼습니다.</li>
      <li>모든 Kubernetes 컨트롤 플레인 구성 요소의 작동이 단일 바이너리 및 프로세스에 캡슐화되어 있고, k3s가 인증서 배포와 같은 복잡한 클러스터 작업을 자동화합니다.</li>
      <li>외부 종속성이 최소화되었습니다. 필요한 것은 최신 커널과 cgroup 마운트뿐입니다.</li>
      <li>손쉬운 클러스터 생성을 위해 필요한 패키지를 기본 제공합니다:
        <ul>
          <li>containerd / cri-dockerd 컨테이너 런타임 (CRI)</li>
          <li>Flannel 컨테이너 네트워크 인터페이스 (CNI)</li>
          <li>CoreDNS 클러스터 DNS</li>
          <li>Traefik Ingress 컨트롤러</li>
          <li>ServiceLB 로드 밸런서 컨트롤러</li>
          <li>Kube-router 네트워크 정책 컨트롤러</li>
          <li>Local-path-provisioner 영구 볼륨 컨트롤러</li>
          <li>Spegel 분산 컨테이너 이미지 레지스트리 미러</li>
          <li>호스트 유틸리티 (iptables, socat 등)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h4 id="k3s의-아키텍쳐">k3s의 아키텍쳐</h4>

<ul>
  <li>k3s는 서버 (Control Plane)와 에이전트 (Worker Node)로 구성되어 있습니다.
    <ul>
      <li>서버 노드는 Kubernetes의 <code class="language-plaintext highlighter-rouge">k3s server</code> 명령으로 실행되며 모든 컨트롤 플레인 구성 요소와 데이터 저장 컴포넌트를 실행하며 k3s가 관리합니다.</li>
      <li>에이전트 노드는 <code class="language-plaintext highlighter-rouge">k3s agent</code> 명령으로 실행되며 컨트롤 플레인 요소등 없이 워커 노드로 동작합니다.</li>
      <li>모든 서버와 에이전트는 kublet, 컨테이너 런타임, CNI 등을 포함한 모든 Kubernetes 구성 요소를 실행합니다.</li>
      <li>더 자세한 내용은 다음 링크를 참고하세요. <a href="https://docs.k3s.io/advanced#running-agentless-servers-experimental">링크</a>
<img src="/assets/2024/kans-3th/w6/20241012_kans_w6_2.svg" alt="20241012_kans_w6_2.svg" /></li>
    </ul>
  </li>
  <li>단일 서버 구성 : 1대 K3S 서버(경량 DB = SQLite), 필요한 만큼의 K3S Agents (Worker Node) 구성
<img src="/assets/2024/kans-3th/w6/20241012_kans_w6_3.png" alt="img.png" /></li>
  <li>고가용성 구성 : Embedded DB (etcd 등), 외부 DB (MySQL, PostgreSQL 등) 사용 가능
<img src="/assets/2024/kans-3th/w6/20241012_kans_w6_4.png" alt="img_1.png" /></li>
</ul>

<h4 id="k3s-설치">k3s 설치</h4>

<ul>
  <li>k3s는 기본적으로 <code class="language-plaintext highlighter-rouge">traefik</code>을 Ingress Controller로 사용하는데 이번 실습에서는 nginx ingress controller를 사용할 것이기 때문에 <code class="language-plaintext highlighter-rouge">traefik</code>을 설치하지 않겠습니다.</li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">INSTALL_K3S_EXEC=" --disable=traefik"</code> 옵션을 사용하여 traefik을 설치하지 않을 수 있습니다.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Install k3s-server</span>
<span class="nv">$ </span>curl <span class="nt">-sfL</span> https://get.k3s.io | <span class="nv">INSTALL_K3S_EXEC</span><span class="o">=</span><span class="s2">" --disable=traefik"</span>  sh <span class="nt">-s</span> - server <span class="nt">--token</span> <span class="o">[[</span>인증토큰]] <span class="nt">--cluster-cidr</span> <span class="s2">"172.16.0.0/16"</span> <span class="nt">--service-cidr</span> <span class="s2">"10.10.200.0/24"</span> <span class="nt">--write-kubeconfig-mode</span> 644 
  
<span class="c"># Install k3s-agent</span>
<span class="nv">$ </span>curl <span class="nt">-sfL</span> https://get.k3s.io | <span class="nv">K3S_URL</span><span class="o">=</span>https://192.168.10.10:6443 <span class="nv">K3S_TOKEN</span><span class="o">=[[</span>인증토큰]]  sh <span class="nt">-s</span> -
</code></pre></div>    </div>
  </li>
  <li>k3s 설치 후, k3의 설정을 확인해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 노드 확인</span>
<span class="nv">$ </span>kubectl get node <span class="nt">-owide</span>
<span class="c"># =&gt; NAME     STATUS   ROLES                  AGE     VERSION        INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION       CONTAINER-RUNTIME</span>
<span class="c">#    k3s-m    Ready    control-plane,master   30m     v1.30.5+k3s1   10.0.2.15     &amp;lt;none&amp;gt;        Ubuntu 22.04.5 LTS   5.15.0-119-generic   containerd://1.7.21-k3s2</span>
<span class="c">#    k3s-w1   Ready    &amp;lt;none&amp;gt;                 4m24s   v1.30.5+k3s1   10.0.2.15     &amp;lt;none&amp;gt;        Ubuntu 22.04.5 LTS   5.15.0-119-generic   containerd://1.7.21-k3s2</span>
<span class="c">#    k3s-w2   Ready    &amp;lt;none&amp;gt;                 26m     v1.30.5+k3s1   10.0.2.15     &amp;lt;none&amp;gt;        Ubuntu 22.04.5 LTS   5.15.0-119-generic   containerd://1.7.21-k3s2</span>
<span class="c">#    k3s-w3   Ready    &amp;lt;none&amp;gt;                 24m     v1.30.5+k3s1   10.0.2.15     &amp;lt;none&amp;gt;        Ubuntu 22.04.5 LTS   5.15.0-119-generic   containerd://1.7.21-k3s2</span>

<span class="nv">$ </span>kubectl describe node k3s-m | <span class="nb">grep </span>Taint  <span class="c"># Taints 없음</span>
<span class="c"># =&gt; Taints:             &amp;lt;none&amp;gt;</span>
<span class="nv">$ </span>kubectl describe node k3s-w1 | <span class="nb">grep </span>Taint  <span class="c"># Taints 없음</span>
<span class="c"># =&gt; Taints:             &amp;lt;none&amp;gt;</span>

<span class="c"># 파드 확인</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-n</span> kube-system
<span class="c"># =&gt; NAME                                      READY   STATUS    RESTARTS   AGE</span>
<span class="c">#    coredns-7b98449c4-8l64d                   1/1     Running   0          31m</span>
<span class="c">#    local-path-provisioner-6795b5f9d8-b5gt6   1/1     Running   0          31m</span>
<span class="c">#    metrics-server-cdcc87586-d87gv            1/1     Running   0          31m</span>

<span class="c">#</span>
<span class="nv">$ </span>kubectl top node
<span class="c"># =&gt; NAME     CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%</span>
<span class="c">#    k3s-m    147m         3%     1128Mi          28%</span>
<span class="c">#    k3s-w1   147m         3%     1128Mi          57%</span>
<span class="c">#    k3s-w2   147m         3%     1128Mi          57%</span>
<span class="c">#    k3s-w3   147m         3%     1128Mi          57%</span>
<span class="nv">$ </span>kubectl top pod <span class="nt">-A</span> <span class="nt">--sort-by</span><span class="o">=</span><span class="s1">'cpu'</span>
<span class="c"># =&gt; NAMESPACE     NAME                                      CPU(cores)   MEMORY(bytes)</span>
<span class="c">#    kube-system   metrics-server-cdcc87586-d87gv            15m          19Mi</span>
<span class="c">#    kube-system   coredns-7b98449c4-8l64d                   4m           13Mi</span>
<span class="c">#    kube-system   local-path-provisioner-6795b5f9d8-b5gt6   1m           6Mi</span>
<span class="nv">$ </span>kubectl top pod <span class="nt">-A</span> <span class="nt">--sort-by</span><span class="o">=</span><span class="s1">'memory'</span>
<span class="nv">$ </span>kubectl get storageclass
<span class="c"># =&gt; NAME                   PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE</span>
<span class="c">#    local-path (default)   rancher.io/local-path   Delete          WaitForFirstConsumer   false                  32m</span>

<span class="c"># config 정보(위치) 확인</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-v</span><span class="o">=</span>6
<span class="c"># =&gt; I1012 05:55:56.507623    6817 loader.go:395] Config loaded from file:  /etc/rancher/k3s/k3s.yaml</span>
<span class="c">#    I1012 05:55:56.518338    6817 round_trippers.go:553] GET https://127.0.0.1:6443/api/v1/namespaces/default/pods?limit=500 200 OK in 5 milliseconds</span>
<span class="c">#    No resources found in default namespace.</span>

<span class="nv">$ </span><span class="nb">cat</span> /etc/rancher/k3s/k3s.yaml
<span class="c"># =&gt; apiVersion: v1</span>
<span class="c">#    clusters:</span>
<span class="c">#    - cluster:</span>
<span class="c">#        certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJlRENDQVIyZ0F3SUJBZ0lCQURBS0JnZ3Foa2pPUFFRREFqQWpNU0V3SHdZRFZRUUREQmhyTTNNdGMyVnkKZG1WeUxXTmhRREUzTWpnM01UQTJNREF3SGhjTk1qUXhNREV5TURVeU16SXdXaGNOTXpReE1ERXdNRFV5TXpJdwpXakFqTVNFd0h3WURWUVFEREJock0zTXRjMlZ5ZG1WeUxXTmhRREUzTWpnM01UQTJNREF3V1RBVEJnY3Foa2pPClBRSUJCZ2dxaGtqT1BRTUJCd05DQUFReEdLOFFEcHMvNmNHdE45RWRCYmZJRmg2UjBpQlFLYUhHYWhVQXVMdjUKWHhpd1JjTVdia1FZNmxBdWM1RC9zWWYrTmhZYUFjcmNzMk01LzAyTkQ5bERvMEl3UURBT0JnTlZIUThCQWY4RQpCQU1DQXFRd0R3WURWUjBUQVFIL0JBVXdBd0VCL3pBZEJnTlZIUTRFRmdRVXJzL1ZVODFCZEJnS3N2YmJDRmhjCkJ5aStxUTB3Q2dZSUtvWkl6ajBFQXdJRFNRQXdSZ0loQUxwWXpzZkVMdjZScG56OGdqcDZXYkZuUFk2S3FrQ2gKTWYwRWZvMnRzM2d5QWlFQXhkaDM4akJCMWJrTWlwWDNSMTFyTnBtZmc2S2huZzliNUJDTUs0M3UyTjA9Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K</span>
<span class="c">#        server: https://127.0.0.1:6443</span>
<span class="c">#      name: default</span>
<span class="c">#    ...</span>
<span class="nv">$ </span><span class="nb">export</span> | <span class="nb">grep </span>KUBECONFIG
<span class="c"># =&gt; (공백)</span>

<span class="c"># 네트워크 정보 확인 : flannel CNI(vxlan mode), podCIDR</span>
<span class="nv">$ </span>ip <span class="nt">-c</span> addr
<span class="c"># =&gt; ...</span>
<span class="c">#    4: flannel.1: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1450 qdisc noqueue state UNKNOWN group default</span>
<span class="c">#        link/ether 02:21:77:da:a3:91 brd ff:ff:ff:ff:ff:ff</span>
<span class="c">#        inet 172.16.0.0/32 scope global flannel.1</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    5: cni0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1450 qdisc noqueue state UP group default qlen 1000</span>
<span class="c">#        link/ether ca:30:a0:c8:5c:cd brd ff:ff:ff:ff:ff:ff</span>
<span class="c">#        inet 172.16.0.1/24 brd 172.16.0.255 scope global cni0</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    6: veth41d9e3b2@if2: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1450 qdisc noqueue master cni0 state UP group default</span>
<span class="c">#        link/ether fa:47:5c:4a:8d:af brd ff:ff:ff:ff:ff:ff link-netns cni-9c26655e-b22f-97a1-f97c-db88daccc77f</span>
<span class="c">#    7: veth5c3de18a@if2: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1450 qdisc noqueue master cni0 state UP group default</span>
<span class="c">#        link/ether 6a:5e:39:72:4f:4c brd ff:ff:ff:ff:ff:ff link-netns cni-cff25bf8-d23b-790a-91d0-ed5c4ee526d5</span>
<span class="c">#    8: vethfaeebb1c@if2: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1450 qdisc noqueue master cni0 state UP group default</span>
<span class="c">#        link/ether c6:f5:31:a1:56:21 brd ff:ff:ff:ff:ff:ff link-netns cni-06a3672f-70dc-7445-48c9-8cf8c26e7fb3</span>
<span class="nv">$ </span>ip <span class="nt">-c</span> route
<span class="c"># =&gt; default via 10.0.2.2 dev enp0s3 proto dhcp src 10.0.2.15 metric 100</span>
<span class="c">#    ...</span>
<span class="c">#    172.16.0.0/24 dev cni0 proto kernel scope link src 172.16.0.1</span>
<span class="c">#    172.16.1.0/24 via 172.16.1.0 dev flannel.1 onlink</span>
<span class="c">#    172.16.2.0/24 via 172.16.2.0 dev flannel.1 onlink</span>
<span class="c">#    172.16.3.0/24 via 172.16.3.0 dev flannel.1 onlink</span>
<span class="c">#    192.168.10.0/24 dev enp0s8 proto kernel scope link src 192.168.10.10</span>
<span class="nv">$ </span><span class="nb">cat</span> /run/flannel/subnet.env
<span class="c"># =&gt; FLANNEL_NETWORK=172.16.0.0/16</span>
<span class="c">#    FLANNEL_SUBNET=172.16.0.1/24</span>
<span class="c">#    FLANNEL_MTU=1450</span>
<span class="c">#    FLANNEL_IPMASQ=true</span>
<span class="nv">$ </span>kubectl get nodes <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.items[*].spec.podCIDR}'</span> <span class="p">;</span><span class="nb">echo</span>
<span class="c"># =&gt; 172.16.0.0/24 172.16.3.0/24 172.16.1.0/24 172.16.2.0/24</span>
<span class="nv">$ </span>kubectl describe node | <span class="nb">grep</span> <span class="nt">-A3</span> Annotations
<span class="c"># =&gt; Annotations:        alpha.kubernetes.io/provided-node-ip: 10.0.2.15</span>
<span class="c">#                        flannel.alpha.coreos.com/backend-data: {&amp;quot;VNI&amp;quot;:1,&amp;quot;VtepMAC&amp;quot;:&amp;quot;02:21:77:da:a3:91&amp;quot;}</span>
<span class="c">#                        flannel.alpha.coreos.com/backend-type: vxlan</span>
<span class="c">#                        flannel.alpha.coreos.com/kube-subnet-manager: true</span>
<span class="c">#    --</span>
<span class="c">#    Annotations:        alpha.kubernetes.io/provided-node-ip: 10.0.2.15</span>
<span class="c">#                        flannel.alpha.coreos.com/backend-data: {&amp;quot;VNI&amp;quot;:1,&amp;quot;VtepMAC&amp;quot;:&amp;quot;72:95:9e:3d:c6:35&amp;quot;}</span>
<span class="c">#                        flannel.alpha.coreos.com/backend-type: vxlan</span>
<span class="c">#                        flannel.alpha.coreos.com/kube-subnet-manager: true</span>
<span class="c">#    --</span>
<span class="c">#    Annotations:        alpha.kubernetes.io/provided-node-ip: 10.0.2.15</span>
<span class="c">#                        flannel.alpha.coreos.com/backend-data: {&amp;quot;VNI&amp;quot;:1,&amp;quot;VtepMAC&amp;quot;:&amp;quot;ae:28:43:65:df:f4&amp;quot;}</span>
<span class="c">#                        flannel.alpha.coreos.com/backend-type: vxlan</span>
<span class="c">#                        flannel.alpha.coreos.com/kube-subnet-manager: true</span>
<span class="c">#    --</span>
<span class="c">#    Annotations:        alpha.kubernetes.io/provided-node-ip: 10.0.2.15</span>
<span class="c">#                        flannel.alpha.coreos.com/backend-data: {&amp;quot;VNI&amp;quot;:1,&amp;quot;VtepMAC&amp;quot;:&amp;quot;8e:91:37:7d:c1:d7&amp;quot;}</span>
<span class="c">#                        flannel.alpha.coreos.com/backend-type: vxlan</span>
<span class="c">#                        flannel.alpha.coreos.com/kube-subnet-manager: true</span>
<span class="nv">$ </span>brctl show
<span class="c"># =&gt; bridge name     bridge id               STP enabled     interfaces</span>
<span class="c">#    cni0            8000.ca30a0c85ccd       no              veth41d9e3b2</span>
<span class="c">#                                                            veth5c3de18a</span>
<span class="c">#                                                            vethfaeebb1c</span>

<span class="c"># 서비스와 엔드포인트 확인</span>
<span class="nv">$ </span>kubectl get svc,ep <span class="nt">-A</span>
<span class="c"># =&gt; NAMESPACE     NAME                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                  AGE</span>
<span class="c">#    default       service/kubernetes       ClusterIP   10.10.200.1     &amp;lt;none&amp;gt;        443/TCP                  38m</span>
<span class="c">#    kube-system   service/kube-dns         ClusterIP   10.10.200.10    &amp;lt;none&amp;gt;        53/UDP,53/TCP,9153/TCP   38m</span>
<span class="c">#    kube-system   service/metrics-server   ClusterIP   10.10.200.103   &amp;lt;none&amp;gt;        443/TCP                  38m</span>
<span class="c">#    </span>
<span class="c">#    NAMESPACE     NAME                       ENDPOINTS                                     AGE</span>
<span class="c">#    default       endpoints/kubernetes       10.0.2.15:6443                                38m</span>
<span class="c">#    kube-system   endpoints/kube-dns         172.16.0.4:53,172.16.0.4:53,172.16.0.4:9153   38m</span>
<span class="c">#    kube-system   endpoints/metrics-server   172.16.0.3:10250                              38m</span>

<span class="c"># iptables 정보 확인</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> filter <span class="nt">-S</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> mangle <span class="nt">-S</span>

<span class="c"># tcp listen 포트 정보 확인</span>
<span class="nv">$ </span>ss <span class="nt">-tnlp</span>
</code></pre></div></div>

<ul>
  <li>flannel CNI를 사용하고 있고, 클러스터 IP는 172.16.0.0/16이며, 컨트롤 플레인의기능들이 많이 내장되어있어 실행중인 파드가 적음을 확인 할 수 있습니다.</li>
</ul>

<h3 id="nginx-ingress-controller-설치">Nginx Ingress Controller 설치</h3>

<ul>
  <li>Nginx Ingress Controller는 가장 많이 사용되는 Ingress Controller 중 하나로 Ingress 실습을 위해 설치해보겠습니다.</li>
  <li>먼저 NGINX Ingress 의 특징을 살펴보겠습니다.
    <ul>
      <li>NGINX Ingress는 고성능 웹서버인 NGINX를 기반으로 동작하며, Layer 7에서 동작합니다.</li>
      <li>k8s의 configmap 설정을 lua 스크립트로 가공하여 nginx config로 변환하여 사용합니다.</li>
      <li>설정을 변경하면 내부의 nginx가 reload 되면서 자동으로 적용되며, 설정을 쉽게 변경할 수 있습니다.</li>
    </ul>
  </li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Ingress-Nginx 컨트롤러 생성</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOT</span><span class="sh">&gt; ingress-nginx-values.yaml
controller:
  service:
    type: NodePort
    nodePorts:
      http: 30080
      https: 30443
  nodeSelector:
    kubernetes.io/hostname: "k3s-s"
  metrics:
    enabled: true
  serviceMonitor:
      enabled: true
</span><span class="no">EOT

</span><span class="nv">$ </span>helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
<span class="c"># =&gt; "ingress-nginx" has been added to your repositories</span>
<span class="nv">$ </span>helm repo update
<span class="c"># =&gt; ...Successfully got an update from the "ingress-nginx" chart repository</span>

<span class="nv">$ </span>kubectl create ns ingress
<span class="c"># =&gt; namespace/ingress created</span>
<span class="nv">$ </span>helm <span class="nb">install </span>ingress-nginx ingress-nginx/ingress-nginx <span class="nt">-f</span> ingress-nginx-values.yaml <span class="nt">--namespace</span> ingress <span class="nt">--version</span> 4.11.2
<span class="c"># =&gt; Release &amp;quot;ingress-nginx&amp;quot; has been upgraded. Happy Helming!</span>
<span class="c">#    NAME: ingress-nginx</span>
<span class="c">#    NAMESPACE: ingress</span>
<span class="c">#    STATUS: deployed</span>
<span class="c">#    ...</span>
<span class="c">#    The ingress-nginx controller has been installed.</span>
<span class="c">#    Get the application URL by running these commands:</span>
<span class="c">#      export HTTP_NODE_PORT=30080</span>
<span class="c">#      export HTTPS_NODE_PORT=30443</span>
<span class="c">#      export NODE_IP=&amp;quot;$(kubectl get nodes --output jsonpath=&amp;quot;{.items[0].status.addresses[1].address}&amp;quot;)&amp;quot;</span>
<span class="c">#    </span>
<span class="c">#      echo &amp;quot;Visit http://${NODE_IP}:${HTTP_NODE_PORT} to access your application via HTTP.&amp;quot;</span>
<span class="c">#      echo &amp;quot;Visit https://${NODE_IP}:${HTTPS_NODE_PORT} to access your application via HTTPS.&amp;quot;</span>
<span class="c">#    ...</span>

<span class="c"># 확인</span>
<span class="nv">$ </span>kubectl get all <span class="nt">-n</span> ingress
<span class="c"># =&gt; NAME                                            READY   STATUS    RESTARTS   AGE</span>
<span class="c">#    pod/ingress-nginx-controller-7b67846f8f-jdt65   1/1     Running   0          47s</span>
<span class="c">#    </span>
<span class="c">#    NAME                                         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE</span>
<span class="c">#    service/ingress-nginx-controller             NodePort    10.10.200.113   &amp;lt;none&amp;gt;        80:30080/TCP,443:30443/TCP   47s</span>
<span class="c">#    service/ingress-nginx-controller-admission   ClusterIP   10.10.200.176   &amp;lt;none&amp;gt;        443/TCP                      47s</span>
<span class="c">#    service/ingress-nginx-controller-metrics     ClusterIP   10.10.200.218   &amp;lt;none&amp;gt;        10254/TCP                    47s</span>
<span class="c">#    </span>
<span class="c">#    NAME                                       READY   UP-TO-DATE   AVAILABLE   AGE</span>
<span class="c">#    deployment.apps/ingress-nginx-controller   1/1     1            1           47s</span>
<span class="c">#    </span>
<span class="c">#    NAME                                                  DESIRED   CURRENT   READY   AGE</span>
<span class="c">#    replicaset.apps/ingress-nginx-controller-7b67846f8f   1         1         1       47s</span>

<span class="nv">$ </span>kubectl describe svc <span class="nt">-n</span> ingress ingress-nginx-controller
<span class="c"># =&gt; Name:                     ingress-nginx-controller</span>
<span class="c">#    Namespace:                ingress</span>
<span class="c">#    Labels:                   app.kubernetes.io/component=controller</span>
<span class="c">#                              app.kubernetes.io/instance=ingress-nginx</span>
<span class="c">#    ...</span>
<span class="c">#    Selector:                 app.kubernetes.io/component=controller,app.kubernetes.io/instance=ingress-nginx,app.kubernetes.io/name=ingress-nginx</span>
<span class="c">#    Type:                     NodePort</span>
<span class="c">#    IP Family Policy:         SingleStack</span>
<span class="c">#    IP Families:              IPv4</span>
<span class="c">#    IP:                       10.10.200.113</span>
<span class="c">#    ...</span>
<span class="c">#    Port:                     http  80/TCP</span>
<span class="c">#    TargetPort:               http/TCP</span>
<span class="c">#    NodePort:                 http  30080/TCP</span>
<span class="c">#    Endpoints:                172.16.0.16:80</span>
<span class="c">#    Port:                     https  443/TCP</span>
<span class="c">#    TargetPort:               https/TCP</span>
<span class="c">#    NodePort:                 https  30443/TCP</span>
<span class="c">#    Endpoints:                172.16.0.16:443</span>
<span class="c">#    ...</span>

<span class="c"># externalTrafficPolicy 설정</span>
<span class="nv">$ </span>kubectl patch svc <span class="nt">-n</span> ingress ingress-nginx-controller <span class="nt">-p</span> <span class="s1">'{"spec":{"externalTrafficPolicy": "Local"}}'</span>
<span class="c"># =&gt; service/ingress-nginx-controller patched</span>

<span class="c"># 기본 nginx conf 파일 확인</span>
<span class="nv">$ </span>kubectl describe cm <span class="nt">-n</span> ingress ingress-nginx-controller
<span class="c"># =&gt; ...</span>
<span class="c">#    Data</span>
<span class="c">#    ====</span>
<span class="c">#    allow-snippet-annotations:</span>
<span class="c">#    ----</span>
<span class="c">#    false</span>
<span class="c">#    ...</span>
<span class="nv">$ </span>kubectl <span class="nb">exec </span>deploy/ingress-nginx-controller <span class="nt">-n</span> ingress <span class="nt">-it</span> <span class="nt">--</span> <span class="nb">cat</span> /etc/nginx/nginx.conf
<span class="c"># =&gt; # Configuration checksum: 13054992059071414660</span>
<span class="c">#    # setup custom paths that do not require root access</span>
<span class="c">#    pid /tmp/nginx/nginx.pid;</span>
<span class="c">#    </span>
<span class="c">#    daemon off;</span>
<span class="c">#    worker_processes 4;</span>
<span class="c">#    worker_rlimit_nofile 1047552;</span>
<span class="c">#    worker_shutdown_timeout 240s ;</span>
<span class="c">#    </span>
<span class="c">#    events {</span>
<span class="c">#            multi_accept        on;</span>
<span class="c">#            worker_connections  16384;</span>
<span class="c">#            use                 epoll;</span>
<span class="c">#    }</span>
<span class="c">#    </span>
<span class="c">#    http {</span>
<span class="c">#            lua_package_path &amp;quot;/etc/nginx/lua/?.lua;;&amp;quot;;</span>
<span class="c">#            lua_shared_dict balancer_ewma 10M;</span>
<span class="c">#    ...</span>

<span class="c"># 관련된 정보 확인 : 포드(Nginx 서버), 서비스, 디플로이먼트, 리플리카셋, 컨피그맵, 롤, 클러스터롤, 서비스 어카운트 등</span>
<span class="nv">$ </span>kubectl get all,sa,cm,secret,roles <span class="nt">-n</span> ingress
<span class="c"># =&gt; NAME                                            READY   STATUS    RESTARTS   AGE</span>
<span class="c">#    pod/ingress-nginx-controller-7b67846f8f-jdt65   1/1     Running   0          4m21s</span>
<span class="c">#    </span>
<span class="c">#    NAME                                         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE</span>
<span class="c">#    service/ingress-nginx-controller             NodePort    10.10.200.113   &amp;lt;none&amp;gt;        80:30080/TCP,443:30443/TCP   4m21s</span>
<span class="c">#    service/ingress-nginx-controller-admission   ClusterIP   10.10.200.176   &amp;lt;none&amp;gt;        443/TCP                      4m21s</span>
<span class="c">#    service/ingress-nginx-controller-metrics     ClusterIP   10.10.200.218   &amp;lt;none&amp;gt;        10254/TCP                    4m21s</span>
<span class="c">#    </span>
<span class="c">#    NAME                                       READY   UP-TO-DATE   AVAILABLE   AGE</span>
<span class="c">#    deployment.apps/ingress-nginx-controller   1/1     1            1           4m21s</span>
<span class="c">#    </span>
<span class="c">#    NAME                                                  DESIRED   CURRENT   READY   AGE</span>
<span class="c">#    replicaset.apps/ingress-nginx-controller-7b67846f8f   1         1         1       4m21s</span>
<span class="c">#    </span>
<span class="c">#    NAME                           SECRETS   AGE</span>
<span class="c">#    serviceaccount/default         0         4m29s</span>
<span class="c">#    serviceaccount/ingress-nginx   0         4m21s</span>
<span class="c">#    </span>
<span class="c">#    NAME                                 DATA   AGE</span>
<span class="c">#    configmap/ingress-nginx-controller   1      4m21s</span>
<span class="c">#    configmap/kube-root-ca.crt           1      4m30s</span>
<span class="c">#    </span>
<span class="c">#    NAME                                         TYPE                 DATA   AGE</span>
<span class="c">#    secret/ingress-nginx-admission               Opaque               3      4m24s</span>
<span class="c">#    secret/sh.helm.release.v1.ingress-nginx.v1   helm.sh/release.v1   1      4m26s</span>
<span class="c">#    </span>
<span class="c">#    NAME                                           CREATED AT</span>
<span class="c">#    role.rbac.authorization.k8s.io/ingress-nginx   2024-01-01T08:53:04Z</span>
<span class="nv">$ </span>kubectl describe clusterroles ingress-nginx
<span class="nv">$ </span>kubectl get pod,svc,ep <span class="nt">-n</span> ingress <span class="nt">-o</span> wide <span class="nt">-l</span> app.kubernetes.io/component<span class="o">=</span>controller

<span class="c"># 버전 정보 확인</span>
<span class="nv">$ POD_NAMESPACE</span><span class="o">=</span>ingress
<span class="nv">$ POD_NAME</span><span class="o">=</span><span class="si">$(</span>kubectl get pods <span class="nt">-n</span> <span class="nv">$POD_NAMESPACE</span> <span class="nt">-l</span> app.kubernetes.io/name<span class="o">=</span>ingress-nginx <span class="nt">--field-selector</span><span class="o">=</span>status.phase<span class="o">=</span>Running <span class="nt">-o</span> name<span class="si">)</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nv">$POD_NAME</span> <span class="nt">-n</span> <span class="nv">$POD_NAMESPACE</span> <span class="nt">--</span> /nginx-ingress-controller <span class="nt">--version</span>
<span class="c"># =&gt; -------------------------------------------------------------------------------</span>
<span class="c">#    NGINX Ingress controller</span>
<span class="c">#      Release:       v1.11.2</span>
<span class="c">#      Build:         46e76e5916813cfca2a9b0bfdc34b69a0000f6b9</span>
<span class="c">#      Repository:    https://github.com/kubernetes/ingress-nginx</span>
<span class="c">#      nginx version: nginx/1.25.5</span>
<span class="c">#    -------------------------------------------------------------------------------</span>
</code></pre></div></div>

<ul>
  <li>Ingress Controller가 설치되었으며, NodePort로 서비스가 생성된것을 확인할 수 있습니다.</li>
  <li>
    <p>또한 Nginx Ingress Controller의 경우 내부적으로는 일반적인 <strong>nginx 서버가 동일하게 동작</strong>하고, <strong>lua 스크립트를 사용하여 configmap의 설정이 적용/관리</strong>되고 있음을 확인할 수 있습니다.</p>
  </li>
  <li>(옵션) kubectl krew 설치 - <a href="https://krew.sigs.k8s.io/docs/user-guide/setup/install/">링크</a> &amp; ingress-nginx plugin 설치 - <a href="https://kubernetes.github.io/ingress-nginx/kubectl-plugin/">링크</a></li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># (참고) 운영체제 확인 : linux</span>
<span class="nv">$ OS</span><span class="o">=</span><span class="s2">"</span><span class="si">$(</span><span class="nb">uname</span> | <span class="nb">tr</span> <span class="s1">'[:upper:]'</span> <span class="s1">'[:lower:]'</span><span class="si">)</span><span class="s2">"</span>
<span class="c"># (참고)  CPU 아키텍처 확인 : amd64</span>
<span class="nv">$ ARCH</span><span class="o">=</span><span class="s2">"</span><span class="si">$(</span><span class="nb">uname</span> <span class="nt">-m</span> | <span class="nb">sed</span> <span class="nt">-e</span> <span class="s1">'s/x86_64/amd64/'</span> <span class="nt">-e</span> <span class="s1">'s/\(arm\)\(64\)\?.*/\1\2/'</span> <span class="nt">-e</span> <span class="s1">'s/aarch64$/arm64/'</span><span class="si">)</span><span class="s2">"</span>
<span class="c"># (참고)  KREW 지정 : krew-linux_amd64</span>
<span class="nv">$ KREW</span><span class="o">=</span><span class="s2">"krew-</span><span class="k">${</span><span class="nv">OS</span><span class="k">}</span><span class="s2">_</span><span class="k">${</span><span class="nv">ARCH</span><span class="k">}</span><span class="s2">"</span>

<span class="c"># kubectl krew 설치</span>
<span class="c"># curl -fsSLO "https://github.com/kubernetes-sigs/krew/releases/latest/download/${KREW}.tar.gz"</span>
<span class="nv">$ </span>curl <span class="nt">-fsSLO</span> <span class="s2">"https://github.com/kubernetes-sigs/krew/releases/latest/download/krew-linux_amd64.tar.gz"</span> <span class="o">&amp;&amp;</span> <span class="nb">tar </span>zxvf krew-linux_amd64.tar.gz <span class="o">&amp;&amp;</span> ./krew-linux_amd64 <span class="nb">install </span>krew
<span class="nv">$ </span><span class="nb">export </span><span class="nv">PATH</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">KREW_ROOT</span><span class="k">:-</span><span class="nv">$HOME</span><span class="p">/.krew</span><span class="k">}</span><span class="s2">/bin:</span><span class="nv">$PATH</span><span class="s2">"</span>

<span class="c"># 플러그인 정보 업데이트 후 확인 - 링크</span>
<span class="nv">$ </span>kubectl krew update
<span class="nv">$ </span>kubectl krew search

<span class="c"># ingress-nginx 플러그인 설치</span>
<span class="nv">$ </span>kubectl krew <span class="nb">install </span>ingress-nginx
<span class="c"># =&gt; (아쉽게도 옛날 버전이라서 설치가 안 됩니다.) </span>

<span class="c"># ingress-nginx 플러그인 명령어 실행(도움말 출력)</span>
<span class="nv">$ </span>kubectl ingress-nginx

<span class="c"># nginx ctrl 의 backends 설정 정보 출력</span>
<span class="nv">$ </span>kubectl ingress-nginx backends <span class="nt">-n</span> ingress-nginx <span class="nt">--list</span>
<span class="nv">$ </span>kubectl ingress-nginx backends <span class="nt">-n</span> ingress-nginx

<span class="c"># conf 출력</span>
<span class="nv">$ </span>kubectl ingress-nginx conf <span class="nt">-n</span> ingress-nginx
<span class="c">## 특정 호스트(도메인) 설정 확인</span>
<span class="nv">$ </span>kubectl ingress-nginx conf <span class="nt">-n</span> ingress-nginx <span class="nt">--host</span> gasida.cndk.link
<span class="nv">$ </span>kubectl ingress-nginx conf <span class="nt">-n</span> ingress-nginx <span class="nt">--host</span> nasida.cndk.link

<span class="c"># 정보 보기 편함!</span>
<span class="nv">$ </span>kubectl ingress-nginx ingresses
<span class="nv">$ </span>kubectl ingress-nginx ingresses <span class="nt">--all-namespaces</span>
</code></pre></div></div>

<h3 id="인그레스ingress-실습-및-통신-흐름-확인">인그레스(Ingress) 실습 및 통신 흐름 확인</h3>

<ul>
  <li>실습 구성도
    <ul>
      <li>컨트롤플레인 노드에 인그레스 컨트롤러(Nginx) 파드를 생성하고, NodePort 로 외부에 노출합니다.</li>
      <li>인그레스 정책 설정 : Host/Path routing, 실습의 편리를 위해서 도메인 없이 IP로 접속 설정 가능하도록 합니다.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/2024/kans-3th/w6/20241012_kans_w6_5.png" alt="img.png" /></p>

<p><img src="/assets/2024/kans-3th/w6/20241012_kans_w6_6.png" alt="img_1.png" /></p>

<h4 id="deployment와-service-생성">deployment와 service 생성</h4>

<ul>
  <li>svc1-pod.yml 생성
    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># svc1-pod.yml</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">deploy1-websrv</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">websrv</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">app</span><span class="pi">:</span> <span class="s">websrv</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">containers</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">pod-web</span>
        <span class="na">image</span><span class="pi">:</span> <span class="s">nginx</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">svc1-web</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">ports</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">web-port</span>
      <span class="na">port</span><span class="pi">:</span> <span class="m">9001</span>
      <span class="na">targetPort</span><span class="pi">:</span> <span class="m">80</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">websrv</span>
  <span class="na">type</span><span class="pi">:</span> <span class="s">ClusterIP</span>
</code></pre></div>    </div>
  </li>
  <li>svc2-pod.yml 생성
    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># svc2-pod.yml </span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">deploy2-guestsrv</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">2</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">guestsrv</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">app</span><span class="pi">:</span> <span class="s">guestsrv</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">containers</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">pod-guest</span>
        <span class="na">image</span><span class="pi">:</span> <span class="s">gcr.io/google-samples/kubernetes-bootcamp:v1</span>
        <span class="na">ports</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">8080</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">svc2-guest</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">ports</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">guest-port</span>
      <span class="na">port</span><span class="pi">:</span> <span class="m">9002</span>
      <span class="na">targetPort</span><span class="pi">:</span> <span class="m">8080</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">guestsrv</span>
  <span class="na">type</span><span class="pi">:</span> <span class="s">NodePort</span>
</code></pre></div>    </div>
  </li>
  <li>svc3-pod.yml 생성
    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># svc3-pod.yml</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">deploy3-adminsrv</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">3</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">adminsrv</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">app</span><span class="pi">:</span> <span class="s">adminsrv</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">containers</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">pod-admin</span>
        <span class="na">image</span><span class="pi">:</span> <span class="s">k8s.gcr.io/echoserver:1.5</span>
        <span class="na">ports</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">8080</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">svc3-admin</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">ports</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">admin-port</span>
      <span class="na">port</span><span class="pi">:</span> <span class="m">9003</span>
      <span class="na">targetPort</span><span class="pi">:</span> <span class="m">8080</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">adminsrv</span>
</code></pre></div>    </div>
  </li>
  <li>생성 및 확인
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 모니터링</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s1">'kubectl get ingress,svc,ep,pod -owide'</span>
  
<span class="c"># 생성</span>
<span class="nv">$ </span>kubectl taint nodes k3s-m <span class="nv">role</span><span class="o">=</span>controlplane:NoSchedule
<span class="c"># &lt;span style="color: green;"&gt;&lt;/span&gt;</span>
  
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> svc1-pod.yml,svc2-pod.yml,svc3-pod.yml
<span class="c"># =&gt; deployment.apps/deploy1-websrv created</span>
<span class="c">#    service/svc1-web created</span>
<span class="c">#    deployment.apps/deploy2-guestsrv created</span>
<span class="c">#    service/svc2-guest created</span>
<span class="c">#    deployment.apps/deploy3-adminsrv created</span>
<span class="c">#    service/svc3-admin created</span>
  
<span class="c"># 확인 : svc1, svc3 은 ClusterIP 로 클러스터 외부에서는 접속할 수 없다 &gt;&gt; Ingress 는 연결 가능!</span>
<span class="nv">$ </span>kubectl get pod,svc,ep
<span class="c"># =&gt; NAME                                    READY   STATUS    RESTARTS   AGE</span>
<span class="c">#    pod/deploy1-websrv-5c6b88bd77-ht5hl     1/1     Running   0          34s</span>
<span class="c">#    pod/deploy2-guestsrv-649875f78b-8wh8r   1/1     Running   0          34s</span>
<span class="c">#    pod/deploy2-guestsrv-649875f78b-jcvrf   1/1     Running   0          34s</span>
<span class="c">#    pod/deploy3-adminsrv-7c8f8b8c87-4mzv7   1/1     Running   0          34s</span>
<span class="c">#    pod/deploy3-adminsrv-7c8f8b8c87-4sqmh   1/1     Running   0          34s</span>
<span class="c">#    pod/deploy3-adminsrv-7c8f8b8c87-ztltl   1/1     Running   0          34s</span>
<span class="c">#    </span>
<span class="c">#    NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE</span>
<span class="c">#    service/kubernetes   ClusterIP   10.10.200.1     &amp;lt;none&amp;gt;        443/TCP          5h52m</span>
<span class="c">#    service/svc1-web     ClusterIP   10.10.200.141   &amp;lt;none&amp;gt;        9001/TCP         34s</span>
<span class="c">#    service/svc2-guest   NodePort    10.10.200.60    &amp;lt;none&amp;gt;        9002:30901/TCP   34s</span>
<span class="c">#    service/svc3-admin   ClusterIP   10.10.200.171   &amp;lt;none&amp;gt;        9003/TCP         34s</span>
<span class="c">#    &lt;span style="color: green;"&gt;# ingress는 pod 정보로 바로 접근 가능하므로 서비스가 ClusterIP이든 NodePort 타입이든 관계 없습니다.&lt;/span&gt;</span>
<span class="c">#    </span>
<span class="c">#    NAME                   ENDPOINTS                                         AGE</span>
<span class="c">#    endpoints/kubernetes   192.168.10.10:6443                                5h52m</span>
<span class="c">#    endpoints/svc1-web     172.16.1.8:80                                     34s</span>
<span class="c">#    endpoints/svc2-guest   172.16.2.8:8080,172.16.3.7:8080                   34s</span>
<span class="c">#    endpoints/svc3-admin   172.16.1.7:8080,172.16.2.9:8080,172.16.3.8:8080   34s</span>
</code></pre></div>    </div>
  </li>
</ul>

<h4 id="인그레스정책-생성">인그레스(정책) 생성</h4>

<p><img src="/assets/2024/kans-3th/w6/20241012_kans_w6_7.png" alt="img.png" class="image-center" />
<em class="image-caption">ingress 정책 적용 구조 (<a href="https://kschoi728.tistory.com/266">출처</a>)</em></p>

<ul>
  <li>ingress1.yml 파일 생성</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOT</span><span class="sh">&gt; ingress1.yml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-1
  annotations:
    #nginx.ingress.kubernetes.io/upstream-hash-by: "true"
spec:
  ingressClassName: nginx
  rules:
  - http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: svc1-web
            port:
              number: 80
      - path: /guest
        pathType: Prefix
        backend:
          service:
            name: svc2-guest
            port:
              number: 8080
      - path: /admin
        pathType: Prefix
        backend:
          service:
            name: svc3-admin
            port:
              number: 8080
</span><span class="no">EOT
</span></code></pre></div></div>

<ul>
  <li>ingress 정책 생성</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 모니터링</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s1">'kubectl get ingress,svc,ep,pod -owide'</span>

<span class="c"># 생성</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> ingress1.yml
<span class="c"># =&gt; ingress.networking.k8s.io/ingress-1 created</span>

<span class="c"># 확인</span>
<span class="nv">$ </span>kubectl get ingress
<span class="c"># =&gt; NAME        CLASS   HOSTS   ADDRESS   PORTS   AGE</span>
<span class="c">#    ingress-1   nginx   *                 80      11s</span>
<span class="nv">$ </span>kubectl describe ingress ingress-1
<span class="c"># =&gt; ...</span>
<span class="c">#    Rules:</span>
<span class="c">#      Host        Path  Backends</span>
<span class="c">#      ----        ----  --------</span>
<span class="c">#      *</span>
<span class="c">#                  /        svc1-web:80 ()</span>
<span class="c">#                  /guest   svc2-guest:8080 ()</span>
<span class="c">#                  /admin   svc3-admin:8080 ()</span>
<span class="c">#    ...</span>

<span class="c"># 설정이 반영된 nginx conf 파일 확인</span>
<span class="nv">$ </span>kubectl <span class="nb">exec </span>deploy/ingress-nginx-controller <span class="nt">-n</span> ingress <span class="nt">-it</span> <span class="nt">--</span> <span class="nb">cat</span> /etc/nginx/nginx.conf
<span class="nv">$ </span>kubectl <span class="nb">exec </span>deploy/ingress-nginx-controller <span class="nt">-n</span> ingress <span class="nt">-it</span> <span class="nt">--</span> <span class="nb">cat</span> /etc/nginx/nginx.conf | <span class="nb">grep</span> <span class="s1">'location /'</span> <span class="nt">-A5</span>
<span class="c"># =&gt;      location /guest/ {</span>
<span class="c">#    </span>
<span class="c">#         set $namespace      &amp;quot;default&amp;quot;;</span>
<span class="c">#         set $ingress_name   &amp;quot;ingress-1&amp;quot;;</span>
<span class="c">#         set $service_name   &amp;quot;svc2-guest&amp;quot;;</span>
<span class="c">#         set $service_port   &amp;quot;8080&amp;quot;;</span>
<span class="c">#    --</span>
<span class="c">#         location /admin/ {</span>
<span class="c">#    </span>
<span class="c">#         set $namespace      &amp;quot;default&amp;quot;;</span>
<span class="c">#         set $ingress_name   &amp;quot;ingress-1&amp;quot;;</span>
<span class="c">#         set $service_name   &amp;quot;svc3-admin&amp;quot;;</span>
<span class="c">#         set $service_port   &amp;quot;8080&amp;quot;;</span>
<span class="c">#    --</span>
<span class="c">#         location / {</span>
<span class="c">#    </span>
<span class="c">#         set $namespace      &amp;quot;default&amp;quot;;</span>
<span class="c">#         set $ingress_name   &amp;quot;ingress-1&amp;quot;;</span>
<span class="c">#         set $service_name   &amp;quot;svc1-web&amp;quot;;</span>
<span class="c">#         set $service_port   &amp;quot;80&amp;quot;;</span>
<span class="c">#    ...</span>
</code></pre></div></div>

<h4 id="ingress를-통한-내부-접속">ingress를 통한 내부 접속</h4>

<ul>
  <li>
    <p>Nginx ingress controller를 통해 접속시 서비스는 파드의 엔드포인트의 정보만 참조되고, 서비스를 거치지 않고 바로 파드로 전달됩니다.
<img src="/assets/2024/kans-3th/w6/20241012_kans_w6_8.png" alt="img.png" class="w-80 image-center" />
<em class="image-caption">인그레스 접속 경로(서비스 Bypass) : Ingress → 애플리케이션(Deploy, Pod 등)</em></p>
  </li>
  <li>참고 : URI(Uniform Resource Identifier)는 RFC 3986에 정의된 통합 자원 식별자로, 흔히 사용되는 URL(Uniform Resource Locator)과 URN(Uniform Resource Name)을 포함합니다.
    <ul>
      <li>Request URI는 서버 주소나 파일이름, 파라미터 등 다양한 리소스를 식별하기 위해 사용되는 문자열입니다.</li>
      <li>절대 URI(absolute URI)는 스키마와 호스트를 포함한 완전한 URI를 의미하며, 상대 URI(relative URI)는 스키마와 호스트를 포함하지 않고 현재 위치에서 상대적인 위치를 기록한 URI를 의미합니다.
        <ul>
          <li>URI의 구조는 아래와 같습니다.
<img src="/assets/2024/kans-3th/w6/20241012_kans_w6_9.png" alt="img.png" class="image-center" />
<em class="image-caption">책 ‘그림으로 공부하는 TCP/IP 구조’ 중 발췌</em></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>참고 : X-Forwarded-For 헤더, X-Forwarded-Proto 헤더
    <ul>
      <li>X-Forwarded-For 헤더는 송신지 IP 주소가 변환되는 환경(장비, 서버, 솔루션 등)에서, 변환 전 송신지(클라이언트) IP 주소를 저장하는 헤더입니다.
        <ul>
          <li>여러 장비나 솔루션을 거칠 경우 <code class="language-plaintext highlighter-rouge">,</code>로 구분하여 여러 건이 넘어올 수도 있습니다. 그럴 경우 가장 왼쪽 것이 클라이언트 IP이고, 오른쪽으로 갈 수록 나중에 처리된 장비/솔루션의 IP가 됩니다.</li>
        </ul>
      </li>
      <li>X-Forwarded-Proto 헤더는 변환 전 프로토콜을 저장합니다. (예. SSL Offload 환경에서 서버 측에서 클라이언트가 요청 시 사용한 원래 프로토콜을 확인)</li>
      <li>이러한 헤더는 클라이언트의 IP 주소를 확인하거나, 프로토콜을 확인하는 등의 용도로 사용되며, 어플리케이션에서 NodePort나 LoadBalancer를 통해서 접속되었을때도 원래의 클라이언트의 IP를 확인할 수 있게 해줍니다. (<code class="language-plaintext highlighter-rouge">externalTrafficPolicy: Local</code>를 사용할 필요가 줄어듭니다!)</li>
      <li>원래의 IP를 가져오는 방법은 다음의 방법들이 있습니다.
        <ul>
          <li>Http request header 중 다음 값들에서 원래의 IP 찾기
            <ol>
              <li>X-Forwarded-For : HTTP RFC 표준에는 없지만 사실상 표준!!!</li>
              <li>Proxy-Client-IP : 특정 웹 어플리케이션에서 사용 (예. WebLogic Connector - mod_wl)</li>
              <li>WL-Proxy-Client-IP : 특정 웹 어플리케이션에서 사용 (예. WebLogic Connector - mod_wl)</li>
              <li>CLIENT_IP</li>
            </ol>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>인그레스(Nginx 인그레스 컨트롤러)를 통한 접속(HTTP 인입)을 확인해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># (krew 플러그인 설치 시) 인그레스 정책 확인</span>
<span class="c"># $ kubectl ingress-nginx ingresses</span>
<span class="c"># INGRESS NAME   HOST+PATH   ADDRESSES       TLS   SERVICE      SERVICE PORT   ENDPOINTS</span>
<span class="c"># ingress-1      /           192.168.10.10   NO    svc1-web     80             1</span>
<span class="c"># ingress-1      /guest      192.168.10.10   NO    svc2-guest   8080           2</span>
<span class="c"># ingress-1      /admin      192.168.10.10   NO    svc3-admin   8080           3</span>

<span class="c">#</span>
<span class="nv">$ </span>kubectl get ingress
<span class="c"># =&gt; NAME        CLASS   HOSTS   ADDRESS         PORTS   AGE</span>
<span class="c">#    ingress-1   nginx   *       10.10.200.113   80      18m</span>
 
<span class="nv">$ </span>kubectl describe ingress ingress-1 | <span class="nb">sed</span> <span class="nt">-n</span> <span class="s2">"5, </span><span class="se">\$</span><span class="s2">p"</span>
<span class="c"># =&gt; ...</span>
<span class="c">#    Rules:</span>
<span class="c">#      Host        Path  Backends</span>
<span class="c">#      ----        ----  --------</span>
<span class="c">#      *</span>
<span class="c">#                  /        svc1-web:80 ()</span>
<span class="c">#                  /guest   svc2-guest:8080 ()</span>
<span class="c">#                  /admin   svc3-admin:8080 ()</span>
<span class="c">#    ...</span>

<span class="c"># 접속 로그 확인 : kubetail 설치되어 있음 - 출력되는 nginx 의 로그의 IP 확인</span>
<span class="nv">$ </span>kubetail <span class="nt">-n</span> ingress <span class="nt">-l</span> app.kubernetes.io/component<span class="o">=</span>controller

<span class="nt">-------------------------------</span>
<span class="c"># 자신의 집 PC에서 인그레스를 통한 접속 : 각각 </span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"Ingress1 sv1-web URL = http://</span><span class="si">$(</span>curl <span class="nt">-s</span> ipinfo.io/ip<span class="si">)</span><span class="s2">:30080"</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"Ingress1 sv2-guest URL = http://</span><span class="si">$(</span>curl <span class="nt">-s</span> ipinfo.io/ip<span class="si">)</span><span class="s2">:30080/guest"</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"Ingress1 sv3-admin URL = http://</span><span class="si">$(</span>curl <span class="nt">-s</span> ipinfo.io/ip<span class="si">)</span><span class="s2">:30080/admin"</span>

<span class="c"># svc1-web 접속</span>
<span class="c"># $ MYIP=&lt;EC2 공인 IP 또는 컨트롤플레인 node ip&gt;</span>
<span class="nv">$ MYIP</span><span class="o">=</span>192.168.10.10
<span class="nv">$ </span>curl <span class="nt">-s</span> <span class="nv">$MYIP</span>:30080
<span class="c"># =&gt; ...</span>
<span class="c">#    &amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;</span>
<span class="c">#    &amp;lt;p&amp;gt;If you see this page, the nginx web server is successfully installed and</span>
<span class="c">#    working. Further configuration is required.&amp;lt;/p&amp;gt;</span>
<span class="c">#    </span>
<span class="c">#    &amp;lt;p&amp;gt;For online documentation and support please refer to</span>
<span class="c">#    &amp;lt;a href=&amp;quot;http://nginx.org/&amp;quot;&amp;gt;nginx.org&amp;lt;/a&amp;gt;.&amp;lt;br/&amp;gt;</span>
<span class="c">#    Commercial support is available at</span>
<span class="c">#    &amp;lt;a href=&amp;quot;http://nginx.com/&amp;quot;&amp;gt;nginx.com&amp;lt;/a&amp;gt;.&amp;lt;/p&amp;gt;</span>
<span class="c">#    </span>
<span class="c">#    &amp;lt;p&amp;gt;&amp;lt;em&amp;gt;Thank you for using nginx.&amp;lt;/em&amp;gt;&amp;lt;/p&amp;gt;</span>
<span class="c">#    ...</span>

<span class="c"># svc2-guest 접속</span>
<span class="nv">$ </span>curl <span class="nt">-s</span> <span class="nv">$MYIP</span>:30080/guest
<span class="c"># =&gt; Hello Kubernetes bootcamp! | Running on: deploy2-guestsrv-649875f78b-jcvrf | v=1</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$MYIP</span>:30080/guest <span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
<span class="c"># =&gt;      51 Hello Kubernetes bootcamp! | Running on: deploy2-guestsrv-649875f78b-8wh8r | v=1</span>
<span class="c">#         49 Hello Kubernetes bootcamp! | Running on: deploy2-guestsrv-649875f78b-jcvrf | v=1</span>

<span class="c"># svc3-admin 접속 &gt; 기본적으로 Nginx 는 라운드로빈 부하분산 알고리즘을 사용 &gt;&gt; Client_address 와 XFF 주소는 어떤 주소인가요?</span>
<span class="nv">$ </span>curl <span class="nt">-s</span> <span class="nv">$MYIP</span>:30080/admin
<span class="nv">$ </span>curl <span class="nt">-s</span> <span class="nv">$MYIP</span>:30080/admin | egrep <span class="s1">'(client_address|x-forwarded-for)'</span>
<span class="c"># =&gt;  client_address=172.16.0.16</span>
<span class="c">#     x-forwarded-for=172.16.0.1</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$MYIP</span>:30080/admin | <span class="nb">grep </span>Hostname <span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
<span class="c"># =&gt;      34 Hostname: deploy3-adminsrv-7c8f8b8c87-4sqmh</span>
<span class="c">#         33 Hostname: deploy3-adminsrv-7c8f8b8c87-ztltl</span>
<span class="c">#         33 Hostname: deploy3-adminsrv-7c8f8b8c87-4mzv7</span>

<span class="c"># (옵션) 디플로이먼트의 파드 갯수를 증가/감소 설정 후 접속 테스트 해보자</span>
<span class="nv">$ </span>kubectl scale deployment deploy3-adminsrv <span class="nt">--replicas</span> 2   <span class="c"># svc3-admin의 파드 갯수를 2개로 감소</span>
<span class="c"># =&gt; deployment.apps/deploy3-adminsrv scaled</span>
<span class="nv">$ </span>kubectl get deploy deploy3-adminsrv
<span class="c"># =&gt; NAME               READY   UP-TO-DATE   AVAILABLE   AGE</span>
<span class="c">#    deploy3-adminsrv   2/2     2            2           80m</span>
<span class="c"># &lt;span style="color: green;"&gt;파드수가 3개 =&gt; 2개로 줄었습니다.&lt;/span&gt;</span>
 
<span class="c"># 접속 테스트</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$MYIP</span>:30080/admin | <span class="nb">grep </span>Hostname <span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
<span class="c"># =&gt;      50 Hostname: deploy3-adminsrv-7c8f8b8c87-4sqmh</span>
<span class="c">#         50 Hostname: deploy3-adminsrv-7c8f8b8c87-4mzv7</span>
<span class="c"># &lt;span style="color: green;"&gt;2개로 줄어든 파드수만큼 2개의 파드에 부하가 분산 되는것을 확인하였습니다.&lt;/span&gt;</span>
</code></pre></div></div>

<ul>
  <li>노드에서 패킷 캡쳐 확인 : flannel vxlan의 파드간 통신시 IP정보 확인</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># ngrep을 이용해 패킷 캡쳐</span>
<span class="nv">$ </span>ngrep <span class="nt">-tW</span> byline <span class="nt">-d</span> enp0s8 <span class="s1">''</span> udp port 8472 or tcp port 80
<span class="c"># =&gt; interface: enp0s8 (192.168.10.0/255.255.255.0)</span>
<span class="c">#    filter: ( udp port 8472 or tcp port 80 ) and ((ip || ip6) || (vlan &amp;amp;&amp;amp; (ip || ip6)))</span>
<span class="c">#    #</span>
<span class="c">#    U 2024/10/12 12:42:39.071289 192.168.10.10:39828 -&amp;gt; 192.168.10.102:8472 #1</span>
<span class="c">#    .........(Ce...!w.....E..&amp;lt;..@.?............|.P...........\...........</span>
<span class="c">#    d9?.........</span>
<span class="c">#    #</span>
<span class="c">#    U 2024/10/12 12:42:39.072521 192.168.10.102:37126 -&amp;gt; 192.168.10.10:8472 #2</span>
<span class="c">#    .........!w....(Ce....E..&amp;lt;..@.?............P.|(3c@.......4.y.........</span>
<span class="c">#    ....d9?.....</span>
<span class="c">#    #</span>
<span class="c">#    U 2024/10/12 12:42:39.072734 192.168.10.10:39828 -&amp;gt; 192.168.10.102:8472 #3</span>
<span class="c">#    .........(Ce...!w.....E..4..@.?............|.P....(3cA.....K.....</span>
<span class="c">#    d9?.....</span>
<span class="c">#    #</span>
<span class="c">#    U 2024/10/12 12:42:39.072855 192.168.10.10:39828 -&amp;gt; 192.168.10.102:8472 #4</span>
<span class="c">#    .........(Ce...!w.....E..c..@.?..Y.........|.P....(3cA...........</span>
<span class="c">#    d9?.....GET / HTTP/1.1.</span>
<span class="c">#    Host: localhost:30080.</span>
<span class="c">#    X-Request-ID: e8aa4e70150ae6ae8de5a34637e294e6.</span>
<span class="c">#    X-Real-IP: 172.16.0.1.</span>
<span class="c">#    X-Forwarded-For: 172.16.0.1.</span>
<span class="c">#    X-Forwarded-Host: localhost:30080.</span>
<span class="c">#    X-Forwarded-Port: 80.</span>
<span class="c">#    X-Forwarded-Proto: http.</span>
<span class="c">#    X-Forwarded-Scheme: http.</span>
<span class="c">#    X-Scheme: http.</span>
<span class="c">#    User-Agent: curl/7.81.0.</span>
<span class="c">#    Accept: */*.</span>
<span class="c">#    ...</span>

<span class="c"># tcp dump를 이용해 vxlan(udp 8472) 통신 확인</span>
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> enp0s8 udp port 8472 <span class="nt">-nn</span>
<span class="c"># =&gt; tcpdump: verbose output suppressed, use -v[v]... for full protocol decode</span>
<span class="c">#    listening on enp0s8, link-type EN10MB (Ethernet), snapshot length 262144 bytes</span>
<span class="c">#    12:42:13.504948 IP 192.168.10.10.55617 &amp;gt; 192.168.10.102.8472: OTV, flags [I] (0x08), overlay 0, instance 1</span>
<span class="c">#    IP 172.16.0.16.57692 &amp;gt; 172.16.1.8.80: Flags [S], seq 911277209, win 64860, options [mss 1410,sackOK,TS val 1681447926 ecr 0,nop,wscale 7], length 0</span>
<span class="c">#    ...</span>

<span class="c"># vethY는 각자 k3s-s 의 가장 마지막 veth 를 지정</span>
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> vethY tcp port 8080 <span class="nt">-nn</span>
<span class="c"># =&gt; tcpdump: verbose output suppressed, use -v[v]... for full protocol decode</span>
<span class="c">#    listening on veth5ae3dd58, link-type EN10MB (Ethernet), snapshot length 262144 bytes</span>
<span class="c">#    12:44:11.609334 IP 172.16.0.16.41240 &amp;gt; 172.16.2.9.8080: Flags [S], seq 1593288127, win 64860, options [mss 1410,sackOK,TS val 3487210526 ecr 0,nop,wscale 7], length 0</span>
<span class="c">#    12:44:11.610875 IP 172.16.2.9.8080 &amp;gt; 172.16.0.16.41240: Flags [S.], seq 1820942288, ack 1593288128, win 64308, options [mss 1410,sackOK,TS val 257720908 ecr 3487210526,nop,wscale 7], length 0</span>
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> vethY tcp port 8080 <span class="nt">-w</span> /tmp/ingress-nginx.pcap

<span class="nt">---</span> 

<span class="c"># 다른 터미널에서 svc3-admin 접속</span>
<span class="nv">$ </span>curl <span class="nt">-s</span> <span class="nv">$MYIP</span>:30080/admin 

<span class="nt">---</span>

<span class="c"># 자신의 PC에서 k3s-s EC2 공인 IP로 pcap 다운로드</span>
<span class="c"># $ scp ubuntu@&lt;k3s-s EC2 공인 IP&gt;:/tmp/ingress-nginx.pcap ~/Downloads</span>
<span class="nv">$ </span>scp ubuntu@43.202.1.177:/tmp/ingress-nginx.pcap ~/Downloads
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w6/20241012_kans_w6_10.png" alt="20241012_kans_w6_10.png" class="image-center" />
<em class="image-caption">인그레스를 통한 접속 흐름</em></p>

<ul>
  <li>패킷 캡쳐 결과 ingress controller에서 파드의 ip로 바로 접속 됨을 확인할 수 있었습니다.</li>
  <li>
    <p>또한, flannel CNI를 사용하기 때문에 vxlan을 통해 통신이 이루어지고 있음을 확인할 수 있습니다.</p>
  </li>
  <li>Nginx 파드가 endpoint 정보 등을 모니터링 가능한 이유는 클러스터롤과 롤(엔드포인트 list, watch)를 바인딩된 서비스 어카운트를 파드가 사용하기 때문입니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl describe deployments.apps <span class="nt">-n</span> ingress ingress-nginx-controller | <span class="nb">grep</span> <span class="s1">'Service Account'</span>
<span class="c"># =&gt;   Service Account:  ingress-nginx</span>

<span class="nv">$ </span>kubectl get <span class="nt">-n</span> ingress clusterrolebindings ingress-nginx
<span class="c"># =&gt; NAME            ROLE                        AGE</span>
<span class="c">#    ingress-nginx   ClusterRole/ingress-nginx   4h9m</span>

<span class="nv">$ </span>kubectl get <span class="nt">-n</span> ingress rolebindings ingress-nginx
<span class="c"># =&gt; NAME            ROLE                 AGE</span>
<span class="c">#    ingress-nginx   Role/ingress-nginx   4h9m</span>

<span class="nv">$ </span>kubectl describe clusterrole ingress <span class="nt">-n</span> ingress | egrep <span class="s1">'(Verbs|endpoints)'</span>
<span class="c"># =&gt;   Resources                           Non-Resource URLs  Resource Names  Verbs</span>
<span class="c">#      endpointslices.discovery.k8s.io     []                 []              [list watch get]</span>
<span class="c">#      endpoints                           []                 []              [list watch]</span>

<span class="nv">$ </span>kubectl describe roles ingress-nginx <span class="nt">-n</span> ingress | egrep <span class="s1">'(Verbs|endpoints)'</span>
<span class="c"># =&gt;   Resources                           Non-Resource URLs  Resource Names          Verbs</span>
<span class="c">#      endpoints                           []                 []                      [get list watch]</span>
<span class="c">#      endpointslices.discovery.k8s.io     []                 []                      [list watch get]</span>
</code></pre></div></div>

<h4 id="패킷-분석">패킷 분석</h4>

<ul>
  <li>클러스터 외부에서 접속 후 내부로 접속하는 패킷을 분석해보겠습니다.</li>
  <li>위의 실습과 동일하지만 veth에서 8080을 캡쳐하고 노드의 nic에서 8472 (vxnet)를 캡쳐하여 병합(merge)하여 확인하였습니다.</li>
  <li>또한, 클라이언트의 IP 주소를 확인하기 위해 X-Forwarded-For 헤더를 확인하였습니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w6/20241012_kans_w6_11.png" alt="20241012_kans_w6_11.png" /></p>

<ul>
  <li>위의 그림과 같이 프로토콜의 정보를 그림으로 보려면 아래와 같이 환경설정에서 Appearance &gt; Layout에서 Pane 3에 
“Packet Diagram”을 선택하시면 됩니다.
<img src="/assets/2024/kans-3th/w6/20241012_kans_w6_12.png" alt="20241012_kans_w6_12.png" class="w-80 image-center" /></li>
</ul>

<h4 id="nginx-분산-알고리즘-변경">Nginx 분산 알고리즘 변경</h4>

<ul>
  <li>nginx는 기본 RR(Round Robin) 방식으로 부하분산을 수행하지만, IP-Hash나 Session Cookie 설정으로 변경할 수 있습니다.</li>
  <li>특히 IP-Hash 나 Session Cookie를 사용하면 각 클라이언트에서 대상 파드를 고정할 수 있습니다.</li>
  <li>이를 변경하기 위해서는 <code class="language-plaintext highlighter-rouge">nginx.ingress.kubernetes.io/upstream-hash-by</code> annotation을 사용하여 변경하여야 하는데 실습을 통해 확인해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># mypc</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$MYIP</span>:30080/admin | <span class="nb">grep </span>Hostname <span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
<span class="c"># =&gt;   51 Hostname: deploy3-adminsrv-7c8f8b8c87-4sqmh</span>
<span class="c">#      49 Hostname: deploy3-adminsrv-7c8f8b8c87-4mzv7</span>
<span class="nv">$ </span><span class="k">while </span><span class="nb">true</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nt">--connect-timeout</span> 1 <span class="nv">$MYIP</span>:30080/admin | <span class="nb">grep </span>Hostname <span class="p">;</span> <span class="nb">date</span> <span class="s2">"+%Y-%m-%d %H:%M:%S"</span> <span class="p">;</span> <span class="nb">echo</span> <span class="s2">"--------------"</span> <span class="p">;</span> <span class="nb">sleep </span>1<span class="p">;</span> <span class="k">done</span>

<span class="c"># 아래 ingress 설정 중 IP-Hash 설정 &gt; # 주석 제거</span>
<span class="nv">$ </span><span class="nb">sed</span> <span class="nt">-i</span> <span class="s1">'s/#nginx.ingress/nginx.ingress/g'</span> ingress1.yml
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> ingress1.yml
<span class="c"># =&gt; ingress.networking.k8s.io/ingress-1 configured</span>

<span class="c"># 접속 확인</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$MYIP</span>:30080/admin | <span class="nb">grep </span>Hostname <span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
<span class="c"># =&gt;  100 Hostname: deploy3-adminsrv-7c8f8b8c87-4sqmh</span>
<span class="nv">$ </span><span class="k">while </span><span class="nb">true</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nt">--connect-timeout</span> 1 <span class="nv">$MYIP</span>:30080/admin | <span class="nb">grep </span>Hostname <span class="p">;</span> <span class="nb">date</span> <span class="s2">"+%Y-%m-%d %H:%M:%S"</span> <span class="p">;</span> <span class="nb">echo</span> <span class="s2">"--------------"</span> <span class="p">;</span> <span class="nb">sleep </span>1<span class="p">;</span> <span class="k">done</span>

<span class="c"># 다시 원복(라운드 로빈) &gt; # 주석 추가</span>
<span class="nv">$ </span><span class="nb">sed</span> <span class="nt">-i</span> <span class="s1">'s/nginx.ingress/#nginx.ingress/g'</span> ingress1.yml
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> ingress1.yml
<span class="c"># =&gt; ingress.networking.k8s.io/ingress-1 configured</span>

<span class="c"># 접속 확인</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$MYIP</span>:30080/admin | <span class="nb">grep </span>Hostname <span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
<span class="c"># =&gt;   50 Hostname: deploy3-adminsrv-7c8f8b8c87-4sqmh</span>
<span class="c">#      50 Hostname: deploy3-adminsrv-7c8f8b8c87-4mzv7</span>
<span class="nv">$ </span><span class="k">while </span><span class="nb">true</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nt">--connect-timeout</span> 1 <span class="nv">$MYIP</span>:30080/admin | <span class="nb">grep </span>Hostname <span class="p">;</span> <span class="nb">date</span> <span class="s2">"+%Y-%m-%d %H:%M:%S"</span> <span class="p">;</span> <span class="nb">echo</span> <span class="s2">"--------------"</span> <span class="p">;</span> <span class="nb">sleep </span>1<span class="p">;</span> <span class="k">done</span>
</code></pre></div></div>

<ul>
  <li>ip-hash 설정을 통해 클라이언트의 IP 주소를 해싱하여 특정 파드로 접속되는 것을 확인할 수 있습니다.</li>
  <li>오브젝트 삭제</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl delete deployments,svc,ingress <span class="nt">--all</span>
</code></pre></div></div>

<h4 id="host-기반-라우팅">Host 기반 라우팅</h4>

<ul>
  <li>ingress2.yml 파일 생성</li>
</ul>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">networking.k8s.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Ingress</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">ingress-2</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">ingressClassName</span><span class="pi">:</span> <span class="s">nginx</span>
  <span class="na">rules</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">host</span><span class="pi">:</span> <span class="s">kans.com</span>
    <span class="na">http</span><span class="pi">:</span>
      <span class="na">paths</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">path</span><span class="pi">:</span> <span class="s">/</span>
        <span class="na">pathType</span><span class="pi">:</span> <span class="s">Prefix</span>
        <span class="na">backend</span><span class="pi">:</span>
          <span class="na">service</span><span class="pi">:</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">svc3-admin</span>
            <span class="na">port</span><span class="pi">:</span>
              <span class="na">number</span><span class="pi">:</span> <span class="m">8080</span>
  <span class="pi">-</span> <span class="na">host</span><span class="pi">:</span> <span class="s2">"</span><span class="s">*.kans.com"</span>
    <span class="na">http</span><span class="pi">:</span>
      <span class="na">paths</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">path</span><span class="pi">:</span> <span class="s">/echo</span>
        <span class="na">pathType</span><span class="pi">:</span> <span class="s">Prefix</span>
        <span class="na">backend</span><span class="pi">:</span>
          <span class="na">service</span><span class="pi">:</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">svc3-admin</span>
            <span class="na">port</span><span class="pi">:</span>
              <span class="na">number</span><span class="pi">:</span> <span class="m">8080</span>
</code></pre></div></div>

<ul>
  <li>ingress 정책 생성</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 터미널1</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s1">'kubectl get ingresses,svc,ep,pod -owide'</span>

<span class="c"># 도메인 변경</span>
<span class="c"># $ MYDOMAIN1=&lt;각자 자신의 닉네임의 도메인&gt; 예시) gasida.com</span>
<span class="nv">$ MYDOMAIN1</span><span class="o">=</span>sweetlittlebird.com
<span class="nv">$ </span><span class="nb">sed</span> <span class="nt">-i</span> <span class="s2">"s/kans.com/</span><span class="nv">$MYDOMAIN1</span><span class="s2">/g"</span> ingress2.yaml

<span class="c"># 생성</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> ingress2.yaml,svc3-pod.yaml
<span class="c"># =&gt; ingress.networking.k8s.io/ingress-2 created</span>
<span class="c">#    deployment.apps/deploy3-adminsrv created</span>
<span class="c">#    service/svc3-admin created</span>

<span class="c"># 확인</span>
<span class="nv">$ </span>kubectl get ingress
<span class="c"># =&gt; NAME        CLASS   HOSTS                                       ADDRESS         PORTS   AGE</span>
<span class="c">#    ingress-2   nginx   sweetlittlebird.com,*.sweetlittlebird.com   10.10.200.113   80      14s</span>
<span class="nv">$ </span>kubectl describe ingress ingress-2
<span class="nv">$ </span>kubectl describe ingress ingress-2 | <span class="nb">sed</span> <span class="nt">-n</span> <span class="s2">"5, </span><span class="se">\$</span><span class="s2">p"</span>
<span class="c"># =&gt; ...</span>
<span class="c">#    Default backend:  &amp;lt;default&amp;gt;</span>
<span class="c">#    Rules:</span>
<span class="c">#      Host                   Path    Backends</span>
<span class="c">#      ----                   ----    --------</span>
<span class="c">#      sweetlittlebird.com    /       svc3-admin:8080 ()</span>
<span class="c">#      *.sweetlittlebird.com  /echo   svc3-admin:8080 ()</span>
<span class="c">#    ...</span>
</code></pre></div></div>

<ul>
  <li>Host 기반 라우팅을 통해 서비스에 접속해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 로그 모니터링</span>
<span class="nv">$ </span>kubetail <span class="nt">-n</span> ingress <span class="nt">-l</span> app.kubernetes.io/component<span class="o">=</span>controller
<span class="c"># =&gt; Will tail 1 logs...</span>
<span class="c">#    ingress-nginx-controller-7b67846f8f-jdt65</span>
<span class="c"># =&gt; [ingress-nginx-controller-7b67846f8f-jdt65] 192.168.10.1 - - [01/Jan/2024:13:52:42 +0000] &amp;quot;GET / HTTP/1.1&amp;quot; 200 677 &amp;quot;-&amp;quot; &amp;quot;curl/8.5.0&amp;quot; 88 0.002 [default-svc3-admin-8080] [] 172.16.3.10:8080 852 0.002 200 f22ddba305f55138796fc866f7416890</span>
<span class="c">#    [ingress-nginx-controller-7b67846f8f-jdt65] 192.168.10.1 - - [01/Jan/2024:13:53:47 +0000] &amp;quot;GET /admin HTTP/1.1&amp;quot; 200 687 &amp;quot;-&amp;quot; &amp;quot;curl/8.5.0&amp;quot; 93 0.002 [default-svc3-admin-8080] [] 172.16.2.10:8080 863 0.002 200 966f7a55060f1497eed20818d4bef890</span>
<span class="c">#    ...</span>

<span class="nt">------------</span>
<span class="c"># 자신의 PC 에서 접속 테스트</span>
<span class="c"># svc3-admin 접속 &gt; 결과 확인 : 왜 접속이 되지 않는가? HTTP 헤더에 Host 필드를 잘 확인해보자!</span>
<span class="nv">$ </span>curl <span class="nv">$MYIP</span>:30080 <span class="nt">-v</span>
<span class="nv">$ </span>curl <span class="nv">$MYIP</span>:30080/echo <span class="nt">-v</span>

<span class="c"># mypc에서 접속을 위한 설정</span>
<span class="c">## /etc/hosts 수정 : 도메인 이름으로 접속하기 위해서 변수 지정</span>
<span class="c">## 윈도우 C:\Windows\System32\drivers\etc\hosts</span>
<span class="c">## 맥 sudo vim /etc/hosts</span>
<span class="c"># $ MYDOMAIN1=&lt;각자 자신의 닉네임의 도메인&gt;</span>
<span class="c"># $ MYDOMAIN2=&lt;test.각자 자신의 닉네임의 도메인&gt;</span>
<span class="nv">$ MYDOMAIN1</span><span class="o">=</span>sweetlittlebird.com
<span class="nv">$ MYDOMAIN2</span><span class="o">=</span>test.sweetlittlebird.com
<span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$MYIP</span> <span class="nv">$MYDOMAIN1</span> <span class="nv">$MYDOMAIN2</span>
<span class="c"># =&gt; 192.168.10.10 sweetlittlebird.com test.sweetlittlebird.com</span>

<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"</span><span class="nv">$MYIP</span><span class="s2"> </span><span class="nv">$MYDOMAIN1</span><span class="s2">"</span> | <span class="nb">sudo tee</span> <span class="nt">-a</span> /etc/hosts
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"</span><span class="nv">$MYIP</span><span class="s2"> </span><span class="nv">$MYDOMAIN2</span><span class="s2">"</span> | <span class="nb">sudo tee</span> <span class="nt">-a</span> /etc/hosts
<span class="nv">$ </span><span class="nb">cat</span> /etc/hosts | <span class="nb">grep</span> <span class="nv">$MYDOMAIN1</span>
<span class="c"># =&gt; 192.168.10.10 sweetlittlebird.com</span>
<span class="c">#    192.168.10.10 test.sweetlittlebird.com</span>

<span class="c"># svc3-admin 접속 &gt; 결과 확인</span>
<span class="nv">$ </span>curl <span class="nv">$MYDOMAIN1</span>:30080 <span class="nt">-v</span>
<span class="c"># =&gt; * Host sweetlittlebird.com:30080 was resolved.</span>
<span class="c">#    * IPv4: 192.168.10.10</span>
<span class="c">#    *   Trying 192.168.10.10:30080...</span>
<span class="c">#    * Connected to sweetlittlebird.com (192.168.10.10) port 30080</span>
<span class="c">#    &amp;gt; GET / HTTP/1.1</span>
<span class="c">#    &amp;gt; Host: sweetlittlebird.com:30080</span>
<span class="c">#    &amp;gt; User-Agent: curl/8.5.0</span>
<span class="c">#    &amp;gt; Accept: */*</span>
<span class="c">#    &amp;gt;</span>
<span class="c">#    &amp;lt; HTTP/1.1 200 OK</span>
<span class="c">#    &amp;lt; Date: Sat, 01 Jan 2024 13:52:42 GMT</span>
<span class="c">#    &amp;lt; Content-Type: text/plain</span>
<span class="c">#    &amp;lt; Transfer-Encoding: chunked</span>
<span class="c">#    &amp;lt; Connection: keep-alive</span>
<span class="c">#    &amp;lt;</span>
<span class="c">#    </span>
<span class="c">#    Hostname: deploy3-adminsrv-7c8f8b8c87-48xwp</span>
<span class="c">#    </span>
<span class="c">#    Pod Information:</span>
<span class="c">#            -no pod information available-</span>
<span class="c">#    </span>
<span class="c">#    Server values:</span>
<span class="c">#            server_version=nginx: 1.13.0 - lua: 10008</span>
<span class="c">#    </span>
<span class="c">#    Request Information:</span>
<span class="c">#            client_address=172.16.0.16</span>
<span class="c">#            method=GET</span>
<span class="c">#            real path=/</span>
<span class="c">#            query=</span>
<span class="c">#            request_version=1.1</span>
<span class="c">#            request_uri=http://sweetlittlebird.com:8080/</span>
<span class="c">#    </span>
<span class="c">#    Request Headers:</span>
<span class="c">#            accept=*/*</span>
<span class="c">#            host=sweetlittlebird.com:30080</span>
<span class="c">#            user-agent=curl/8.5.0</span>
<span class="c">#            x-forwarded-for=192.168.10.1</span>
<span class="c">#            x-forwarded-host=sweetlittlebird.com:30080</span>
<span class="c">#            x-forwarded-port=80</span>
<span class="c">#            x-forwarded-proto=http</span>
<span class="c">#            ...</span>
<span class="nv">$ </span>curl <span class="nv">$MYDOMAIN1</span>:30080/admin
<span class="c"># =&gt; </span>
<span class="c">#    </span>
<span class="c">#    Hostname: deploy3-adminsrv-7c8f8b8c87-bm7dq</span>
<span class="c">#            ...</span>
<span class="c">#            client_address=172.16.0.16</span>
<span class="c">#            method=GET</span>
<span class="c">#            real path=/admin</span>
<span class="c">#            ...</span>
<span class="c">#            request_uri=http://sweetlittlebird.com:8080/admin</span>
<span class="c">#    </span>
<span class="c">#    Request Headers:</span>
<span class="c">#            accept=*/*</span>
<span class="c">#            host=sweetlittlebird.com:30080</span>
<span class="c">#            user-agent=curl/8.5.0</span>
<span class="c">#            x-forwarded-for=192.168.10.1</span>
<span class="c">#            x-forwarded-host=sweetlittlebird.com:30080</span>
<span class="c">#            x-forwarded-port=80</span>
<span class="c">#            x-forwarded-proto=http</span>
<span class="c">#            x-forwarded-scheme=http</span>
<span class="c">#            ...</span>
<span class="nv">$ </span>curl <span class="nv">$MYDOMAIN1</span>:30080/echo
<span class="c"># =&gt; Hostname: deploy3-adminsrv-7c8f8b8c87-ndwkj</span>
<span class="c">#            ...</span>
<span class="c">#            client_address=172.16.0.16</span>
<span class="c">#            method=GET</span>
<span class="c">#            real path=/echo</span>
<span class="c">#            ...</span>
<span class="c">#            request_uri=http://sweetlittlebird.com:8080/echo</span>
<span class="c">#    </span>
<span class="c">#    Request Headers:</span>
<span class="c">#            accept=*/*</span>
<span class="c">#            host=sweetlittlebird.com:30080</span>
<span class="c">#            user-agent=curl/8.5.0</span>
<span class="c">#            x-forwarded-for=192.168.10.1</span>
<span class="c">#            x-forwarded-host=sweetlittlebird.com:30080</span>
<span class="c">#            x-forwarded-port=80</span>
<span class="c">#            x-forwarded-proto=http</span>
<span class="c">#            x-forwarded-scheme=http</span>
<span class="c">#            ...    </span>
<span class="nv">$ </span>curl <span class="nv">$MYDOMAIN1</span>:30080/echo/1
<span class="c"># =&gt; Hostname: deploy3-adminsrv-7c8f8b8c87-48xwp</span>
<span class="c">#    </span>
<span class="c">#    Pod Information:</span>
<span class="c">#            -no pod information available-</span>
<span class="c">#    </span>
<span class="c">#    Server values:</span>
<span class="c">#            server_version=nginx: 1.13.0 - lua: 10008</span>
<span class="c">#    </span>
<span class="c">#    Request Information:</span>
<span class="c">#            client_address=172.16.0.16</span>
<span class="c">#            method=GET</span>
<span class="c">#            real path=/echo/1</span>
<span class="c">#            query=</span>
<span class="c">#            request_version=1.1</span>
<span class="c">#            request_uri=http://sweetlittlebird.com:8080/echo/1</span>
<span class="c">#            ...</span>

<span class="nv">$ </span>curl <span class="nv">$MYDOMAIN2</span>:30080 <span class="nt">-v</span>
<span class="c"># =&gt; * Host test.sweetlittlebird.com:30080 was resolved.</span>
<span class="c">#    * IPv4: 192.168.10.10</span>
<span class="c">#    *   Trying 192.168.10.10:30080...</span>
<span class="c">#    * Connected to test.sweetlittlebird.com (192.168.10.10) port 30080</span>
<span class="c">#    &amp;gt; GET / HTTP/1.1</span>
<span class="c">#    &amp;gt; Host: test.sweetlittlebird.com:30080</span>
<span class="c">#    ...</span>
<span class="c">#    &amp;lt; HTTP/1.1 404 Not Found</span>
<span class="c">#    ...</span>
<span class="nv">$ </span>curl <span class="nv">$MYDOMAIN2</span>:30080/admin
<span class="c"># =&gt; &amp;lt;html&amp;gt;</span>
<span class="c">#    &amp;lt;head&amp;gt;&amp;lt;title&amp;gt;404 Not Found&amp;lt;/title&amp;gt;&amp;lt;/head&amp;gt;</span>
<span class="c">#    &amp;lt;body&amp;gt;</span>
<span class="c">#    &amp;lt;center&amp;gt;&amp;lt;h1&amp;gt;404 Not Found&amp;lt;/h1&amp;gt;&amp;lt;/center&amp;gt;</span>
<span class="c">#    &amp;lt;hr&amp;gt;&amp;lt;center&amp;gt;nginx&amp;lt;/center&amp;gt;</span>
<span class="c">#    &amp;lt;/body&amp;gt;</span>
<span class="c">#    &amp;lt;/html&amp;gt;</span>
<span class="nv">$ </span>curl <span class="nv">$MYDOMAIN2</span>:30080/echo
<span class="c"># =&gt; Hostname: deploy3-adminsrv-7c8f8b8c87-ndwkj</span>
<span class="c">#    </span>
<span class="c">#    Pod Information:</span>
<span class="c">#            -no pod information available-</span>
<span class="c">#    </span>
<span class="c">#    Server values:</span>
<span class="c">#            server_version=nginx: 1.13.0 - lua: 10008</span>
<span class="c">#    </span>
<span class="c">#    Request Information:</span>
<span class="c">#            client_address=172.16.0.16</span>
<span class="c">#            method=GET</span>
<span class="c">#            real path=/echo</span>
<span class="c">#            ...</span>
<span class="c">#            request_uri=http://test.sweetlittlebird.com:8080/echo</span>
<span class="c">#    </span>
<span class="c">#    Request Headers:</span>
<span class="c">#            accept=*/*</span>
<span class="c">#            host=test.sweetlittlebird.com:30080</span>
<span class="c">#            user-agent=curl/8.5.0</span>
<span class="c">#            x-forwarded-for=192.168.10.1</span>
<span class="c">#            x-forwarded-host=test.sweetlittlebird.com:30080</span>
<span class="c">#            x-forwarded-port=80</span>
<span class="c">#            x-forwarded-proto=http</span>
<span class="c">#            x-forwarded-scheme=http</span>
<span class="c">#            ...</span>
<span class="nv">$ </span>curl <span class="nv">$MYDOMAIN2</span>:30080/echo/1
<span class="nv">$ </span>curl <span class="nv">$MYDOMAIN2</span>:30080/echo/1/2

<span class="c">## (옵션) /etc/hosts 파일 변경 없이 접속 방안</span>
<span class="nv">$ </span>curl <span class="nt">-H</span> <span class="s2">"host: </span><span class="nv">$MYDOMAIN1</span><span class="s2">"</span> <span class="nv">$MYIP</span>:30080
<span class="c"># =&gt; (정상)</span>
<span class="nv">$ </span>curl <span class="nt">-H</span> <span class="s2">"host: </span><span class="nv">$MYDOMAIN2</span><span class="s2">"</span> <span class="nv">$MYIP</span>:30080
<span class="c"># =&gt; (404 에러)</span>
<span class="nv">$ </span>curl <span class="nt">-H</span> <span class="s2">"host: </span><span class="nv">$MYDOMAIN2</span><span class="s2">"</span> <span class="nv">$MYIP</span>:30080/echo
<span class="c"># =&gt; (정상 응답 옴)</span>
</code></pre></div></div>

<ul>
  <li>실습결과 sweetlittlebird.com으로는 모든 응답이 200 OK 응답이 오고,
test.sweetlittlebird.com으로 접속시에는 /echo 경로로 접속해야만 200 OK 응답이 오고, 그 외의 경로로 접속시에는 404 에러가 발생하는 것을 확인할 수 있습니다.</li>
  <li>아래의 룰대로 잘 접속이 되는 것을 확인할 수 있습니다.
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># sweetlittlebird.com이라는 호스트로 접속시 모든 경로에 대해서 200 OK 응답</span>
sweetlittlebird.com    /       svc3-admin:8080   
  
<span class="c"># test.sweetlittlebird.com 처럼 서브 도메인이 있는 호스트명으로 접속시 /echo 경로로만 200 OK 응답</span>
<span class="k">*</span>.sweetlittlebird.com  /echo   svc3-admin:8080   
</code></pre></div>    </div>
  </li>
  <li>오브젝트 삭제
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl delete deployments,svc,ingress <span class="nt">--all</span>
</code></pre></div>    </div>
  </li>
</ul>

<h4 id="카나리-업데이트">카나리 업데이트</h4>

<ul>
  <li>카나리 업데이트는 새로운 버전의 파드를  배포하고, 일부 트래픽만 새로운 버전으로 전환하고, 새로운 버전의 정상동작 확인 후 전체를 새로운 버전으로 전환하는 업데이트 방식입니다.</li>
  <li>배포 자동화시 최소 중단/무중단으로 하는 방법을 몇가지 살펴보겠습니다.
    <ol>
      <li>롤링 업데이트
  <img src="/assets/2024/kans-3th/w6/20241012_kans_w6_13.png" alt="img.png" class="w-80 image-center" />
        <ul>
          <li>파드를 하나씩 새로운 버전으로 교체하는 방식으로, 기존 버전의 파드가 정상동작하는지 확인 후 다음 파드로 교체하는 방식입니다.</li>
        </ul>
      </li>
      <li>카나리 업데이트
  <img src="/assets/2024/kans-3th/w6/20241012_kans_w6_14.png" alt="img_1.png" class="w-80 image-center" />
        <ul>
          <li>일부 트래픽을 새로운 버전으로 전환하고, 정상동작 확인 후 전체로 전환하는 방식입니다.</li>
        </ul>
      </li>
      <li>블루/그린 업데이트
  <img src="/assets/2024/kans-3th/w6/20241012_kans_w6_15.png" alt="img_2.png" class="w-80 image-center" />
        <ul>
          <li>새로운 버전의 파드를 새로운 서비스로 배포하고, 모든 파드 배포 후, 하나씩 전환하는 롤링 업데이트와는 다르게 전체 트래픽을 한꺼번에 새로운 서비스로 전환하는 방식입니다.</li>
        </ul>
      </li>
    </ol>
  </li>
  <li>실습을 통해 nginx ingress controller를 이용한 카나리 업데이트를 진행해보겠습니다.
    <ul>
      <li>
        <p>canary-svc1-pod.yml 파일 생성</p>

        <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">dp-v1</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">3</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">svc-v1</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">app</span><span class="pi">:</span> <span class="s">svc-v1</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">containers</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">pod-v1</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">k8s.gcr.io/echoserver:1.5</span>
          <span class="na">ports</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">8080</span>
      <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">svc-v1</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">ports</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">web-port</span>
      <span class="na">port</span><span class="pi">:</span> <span class="m">9001</span>
      <span class="na">targetPort</span><span class="pi">:</span> <span class="m">8080</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">svc-v1</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>canary-svc2-pod.yml 파일 생성</p>

        <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">dp-v2</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">3</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">svc-v2</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">app</span><span class="pi">:</span> <span class="s">svc-v2</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">containers</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">pod-v2</span>
        <span class="na">image</span><span class="pi">:</span> <span class="s">k8s.gcr.io/echoserver:1.6</span>
        <span class="na">ports</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">8080</span>
      <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">svc-v2</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">ports</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">web-port</span>
      <span class="na">port</span><span class="pi">:</span> <span class="m">9001</span>
      <span class="na">targetPort</span><span class="pi">:</span> <span class="m">8080</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">svc-v2</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>생성 및 확인</p>

        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 터미널1</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s1">'kubectl get ingress,svc,ep,pod -owide'</span>
    
<span class="c"># 생성</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> canary-svc1-pod.yml,canary-svc2-pod.yml
<span class="c"># =&gt; deployment.apps/dp-v1 created</span>
<span class="c">#    service/svc-v1 created</span>
<span class="c">#    deployment.apps/dp-v2 created</span>
<span class="c">#    service/svc-v2 created</span>
    
<span class="c"># 확인</span>
<span class="nv">$ </span>kubectl get svc,ep,pod
<span class="c"># =&gt; NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE</span>
<span class="c">#    service/kubernetes   ClusterIP   10.10.200.1     &amp;lt;none&amp;gt;        443/TCP    12m</span>
<span class="c">#    service/svc-v1       ClusterIP   10.10.200.231   &amp;lt;none&amp;gt;        9001/TCP   18s</span>
<span class="c">#    service/svc-v2       ClusterIP   10.10.200.216   &amp;lt;none&amp;gt;        9001/TCP   18s</span>
<span class="c">#    </span>
<span class="c">#    NAME                   ENDPOINTS                                            AGE</span>
<span class="c">#    endpoints/kubernetes   192.168.10.10:6443                                   12m</span>
<span class="c">#    endpoints/svc-v1       172.16.1.10:8080,172.16.2.12:8080,172.16.3.11:8080   18s</span>
<span class="c">#    endpoints/svc-v2       172.16.1.11:8080,172.16.2.11:8080,172.16.3.12:8080   18s</span>
<span class="c">#    </span>
<span class="c">#    NAME                         READY   STATUS    RESTARTS   AGE</span>
<span class="c">#    pod/dp-v1-8684d45558-22nbv   1/1     Running   0          18s</span>
<span class="c">#    pod/dp-v1-8684d45558-59pnl   1/1     Running   0          18s</span>
<span class="c">#    pod/dp-v1-8684d45558-87xrs   1/1     Running   0          18s</span>
<span class="c">#    pod/dp-v2-7757c4bdc-5xmcm    1/1     Running   0          18s</span>
<span class="c">#    pod/dp-v2-7757c4bdc-bm2gq    1/1     Running   0          18s</span>
<span class="c">#    pod/dp-v2-7757c4bdc-h7fzl    1/1     Running   0          18s</span>
    
<span class="c"># 파드 버전 확인: 1.13.0 vs 1.13.1</span>
<span class="nv">$ </span><span class="k">for </span>pod <span class="k">in</span> <span class="si">$(</span>kubectl get pod <span class="nt">-o</span> wide <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>svc-v1 |awk <span class="s1">'NR&gt;1 {print $6}'</span><span class="si">)</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$pod</span>:8080 | egrep <span class="s1">'(Hostname|nginx)'</span><span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; Hostname: dp-v1-8684d45558-22nbv</span>
<span class="c">#     server_version=nginx: 1.13.0 - lua: 10008</span>
<span class="c">#    ...</span>
<span class="nv">$ </span><span class="k">for </span>pod <span class="k">in</span> <span class="si">$(</span>kubectl get pod <span class="nt">-o</span> wide <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>svc-v2 |awk <span class="s1">'NR&gt;1 {print $6}'</span><span class="si">)</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$pod</span>:8080 | egrep <span class="s1">'(Hostname|nginx)'</span><span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; Hostname: dp-v2-7757c4bdc-5xmcm</span>
<span class="c">#     server_version=nginx: 1.13.1 - lua: 10008</span>
<span class="c">#    ...		</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>canary-ingress1.yml</p>

        <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">networking.k8s.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Ingress</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">ingress-canary-v1</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">ingressClassName</span><span class="pi">:</span> <span class="s">nginx</span>
  <span class="na">rules</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">host</span><span class="pi">:</span> <span class="s">kans.com</span>
    <span class="na">http</span><span class="pi">:</span>
      <span class="na">paths</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">path</span><span class="pi">:</span> <span class="s">/</span>
        <span class="na">pathType</span><span class="pi">:</span> <span class="s">Prefix</span>
        <span class="na">backend</span><span class="pi">:</span>
          <span class="na">service</span><span class="pi">:</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">svc-v1</span>
            <span class="na">port</span><span class="pi">:</span>
              <span class="na">number</span><span class="pi">:</span> <span class="m">8080</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>canary-ingress2.yml</p>

        <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">networking.k8s.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Ingress</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">ingress-canary-v2</span>
  <span class="na">annotations</span><span class="pi">:</span>
    <span class="na">nginx.ingress.kubernetes.io/canary</span><span class="pi">:</span> <span class="s2">"</span><span class="s">true"</span>
    <span class="na">nginx.ingress.kubernetes.io/canary-weight</span><span class="pi">:</span> <span class="s2">"</span><span class="s">10"</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">ingressClassName</span><span class="pi">:</span> <span class="s">nginx</span>
  <span class="na">rules</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">host</span><span class="pi">:</span> <span class="s">kans.com</span>
    <span class="na">http</span><span class="pi">:</span>
      <span class="na">paths</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">path</span><span class="pi">:</span> <span class="s">/</span>
        <span class="na">pathType</span><span class="pi">:</span> <span class="s">Prefix</span>
        <span class="na">backend</span><span class="pi">:</span>
          <span class="na">service</span><span class="pi">:</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">svc-v2</span>
            <span class="na">port</span><span class="pi">:</span>
              <span class="na">number</span><span class="pi">:</span> <span class="m">8080</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>카나리 업그레이드 확인</p>
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 터미널1</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s1">'kubectl get ingress,svc,ep'</span>
    
<span class="c"># 도메인 변경</span>
<span class="c"># $ MYDOMAIN1=&lt;각자 자신의 닉네임의 도메인&gt; 예시) gasida.com</span>
<span class="nv">$ MYDOMAIN1</span><span class="o">=</span>sweetlittlebird.com
<span class="nv">$ </span><span class="nb">sed</span> <span class="nt">-i</span> <span class="s2">"s/kans.com/</span><span class="nv">$MYDOMAIN1</span><span class="s2">/g"</span> canary-ingress1.yml
<span class="nv">$ </span><span class="nb">sed</span> <span class="nt">-i</span> <span class="s2">"s/kans.com/</span><span class="nv">$MYDOMAIN1</span><span class="s2">/g"</span> canary-ingress2.yml
    
<span class="c"># 생성</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> canary-ingress1.yml,canary-ingress2.yml
<span class="c"># =&gt; ingress.networking.k8s.io/ingress-canary-v1 created</span>
<span class="c">#    ingress.networking.k8s.io/ingress-canary-v2 created</span>
    
<span class="c"># 로그 모니터링</span>
<span class="nv">$ </span>kubetail <span class="nt">-n</span> ingress <span class="nt">-l</span> app.kubernetes.io/component<span class="o">=</span>controller
    
<span class="c"># 접속 테스트</span>
<span class="nv">$ </span>curl <span class="nt">-s</span> <span class="nv">$MYDOMAIN1</span>:30080
<span class="nv">$ </span>curl <span class="nt">-s</span> <span class="nv">$MYDOMAIN1</span>:30080 | <span class="nb">grep </span>nginx
<span class="c"># =&gt; &amp;lt;hr&amp;gt;&amp;lt;center&amp;gt;nginx&amp;lt;/center&amp;gt;</span>
    
<span class="c"># 접속 시 v1 v2 버전별 비율이 어떻게 되나요? 왜 이렇게 되나요?</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span>  <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$MYDOMAIN1</span>:30080 | <span class="nb">grep </span>nginx <span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
<span class="c"># =&gt;      84         server_version=nginx: 1.13.0 - lua: 10008</span>
<span class="c">#         16         server_version=nginx: 1.13.1 - lua: 10008</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..1000<span class="o">}</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$MYDOMAIN1</span>:30080 | <span class="nb">grep </span>nginx <span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
<span class="c"># =&gt;     919         server_version=nginx: 1.13.0 - lua: 10008</span>
<span class="c">#         81         server_version=nginx: 1.13.1 - lua: 10008</span>
<span class="nv">$ </span><span class="k">while </span><span class="nb">true</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nt">--connect-timeout</span> 1 <span class="nv">$MYDOMAIN1</span>:30080 | <span class="nb">grep </span>Hostname <span class="p">;</span> <span class="nb">echo</span> <span class="s2">"--------------"</span> <span class="p">;</span> <span class="nb">date</span> <span class="s2">"+%Y-%m-%d %H:%M:%S"</span> <span class="p">;</span> <span class="nb">sleep </span>1<span class="p">;</span> <span class="k">done</span>
<span class="c"># &lt;span style="color: green;"&gt;👉 v2의 canary 비율을 10%로 두어서 그렇습니다.&lt;/span&gt;</span>
    
<span class="c"># 비율 조정하여 절반을 v2로 전환하겠습니다. &gt;&gt; 개발 배포 버전 전략에 유용하다!</span>
<span class="nv">$ </span>kubectl annotate <span class="nt">--overwrite</span> ingress ingress-canary-v2 nginx.ingress.kubernetes.io/canary-weight<span class="o">=</span>50
    
<span class="c"># 접속 테스트</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span>  <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$MYDOMAIN1</span>:30080 | <span class="nb">grep </span>nginx <span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
<span class="c"># =&gt;      53         server_version=nginx: 1.13.1 - lua: 10008</span>
<span class="c">#         47         server_version=nginx: 1.13.0 - lua: 10008</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..1000<span class="o">}</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$MYDOMAIN1</span>:30080 | <span class="nb">grep </span>nginx <span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
<span class="c"># =&gt;     526         server_version=nginx: 1.13.1 - lua: 10008</span>
<span class="c">#        474         server_version=nginx: 1.13.0 - lua: 10008</span>
        
<span class="c"># (옵션) 비율 조정 &lt;&lt; 어떻게 비율이 조정될까요?</span>
<span class="nv">$ </span>kubectl annotate <span class="nt">--overwrite</span> ingress ingress-canary-v2 nginx.ingress.kubernetes.io/canary-weight<span class="o">=</span>100
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span>  <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$MYDOMAIN1</span>:30080 | <span class="nb">grep </span>nginx <span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
<span class="c"># =&gt;    100         server_version=nginx: 1.13.1 - lua: 10008</span>
<span class="c"># &lt;span style="color: green;"&gt;👉 카나리 비율을 100%로 하니 v2버전으로 100% 전환 되었습니다.&lt;/span&gt;</span>
    
<span class="c"># (옵션) 비율 조정 &lt;&lt; 어떻게 비율이 조정될까요?</span>
<span class="nv">$ </span>kubectl annotate <span class="nt">--overwrite</span> ingress ingress-canary-v2 nginx.ingress.kubernetes.io/canary-weight<span class="o">=</span>0
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span>  <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$MYDOMAIN1</span>:30080 | <span class="nb">grep </span>nginx <span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
<span class="c"># =&gt;     100         server_version=nginx: 1.13.0 - lua: 10008</span>
<span class="c"># &lt;span style="color: green;"&gt;👉 카나리 비율을 0%로 하니 v2버전으로 0% 로 전환 되고 모든 트래픽이 v1으로 전달되었습니다.&lt;/span&gt;</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>오브젝트 삭제
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl delete deployments,svc,ingress <span class="nt">--all</span>
</code></pre></div>    </div>
  </li>
</ul>

<hr />

<h2 id="gateway-api">Gateway API</h2>

<h3 id="gateway-api-소개">Gateway API 소개</h3>

<p>앞서 Ingress를 살펴볼때 말씀드린것 처럼 Ingress는 Frozen 되어서 더이상 업데이트 되지 않고, Gateway API에 기능을 추가할 계획이라고 합니다.
이어서 Gateway API에 대해 알아보겠습니다.</p>

<p>Gateway API는 Kubernetes에서 API Gateway를 정의하고 구성하기 위한 API를 제공하는 프로젝트입니다. <a href="https://medium.com/@disha.20.10/introduction-to-the-gateway-api-revolutionizing-kubernetes-networking-7b0c9a696038">Gateway API 소개</a>
Gateway API는 서비스 메시(예) istio 등)에서 제공하는 풍부한 기능 중 일부 기능들과 운영 관리에 필요한 기능들을 추가하였습니다.
추가된 기능의 예로는 헤더 기반 라우팅, 헤더 변조, 트래픽 미러링(쉽게 트래픽 복제), 역할 기반 접근 제어 등이 있습니다.</p>

<p><img src="/assets/2024/kans-3th/w6/20241012_kans_w6_16.png" alt="img.png" class="w-80 image-center" /></p>

<p>Gateway API는 이를 통해 동적 인프라 구성을 지원하고, 고급 트래픽 라우팅을 지원합니다.</p>

<ul>
  <li>Gateway API의 주요 기능은 다음과 같습니다.
    <ol>
      <li>
        <dl>
          <dt><strong>개선된 리소스 모델</strong></dt>
          <dd>API는 GatewayClass, Gateway 및 Route(HTTPRoute, TCPRoute 등)와 같은 새로운 사용자 정의 리소스를 도입하여 라우팅 규칙을 정의하는 보다 세부적이고 표현력 있는 방법을 제공합니다.</dd>
        </dl>
      </li>
      <li>
        <dl>
          <dt><strong>프로토콜 독립적</strong></dt>
          <dd>주로 HTTP용으로 설계된 Ingress와 달리 Gateway API는 TCP, UDP, TLS를 포함한 여러 프로토콜을 지원합니다.</dd>
        </dl>
      </li>
      <li>
        <dl>
          <dt><strong>강화된 보안</strong></dt>
          <dd>TLS 구성 및 보다 세부적인 액세스 제어에 대한 기본 제공 지원.</dd>
        </dl>
      </li>
      <li>
        <dl>
          <dt><strong>교차 네임스페이스 지원</strong></dt>
          <dd>서로 다른 네임스페이스의 서비스로 트래픽을 라우팅하여 보다 유연한 아키텍처를 구축할 수 있는 기능을 제공합니다.</dd>
        </dl>
      </li>
      <li>
        <dl>
          <dt><strong>확장성</strong></dt>
          <dd>API는 사용자 정의 리소스 및 정책으로 쉽게 확장할 수 있도록 설계되었습니다.</dd>
        </dl>
      </li>
      <li>
        <dl>
          <dt><strong>역할 지향</strong></dt>
          <dd>클러스터 운영자, 애플리케이션 개발자, 보안 팀 간의 우려를 명확하게 분리합니다.</dd>
        </dl>
      </li>
    </ol>
  </li>
  <li>다음의 구성요소 (Resource)를 가집니다.
    <ul>
      <li>GatewayClass, Gateway, HTTPRoute, TCPRoute, Service
<img src="/assets/2024/kans-3th/w6/20241012_kans_w6_17.png" alt="img.png" />
        <ul>
          <li><strong>GatewayClass:</strong> 공통 구성을 가진 게이트웨이 세트를 정의하고 클래스를 구현하는 컨트롤러에 의해 관리됩니다.</li>
          <li><strong>Gateway:</strong> 클라우드 로드 밸런서와 같은 트래픽 처리 인프라의 인스턴스를 정의합니다.</li>
          <li><strong>HTTPRoute:</strong> Gateway 리스너에서 백엔드 네트워크 엔드포인트의 표현으로 트래픽을 매핑하기 위한 HTTP 전용 규칙을 정의합니다. 이러한 엔드포인트는 종종 <a href="https://kubernetes.io/docs/concepts/services-networking/service/">Service</a>로 표현됩니다<br />
<img src="/assets/2024/kans-3th/w6/20241012_kans_w6_18.png" alt="img.png" class="image-center" />
<em class="image-caption">출처 : <a href="https://gateway-api.sigs.k8s.io/">https://gateway-api.sigs.k8s.io/</a></em></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>리퀘스트 흐름
<img src="/assets/2024/kans-3th/w6/20241012_kans_w6_19.svg" alt="20241012_kans_w6_19.svg" /></li>
  <li>role-oriented  API 가 중요한 이유
    <ul>
      <li>담당 업무의 역할에 따라서 동작/권한을 유연하게 제공할 수 있습니다.</li>
      <li>아래 그림 처럼 ‘스토어 개발자’는 Store 네임스페이스내에서 해당 store PATH 라우팅 관련 정책을 스스로 관리 할 수 있습니다.
<img src="/assets/2024/kans-3th/w6/20241012_kans_w6_20.png" alt="img.png" /></li>
      <li>역할의 예시입니다.
        <ul>
          <li><strong>인프라 제공자:</strong> 여러 격리된 클러스터가 여러 테넌트를 서비스할 수 있도록 인프라를 관리합니다. 예: 클라우드 제공자.</li>
          <li><strong>클러스터 운영자:</strong> 클러스터를 관리하며 주로 정책, 네트워크 접근, 애플리케이션 권한 등을 관리합니다.</li>
          <li><strong>애플리케이션 개발자:</strong> 클러스터에서 실행되는 애플리케이션을 관리하며 주로 애플리케이션 수준의 구성 및 <a href="https://kubernetes.io/docs/concepts/services-networking/service/">서비스</a> 구성에 관심이 있습니다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>추천글
    <ul>
      <li><a href="https://www.anyflow.net/sw-engineer/kubernetes-gateway-api-1">Ingress + API Gateway = Kubernetes Gateway API</a></li>
      <li><a href="https://www.anyflow.net/sw-engineer/kubernetes-gateway-api-2">API Gateway + Service Mesh = Kubernetes Gateway API</a></li>
    </ul>
  </li>
</ul>

<h3 id="gloo-gateway">Gloo Gateway</h3>

<p>Gloo Gateway는 Solo.io에서 개발한 API Gateway로, Gateway API를 구현한 대표적인 제품 중 하나입니다. 
Gloo Gateway는 다양한 환경에서 사용할 수 있도록 설계되어 있으며, 다음과 같은 특징을 가지고 있습니다.</p>

<ul>
  <li><strong>Envoy Proxy 기반</strong> : Gloo Gateway는 고성능의 Envoy Proxy를 기반으로 하여 뛰어난 확장성과 성능을 제공합니다.</li>
  <li><strong>API 관리 및 라우팅</strong> : 다양한 API 라우팅 옵션을 제공하며, REST, gRPC, GraphQL 등의 다양한 프로토콜을 지원합니다. 이를 통해 복잡한 트래픽 관리와 라우팅이 가능합니다.</li>
  <li><strong>보안 기능</strong> : 인증, 인가, TLS 암호화, OAuth, OpenID Connect 등 다양한 보안 기능을 제공합니다. 이를 통해 API를 안전하게 보호할 수 있습니다.</li>
  <li><strong>확장성</strong> : 플러그인 아키텍처를 통해 쉽게 확장할 수 있으며, 다양한 서드파티 통합을 지원합니다. 필요에 따라 기능을 확장하거나 사용자 정의 기능을 추가할 수 있습니다.</li>
  <li><strong>서비스 디스커버리</strong> : Kubernetes, Consul, EC2 등 다양한 서비스 디스커버리 메커니즘을 지원하여 동적 환경에서도 효율적으로 작동합니다.</li>
  <li><strong>Observability</strong> : 트래픽 모니터링, 로깅, 트레이싱 등의 기능을 제공하여 운영 중인 시스템의 상태를 쉽게 파악하고 문제를 해결할 수 있습니다.</li>
  <li><strong>유연한</strong> 배포 : 클라우드, 온프레미스, 하이브리드 환경 등 다양한 배포 옵션을 지원합니다. 이를 통해 다양한 인프라 환경에 맞춰 유연하게 배포할 수 있습니다.</li>
</ul>

<h4 id="gloo-gateway-architecture">Gloo Gateway Architecture</h4>

<p><img src="/assets/2024/kans-3th/w6/20241012_kans_w6_21.png" alt="img.png" /></p>

<ul>
  <li>Envoy를 통해서 Http의 L7 라우팅을 지원하며, Gloo의 역할은 라우팅 규칙을 관리하고 Envoy에 전달하는 역할을 합니다.</li>
</ul>

<p>Gloo Gateway는 내용이 방대하기 때문에 아래의 링크들로 설명을 대체하겠습니다.</p>

<ul>
  <li><a href="https://www.solo.io/blog/">Gloo Blog</a></li>
  <li><a href="https://docs.solo.io/gateway/latest/quickstart/">Docs</a></li>
  <li><a href="https://www.solo.io/blog/gateway-api-tutorial-blog/">https://www.solo.io/blog/gateway-api-tutorial-blog/</a></li>
  <li><a href="https://www.solo.io/blog/gateway-api-workshop/">https://www.solo.io/blog/gateway-api-workshop/</a></li>
  <li><a href="https://www.solo.io/blog/fast-and-furious-gateway-api-at-scale-with-envoy-proxy-and-gloo-gateway/">https://www.solo.io/blog/fast-and-furious-gateway-api-at-scale-with-envoy-proxy-and-gloo-gateway/</a></li>
  <li><a href="https://www.solo.io/blog/getting-the-most-out-of-gateway-api-lessons-learned-from-gloo-migrations/">https://www.solo.io/blog/getting-the-most-out-of-gateway-api-lessons-learned-from-gloo-migrations/</a></li>
  <li><a href="https://www.solo.io/blog/solving-an-information-leakage-problem-with-the-envoy-extproc-filter-and-kube-gateway-api/">https://www.solo.io/blog/solving-an-information-leakage-problem-with-the-envoy-extproc-filter-and-kube-gateway-api/</a></li>
  <li><a href="https://www.solo.io/blog/gateway-api-gitops-argo-tutorial-blog/">https://www.solo.io/blog/gateway-api-gitops-argo-tutorial-blog/</a></li>
</ul>

<h4 id="실습">실습</h4>

<p>실습을 통해 Gloo Gateway를 설치하고, Gateway API를 사용해보겠습니다.
실습은 <a href="https://www.solo.io/blog/gateway-api-tutorial-blog/">[Tutorial] Hands-On with the Kubernetes Gateway API and Envoy Proxy</a>를 참고하였습니다.
kind를 통해서 실습할 수 있도록 잘 구성되었고 30분 정도면 따라할 수 있다고 합니다.</p>

<h5 id="install">Install</h5>

<p><strong>Install KinD Cluster</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOT</span><span class="sh">&gt; kind-1node.yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  extraPortMappings:
  - containerPort: 30000
    hostPort: 30000
  - containerPort: 30001
    hostPort: 30001
  - containerPort: 30002
    hostPort: 30002
</span><span class="no">EOT

</span><span class="c"># Install KinD Cluster</span>
<span class="nv">$ </span>kind create cluster <span class="nt">--image</span> kindest/node:v1.30.0 <span class="nt">--config</span> kind-1node.yaml <span class="nt">--name</span> myk8s

<span class="c"># 노드에 기본 툴 설치</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane sh <span class="nt">-c</span> <span class="s1">'apt update &amp;&amp; apt install tree psmisc lsof wget bsdmainutils bridge-utils net-tools dnsutils tcpdump ngrep iputils-ping git vim -y'</span>

<span class="c"># 노드/파드 확인</span>
<span class="nv">$ </span>kubectl get nodes <span class="nt">-o</span> wide
<span class="c"># =&gt; NAME                  STATUS   ROLES           AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                         KERNEL-VERSION     CONTAINER-RUNTIME</span>
<span class="c">#    myk8s-control-plane   Ready    control-plane   32s   v1.30.0   172.20.0.2    &amp;lt;none&amp;gt;        Debian GNU/Linux 12 (bookworm)   5.10.76-linuxkit   containerd://1.7.15</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-A</span>
<span class="c"># =&gt; NAMESPACE            NAME                                          READY   STATUS    RESTARTS   AGE</span>
<span class="c">#    kube-system          coredns-7db6d8ff4d-45mzg                      1/1     Running   0          38s</span>
<span class="c">#    kube-system          coredns-7db6d8ff4d-gc4zp                      1/1     Running   0          38s</span>
<span class="c">#    kube-system          etcd-myk8s-control-plane                      1/1     Running   0          52s</span>
<span class="c">#    kube-system          kindnet-h4dwk                                 1/1     Running   0          38s</span>
<span class="c">#    kube-system          kube-apiserver-myk8s-control-plane            1/1     Running   0          54s</span>
<span class="c">#    kube-system          kube-controller-manager-myk8s-control-plane   1/1     Running   0          52s</span>
<span class="c">#    kube-system          kube-proxy-sptf6                              1/1     Running   0          38s</span>
<span class="c">#    kube-system          kube-scheduler-myk8s-control-plane            1/1     Running   0          52s</span>
<span class="c">#    local-path-storage   local-path-provisioner-988d74bc-gl679         1/1     Running   0          38s</span>
</code></pre></div></div>

<p><strong>Install Gateway API CRDs</strong> : The Kubernetes Gateway API abstractions are expressed using Kubernetes CRDs.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># CRDs 설치 및 확인</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> https://github.com/kubernetes-sigs/gateway-api/releases/download/v1.0.0/standard-install.yaml
<span class="nv">$ </span>kubectl get crd
<span class="c"># =&gt; NAME                                        CREATED AT</span>
<span class="c">#    gatewayclasses.gateway.networking.k8s.io    2024-10-01T15:50:32Z</span>
<span class="c">#    gateways.gateway.networking.k8s.io          2024-10-01T15:50:32Z</span>
<span class="c">#    httproutes.gateway.networking.k8s.io        2024-10-01T15:50:32Z</span>
<span class="c">#    referencegrants.gateway.networking.k8s.io   2024-10-01T15:50:32Z</span>
</code></pre></div></div>

<p><strong>Install Glooctl Utility</strong> : GLOOCTL is a command-line utility that allows users to view, manage, and debug Gloo Gateway deployments - <a href="https://docs.solo.io/gloo-edge/latest/installation/glooctl_setup/">Link</a></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># [신규 터미널] 아래 bash 진입 후 glooctl 툴 사용</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane bash
<span class="nt">----------------------------------------</span>
<span class="c"># Install Glooctl Utility</span>
<span class="c">## glooctl install gateway     # install gloo's function gateway functionality into the 'gloo-system' namespace</span>
<span class="c">## glooctl install ingress     # install very basic Kubernetes Ingress support with Gloo into namespace gloo-system</span>
<span class="c">## glooctl install knative     # install Knative serving with Gloo configured as the default cluster ingress</span>
<span class="c">## curl -sL https://run.solo.io/gloo/install | sh</span>
<span class="nv">$ </span>curl <span class="nt">-sL</span> https://run.solo.io/gloo/install | <span class="nv">GLOO_VERSION</span><span class="o">=</span>v1.17.7 sh
<span class="nv">$ </span><span class="nb">export </span><span class="nv">PATH</span><span class="o">=</span><span class="nv">$HOME</span>/.gloo/bin:<span class="nv">$PATH</span>

<span class="c"># 버전 확인</span>
<span class="nv">$ </span>glooctl version
<span class="c"># =&gt; Server: version undefined, could not find any version of gloo running</span>
<span class="c">#    </span>
<span class="c">#    {</span>
<span class="c">#      &amp;quot;client&amp;quot;: {</span>
<span class="c">#        &amp;quot;version&amp;quot;: &amp;quot;1.17.7&amp;quot;</span>
<span class="c">#      },</span>
<span class="c">#      &amp;quot;kubernetesCluster&amp;quot;: {</span>
<span class="c">#        &amp;quot;major&amp;quot;: &amp;quot;1&amp;quot;,</span>
<span class="c">#        &amp;quot;minor&amp;quot;: &amp;quot;30&amp;quot;,</span>
<span class="c">#        &amp;quot;gitVersion&amp;quot;: &amp;quot;v1.30.0&amp;quot;,</span>
<span class="c">#        &amp;quot;buildDate&amp;quot;: &amp;quot;2024-05-13T22:02:25Z&amp;quot;,</span>
<span class="c">#        &amp;quot;platform&amp;quot;: &amp;quot;linux/arm64&amp;quot;</span>
<span class="c">#      }</span>
<span class="c">#    }</span>

<span class="c"># &lt;span style="color: green;"&gt;👉 서버가 설치되지 않았기 때문에 클라이언트 정보만 나옵니다.&lt;/span&gt;</span>
<span class="nt">----------------------------------------</span>
</code></pre></div></div>

<p><strong>Install Gloo Gateway : 오픈소스 버전</strong></p>

<p><strong>rosetta 비활성화 방법</strong></p>

<ul>
  <li>
    <p>[macOS m시리즈] <strong>Docker Desktop</strong> : 아래 옵션 Uncheck 해둘 것 → Apply &amp; restart</p>

    <p><img src="/assets/2024/kans-3th/w6/20241012_kans_w6_22.png" alt="img.png" /></p>
  </li>
  <li>
    <p>[macOS m시리즈] <strong>Orbstack</strong> : 터미널에서 아래 입력</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c"># rosetta 비활성화 </span>
  <span class="nv">$ </span>orb config <span class="nb">set </span>rosetta <span class="nb">false</span>
    
  <span class="c">#  orb 설정 확인 </span>
  <span class="nv">$ </span>orb config show
    
  <span class="c"># orbstack 재시작 </span>
  <span class="nv">$ </span>orb stop 
  <span class="nv">$ </span>orb start 
</code></pre></div>    </div>
  </li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># [신규 터미널] 모니터링</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> kubectl get pod,svc,endpointslices,ep <span class="nt">-n</span> gloo-system

<span class="c"># Install Gloo Gateway</span>
<span class="c">## --set kubeGateway.enabled=true: Kubernetes Gateway 기능을 활성화합니다.</span>
<span class="c">## --set gloo.disableLeaderElection=true: Gloo의 리더 선출 기능을 비활성화합니다. (단일 인스턴스에서 Gloo를 실행 시 유용)</span>
<span class="c">## --set discovery.enabled=false: 서비스 디스커버리 기능을 비활성화합니다.</span>
<span class="nv">$ </span>helm repo add gloo https://storage.googleapis.com/solo-public-helm
<span class="c"># =&gt; &amp;quot;gloo&amp;quot; has been added to your repositories</span>
<span class="nv">$ </span>helm repo update
<span class="nv">$ </span>helm <span class="nb">install</span> <span class="nt">-n</span> gloo-system gloo-gateway gloo/gloo <span class="se">\</span>
<span class="nt">--create-namespace</span> <span class="se">\</span>
<span class="nt">--version</span> 1.17.7 <span class="se">\</span>
<span class="nt">--set</span> kubeGateway.enabled<span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
<span class="nt">--set</span> gloo.disableLeaderElection<span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
<span class="nt">--set</span> discovery.enabled<span class="o">=</span><span class="nb">false</span>
<span class="c"># =&gt; NAME: gloo-gateway</span>
<span class="c">#    LAST DEPLOYED: Sun Oct 13 00:57:27 2024</span>
<span class="c">#    NAMESPACE: gloo-system</span>
<span class="c">#    STATUS: deployed</span>
<span class="c">#    REVISION: 1</span>
<span class="c">#    TEST SUITE: None</span>

<span class="c"># Confirm that the Gloo control plane has successfully been deployed using this command</span>
<span class="nv">$ </span>kubectl rollout status deployment/gloo <span class="nt">-n</span> gloo-system
<span class="c"># =&gt; deployment &amp;quot;gloo&amp;quot; successfully rolled out</span>

<span class="c"># 설치 확인</span>
<span class="nv">$ </span>kubectl get crd | <span class="nb">grep</span> <span class="s1">'networking.k8s.io'</span>
<span class="c"># =&gt; gatewayclasses.gateway.networking.k8s.io    2024-10-01T15:50:32Z</span>
<span class="c">#    gateways.gateway.networking.k8s.io          2024-10-01T15:50:32Z</span>
<span class="c">#    httproutes.gateway.networking.k8s.io        2024-10-01T15:50:32Z</span>
<span class="c">#    referencegrants.gateway.networking.k8s.io   2024-10-01T15:50:32Z</span>
<span class="nv">$ </span>kubectl get crd | <span class="nb">grep</span> <span class="nt">-v</span> <span class="s1">'networking.k8s.io'</span>
<span class="nv">$ </span>kubectl get pod,svc,endpointslices <span class="nt">-n</span> gloo-system
<span class="c"># =&gt; NAME                                    READY   STATUS      RESTARTS   AGE</span>
<span class="c">#    pod/gateway-proxy-57c49d4f48-xm8vv      1/1     Running     0          87s</span>
<span class="c">#    pod/gloo-748d877c4-24ngk                1/1     Running     0          87s</span>
<span class="c">#    pod/gloo-resource-rollout-5bt7d         0/1     Completed   0          87s</span>
<span class="c">#    pod/gloo-resource-rollout-check-xwxd4   0/1     Completed   0          86s</span>
<span class="c">#    </span>
<span class="c">#    NAME                    TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                AGE</span>
<span class="c">#    service/gateway-proxy   LoadBalancer   10.96.69.126    &amp;lt;pending&amp;gt;     80:30172/TCP,443:32484/TCP                             87s</span>
<span class="c">#    service/gloo            ClusterIP      10.96.100.145   &amp;lt;none&amp;gt;        9977/TCP,9976/TCP,9988/TCP,9966/TCP,9979/TCP,443/TCP   87s</span>
<span class="c">#    </span>
<span class="c">#    NAME                                                 ADDRESSTYPE   PORTS                        ENDPOINTS    AGE</span>
<span class="c">#    endpointslice.discovery.k8s.io/gateway-proxy-n7f7v   IPv4          8080,8443                    10.244.0.7   87s</span>
<span class="c">#    endpointslice.discovery.k8s.io/gloo-9bf7g            IPv4          9979,9988,9966 + 3 more...   10.244.0.8   87s</span>

<span class="c">#</span>
<span class="nv">$ </span>kubectl explain gatewayclasses
<span class="nv">$ </span>kubectl get gatewayclasses
<span class="c"># =&gt; NAME           CONTROLLER             ACCEPTED   AGE</span>
<span class="c">#    gloo-gateway   solo.io/gloo-gateway   True       2m18s</span>

<span class="nv">$ </span>kubectl get gatewayclasses <span class="nt">-o</span> yaml
<span class="c"># =&gt; apiVersion: v1</span>
<span class="c">#    items:</span>
<span class="c">#    - apiVersion: gateway.networking.k8s.io/v1</span>
<span class="c">#      kind: GatewayClass</span>
<span class="c">#      metadata:</span>
<span class="c">#        labels:</span>
<span class="c">#          app: gloo</span>
<span class="c">#        name: gloo-gateway</span>
<span class="c">#      spec:</span>
<span class="c">#        controllerName: solo.io/gloo-gateway</span>
<span class="c">#    ...</span>
</code></pre></div></div>

<p><strong>Install Httpbin Application</strong> : A simple HTTP Request &amp; Response Service - <a href="https://httpbin.org/">Link</a></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> kubectl get pod,svc,endpointslices,ep <span class="nt">-n</span> httpbin

<span class="c"># Install Httpbin Application</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> https://raw.githubusercontent.com/solo-io/solo-blog/main/gateway-api-tutorial/01-httpbin-svc.yaml
<span class="c"># =&gt; namespace/httpbin created</span>
<span class="c">#    serviceaccount/httpbin created</span>
<span class="c">#    service/httpbin created</span>
<span class="c">#    deployment.apps/httpbin created</span>

<span class="c"># 설치 확인</span>
<span class="nv">$ </span>kubectl get deploy,pod,svc,endpointslices,sa <span class="nt">-n</span> httpbin
<span class="c"># =&gt; NAME                      READY   UP-TO-DATE   AVAILABLE   AGE</span>
<span class="c">#    deployment.apps/httpbin   0/1     1            0           10s</span>
<span class="c">#    </span>
<span class="c">#    NAME                           READY   STATUS              RESTARTS   AGE</span>
<span class="c">#    pod/httpbin-5855dc8bdd-xh2vf   0/1     ContainerCreating   0          10s</span>
<span class="c">#    </span>
<span class="c">#    NAME              TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE</span>
<span class="c">#    service/httpbin   ClusterIP   10.96.169.139   &amp;lt;none&amp;gt;        8000/TCP   10s</span>
<span class="c">#    </span>
<span class="c">#    NAME                                           ADDRESSTYPE   PORTS     ENDPOINTS   AGE</span>
<span class="c">#    endpointslice.discovery.k8s.io/httpbin-6zhsk   IPv4          &amp;lt;unset&amp;gt;   &amp;lt;unset&amp;gt;     10s</span>
<span class="c">#    </span>
<span class="c">#    NAME                     SECRETS   AGE</span>
<span class="c">#    serviceaccount/default   0         10s</span>
<span class="c">#    serviceaccount/httpbin   0         10s</span>
<span class="nv">$ </span>kubectl rollout status deploy/httpbin <span class="nt">-n</span> httpbin
<span class="c"># =&gt; deployment &amp;quot;httpbin&amp;quot; successfully rolled out</span>

<span class="c"># (옵션) NodePort 설정</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  labels:
    app: httpbin
    service: httpbin
  name: httpbin
  namespace: httpbin
spec:
  type: NodePort
  ports:
  - name: http
    port: 8000
    targetPort: 80
    nodePort: 30000
  selector:
    app: httpbin
</span><span class="no">EOF
</span><span class="c"># =&gt; service/httpbin configured</span>

<span class="c"># (옵션) 로컬 접속 확인</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"httpbin web - http://localhost:30000"</span>     <span class="c"># macOS 사용자</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"httpbin web - http://192.168.50.10:30000"</span> <span class="c"># Windows 사용자</span>
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w6/20241012_kans_w6_23.png" alt="img.png" class="w-80 image-center" />
<em class="image-caption">httpbin 설치결과</em></p>

<p><strong>Gateway API 종류</strong> - <a href="https://kubernetes.io/docs/concepts/services-networking/gateway/#resource-model">Docs</a></p>

<ul>
  <li><strong>GatewayClass:</strong> Defines a set of gateways with <strong>common configuration</strong> and managed by a controller that implements the <strong>class</strong>. - 예) 인프라 엔지니어가 관리</li>
  <li><strong>Gateway:</strong> Defines an instance of traffic handling <strong>infrastructure</strong>, such as cloud load balancer. - 예) 데브옵스 엔지니어가 관리</li>
  <li><strong>HTTPRoute:</strong> Defines <strong>HTTP-specific rules</strong> for mapping traffic from a Gateway listener to a representation of backend network endpoints. These endpoints are often represented as a <a href="https://kubernetes.io/docs/concepts/services-networking/service/">Service</a>. - 예) 개발자가 관리</li>
</ul>

<p><img src="/assets/2024/kans-3th/w6/20241012_kans_w6_24.png" alt="img.png" class="image-center" /></p>

<h5 id="control--envoy-data-plane-and-the-gloo-control-plane">Control : <strong>Envoy</strong> data plane and the <strong>Gloo</strong> control plane.</h5>

<ul>
  <li>Now we’ll configure a <strong>Gateway listener</strong>, establish external access to <strong>Gloo Gateway,</strong> and test the <strong>routing</strong> <strong>rules</strong> that are the core of the proxy configuration.</li>
</ul>

<p><strong>Configure a Gateway Listener</strong></p>

<ul>
  <li>Let’s begin by establishing a Gateway resource that sets up an HTTP listener on port 8080 to expose routes from all our namespaces. Gateway custom resources like this are part of the Gateway API standard.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 02-gateway.yaml</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> &gt; 02-gateway.yaml
kind: Gateway
apiVersion: gateway.networking.k8s.io/v1
metadata:
  name: http
  namespace: gloo-system
spec:
  gatewayClassName: gloo-gateway
  listeners:
  - protocol: HTTP
    port: 8080
    name: http
    allowedRoutes:
      namespaces:
        from: All
</span><span class="no">EOF

</span><span class="c"># gateway 리소스 생성</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> 02-gateway.yaml
<span class="c"># =&gt; gateway.gateway.networking.k8s.io/http created</span>

<span class="c"># 확인 : Now we can confirm that the Gateway has been activated</span>
<span class="nv">$ </span>kubectl get gateway <span class="nt">-n</span> gloo-system
<span class="c"># =&gt; NAME   CLASS          ADDRESS   PROGRAMMED   AGE</span>
<span class="c">#    http   gloo-gateway             True         8s</span>
<span class="nv">$ </span>kubectl get gateway <span class="nt">-n</span> gloo-system <span class="nt">-o</span> yaml
<span class="c"># =&gt; apiVersion: v1</span>
<span class="c">#    items:</span>
<span class="c">#    - apiVersion: gateway.networking.k8s.io/v1</span>
<span class="c">#      kind: Gateway</span>
<span class="c">#      metadata:</span>
<span class="c">#        name: http</span>
<span class="c">#        namespace: gloo-system</span>
<span class="c">#      spec:</span>
<span class="c">#        gatewayClassName: gloo-gateway</span>
<span class="c">#        listeners:</span>
<span class="c">#        - allowedRoutes:</span>
<span class="c">#            namespaces:</span>
<span class="c">#              from: All</span>
<span class="c">#          name: http</span>
<span class="c">#          port: 8080</span>
<span class="c">#          protocol: HTTP</span>
<span class="c">#    ...</span>

<span class="c"># You can also confirm that Gloo Gateway has spun up an Envoy proxy instance in response to the creation of this Gateway object by deploying gloo-proxy-http:</span>
<span class="nv">$ </span>kubectl get deployment gloo-proxy-http <span class="nt">-n</span> gloo-system
<span class="c"># =&gt; NAME              READY   UP-TO-DATE   AVAILABLE   AGE</span>
<span class="c">#    gloo-proxy-http   1/1     1            1           66s</span>

<span class="c"># envoy 사용 확인</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-n</span> gloo-system
<span class="c"># =&gt; NAME                               READY   STATUS    RESTARTS   AGE</span>
<span class="c">#    gateway-proxy-57c49d4f48-xm8vv     1/1     Running   0          13m</span>
<span class="c">#    gloo-748d877c4-24ngk               1/1     Running   0          13m</span>
<span class="c">#    gloo-proxy-http-587765f6b6-mpnt5   1/1     Running   0          78s</span>
<span class="nv">$ </span>kubectl describe pod <span class="nt">-n</span> gloo-system  |grep Image:
<span class="c"># =&gt;     Image:         quay.io/solo-io/gloo-envoy-wrapper:1.17.7 # &lt;span style="color: green;"&gt;👉 이름에서 알 수 있듯이 envoy가 들어있고 감싸고 있는것으로 보입니다.&lt;/span&gt;</span>
<span class="c">#        Image:          quay.io/solo-io/gloo:1.17.7</span>
<span class="c">#        Image:         quay.io/solo-io/gloo-envoy-wrapper:1.17.7</span>

<span class="c"># gloo-proxy-http 서비스는 External-IP는 Pending 상태</span>
<span class="nv">$ </span>kubectl get svc <span class="nt">-n</span> gloo-system gloo-proxy-http
<span class="c"># =&gt; NAME              TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE</span>
<span class="c">#    gloo-proxy-http   LoadBalancer   10.96.104.226   &amp;lt;pending&amp;gt;     8080:31461/TCP   101s</span>

<span class="c"># gloo-proxy-http NodePort 30001 설정</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: http
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: gloo-proxy-http
    app.kubernetes.io/version: 1.17.7
    gateway.networking.k8s.io/gateway-name: http
    gloo: kube-gateway
    helm.sh/chart: gloo-gateway-1.17.7
  name: gloo-proxy-http
  namespace: gloo-system
spec:
  ports:
  - name: http
    nodePort: 30001
    port: 8080
  selector:
    app.kubernetes.io/instance: http
    app.kubernetes.io/name: gloo-proxy-http
    gateway.networking.k8s.io/gateway-name: http
  type: LoadBalancer
</span><span class="no">EOF
</span><span class="c"># =&gt; service/gloo-proxy-http configured</span>

<span class="nv">$ </span>kubectl get svc <span class="nt">-n</span> gloo-system gloo-proxy-http
<span class="c"># =&gt; NAME              TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE</span>
<span class="c">#    gloo-proxy-http   LoadBalancer   10.96.104.226   &amp;lt;pending&amp;gt;     8080:30001/TCP   2m17s </span>
<span class="c"># &lt;span style="color: green;"&gt;👉 노드포트가 30001로 변경되었습니다.&lt;/span&gt;</span>
</code></pre></div></div>

<p><strong>Establish External Access to Proxy</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 간편한 테스트를 위해 port-forward를 사용하여 외부로 노출하겠습니다.</span>
<span class="nv">$ </span>kubectl port-forward deployment/gloo-proxy-http <span class="nt">-n</span> gloo-system 8080:8080 &amp;
</code></pre></div></div>

<p><strong>Configure Simple Routing with an HTTPRoute</strong></p>

<p>Let’s begin our routing configuration with the simplest possible <strong>route</strong> to expose the <strong>/get</strong> operation on <strong>httpbin</strong></p>

<p><code class="language-plaintext highlighter-rouge">HTTPRoute</code> is one of the new Kubernetes CRDs introduced by the Gateway API, as documented <a href="https://gateway-api.sigs.k8s.io/api-types/httproute/">here</a>. We’ll start by introducing a simple <code class="language-plaintext highlighter-rouge">HTTPRoute</code> for our service.</p>

<p><strong>HTTPRoute Spec</strong></p>

<ul>
  <li><a href="https://gateway-api.sigs.k8s.io/reference/spec/#gateway.networking.k8s.io/v1.ParentRef">ParentRefs</a>-Define which <strong>Gateways</strong> this <strong>Route</strong> wants to be <strong>attached</strong> to.</li>
  <li><a href="https://gateway-api.sigs.k8s.io/reference/spec/#gateway.networking.k8s.io/v1.Hostname">Hostnames</a> (optional)- Define a list of <strong>hostnames</strong> to use for matching the <strong>Host header</strong> of HTTP requests.</li>
  <li><a href="https://gateway-api.sigs.k8s.io/reference/spec/#gateway.networking.k8s.io/v1.HTTPRouteRule">Rules</a>-Define a list of <strong>rules</strong> to perform <strong>actions</strong> against matching HTTP requests.
    <ul>
      <li>Each rule consists of <a href="https://gateway-api.sigs.k8s.io/reference/spec/#gateway.networking.k8s.io/v1.HTTPRouteMatch">matches</a>, <a href="https://gateway-api.sigs.k8s.io/reference/spec/#gateway.networking.k8s.io/v1.HTTPRouteFilter">filters</a> (optional), <a href="https://gateway-api.sigs.k8s.io/reference/spec/#gateway.networking.k8s.io/v1.HTTPBackendRef">backendRefs</a> (optional) and <a href="https://gateway-api.sigs.k8s.io/reference/spec/#gateway.networking.k8s.io/v1.HTTPRouteTimeouts">timeouts</a> (optional) fields.</li>
    </ul>
  </li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>apiVersion: gateway.networking.k8s.io/v1beta1
kind: HTTPRoute
metadata:
  name: httpbin
  namespace: httpbin
  labels:
    example: httpbin-route
spec:
  parentRefs:
    - name: http
      namespace: gloo-system
  hostnames:
    - <span class="s2">"api.example.com"</span>
  rules:
  - matches:
    - path:
        <span class="nb">type</span>: Exact
        value: /get
    backendRefs:
      - name: httpbin
        port: 8000
</code></pre></div></div>

<p>This example <strong>attaches</strong> to the default <code class="language-plaintext highlighter-rouge">Gateway</code> object created for us when we installed Gloo Gateway earlier.</p>

<p>See the <code class="language-plaintext highlighter-rouge">gloo-system/http</code> reference in the <code class="language-plaintext highlighter-rouge">parentRefs</code> stanza.</p>

<p>The <a href="https://gateway-api.sigs.k8s.io/api-types/gateway/">Gateway</a> object simply represents a host:port listener that the proxy will expose to accept ingress traffic.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Our route watches for HTTP requests directed at the host api.example.com with the request path /get and then forwards the request to the httpbin service on port 8000.</span>
<span class="c"># Let’s establish this route now:</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> https://raw.githubusercontent.com/solo-io/gloo-gateway-use-cases/main/gateway-api-tutorial/03-httpbin-route.yaml
<span class="c"># =&gt; httproute.gateway.networking.k8s.io/httpbin created</span>

<span class="c">#</span>
<span class="nv">$ </span>kubectl get httproute <span class="nt">-n</span> httpbin
<span class="c"># =&gt; NAME      HOSTNAMES             AGE</span>
<span class="c">#    httpbin   [&amp;quot;api.example.com&amp;quot;]   12s</span>

<span class="nv">$ </span>kubectl describe httproute <span class="nt">-n</span> httpbin
<span class="c"># =&gt; ...</span>
<span class="c">#    Spec:</span>
<span class="c">#      Hostnames:</span>
<span class="c">#        api.example.com</span>
<span class="c">#      Parent Refs:</span>
<span class="c">#        Group:      gateway.networking.k8s.io</span>
<span class="c">#        Kind:       Gateway</span>
<span class="c">#        Name:       http</span>
<span class="c">#        Namespace:  gloo-system</span>
<span class="c">#      Rules:</span>
<span class="c">#        Backend Refs:</span>
<span class="c">#          Group:   </span>
<span class="c">#          Kind:    Service</span>
<span class="c">#          Name:    httpbin</span>
<span class="c">#          Port:    8000</span>
<span class="c">#          Weight:  1</span>
<span class="c">#        Matches:</span>
<span class="c">#          Path:</span>
<span class="c">#            Type:   Exact</span>
<span class="c">#            Value:  /get</span>
<span class="c">#    ...</span>
</code></pre></div></div>

<p><strong>Test the Simple Route with Curl</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># let’s use curl to display the response with the -i option to additionally show the HTTP response code and headers.</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"127.0.0.1 api.example.com"</span> | <span class="nb">sudo tee</span> <span class="nt">-a</span> /etc/hosts
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"httproute - http://api.example.com:30001/get"</span> <span class="c"># 웹브라우저</span>
<span class="c"># 혹은</span>
<span class="nv">$ </span>curl <span class="nt">-is</span> <span class="nt">-H</span> <span class="s2">"Host: api.example.com"</span> http://localhost:8080/get <span class="c"># kubectl port-forward 사용 시</span>
<span class="c"># =&gt; HTTP/1.1 200 OK</span>
<span class="c">#    &lt;span style="color: red;"&gt;server: envoy&lt;/span&gt; # &lt;span style="color: green;"&gt;👉 서버가 envoy임을 확인할 수 있습니다.&lt;/span&gt;</span>
<span class="c">#    date: Sat, 1 Oct 2024 16:19:18 GMT</span>
<span class="c">#    content-type: application/json</span>
<span class="c">#    content-length: 239</span>
<span class="c">#    access-control-allow-origin: *</span>
<span class="c">#    access-control-allow-credentials: true</span>
<span class="c">#    x-envoy-upstream-service-time: 13</span>
<span class="c">#    </span>
<span class="c">#    {</span>
<span class="c">#      &amp;quot;args&amp;quot;: {},</span>
<span class="c">#      &amp;quot;headers&amp;quot;: {</span>
<span class="c">#        &amp;quot;Accept&amp;quot;: &amp;quot;*/*&amp;quot;,</span>
<span class="c">#        &amp;quot;Host&amp;quot;: &amp;quot;api.example.com&amp;quot;,</span>
<span class="c">#        &amp;quot;User-Agent&amp;quot;: &amp;quot;curl/8.1.2&amp;quot;,</span>
<span class="c">#        &amp;quot;X-Envoy-Expected-Rq-Timeout-Ms&amp;quot;: &amp;quot;15000&amp;quot;</span>
<span class="c">#      },</span>
<span class="c">#      &amp;quot;origin&amp;quot;: &amp;quot;10.244.0.12&amp;quot;,</span>
<span class="c">#      &amp;quot;url&amp;quot;: &amp;quot;http://api.example.com/get&amp;quot;</span>
<span class="c">#    }</span>
</code></pre></div></div>

<p>Note that if we attempt to invoke another valid endpoint <code class="language-plaintext highlighter-rouge">/delay</code> on the <code class="language-plaintext highlighter-rouge">httpbin</code> service, it will fail with a <code class="language-plaintext highlighter-rouge">404 Not Found</code> error. Why? Because our <code class="language-plaintext highlighter-rouge">HTTPRoute</code> policy is only exposing access to <code class="language-plaintext highlighter-rouge">/get</code>, one of the many endpoints available on the service. If we try to consume an alternative <code class="language-plaintext highlighter-rouge">httpbin</code> endpoint like <code class="language-plaintext highlighter-rouge">/delay</code>:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 호출 응답 왜 그럴까?</span>
<span class="nv">$ </span>curl <span class="nt">-is</span> <span class="nt">-H</span> <span class="s2">"Host: api.example.com"</span> http://localhost:8080/delay/1
<span class="c"># =&gt; Handling connection for 8080</span>
<span class="c">#    HTTP/1.1 404 Not Found</span>
<span class="c">#    date: Sat, 1 Oct 2024 16:20:23 GMT</span>
<span class="c">#    server: envoy</span>
<span class="c">#    content-length: 0</span>
<span class="c"># &lt;span style="color: green;"&gt;👉 gloo를 통했을때는 HTTProute 설정에서 /get 이라는 경로에대해서 정확하게 일치(Exact) 할 경우에만 라우팅하도록 해서 그렇습니다.&lt;/span&gt;</span>

<span class="c"># nodeport 직접 접속 테스트</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"httproute - http://api.example.com:30000/delay/1"</span> <span class="c"># 1초 후 응답</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"httproute - http://api.example.com:30000/delay/5"</span> <span class="c"># 5초 후 응답</span>
<span class="c"># &lt;span style="color: green;"&gt;👉 노드포트로 직접 접속할 경우 gloo HTTPRoute를 거치지 않기 때문에 접속이 가능합니다.&lt;/span&gt;</span>
</code></pre></div></div>

<p>kind 클러스터를 구성할때 30000 포트를 열었기 때문에 NodePort로 직접 접속이 가능합니다.</p>

<p><img src="/assets/2024/kans-3th/w6/20241012_kans_w6_25.png" alt="img.png" class="w-80 image-center" />
<em class="image-caption">http://api.example.com:30000/delay/1 호출 결과 =&gt; 1초후 응답</em></p>

<p><strong>[정규식 패턴 매칭] Explore Routing with Regex Matching Patterns</strong></p>

<p>Let’s assume that now we DO want to expose other <code class="language-plaintext highlighter-rouge">httpbin</code> endpoints like <code class="language-plaintext highlighter-rouge">/delay</code>. Our initial <code class="language-plaintext highlighter-rouge">HTTPRoute</code> is inadequate, because it is looking for an exact path match with <code class="language-plaintext highlighter-rouge">/get</code>.</p>

<p>We’ll <strong>modify</strong> it in a couple of ways. <strong>First</strong>, we’ll modify the matcher to look for <strong>path prefix matches</strong> instead of an <strong>exact match</strong>. <strong>Second</strong>, we’ll add a <strong>new request filter</strong> to <strong>rewrite</strong> the matched <code class="language-plaintext highlighter-rouge">/api/httpbin/</code> prefix with just a <code class="language-plaintext highlighter-rouge">/</code> prefix, which will give us the flexibility to access any endpoint available on the <code class="language-plaintext highlighter-rouge">httpbin</code> service. So a path like <code class="language-plaintext highlighter-rouge">/api/httpbin/delay/1</code> will be sent to <code class="language-plaintext highlighter-rouge">httpbin</code> with the path <code class="language-plaintext highlighter-rouge">/delay/1</code>.</p>

<p>URL에 패턴이 매치가 되면 rewrite해서 실제 접속되는 경로를 변경할 수 있습니다.</p>
<ul>
  <li>예시) <code class="language-plaintext highlighter-rouge">/api/httpbin/delay/1</code> ⇒ <code class="language-plaintext highlighter-rouge">/delay/1</code></li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Here are the modifications we’ll apply to our HTTPRoute:</span>

    - matches:
        <span class="c"># Switch from an Exact Matcher(정확한 매팅) to a PathPrefix (경로 매팅) Matcher</span>
        - path:
            <span class="nb">type</span>: PathPrefix
            value: /api/httpbin/
      filters:
        <span class="c"># Replace(변경) the /api/httpbin matched prefix with /</span>
        - <span class="nb">type</span>: URLRewrite
          urlRewrite:
            path:
              <span class="nb">type</span>: ReplacePrefixMatch
              replacePrefixMatch: /
</code></pre></div></div>

<ul>
  <li>2가지 수정 내용 적용 후 확인</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> https://raw.githubusercontent.com/solo-io/gloo-gateway-use-cases/main/gateway-api-tutorial/04-httpbin-rewrite.yaml
<span class="c"># =&gt; httproute.gateway.networking.k8s.io/httpbin configured</span>

<span class="c"># 확인</span>
<span class="nv">$ </span>kubectl describe httproute <span class="nt">-n</span> httpbin
<span class="c"># =&gt; ...</span>
<span class="c">#    Spec:</span>
<span class="c">#      Hostnames:</span>
<span class="c">#        api.example.com</span>
<span class="c">#      Parent Refs:</span>
<span class="c">#        Group:      gateway.networking.k8s.io</span>
<span class="c">#        Kind:       Gateway</span>
<span class="c">#        Name:       http</span>
<span class="c">#        Namespace:  gloo-system</span>
<span class="c">#      Rules:</span>
<span class="c">#        Backend Refs:</span>
<span class="c">#          Group:</span>
<span class="c">#          Kind:    Service</span>
<span class="c">#          Name:    httpbin</span>
<span class="c">#          Port:    8000</span>
<span class="c">#          Weight:  1</span>
<span class="c">#        Filters:</span>
<span class="c">#          Type:  URLRewrite</span>
<span class="c">#          URL Rewrite:</span>
<span class="c">#            Path:</span>
<span class="c">#              Replace Prefix Match:  /</span>
<span class="c">#              Type:                  ReplacePrefixMatch</span>
<span class="c">#        Matches:</span>
<span class="c">#          Path:</span>
<span class="c">#            Type:   PathPrefix</span>
<span class="c">#            Value:  /api/httpbin/</span>
<span class="c">#    ...</span>
</code></pre></div></div>

<p><strong>Test Routing with Regex Matching Patterns</strong></p>

<p>When we used only a single route with an exact match pattern, we could only exercise the httpbin <code class="language-plaintext highlighter-rouge">/get</code> endpoint. Let’s now use <code class="language-plaintext highlighter-rouge">curl</code> to confirm that both <code class="language-plaintext highlighter-rouge">/get</code> and <code class="language-plaintext highlighter-rouge">/delay</code> work as expected.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"httproute - http://api.example.com:30001/api/httpbin/get"</span> <span class="c"># 웹브라우저</span>
<span class="c"># 혹은</span>
<span class="nv">$ </span>curl <span class="nt">-is</span> <span class="nt">-H</span> <span class="s2">"Host: api.example.com"</span> http://localhost:8080/api/httpbin/get <span class="c"># kubectl port-forward 사용 시</span>
<span class="c"># =&gt; HTTP/1.1 200 OK</span>
<span class="c">#    server: envoy</span>
<span class="c">#    date: Sat, 1 Oct 2024 16:33:20 GMT</span>
<span class="c">#    content-type: application/json</span>
<span class="c">#    content-length: 289</span>
<span class="c">#    access-control-allow-origin: *</span>
<span class="c">#    access-control-allow-credentials: true</span>
<span class="c">#    x-envoy-upstream-service-time: 20</span>
<span class="c">#    </span>
<span class="c">#    {</span>
<span class="c">#      &amp;quot;args&amp;quot;: {},</span>
<span class="c">#      &amp;quot;headers&amp;quot;: {</span>
<span class="c">#        &amp;quot;Accept&amp;quot;: &amp;quot;*/*&amp;quot;,</span>
<span class="c">#        &amp;quot;Host&amp;quot;: &amp;quot;api.example.com&amp;quot;,</span>
<span class="c">#        &amp;quot;User-Agent&amp;quot;: &amp;quot;curl/8.1.2&amp;quot;,</span>
<span class="c">#        &amp;quot;X-Envoy-Expected-Rq-Timeout-Ms&amp;quot;: &amp;quot;15000&amp;quot;,</span>
<span class="c">#        &amp;quot;X-Envoy-Original-Path&amp;quot;: &amp;quot;/api/httpbin/get&amp;quot;</span>
<span class="c">#      },</span>
<span class="c">#      &amp;quot;origin&amp;quot;: &amp;quot;10.244.0.12&amp;quot;,</span>
<span class="c">#      &amp;quot;url&amp;quot;: &amp;quot;http://api.example.com/get&amp;quot;</span>
<span class="c">#    }</span>

<span class="c"># 아래 NodePort 와 GW API 통한 접속 비교</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"httproute - http://api.example.com:30001/api/httpbin/get"</span>
<span class="c"># =&gt; HTTP/1.1 200 OK</span>
<span class="c">#    ...</span>
<span class="c">#      &amp;quot;url&amp;quot;: &amp;quot;&lt;span style="color: red;"&gt;http://api.example.com/get&lt;/span&gt;&amp;quot;</span>
<span class="c">#    }</span>
<span class="c"># &lt;span style="color: green;"&gt;👉 gloo gateway가 /app/httpbin/get =&gt; / 로 변경하여 잘 접속이 되었습니다.&lt;/span&gt;</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"httproute - http://api.example.com:30000/api/httpbin/get"</span> <span class="c"># NodePort 직접 접근</span>
<span class="c"># =&gt; HTTP/1.1 404 NOT FOUND</span>
<span class="c">#    ...</span>
<span class="c"># &lt;span style="color: green;"&gt;👉 NodePort에 직접 접근시에는 /app/httpbin/get이 그대로 파드에 전달되어 없는 경로라서 404 에러가 납니다.&lt;/span&gt;</span>

<span class="nt">---</span>
<span class="c">#</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"httproute - http://api.example.com:30001/api/httpbin/delay/1"</span> <span class="c"># 웹브라우저</span>
<span class="c"># 혹은</span>
<span class="nv">$ </span>curl <span class="nt">-is</span> <span class="nt">-H</span> <span class="s2">"Host: api.example.com"</span> http://localhost:8080/api/httpbin/delay/1 <span class="c"># kubectl port-forward 사용 시</span>
<span class="c"># =&gt; HTTP/1.1 200 OK</span>
<span class="c">#    server: envoy</span>
<span class="c">#    date: Sat, 1 Oct 2024 16:36:49 GMT</span>
<span class="c">#    content-type: application/json</span>
<span class="c">#    content-length: 343</span>
<span class="c">#    access-control-allow-origin: *</span>
<span class="c">#    access-control-allow-credentials: true</span>
<span class="c">#    x-envoy-upstream-service-time: 1049     # envoy 가 업스트림 httpbin 요청 처리에 걸리 시간 1초 이상</span>
<span class="c">#    </span>
<span class="c">#    {</span>
<span class="c">#      &amp;quot;args&amp;quot;: {},</span>
<span class="c">#      &amp;quot;data&amp;quot;: &amp;quot;&amp;quot;,</span>
<span class="c">#      &amp;quot;files&amp;quot;: {},</span>
<span class="c">#      &amp;quot;form&amp;quot;: {},</span>
<span class="c">#      &amp;quot;headers&amp;quot;: {</span>
<span class="c">#        &amp;quot;Accept&amp;quot;: &amp;quot;*/*&amp;quot;,</span>
<span class="c">#        &amp;quot;Host&amp;quot;: &amp;quot;api.example.com&amp;quot;,</span>
<span class="c">#        &amp;quot;User-Agent&amp;quot;: &amp;quot;curl/8.1.2&amp;quot;,</span>
<span class="c">#        &amp;quot;X-Envoy-Expected-Rq-Timeout-Ms&amp;quot;: &amp;quot;15000&amp;quot;,</span>
<span class="c">#        &amp;quot;X-Envoy-Original-Path&amp;quot;: &amp;quot;/api/httpbin/delay/1&amp;quot;</span>
<span class="c">#      },</span>
<span class="c">#      &amp;quot;origin&amp;quot;: &amp;quot;10.244.0.12&amp;quot;,</span>
<span class="c">#      &amp;quot;url&amp;quot;: &amp;quot;http://api.example.com/delay/1&amp;quot;</span>
<span class="c">#    }</span>

<span class="nv">$ </span>curl <span class="nt">-is</span> <span class="nt">-H</span> <span class="s2">"Host: api.example.com"</span> http://localhost:8080/api/httpbin/delay/2
<span class="c"># =&gt; ...</span>
<span class="c">#    x-envoy-upstream-service-time: 2133</span>
<span class="c">#    ...</span>
</code></pre></div></div>

<p>Perfect! It works just as expected! Note that the <code class="language-plaintext highlighter-rouge">/delay</code> operation completed successfully and that the 1-second delay was applied. The response header <code class="language-plaintext highlighter-rouge">x-envoy-upstream-service-time: 1023</code> indicates that Envoy reported that the upstream <code class="language-plaintext highlighter-rouge">httpbin</code> service required just over 1 second (1,023 milliseconds) to process the request. In the initial <code class="language-plaintext highlighter-rouge">/get</code> operation, which doesn’t inject an artificial delay, observe that the same header reported only 14 milliseconds of upstream processing time.</p>

<p><strong>[업스트림 베어러 토큰을 사용한 변환] Test Transformations with Upstream Bearer Tokens</strong></p>

<p><strong>목적</strong> : 요청을 라우팅하는 <strong>백엔드</strong> 시스템 중 하나에서 <strong>인증</strong>해야 하는 <strong>요구</strong> 사항이 있는 경우는 어떻게 할까요? 이 업스트림 시스템에는 권한 부여를 위한 API 키가 필요하고, 이를 소비하는 <strong>클라이언트에 직접 노출하고 싶지 않다</strong>고 가정해 보겠습니다. 즉<strong>, 프록시 계층</strong>에서 <strong>요청</strong>에 <strong>주입</strong>할 간단한 <strong>베어러 토큰</strong>을 구성하고 싶습니다. (정적 API 키 토큰을 직접 주입)</p>

<p>What if we have a requirement to <strong>authenticate</strong> with one of the <strong>backend</strong> systems to which we route our requests?</p>

<p>Let’s assume that this <strong>upstream</strong> system requires an <strong>API key</strong> for authorization, and that we <strong>don’t</strong> want to expose this directly to the <strong>consuming client</strong>. In other words, we’d like to configure a <strong>simple bearer toke</strong>n to be <strong>injected</strong> into the <strong>request</strong> at the <strong>proxy layer.</strong></p>

<p>We can <strong>express</strong> this in the <strong>Gateway API</strong> by adding a <strong>filter</strong> that applies a simple <strong>transformation</strong> to the i<strong>ncoming request</strong>.</p>

<p>This will be applied along with the <strong>URLRewrite</strong> filter we created in the previous step.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># The new filters stanza in our HTTPRoute now looks like this:</span>

      filters:
        - <span class="nb">type</span>: URLRewrite
          urlRewrite:
            path:
              <span class="nb">type</span>: ReplacePrefixMatch
              replacePrefixMatch: /
              
        <span class="c"># Add a Bearer token to supply a static API key when routing to backend system</span>
        - <span class="nb">type</span>: RequestHeaderModifier
          requestHeaderModifier:
            add:
              - name: Authorization
                value: Bearer my-api-key
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> https://raw.githubusercontent.com/solo-io/gloo-gateway-use-cases/main/gateway-api-tutorial/05-httpbin-rewrite-xform.yaml

<span class="c">#</span>
<span class="nv">$ </span>kubectl describe httproute <span class="nt">-n</span> httpbin
<span class="c"># =&gt; ...</span>
<span class="c">#    Spec:</span>
<span class="c">#      ...</span>
<span class="c">#      Rules:</span>
<span class="c">#        Backend Refs:</span>
<span class="c">#          Group:</span>
<span class="c">#          Kind:    Service</span>
<span class="c">#          Name:    httpbin</span>
<span class="c">#          Port:    8000</span>
<span class="c">#          Weight:  1</span>
<span class="c">#        Filters:</span>
<span class="c">#          Type:  URLRewrite</span>
<span class="c">#          URL Rewrite:</span>
<span class="c">#            Path:</span>
<span class="c">#              Replace Prefix Match:  /</span>
<span class="c">#              Type:                  ReplacePrefixMatch</span>
<span class="c">#          Request Header Modifier:</span>
<span class="c">#            Add:</span>
<span class="c">#              Name:   Authorization</span>
<span class="c">#              Value:  Bearer my-api-key</span>
<span class="c">#          Type:       RequestHeaderModifier</span>
<span class="c">#        Matches:</span>
<span class="c">#          Path:</span>
<span class="c">#            Type:   PathPrefix</span>
<span class="c">#            Value:  /api/httpbin/</span>
</code></pre></div></div>

<ul>
  <li>동작 테스트</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"httproute - http://api.example.com:30001/api/httpbin/get"</span> <span class="c"># 웹브라우저</span>
<span class="c"># 혹은</span>
<span class="nv">$ </span>curl <span class="nt">-is</span> <span class="nt">-H</span> <span class="s2">"Host: api.example.com"</span> http://localhost:8080/api/httpbin/get <span class="c"># kubectl port-forward 사용 시</span>
<span class="c"># =&gt; HTTP/1.1 200 OK</span>
<span class="c">#    server: envoy</span>
<span class="c">#    date: Sat, 1 Oct 2024 16:40:59 GMT</span>
<span class="c">#    content-type: application/json</span>
<span class="c">#    content-length: 332</span>
<span class="c">#    access-control-allow-origin: *</span>
<span class="c">#    access-control-allow-credentials: true</span>
<span class="c">#    x-envoy-upstream-service-time: 19</span>
<span class="c">#    </span>
<span class="c">#    {</span>
<span class="c">#      &amp;quot;args&amp;quot;: {},</span>
<span class="c">#      &amp;quot;headers&amp;quot;: {</span>
<span class="c">#        &amp;quot;Accept&amp;quot;: &amp;quot;*/*&amp;quot;,</span>
<span class="c">#        &lt;span style="color: red"&gt;&amp;quot;Authorization&amp;quot;: &amp;quot;Bearer my-api-key&amp;quot;,&lt;/span&gt; </span>
<span class="c">#        &amp;quot;Host&amp;quot;: &amp;quot;api.example.com&amp;quot;,</span>
<span class="c">#        &amp;quot;User-Agent&amp;quot;: &amp;quot;curl/8.1.2&amp;quot;,</span>
<span class="c">#        &amp;quot;X-Envoy-Expected-Rq-Timeout-Ms&amp;quot;: &amp;quot;15000&amp;quot;,</span>
<span class="c">#        &amp;quot;X-Envoy-Original-Path&amp;quot;: &amp;quot;/api/httpbin/get&amp;quot;</span>
<span class="c">#      },</span>
<span class="c">#      &amp;quot;origin&amp;quot;: &amp;quot;10.244.0.12&amp;quot;,</span>
<span class="c">#      &amp;quot;url&amp;quot;: &amp;quot;http://api.example.com/get&amp;quot;</span>
<span class="c">#    }</span>
<span class="c"># &lt;span style="color: green;"&gt;👉 클라이언트에서는 Authorization 헤더를 안 주었지만, Gloo gateway를 통하자&lt;/span&gt; </span>
<span class="c"># &lt;span style="color: green;"&gt;    Authorization 헤더에 Bearer my-api-key 가 추가되어 있습니다.&lt;/span&gt;</span>
</code></pre></div></div>

<h5 id="migrate">Migrate</h5>

<p>In this section, we’ll explore how a couple of common service migration techniques, <strong>dark launches with header-based routing</strong> and <strong>canary releases with percentage-based routing,</strong> are supported by the Gateway API standard.</p>

<p><strong>Configure Two Workloads for Migration Routing</strong></p>

<p>Let’s first establish <strong>two versions</strong> of a <strong>workload</strong> to facilitate our migration example. We’ll use the open-source <a href="https://github.com/nicholasjackson/fake-service">Fake Service</a> to enable this.</p>

<ul>
  <li><strong>Fake service</strong> that can handle both <strong>HTTP</strong> and <strong>gRPC</strong> traffic, for <strong>testing</strong> upstream service communications and testing service mesh and other scenarios.</li>
</ul>

<p>Let’s establish a <code class="language-plaintext highlighter-rouge">v1</code> of our <code class="language-plaintext highlighter-rouge">my-workload</code> service that’s configured to return a response string containing “v1”. We’ll create a corresponding <code class="language-plaintext highlighter-rouge">my-workload-v2</code> service as well.</p>

<ul>
  <li>ingress의 카나리 배포와 유사하게 V1의 일부 트래픽을 V2로 라우팅할 수 있습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># You should see the response below, indicating deployments for both v1 and v2 of my-workload have been created in the my-workload namespace.</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> https://raw.githubusercontent.com/solo-io/gloo-gateway-use-cases/main/gateway-api-tutorial/06-workload-svcs.yaml
<span class="c"># =&gt; namespace/my-workload created</span>
<span class="c">#    serviceaccount/my-workload created</span>
<span class="c">#    deployment.apps/my-workload-v1 created</span>
<span class="c">#    deployment.apps/my-workload-v2 created</span>
<span class="c">#    service/my-workload-v1 created</span>
<span class="c">#    service/my-workload-v2 created</span>

<span class="c"># v1,v2 2가지 버전 워크로드 확인</span>
<span class="nv">$ </span>kubectl get deploy,pod,svc,endpointslices <span class="nt">-n</span> my-workload
<span class="c"># =&gt; NAME                             READY   UP-TO-DATE   AVAILABLE   AGE</span>
<span class="c">#    deployment.apps/my-workload-v1   1/1     1            1           15s</span>
<span class="c">#    deployment.apps/my-workload-v2   1/1     1            1           15s</span>
<span class="c">#    </span>
<span class="c">#    NAME                                  READY   STATUS    RESTARTS   AGE</span>
<span class="c">#    pod/my-workload-v1-644f98bbd9-q6cs5   1/1     Running   0          15s</span>
<span class="c">#    pod/my-workload-v2-5bb5fcfcbc-bq88c   1/1     Running   0          15s</span>
<span class="c">#    </span>
<span class="c">#    NAME                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE</span>
<span class="c">#    service/my-workload-v1   ClusterIP   10.96.203.193   &amp;lt;none&amp;gt;        8080/TCP   15s</span>
<span class="c">#    service/my-workload-v2   ClusterIP   10.96.210.160   &amp;lt;none&amp;gt;        8080/TCP   15s</span>
<span class="c">#    </span>
<span class="c">#    NAME                                                  ADDRESSTYPE   PORTS   ENDPOINTS     AGE</span>
<span class="c">#    endpointslice.discovery.k8s.io/my-workload-v1-d9sqd   IPv4          8080    10.244.0.14   15s</span>
<span class="c">#    endpointslice.discovery.k8s.io/my-workload-v2-mv7fq   IPv4          8080    10.244.0.13   15s</span>
</code></pre></div></div>

<p><strong>Test Simple V1 Routing</strong></p>

<p>Before we dive into routing to multiple services, we’ll start by building a simple <strong><code class="language-plaintext highlighter-rouge">HTTPRoute</code></strong> that sends HTTP requests to host <code class="language-plaintext highlighter-rouge">api.example.com</code> whose paths begin with <strong><code class="language-plaintext highlighter-rouge">/api/my-workload</code></strong> to the <strong><code class="language-plaintext highlighter-rouge">v1</code></strong> workload:</p>

<p><img src="/assets/2024/kans-3th/w6/20241012_kans_w6_26.png" alt="img.png" /></p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">gateway.networking.k8s.io/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">HTTPRoute</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">my-workload</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">my-workload</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">example</span><span class="pi">:</span> <span class="s">my-workload-route</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">parentRefs</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">http</span>
      <span class="na">namespace</span><span class="pi">:</span> <span class="s">gloo-system</span>
  <span class="na">hostnames</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s2">"</span><span class="s">api.example.com"</span>
  <span class="na">rules</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">matches</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">path</span><span class="pi">:</span>
          <span class="na">type</span><span class="pi">:</span> <span class="s">PathPrefix</span>
          <span class="na">value</span><span class="pi">:</span> <span class="s">/api/my-workload</span>
      <span class="na">backendRefs</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">my-workload-v1</span>
          <span class="na">namespace</span><span class="pi">:</span> <span class="s">my-workload</span>
          <span class="na">port</span><span class="pi">:</span> <span class="m">8080</span>
</code></pre></div></div>

<p>Now apply this route:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> https://raw.githubusercontent.com/solo-io/gloo-gateway-use-cases/main/gateway-api-tutorial/07-workload-route.yaml
<span class="c"># =&gt; httproute.gateway.networking.k8s.io/my-workload created</span>

<span class="c">#</span>
<span class="nv">$ </span>kubectl get httproute <span class="nt">-A</span>
<span class="c"># =&gt; NAMESPACE     NAME          HOSTNAMES             AGE</span>
<span class="c">#    httpbin       httpbin       [&amp;quot;api.example.com&amp;quot;]   29m</span>
<span class="c">#    my-workload   my-workload   [&amp;quot;api.example.com&amp;quot;]   29s</span>

<span class="c">#</span>
<span class="nv">$ </span>kubectl describe httproute <span class="nt">-n</span> my-workload
<span class="c"># =&gt; ...</span>
<span class="c">#    Spec:</span>
<span class="c">#      Hostnames:</span>
<span class="c">#        api.example.com</span>
<span class="c">#      Parent Refs:</span>
<span class="c">#        Group:      gateway.networking.k8s.io</span>
<span class="c">#        Kind:       Gateway</span>
<span class="c">#        Name:       http</span>
<span class="c">#        Namespace:  gloo-system</span>
<span class="c">#      Rules:</span>
<span class="c">#        Backend Refs:</span>
<span class="c">#          Group:</span>
<span class="c">#          Kind:       Service</span>
<span class="c">#          Name:       my-workload-v1</span>
<span class="c">#          Namespace:  my-workload</span>
<span class="c">#          Port:       8080</span>
<span class="c">#          Weight:     1</span>
<span class="c">#        Matches:</span>
<span class="c">#          Path:</span>
<span class="c">#            Type:   PathPrefix</span>
<span class="c">#            Value:  /api/my-workload</span>
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> http://api.example.com:8080/api/my-workload | <span class="nb">grep </span>Workload<span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
<span class="c"># =&gt;  100   &amp;quot;body&amp;quot;: &amp;quot;Hello From My Workload (v1)!&amp;quot;,</span>

<span class="c"># &lt;span style="color: green;"&gt;👉 현재는 모든 연결이 v1으로 향합니다.&lt;/span&gt;</span>
</code></pre></div></div>

<p><strong>Simulate a v2 Dark Launch with Header-Based Routing</strong></p>

<p><img src="/assets/2024/kans-3th/w6/20241012_kans_w6_27.png" alt="img.png" /></p>

<p><a href="https://www.cloudbees.com/blog/when-dark-launch-right-release-strategy">Dark Launch</a> is a great cloud migration technique that <strong>releases new feature</strong>s to a select <strong>subset of users</strong> to gather <strong>feedback</strong> and experiment with improvements <strong>before</strong> potentially disrupting a larger user community.</p>

<ul>
  <li>Dark Launch : 일부 사용자에게 새로운 기능을 출시하여 피드백을 수집하고 잠재적으로 더 큰 사용자 커뮤니티를 방해하기 전에 개선 사항을 실험하는 훌륭한 클라우드 마이그레이션 기술</li>
</ul>

<p>We will simulate a dark launch in our example by installing the <strong>new cloud version</strong> of our <strong>service</strong> in our Kubernetes cluster, and then using declarative policy to route only requests containing a <strong>particular heade</strong>r to the new <code class="language-plaintext highlighter-rouge">v2</code> instance. The <strong>vast majority of users</strong> will continue to use the original <strong><code class="language-plaintext highlighter-rouge">v1</code></strong> of the service just as before.</p>

<ul>
  <li>우리는 Kubernetes 클러스터에 서비스의 새로운 클라우드 버전을 설치한 다음 선언적 정책을 사용하여 특정 헤더를 포함하는 요청만 새 인스턴스로 라우팅하여 예제에서 다크 런치를 시뮬레이션할 것입니다 . 대다수의 사용자는 이전과 마찬가지로 서비스의 <code class="language-plaintext highlighter-rouge">v1</code>을 계속 사용할 것 입니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  rules:
    - matches:
      - path:
          <span class="nb">type</span>: PathPrefix
          value: /api/my-workload
        <span class="c"># Add a matcher to route requests with a v2 version header to v2</span>
        <span class="c"># version=v2 헤더값이 있는 사용자만 v2 라우팅</span>
        headers:
        - name: version
          value: v2
      backendRefs:
        - name: my-workload-v2
          namespace: my-workload
          port: 8080      
    - matches:
      <span class="c"># Route requests without the version header to v1 as before</span>
      <span class="c"># 대다수 일반 사용자는 기존 처럼 v1 라우팅</span>
      - path:
          <span class="nb">type</span>: PathPrefix
          value: /api/my-workload
      backendRefs:
        - name: my-workload-v1
          namespace: my-workload
          port: 8080
</code></pre></div></div>

<p>Configure two separate routes, one for <code class="language-plaintext highlighter-rouge">v1</code> that the majority of service consumers will still use, and another route for <code class="language-plaintext highlighter-rouge">v2</code> that will be accessed by specifying a request header with name <code class="language-plaintext highlighter-rouge">version</code> and value <code class="language-plaintext highlighter-rouge">v2</code>. Let’s apply the modified <code class="language-plaintext highlighter-rouge">HTTPRoute</code>:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> https://raw.githubusercontent.com/solo-io/gloo-gateway-use-cases/main/gateway-api-tutorial/08-workload-route-header.yaml
<span class="c"># =&gt; httproute.gateway.networking.k8s.io/my-workload configured</span>

<span class="c"># </span>
<span class="nv">$ </span>kubectl describe httproute <span class="nt">-n</span> my-workload
<span class="c"># =&gt; ...</span>
<span class="c">#    Spec:  </span>
<span class="c">#      Rules:</span>
<span class="c">#        Backend Refs:</span>
<span class="c">#          Group:</span>
<span class="c">#          Kind:       Service</span>
<span class="c">#          Name:       my-workload-v2</span>
<span class="c">#          Namespace:  my-workload</span>
<span class="c">#          Port:       8080</span>
<span class="c">#          Weight:     1</span>
<span class="c">#        Matches:</span>
<span class="c">#          Headers:</span>
<span class="c">#            Name:   version</span>
<span class="c">#            Type:   Exact</span>
<span class="c">#            Value:  v2</span>
<span class="c">#          Path:</span>
<span class="c">#            Type:   PathPrefix</span>
<span class="c">#            Value:  /api/my-workload</span>
<span class="c">#        Backend Refs:</span>
<span class="c">#          Group:</span>
<span class="c">#          Kind:       Service</span>
<span class="c">#          Name:       my-workload-v1</span>
<span class="c">#          Namespace:  my-workload</span>
<span class="c">#          Port:       8080</span>
<span class="c">#          Weight:     1</span>
<span class="c">#        Matches:</span>
<span class="c">#          Path:</span>
<span class="c">#            Type:   PathPrefix</span>
<span class="c">#            Value:  /api/my-workload</span>
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># version: v2 헤더가 없는 경우 v1으로 라우팅됩니다.</span>
<span class="nv">$ </span>curl <span class="nt">-is</span> <span class="nt">-H</span> <span class="s2">"Host: api.example.com"</span> http://localhost:8080/api/my-workload
<span class="nv">$ </span>curl <span class="nt">-is</span> <span class="nt">-H</span> <span class="s2">"Host: api.example.com"</span> http://localhost:8080/api/my-workload | <span class="nb">grep </span>body
<span class="c"># =&gt; "body": "Hello From My Workload (v1)!",</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> http://api.example.com:8080/api/my-workload | <span class="nb">grep </span>Workload<span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
<span class="c"># =&gt;  100   &amp;quot;body&amp;quot;: &amp;quot;Hello From My Workload (v1)!&amp;quot;,</span>

<span class="c"># 하지만 version: v2 헤더가 있는 경우 v2로 라우팅됩니다.</span>
<span class="nv">$ </span>curl <span class="nt">-is</span> <span class="nt">-H</span> <span class="s2">"Host: api.example.com"</span> <span class="nt">-H</span> <span class="s2">"version: v2"</span> http://localhost:8080/api/my-workload
<span class="nv">$ </span>curl <span class="nt">-is</span> <span class="nt">-H</span> <span class="s2">"Host: api.example.com"</span> <span class="nt">-H</span> <span class="s2">"version: v2"</span> http://localhost:8080/api/my-workload | <span class="nb">grep </span>body
<span class="c"># =&gt;   &amp;quot;body&amp;quot;: &amp;quot;Hello From My Workload (v2)!&amp;quot;,</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> http://api.example.com:8080/api/my-workload | <span class="nb">grep </span>Workload<span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
<span class="c"># =&gt;  100   &amp;quot;body&amp;quot;: &amp;quot;Hello From My Workload (v1)!&amp;quot;,</span>
</code></pre></div></div>

<p><strong>Expand V2 Testing with Percentage-Based Routing</strong></p>

<p>After a successful dark-launch, we may want a period where we use a <strong>blue-green strategy</strong> of gradually <strong>shifting</strong> user traffic from the <strong>old</strong> version to the <strong>new</strong> one. Let’s explore this with a routing policy that splits our traffic evenly, sending half our traffic to <strong><code class="language-plaintext highlighter-rouge">v1</code></strong> and the other <strong>half</strong> to <strong><code class="language-plaintext highlighter-rouge">v2</code></strong>.</p>

<ul>
  <li>성공적인 다크 런칭 이후, 우리는 <strong>점진적</strong>으로 이전 버전에서 새 버전으로 사용자 트래픽을 옮기는 <strong>블루-그린 전략</strong>을 사용하는 기간을 원할 수 있습니다. 트래픽을 균등하게 분할하고 트래픽의 절반을 로 보내고 <code class="language-plaintext highlighter-rouge">v1</code>나머지 절반을 로 보내는 라우팅 정책으로 이를 살펴보겠습니다 <code class="language-plaintext highlighter-rouge">v2</code>.</li>
</ul>

<p>We will modify our <strong><code class="language-plaintext highlighter-rouge">HTTPRoute</code></strong> to accomplish this by removing the header-based routing rule that drove our dark launch. Then we will <strong>replace</strong> that with a <strong>50-50 <code class="language-plaintext highlighter-rouge">weight</code></strong> applied to each of the routes, as shown below:</p>

<p><img src="/assets/2024/kans-3th/w6/20241012_kans_w6_28.png" alt="img.png" /></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  rules:
    - matches:
      - path:
          <span class="nb">type</span>: PathPrefix
          value: /api/my-workload
      <span class="c"># Configure a 50-50 traffic split across v1 and v2 : 버전 1,2 50:50 비율</span>
      backendRefs:
        - name: my-workload-v1
          namespace: my-workload
          port: 8080
          weight: 50
        - name: my-workload-v2
          namespace: my-workload
          port: 8080
          weight: 50
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Apply this 50-50 routing policy with kubectl:</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> https://raw.githubusercontent.com/solo-io/gloo-gateway-use-cases/main/gateway-api-tutorial/09-workload-route-split.yaml
<span class="c"># =&gt; httproute.gateway.networking.k8s.io/my-workload configured</span>

<span class="c">#</span>
<span class="nv">$ </span>kubectl describe httproute <span class="nt">-n</span> my-workload
<span class="c"># =&gt; Spec:</span>
<span class="c">#      ...</span>
<span class="c">#      Rules:</span>
<span class="c">#        Backend Refs:</span>
<span class="c">#          Group:</span>
<span class="c">#          Kind:       Service</span>
<span class="c">#          Name:       my-workload-v1</span>
<span class="c">#          Namespace:  my-workload</span>
<span class="c">#          Port:       8080</span>
<span class="c">#          Weight:     50</span>
<span class="c">#          Group:</span>
<span class="c">#          Kind:       Service</span>
<span class="c">#          Name:       my-workload-v2</span>
<span class="c">#          Namespace:  my-workload</span>
<span class="c">#          Port:       8080</span>
<span class="c">#          Weight:     50</span>
<span class="c">#        Matches:</span>
<span class="c">#          Path:</span>
<span class="c">#            Type:   PathPrefix</span>
<span class="c">#            Value:  /api/my-workload</span>
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 반복 접속 후 대략 비률 확인</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nt">-H</span> <span class="s2">"Host: api.example.com"</span> http://localhost:8080/api/my-workload/ | <span class="nb">grep </span>body<span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
<span class="c"># =&gt;   51   &amp;quot;body&amp;quot;: &amp;quot;Hello From My Workload (v1)!&amp;quot;,</span>
<span class="c">#      49   &amp;quot;body&amp;quot;: &amp;quot;Hello From My Workload (v2)!&amp;quot;,</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..200<span class="o">}</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nt">-H</span> <span class="s2">"Host: api.example.com"</span> http://localhost:8080/api/my-workload/ | <span class="nb">grep </span>body<span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
<span class="c"># =&gt;  116   &amp;quot;body&amp;quot;: &amp;quot;Hello From My Workload (v1)!&amp;quot;,</span>
<span class="c">#      84   &amp;quot;body&amp;quot;: &amp;quot;Hello From My Workload (v2)!&amp;quot;,</span>
</code></pre></div></div>

<h5 id="debug">Debug</h5>

<p><strong>Solve a Problem with Glooctl CLI</strong></p>

<p>A common source of Gloo configuration <strong>errors</strong> is <strong>mistyping</strong> an upstream reference, perhaps when copy/pasting it from another source but “missing a spot” when changing the name of the backend service target. In this example, we’ll simulate making an error like that, and then demonstrating how <code class="language-plaintext highlighter-rouge">glooctl</code> can be used to detect it.</p>

<ul>
  <li>Gloo 구성 오류의 일반적인 원인은 <strong>업스트림 참조를 잘못 입력</strong>하는 것입니다. 아마도 다른 소스에서 복사/붙여넣을 때이지만 백엔드 서비스 대상의 이름을 변경할 때 “한 군데를 놓친” 것입니다. 이 예에서 우리는 그런 오류를 만드는 것을 시뮬레이션하고, <code class="language-plaintext highlighter-rouge">glooctl</code>그것을 감지하는 데 어떻게 사용할 수 있는지 보여줍니다.</li>
</ul>

<p><strong>First</strong>, let’s apply a change to simulate the <strong>mistyping</strong> of an upstream config so that it is targeting a <strong>non-existent <code class="language-plaintext highlighter-rouge">my-bad-workload-v2</code></strong> backend service, rather than the correct <strong><code class="language-plaintext highlighter-rouge">my-workload-v2</code></strong>.</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">my-bad-workload-v2</code> 업스트림 구성의 오타를 시뮬레이션하여 올바른 타겟팅하는 대신 존재하지 않는 백엔드 서비스를 타겟팅하도록 변경</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># [신규 터미널] 모니터링</span>
<span class="nv">$ </span>kubectl get httproute <span class="nt">-n</span> my-workload my-workload <span class="nt">-o</span> yaml <span class="nt">-w</span>

<span class="c">#</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> https://raw.githubusercontent.com/solo-io/gloo-gateway-use-cases/main/gateway-api-tutorial/10-workload-route-split-bad-dest.yaml
<span class="c"># =&gt; httproute.gateway.networking.k8s.io/my-workload configured</span>

<span class="c">#</span>
<span class="nv">$ </span>kubectl describe httproute <span class="nt">-n</span> my-workload
<span class="c"># =&gt; ...</span>
<span class="c">#    Spec:</span>
<span class="c">#      Rules:</span>
<span class="c">#        Backend Refs:</span>
<span class="c">#          Group:</span>
<span class="c">#          Kind:       Service</span>
<span class="c">#          Name:       my-workload-v1</span>
<span class="c">#          Namespace:  my-workload</span>
<span class="c">#          Port:       8080</span>
<span class="c">#          Weight:     50</span>
<span class="c">#          Group:</span>
<span class="c">#          Kind:       Service</span>
<span class="c">#          Name:       my-bad-workload-v2</span>
<span class="c">#          Namespace:  my-workload</span>
<span class="c">#          Port:       8080</span>
<span class="c">#          Weight:     50</span>
<span class="c">#        Matches:</span>
<span class="c">#          Path:</span>
<span class="c">#            Type:   PathPrefix</span>
<span class="c">#            Value:  /api/my-workload</span>
<span class="c">#    Status:</span>
<span class="c">#      Parents:</span>
<span class="c">#        Conditions:</span>
<span class="c">#          Last Transition Time:  2024-10-12T16:55:06Z</span>
<span class="c">#          Message:               Service &amp;quot;my-bad-workload-v2&amp;quot; not found</span>
<span class="c">#          Observed Generation:   4</span>
<span class="c">#          Reason:                BackendNotFound</span>
<span class="c">#          Status:                False</span>
<span class="c">#          Type:                  ResolvedRefs</span>
<span class="c">#          Last Transition Time:  2024-10-12T16:45:41Z</span>
<span class="c">#          Message:</span>
<span class="c">#          Observed Generation:   4</span>
<span class="c">#          Reason:                Accepted</span>
<span class="c">#          Status:                True</span>
<span class="c">#          Type:                  Accepted</span>
<span class="c">#        Controller Name:         solo.io/gloo-gateway</span>
<span class="c">#        Parent Ref:</span>
<span class="c">#          Group:      gateway.networking.k8s.io</span>
<span class="c">#          Kind:       Gateway</span>
<span class="c">#          Name:       http</span>
<span class="c">#          Namespace:  gloo-system</span>
<span class="c">#    Events:           &amp;lt;none&amp;gt;</span>
</code></pre></div></div>

<p>When we test this out, note that the 50-50 traffic split is still in place. This means that about half of the requests will be routed to <code class="language-plaintext highlighter-rouge">my-workload-v1</code> and succeed, while the others will attempt to use the non-existent <code class="language-plaintext highlighter-rouge">my-bad-workload-v2</code> and fail like this:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#</span>
<span class="nv">$ </span>curl <span class="nt">-is</span> <span class="nt">-H</span> <span class="s2">"Host: api.example.com"</span> http://localhost:8080/api/my-workload
<span class="c"># =&gt; HTTP/1.1 500 Internal Server Error</span>
<span class="nv">$ </span>curl <span class="nt">-is</span> <span class="nt">-H</span> <span class="s2">"Host: api.example.com"</span> http://localhost:8080/api/my-workload
<span class="c"># =&gt; HTTP/1.1 200 OK</span>
<span class="c">#    vary: Origin</span>
<span class="c">#    date: Sat, 12 Oct 2024 16:56:37 GMT</span>
<span class="c">#    content-length: 292</span>
<span class="c">#    content-type: text/plain; charset=utf-8</span>
<span class="c">#    x-envoy-upstream-service-time: 5</span>
<span class="c">#    server: envoy</span>
<span class="c">#    ...</span>

<span class="c"># </span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nt">-H</span> <span class="s2">"Host: api.example.com"</span> http://localhost:8080/api/my-workload/ | <span class="nb">grep </span>body<span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
<span class="c"># =&gt;   55   &amp;quot;body&amp;quot;: &amp;quot;Hello From My Workload (v1)!&amp;quot;,</span>

<span class="c"># &lt;span style="color: green;"&gt;👉 디버깅 테스트를 위해 일부러 50%의 워크로드에는 오타를 내어서&lt;/span&gt;</span>
<span class="c"># &lt;span style="color: green;"&gt;    50%의 요청은 v1로 라우팅되어 성공하고 나머지 50%는 실패합니다.&lt;/span&gt;</span>
</code></pre></div></div>

<p>So we’ll deploy one of the first weapons from the Gloo debugging arsenal, the <code class="language-plaintext highlighter-rouge">glooctl check</code> utility. It verifies a number of Gloo resources, confirming that they are configured correctly and are interconnected with other resources correctly. For example, in this case, <code class="language-plaintext highlighter-rouge">glooctl</code> will detect the error in the mis-connection between the <code class="language-plaintext highlighter-rouge">HTTPRoute</code> and its backend target:</p>

<ul>
  <li>gloo에서 제공하는 <code class="language-plaintext highlighter-rouge">glooctl check</code> 명령으로 구성 오류를 확인합니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane bash
<span class="c"># -----------------------------------</span>
<span class="nv">$ </span><span class="nb">export </span><span class="nv">PATH</span><span class="o">=</span><span class="nv">$HOME</span>/.gloo/bin:<span class="nv">$PATH</span>
<span class="nv">$ </span>glooctl check
<span class="c"># =&gt; ...</span>
<span class="c">#    Checking Gateways... OK</span>
<span class="c">#    Checking Proxies... 1 Errors!</span>
<span class="c">#    </span>
<span class="c">#    Detected Kubernetes Gateway integration!</span>
<span class="c">#    Checking Kubernetes GatewayClasses... OK</span>
<span class="c">#    Checking Kubernetes Gateways... OK</span>
<span class="c">#    Checking Kubernetes HTTPRoutes... 1 Errors!</span>
<span class="c">#    </span>
<span class="c">#    Skipping Gloo Instance check -- Gloo Federation not detected.</span>
<span class="c">#    Error: 2 errors occurred:</span>
<span class="c">#     * Found proxy with warnings by 'gloo-system': gloo-system gloo-system-http</span>
<span class="c">#    Reason: warning:</span>
<span class="c">#      Route Warning: InvalidDestinationWarning. Reason: invalid destination in weighted destination list: *v1.Upstream { blackhole_ns.kube-svc:blackhole-ns-blackhole-cluster-8080 } not found</span>
<span class="c">#    </span>
<span class="c">#     * HTTPRoute my-workload.my-workload.http status (ResolvedRefs) is not set to expected (True). Reason: BackendNotFound, Message: Service &amp;quot;my-bad-workload-v2&amp;quot; not found</span>

<span class="c"># 원인 관련 정보 확인</span>
<span class="nv">$ </span>kubectl get httproute my-workload <span class="nt">-n</span> my-workload <span class="nt">-o</span> yaml
<span class="c"># =&gt; ...</span>
<span class="c">#    status:</span>
<span class="c">#      parents:</span>
<span class="c">#      - conditions:</span>
<span class="c">#        - lastTransitionTime: &amp;quot;2024-10-12T16:55:06Z&amp;quot;</span>
<span class="c">#          message: Service &amp;quot;my-bad-workload-v2&amp;quot; not found</span>
<span class="c">#          observedGeneration: 4</span>
<span class="c">#          reason: BackendNotFound</span>
<span class="c">#          status: &amp;quot;False&amp;quot;</span>
<span class="c">#          type: ResolvedRefs</span>
<span class="c">#          ...</span>

<span class="c"># 정상 설정으로 해결 configuration is again clean.</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> https://raw.githubusercontent.com/solo-io/gloo-gateway-use-cases/main/gateway-api-tutorial/09-workload-route-split.yaml
<span class="c"># =&gt; httproute.gateway.networking.k8s.io/my-workload configured</span>
<span class="nv">$ </span>kubectl get httproute my-workload <span class="nt">-n</span> my-workload <span class="nt">-o</span> yaml

<span class="c">#</span>
<span class="nv">$ </span>glooctl check
<span class="c"># =&gt; Checking Deployments... OK</span>
<span class="c">#    Checking Pods... OK</span>
<span class="c">#    Checking Upstreams... OK</span>
<span class="c">#    Checking UpstreamGroups... OK</span>
<span class="c">#    Checking AuthConfigs... OK</span>
<span class="c">#    Checking RateLimitConfigs... OK</span>
<span class="c">#    Checking VirtualHostOptions... OK</span>
<span class="c">#    Checking RouteOptions... OK</span>
<span class="c">#    Checking Secrets... OK</span>
<span class="c">#    Checking VirtualServices... OK</span>
<span class="c">#    Checking Gateways... OK</span>
<span class="c">#    Checking Proxies... OK</span>
<span class="c">#    </span>
<span class="c">#    Detected Kubernetes Gateway integration!</span>
<span class="c">#    Checking Kubernetes GatewayClasses... OK</span>
<span class="c">#    Checking Kubernetes Gateways... OK</span>
<span class="c">#    Checking Kubernetes HTTPRoutes... OK</span>
<span class="c">#    </span>
<span class="c">#    Skipping Gloo Instance check -- Gloo Federation not detected.</span>
<span class="c">#    No problems detected.</span>
<span class="c"># &lt;span style="color: green;"&gt;👉 이제 문제가 없다고 합니다. 😀&lt;/span&gt;</span>
</code></pre></div></div>

<h5 id="observe">Observe</h5>

<p><strong>Explore Envoy Metrics</strong></p>

<p><strong>Envoy</strong> publishes a host of <strong>metrics</strong> that may be useful for observing system behavior. In our very modest kind cluster for this exercise, you can count over <strong>3,000 individual metrics</strong>! You can learn more about them in the Envoy documentation <a href="https://www.envoyproxy.io/docs/envoy/latest/configuration/upstream/cluster_manager/cluster_stats">here</a>.</p>

<p>For this 30-minute exercise, let’s take a quick look at a couple of the useful metrics that Envoy produces for every one of our backend targets.</p>

<p>First, we’ll <strong>port-forward</strong> the <strong>Envoy</strong> <strong>administrative</strong> <strong>port</strong> <strong>19000</strong> to our local workstation:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#</span>
<span class="nv">$ </span>kubectl <span class="nt">-n</span> gloo-system port-forward deployment/gloo-proxy-http 19000 &amp;

<span class="c"># 아래 관리 페이지에서 각각 메뉴 링크 클릭 확인</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"Envoy Proxy Admin - http://localhost:19000"</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"Envoy Proxy Admin - http://localhost:19000/stats/prometheus"</span>tZBli7jsXv<span class="s1">'XALZnaKnB2MSBvJNI
</span></code></pre></div></div>

<p>For this exercise, let’s view <strong>two</strong> of the relevant <strong>metrics</strong> from the first part of this exercise: one that counts the <strong>number</strong> of <strong>successful</strong> (HTTP 2xx) requests processed by our <code class="language-plaintext highlighter-rouge">httpbin</code> backend (or <strong><code class="language-plaintext highlighter-rouge">cluster</code></strong>, in Envoy terminology), and another that <strong>counts</strong> the number of requests <strong>returning</strong> server errors (HTTP <strong>5xx</strong>) from that same backend:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 2xx, 5xx 요청 확인</span>
<span class="nv">$ </span>curl <span class="nt">-s</span> http://localhost:19000/stats | <span class="nb">grep</span> <span class="nt">-E</span> <span class="s2">"(^cluster.kube-svc_httpbin-httpbin-8000_httpbin.upstream.*(2xx|5xx))"</span>
<span class="c"># =&gt; cluster.kube-svc_httpbin-httpbin-8000_httpbin.upstream_rq_2xx: 7</span>

<span class="c"># If we apply a curl request that forces a 500 failure from the httpbin backend, using the /status/500 endpoint, I’d expect the number of 2xx requests to remain the same, and the number of 5xx requests to increment by one:</span>
<span class="nv">$ </span>curl <span class="nt">-is</span> <span class="nt">-H</span> <span class="s2">"Host: api.example.com"</span> http://localhost:8080/api/httpbin/status/500
<span class="c"># =&gt; HTTP/1.1 500 Internal Server Error</span>
<span class="c">#    server: envoy</span>
<span class="c">#    date: Sat, 12 Oct 2024 17:02:53 GMT</span>
<span class="c">#    content-type: text/html; charset=utf-8</span>
<span class="c">#    access-control-allow-origin: *</span>
<span class="c">#    access-control-allow-credentials: true</span>
<span class="c">#    content-length: 0</span>
<span class="c">#    x-envoy-upstream-service-time: 38</span>

<span class="c"># 500에러를 발생시키자 500에러가 1개 증가하고 2xx는 변화가 없습니다.</span>
<span class="nv">$ </span>curl <span class="nt">-s</span> http://localhost:19000/stats | <span class="nb">grep</span> <span class="nt">-E</span> <span class="s2">"(^cluster.kube-svc_httpbin-httpbin-8000_httpbin.upstream.*(2xx|5xx))"</span>
<span class="c"># =&gt; cluster.kube-svc_httpbin-httpbin-8000_httpbin.upstream_rq_2xx: 7</span>
<span class="c">#    cluster.kube-svc_httpbin-httpbin-8000_httpbin.upstream_rq_5xx: 1</span>
</code></pre></div></div>

<h5 id="정리">정리</h5>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kind delete cluster <span class="nt">--name</span> myk8s
<span class="c"># =&gt; Deleted nodes: [&amp;quot;myk8s-control-plane&amp;quot;]</span>
</code></pre></div></div>

<h3 id="기타-gateway-api-구현체">기타 Gateway API 구현체</h3>

<ul>
  <li>
    <dl>
      <dt><strong><code class="language-plaintext highlighter-rouge">Cilium</code></strong></dt>
      <dd>Cilium은 CNI로 알려져있지만 Gateway API 역할도 지원합니다.</dd>
    </dl>
    <ul>
      <li><strong>(참고) [OnlineLab] Cilium Gateway API - <a href="https://isovalent.com/labs/cilium-gateway-api/">Link</a></strong></li>
      <li><strong>(참고) [OnlineLab] Advanced Gateway API Use Cases - <a href="https://isovalent.com/labs/cilium-gateway-api-advanced/">Link</a></strong></li>
    </ul>
  </li>
  <li>
    <dl>
      <dt><strong><code class="language-plaintext highlighter-rouge">Istio</code></strong></dt>
      <dd>Istio는 Service Mesh로 알려져있지만 Gateway API 역할도 지원합니다. Gateway API 자체가 Service Mesh인 Istio 등을 참조하였기에 어찌보면 당연한 일입니다.</dd>
    </dl>
    <ul>
      <li>Kubernetes Traffic Management: Combining Gateway API with Service Mesh for North-South and East-West Use Cases - <a href="https://medium.com/@disha.20.10/kubernetes-traffic-management-combining-gateway-api-with-service-mesh-for-north-south-and-63e39ad95dcc">Blog</a></li>
      <li>Istio Gateway API 활용하기 <a href="https://devops-james.tistory.com/317">https://devops-james.tistory.com/317</a></li>
    </ul>
  </li>
  <li><strong><code class="language-plaintext highlighter-rouge">Kong API Gateway</code></strong>
    <ul>
      <li>Kong API Gateway 를 Gateway API 형태 설치 <a href="https://mokpolar.tistory.com/68">https://mokpolar.tistory.com/68</a></li>
    </ul>
  </li>
  <li><strong><code class="language-plaintext highlighter-rouge">Envoy Gateway</code></strong>
    <ul>
      <li>Envoy Gateway 사용하여 + 부하분산 <a href="https://devops-james.tistory.com/320">https://devops-james.tistory.com/320</a></li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="마치며">마치며</h2>

<p>파드 통신에서 부터 CNI, 서비스(ClusterIP, NodePort, LoadBalancer)를 거쳐, ingress, gateway api까지 왔습니다.
나중에 배운 기술이 이전 기술을 필요없게 만드는 부분도 있지만, 기초의 중요성을 알기에 더욱 중요하다고 생각합니다.</p>

<p>그런데 gateway api를 만들면서 ingress를 frozen 하게 된것은 살짝 충격적입니다.
ingress를 없앤다는 얘기는 없지만 결국 gateway api가 더 좋은 기술이고, 
ingress는 점점 점유율을 잃다가 조용히 deprecated 될것 같은 느낌입니다.
ingress가 심심하지 않도록 더 자주 써줘야겠습니다.</p>

<p>이번주는 특히나 실습이 많았던것 같은데, 다른 분들도 다들 잘 생존했으면 좋겠습니다.
(일단 저부터 스터디에서 생존하기를 빕니다.. :smile:)</p>]]></content><author><name></name></author><category term="kans" /><category term="kubernetes," /><category term="network," /><category term="linux" /><summary type="html"><![CDATA[이번주에는 ingress와 gateway api 에대해 알아 보겠습니다.]]></summary></entry><entry><title type="html">[KANS 3기] LoadBalancer(MetalLB), IPVS</title><link href="https://sweetlittlebird.github.io/posts/2024-10-05-KANS-Study-Week5/" rel="alternate" type="text/html" title="[KANS 3기] LoadBalancer(MetalLB), IPVS" /><published>2024-10-05T01:00:18+09:00</published><updated>2024-10-05T01:00:18+09:00</updated><id>https://sweetlittlebird.github.io/posts/KANS%20Study%20-%20Week5</id><content type="html" xml:base="https://sweetlittlebird.github.io/posts/2024-10-05-KANS-Study-Week5/"><![CDATA[<h2 id="들어가며">들어가며</h2>

<p>이번 주에는 LoadBalancer 서비스와 MetalLB, 그리고 kube-proxy의 모드중 하나인 IPVS에 대해 알아보겠습니다.
KANS 3기 5주차 스터디를 시작하겠습니다.</p>

<hr />

<h2 id="loadbalancer-서비스">LoadBalancer 서비스</h2>

<h3 id="loadbalancer란">LoadBalancer란?</h3>

<ul>
  <li>LoadBalancer는 Kubernetes의 Service 유형의 하나로, 클러스터 외부에서 클러스터 내부의 서비스에 접근할 수 있도록 서비스를
노출시키는 역할을 합니다.</li>
  <li>Kubernetes에서는 자체적으로 LoadBalancer를 제공하지 않고, 클라우드 서비스 제공업체의 LoadBalancer(AWS의 ALB, NLB),
LoadBalancer 하드웨어 장비(Citrix, F5 networks), 또는 오픈소스 LoadBalancer (MetalLB 등)를 사용합니다.</li>
  <li>기본적으로 LoadBalancer를 사용하면 NodePort를 먼저 생성한 다음 LoadBalancer와 연결해야 하지만 (NodePort 접근 방식),
구성에 따라 NodePort 없이 바로 LoadBalancer를 생성할 수도 (Pod Direct 접근 방식) 있습니다.</li>
</ul>

<h3 id="환경별-loadbalancer">환경별 LoadBalancer</h3>

<p><img src="/assets/2024/kans-3th/w5/20241005_kans_w5_1.png" alt="환경별 LoadBalancer 비교" class="image-center" />
<em class="image-caption">환경별 LoadBalancer 비교</em></p>

<h4 id="클라우드-서비스-제공업체의-loadbalancer">클라우드 서비스 제공업체의 LoadBalancer</h4>

<ul>
  <li>클라우드 서비스 제공업체의 LoadBalancer는 클라우드 서비스 제공업체가 제공하는 서비스로, 클라우드 서비스 제공업체의
LoadBalancer를 사용하면 클라우드 서비스 제공업체의 LoadBalancer를 통해 클러스터 외부에서 클러스터 내부의 서비스에
접근할 수 있습니다.</li>
  <li>하지만 클라우드 서비스 제공업체마다 동작 방식과 기능이 다르기 때문에 각 클라우드 서비스 제공업체의 LoadBalancer를
사용할 때는 해당 클라우드 서비스 제공업체의 LoadBalancer의 동작 방식과 기능을 확인해야 합니다.</li>
  <li>대표적인 클라우드 서비스 제공업체인 Amazon Web Service는 다음의 LoadBalancer를 제공합니다.
    <ul>
      <li><strong>Classic Load Balancer (CLB)</strong> : 가장 오래된 로드밸런서로 NLB, ALB보다 기능이 적습니다.</li>
      <li><strong>Network Load Balancer (NLB)</strong> : Layer 4 계층의 네트워크 로드밸런서로 TCP/UDP/TLS 트래픽을 지원합니다. CLB/ALB에 비해서 처리속도가 빠릅니다.
(<strong>Application Load Balancer (ALB)</strong>는 Layer 7 계층의 애플리케이션 로드밸런서로 http/https/gRPC 트래픽을 지원합니다. ALB는 Ingress시 생성됩니다.)</li>
    </ul>
  </li>
</ul>

<h5 id="클라우드-서비스-제공업체의-loadbalancer-서비스-동작-방식">클라우드 서비스 제공업체의 LoadBalancer 서비스 동작 방식</h5>

<ol>
  <li>NodePort 접근 방식
<img src="/assets/2024/kans-3th/w5/20241005_kans_w5_3.png" alt="img.png" class="w-80 image-center" />
    <ul>
      <li>외부 클라이언트는 LoadBalancer의 IP 주소로 요청을 보내면 LoadBalancer는 요청을 받아서 노드들의 NodePort로 부하를 분산하여 전달합니다.</li>
      <li>이때 NodePort로 인입 후에 iptables를 통해 파드로 랜덤 부하분산을 통해 전달합니다.</li>
      <li>이 과정에서 DNAT를 통한 부하 분산과정이 두번 수행됩니다. (LoadBalancer에서 NodePort로 전달될때, 노드의 iptables 룰로 파드 IP로 전달될때)</li>
    </ul>
  </li>
  <li>Pod Direct 접근 방식
<img src="/assets/2024/kans-3th/w5/20241005_kans_w5_4.png" alt="img.png" class="w-80 image-center" />
    <ul>
      <li>LoadBalancer에서 파드의 IP로 직접 부하분산해서 전달합니다.</li>
      <li>LoadBalancer가 파드의 IP 정보를 알기 위해서, 별도의 LoadBalancer Controller를 구성하고 LoadBalancer Controller가 
LoadBalancer에게 파드의 IP를 동적으로 전달합니다.</li>
      <li>이 과정에서 부하 분산과정이 한번 수행되며 NodePort 방식 보다 효율 적입니다.</li>
    </ul>
  </li>
</ol>

<p><img src="/assets/2024/kans-3th/w5/20241005_kans_w5_2.png" alt="클라우드의 LoadBalancer 제공 방식 비교" class="image-center" />
<em class="image-caption">클라우드의 LoadBalancer 제공 방식 비교</em></p>

<h4 id="온프레미스-환경에서의-loadbalancer">온프레미스 환경에서의 LoadBalancer</h4>

<h5 id="하드웨어-장비-기반-loadbalancer-서비스-동작-방식">하드웨어 장비 기반 LoadBalancer 서비스 동작 방식</h5>

<p><img src="/assets/2024/kans-3th/w5/20241005_kans_w5_5.png" alt="img.png" class="w-80 image-center" /></p>
<ul>
  <li>하드웨어 장비 기반 LoadBalancer는 AWS LoadBalancer 서비스와 거의 동일하게 별도의 장비로 접속 후 노드에 NodePort 혹은 파드로 직접
전달하여 통신할 수 있습니다.</li>
  <li>대표적으로 Citrix, F5 Networks의 제품 등이 있습니다.</li>
  <li>예시) Citrix ADC for K8S - <a href="https://www.citrix.com/blogs/2019/09/16/citrix-adc-for-kubernetes-service-of-type-loadbalancer/">링크</a> &amp; Citrix ADC(Ingress/Service) with k8s - <a href="https://www.notion.so/e57b6056f1334c9094f444d1c183f378">링크</a>
<img src="/assets/2024/kans-3th/w5/20241005_kans_w5_17.png" alt="img.png" /></li>
</ul>

<h5 id="소프트웨어-기반-loadbalancer-서비스-동작-방식">소프트웨어 기반 LoadBalancer 서비스 동작 방식</h5>

<ul>
  <li>소프트웨어 기반 LoadBalancer는 별도의 네트워크 장비 없이 소프트웨어로 동작합니다.</li>
  <li>대표적으로 MetalLB, OpenELB, PubeLB, kube-vip, LoxiLB 등이 있습니다.</li>
  <li>MetalLB에 대해서는 좀 더 자세히 알아보겠습니다.</li>
</ul>

<h3 id="metallb">MetalLB</h3>

<ul>
  <li>MetalLB는 Bare<strong>MetalL</strong>oad<strong>B</strong>alancer의 약자로, 온프레미스 환경에서 사용할 수 있는 오픈소스 LoadBalancer입니다.</li>
  <li>쿠버네티스는 DaemonSet으로 Speaker 파드를 생성하여 External IP를 전파합니다. External IP는 노드의 IP 대신 외부에서 
접속할 수 있는 IP 입니다.</li>
  <li>이를 통해 노드의 IP를 외부에 노출하지 않을 수 있어서 보안성을 높일 수 있습니다.</li>
  <li>Speaker 파드는 External IP 전파를 위해 표준 프로토콜인 ARP(Address Resolution Protocol) 혹은 BGP(Border Gateway Protocol)를 사용합니다.</li>
  <li>MetalLB는 일부 퍼블릭 클라우드 플랫폼 환경에서 동작하지 않습니다. 이유는 가상서버 IP에 매칭되는 MAC 주소가 아닌 IP에 대한 ARP 요청을 차단하기 때문입니다.</li>
  <li>또한 일부 CNI에서의 동작에 이슈가 있습니다. Calico의 IPIP 모드에서 BGP 사용시 MetalLB의 BGP와 충돌이 생겨 문제가 발생하곤 합니다.</li>
  <li>실무에서 사용시에는 이슈나 제약사항을 확인하고, 사전 테스트 진행후 사용할 필요가 있습니다.</li>
</ul>

<h4 id="layer2-모드">Layer2 모드</h4>

<ul>
  <li>Layer2 모드는 ARP(Address Resolution Protocol)를 통해서 External IP를 전파합니다.</li>
  <li>ARP란?
<img src="/assets/2024/kans-3th/w5/20241005_kans_w5_6.png" alt="img.png" class="w-80 image-center" />
<em class="image-caption">ARP 동작 모식도 (<a href="https://velog.io/@louie/ARPAddress-Resolution-Protocol">출처</a>)</em>
    <ul>
      <li>동일 네트워크 내부에서 통신을 위해서는 상대방의 MAC(Media Access Control) 주소를 알아야 합니다.</li>
      <li>이때 IP 주소를 전송하면서 이 IP의 주인의 MAC 주소를 알려달라는 패킷을 보내면, 해당 IP 주소를 가진 호스트에서 자신의 MAC 주소를 응답합니다.</li>
      <li>이것이 ARP의 동작 방식이며, ARP 테이블에 IP와 MAC 주소를 저장하고, 이후 통신시 ARP 테이블을 참조하여 통신을 합니다.</li>
    </ul>
  </li>
  <li>ARP에 대해서 알아보았으니 Layer2 동작에 대해 다시 알아보겠습니다.
<img src="/assets/2024/kans-3th/w5/20241005_kans_w5_7.png" alt="img.png" class="image-center" />
<em class="image-caption">MetalLB Layer2 동작 (출처: 추가예정)</em></li>
  <li>위의 그림에서 호스트 NS/파드 NS의 NS는 네임스페이스를 의미하며, 여기서의 네임스페이스는 첫주차 컨테이너 격리에서 배웠던
Linux OS 차원의 네임스페이스를 의미합니다.</li>
  <li>흐름을 파악해보면 아래와 같습니다.
    <ol>
      <li>LoadBalancer 서비스 리소스 생성시 MetalLB 스피커 파드중에 리더(Leader) 스피커 파드가 선택됩니다. 리더 스피커 파드는
해당 LoadBalancer 서비스의 External IP를 가지고 ARP 응답을 합니다. 또한 GARP(Gratuitous ARP)를 통해 네트워크 내의 모든 호스트에게
해당 External IP의 MAC 주소를 전파합니다.
        <ul>
          <li>데몬셋으로 배포된 speaker 파드는 <code class="language-plaintext highlighter-rouge">NetworkMode: host</code>로 호스트 네임스페이스를 공유하며, 호스트 네임스페이스에서 ARP 응답을 합니다.</li>
          <li>만약 리더 스피커 파드에 장애가 발생하면, 다른 스피커 파드가 리더 스피커 파드로 선출됩니다.
            <ul>
              <li>멤버 리스터 및 자애 발견은 hashicorp의 memberlist를 사용합니다.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>클라이언트1이 SVC1의 External IP로 접속을 시도하면, 해당 트래픽은 SVC1의 External IP 정보를 전파하는 리더 스피커파드가 
있는 노드1으로 전달됩니다. 또한 클라이언트2는 SVC2의 External IP로 접속을 시도하면, 해당 트래픽은 SVC2의 External IP 정보를 전파하는
리더 스피커파드가 있는 노드3로 전달됩니다.</li>
      <li>노드에 도착한 트래픽은 해당 노드의 iptables를 통해 ClusterIP와 동일하게 해당 서비스에 연동된 엔드포인트 파드들로
(4) 랜덤 부하분산 되어 전달됩니다.</li>
    </ol>
  </li>
  <li>Layer2 모드의 단점
    <ul>
      <li>single-node bottlenecking : 리더 스피커 파드가 있는 노드에만 트래픽이 인입되어 부하가 집중 됩니다.</li>
      <li>potentially slow failover : 리더 스피커 파드에 장애가 발생하면, 나머지 노드 리더가 선출되고, ARP 전파 및 갱신 완료전까지는
장애가 발생됩니다. (대략 10초~20초 소요)</li>
    </ul>
  </li>
</ul>

<h4 id="bgp-모드">BGP 모드</h4>

<p><img src="/assets/2024/kans-3th/w5/20241005_kans_w5_8.png" alt="img.png" /></p>

<ul>
  <li>BGP 모드는 Routing 프로토콜인 BGP(Border Gateway Protocol)를 통해서 External IP를 전파합니다.
    <ul>
      <li>기본은 IP주소(32bit)를 전파하며, 설정으로 축약된 네트워크 정보를 전파할 수 있습니다. (bgp-advertisements에 aggregation-length 설정)</li>
      <li>BGP 커뮤니티, localpref 등 다양한 BGP 속성을 사용할 수 있습니다.</li>
      <li>IP 주소의 마지막이 0과 255로 끝나는 IP를 처리 못하는 라우터 장비가 있는 경우 <code class="language-plaintext highlighter-rouge">avoid-buggy-ips: true</code> 설정을 통해 IP가 0과 255로 끝나는 IP를 사용하지 않도록 설정할 수 있습니다.</li>
    </ul>
  </li>
  <li>외부에서 라우터를 통해 ECMP(Equal Cost Multi Path) 라우팅을 통해 부하 분산을 지원합니다.
    <ul>
      <li>일반적으로 ECMP는 5-tuple(프로토콜, 출발지 IP, 목적지 IP, 출발지 포트, 목적지 포트)을 기반으로 동작합니다.</li>
      <li>라우터 장비에 따라 다양한 라우팅(분산) 처리가 가능합니다.</li>
    </ul>
  </li>
  <li>BGP 모드의 제한사항
    <ul>
      <li>라우터에서 서비스로 인입이 되기 때문에, 라우터 설정이 중요하며 네트워크 팀과 협업이 권장됩니다.</li>
      <li>Speaker 노드 파드 장애시 BGP Timer 설정 등, 구성하고 있는 네트워크 환경에 맞게 최적화 작업이 필요합니다.</li>
      <li>ECMP 부하 분산 접속시 특정 파드에 부하가 집중되거나, 세션 고정, flapping 등 다양한 환경에 대응이 필요합니다.</li>
      <li>BGP 라우팅 설정 및 라우팅 전파 관련 최적화 설정이 필요합니다.</li>
    </ul>
  </li>
</ul>

<h3 id="metallb-실습">MetalLB 실습</h3>

<h4 id="실습환경-준비">실습환경 준비</h4>

<ul>
  <li>이번에도 KIND를 통해 실습을 진행해보겠습니다.</li>
</ul>

<h5 id="kind-클러스터-구성">KIND 클러스터 구성</h5>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># kind 클러스터 설정 파일 작성</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOT</span><span class="sh">&gt; kind-svc-2w.yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
featureGates:
  "InPlacePodVerticalScaling": true  #실행 중인 파드의 리소스 요청 및 제한을 변경할 수 있게 합니다.
  "MultiCIDRServiceAllocator": true  #서비스에 대해 여러 CIDR 블록을 사용할 수 있게 합니다.
nodes:
- role: control-plane
  labels:
    mynode: control-plane
    topology.kubernetes.io/zone: ap-northeast-2a
  extraPortMappings:  #컨테이너 포트를 호스트 포트에 매핑하여 클러스터 외부에서 서비스에 접근할 수 있도록 합니다.
  - containerPort: 30000
    hostPort: 30000
  - containerPort: 30001
    hostPort: 30001
  - containerPort: 30002
    hostPort: 30002
  - containerPort: 30003
    hostPort: 30003
  - containerPort: 30004
    hostPort: 30004
  kubeadmConfigPatches:
  - |
    kind: ClusterConfiguration
    apiServer:
      extraArgs:  #API 서버에 추가 인수를 제공
        runtime-config: api/all=true  #모든 API 버전을 활성화
    controllerManager:
      extraArgs:
        bind-address: 0.0.0.0
    etcd:
      local:
        extraArgs:
          listen-metrics-urls: http://0.0.0.0:2381
    scheduler:
      extraArgs:
        bind-address: 0.0.0.0
  - |
    kind: KubeProxyConfiguration
    metricsBindAddress: 0.0.0.0
- role: worker
  labels:
    mynode: worker1
    topology.kubernetes.io/zone: ap-northeast-2a
- role: worker
  labels:
    mynode: worker2
    topology.kubernetes.io/zone: ap-northeast-2b
- role: worker
  labels:
    mynode: worker3
    topology.kubernetes.io/zone: ap-northeast-2c
networking:
  podSubnet: 10.10.0.0/16  #파드 IP를 위한 CIDR 범위를 정의합니다. 파드는 이 범위에서 IP를 할당받습니다.
  serviceSubnet: 10.200.1.0/24  #서비스 IP를 위한 CIDR 범위를 정의합니다. 서비스는 이 범위에서 IP를 할당받습니다.
</span><span class="no">EOT

</span><span class="c"># k8s 클러스터 설치</span>
<span class="nv">$ </span>kind create cluster <span class="nt">--config</span> kind-svc-2w.yaml <span class="nt">--name</span> myk8s <span class="nt">--image</span> kindest/node:v1.31.0
<span class="nv">$ </span>docker ps
<span class="c"># =&gt; CONTAINER ID   IMAGE                  COMMAND                  CREATED          STATUS          PORTS                                                             NAMES</span>
<span class="c">#    83661e652fb1   kindest/node:v1.31.0   &amp;quot;/usr/local/bin/entr…&amp;quot;   39 seconds ago   Up 34 seconds   0.0.0.0:30000-30004-&amp;gt;30000-30004/tcp, 127.0.0.1:59215-&amp;gt;6443/tcp   myk8s-control-plane</span>
<span class="c">#    242777ad8f3c   kindest/node:v1.31.0   &amp;quot;/usr/local/bin/entr…&amp;quot;   39 seconds ago   Up 34 seconds                                                                     myk8s-worker</span>
<span class="c">#    f8022585c864   kindest/node:v1.31.0   &amp;quot;/usr/local/bin/entr…&amp;quot;   39 seconds ago   Up 34 seconds                                                                     myk8s-worker2</span>
<span class="c">#    80988133cdfc   kindest/node:v1.31.0   &amp;quot;/usr/local/bin/entr…&amp;quot;   39 seconds ago   Up 34 seconds                                                                     myk8s-worker3</span>

<span class="c"># 노드에 기본 툴 설치</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane sh <span class="nt">-c</span> <span class="s1">'apt update &amp;&amp; apt install tree psmisc lsof wget bsdmainutils bridge-utils net-tools dnsutils ipset ipvsadm nfacct tcpdump ngrep iputils-ping arping git vim arp-scan -y'</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> sh <span class="nt">-c</span> <span class="s1">'apt update &amp;&amp; apt install tree psmisc lsof wget bsdmainutils bridge-utils net-tools dnsutils ipset ipvsadm nfacct tcpdump ngrep iputils-ping arping -y'</span><span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>

<span class="c"># k8s v1.31.0 버전 확인</span>
<span class="nv">$ </span>kubectl get node
<span class="c"># =&gt; NAME                  STATUS   ROLES           AGE    VERSION</span>
<span class="c">#    myk8s-control-plane   Ready    control-plane   110s   v1.31.0</span>
<span class="c">#    myk8s-worker          Ready    &amp;lt;none&amp;gt;          100s   v1.31.0</span>
<span class="c">#    myk8s-worker2         Ready    &amp;lt;none&amp;gt;          100s   v1.31.0</span>
<span class="c">#    myk8s-worker3         Ready    &amp;lt;none&amp;gt;          100s   v1.31.0</span>

<span class="c"># 노드 labels 확인</span>
<span class="nv">$ </span>kubectl get nodes <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.items[*].metadata.labels}"</span> | jq
<span class="c"># =&gt; {</span>
<span class="c">#      &amp;quot;kubernetes.io/hostname&amp;quot;: &amp;quot;myk8s-control-plane&amp;quot;,</span>
<span class="c">#      &amp;quot;mynode&amp;quot;: &amp;quot;control-plane&amp;quot;,</span>
<span class="c">#      ...</span>
<span class="c">#    }</span>
<span class="c">#    {</span>
<span class="c">#      &amp;quot;kubernetes.io/hostname&amp;quot;: &amp;quot;myk8s-worker&amp;quot;,</span>
<span class="c">#      &amp;quot;mynode&amp;quot;: &amp;quot;worker1&amp;quot;,</span>
<span class="c">#      ...</span>
<span class="c">#    }</span>
<span class="c">#    {</span>
<span class="c">#      &amp;quot;kubernetes.io/hostname&amp;quot;: &amp;quot;myk8s-worker2&amp;quot;,</span>
<span class="c">#      &amp;quot;mynode&amp;quot;: &amp;quot;worker2&amp;quot;,</span>
<span class="c">#      ...</span>
<span class="c">#    }</span>
<span class="c">#    {</span>
<span class="c">#      &amp;quot;kubernetes.io/hostname&amp;quot;: &amp;quot;myk8s-worker3&amp;quot;,</span>
<span class="c">#      &amp;quot;mynode&amp;quot;: &amp;quot;worker3&amp;quot;,</span>
<span class="c">#      ...</span>
<span class="c">#    }</span>

<span class="c"># kind network 중 컨테이너(노드) IP(대역) 확인</span>
<span class="nv">$ </span>docker ps <span class="nt">-q</span> | xargs docker inspect <span class="nt">--format</span> <span class="s1">' '</span>
<span class="c"># =&gt; /myk8s-control-plane 172.20.0.5</span>
<span class="c">#    /myk8s-worker 172.20.0.4</span>
<span class="c">#    /myk8s-worker2 172.20.0.2</span>
<span class="c">#    /myk8s-worker3 172.20.0.3</span>

<span class="c"># 파드CIDR 과 Service 대역 확인 : CNI는 kindnet 사용</span>
<span class="nv">$ </span>kubectl get cm <span class="nt">-n</span> kube-system kubeadm-config <span class="nt">-oyaml</span> | <span class="nb">grep</span> <span class="nt">-i</span> subnet
<span class="c"># =&gt; podSubnet: 10.10.0.0/16</span>
<span class="c">#    serviceSubnet: 10.200.1.0/24</span>
<span class="nv">$ </span>kubectl cluster-info dump | <span class="nb">grep</span> <span class="nt">-m</span> 2 <span class="nt">-E</span> <span class="s2">"cluster-cidr|service-cluster-ip-range"</span>
<span class="c"># =&gt; &amp;quot;--service-cluster-ip-range=10.200.1.0/24&amp;quot;,</span>
<span class="c">#    &amp;quot;--cluster-cidr=10.10.0.0/16&amp;quot;,</span>

<span class="c"># MultiCIDRServiceAllocator : https://kubernetes.io/docs/tasks/network/extend-service-ip-ranges/</span>
<span class="nv">$ </span>kubectl get servicecidr
<span class="c"># =&gt; NAME         CIDRS           AGE</span>
<span class="c">#    kubernetes   10.200.1.0/24   4m59s</span>

<span class="c"># 노드마다 할당된 dedicated subnet (podCIDR) 확인</span>
<span class="nv">$ </span>kubectl get nodes <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.items[*].spec.podCIDR}"</span>
<span class="c"># =&gt; 10.10.0.0/24 10.10.3.0/24 10.10.2.0/24 10.10.1.0/24</span>

<span class="c"># kube-proxy configmap 확인</span>
<span class="nv">$ </span>kubectl describe cm <span class="nt">-n</span> kube-system kube-proxy
<span class="c"># =&gt; ...</span>
<span class="c">#    mode: iptables</span>
<span class="c">#    iptables:</span>
<span class="c">#      localhostNodePorts: null</span>
<span class="c">#      masqueradeAll: false</span>
<span class="c">#      masqueradeBit: null</span>
<span class="c">#      minSyncPeriod: 1s</span>
<span class="c">#      syncPeriod: 0s</span>
<span class="c">#    ...</span>

<span class="c"># 노드 별 네트워트 정보 확인 : CNI는 kindnet 사용</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> <span class="nb">cat</span> /etc/cni/net.d/10-kindnet.conflist<span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; &amp;gt;&amp;gt; node myk8s-control-plane &amp;lt;&amp;lt;</span>
<span class="c">#    {</span>
<span class="c">#     &amp;quot;cniVersion&amp;quot;: &amp;quot;0.3.1&amp;quot;,</span>
<span class="c">#     &amp;quot;name&amp;quot;: &amp;quot;kindnet&amp;quot;,</span>
<span class="c">#     &amp;quot;plugins&amp;quot;: [</span>
<span class="c">#     {</span>
<span class="c">#       &amp;quot;type&amp;quot;: &amp;quot;ptp&amp;quot;,</span>
<span class="c">#       &amp;quot;ipMasq&amp;quot;: false,</span>
<span class="c">#       &amp;quot;ipam&amp;quot;: {</span>
<span class="c">#         &amp;quot;type&amp;quot;: &amp;quot;host-local&amp;quot;,</span>
<span class="c">#         &amp;quot;dataDir&amp;quot;: &amp;quot;/run/cni-ipam-state&amp;quot;,</span>
<span class="c">#         &amp;quot;routes&amp;quot;: [</span>
<span class="c">#           { &amp;quot;dst&amp;quot;: &amp;quot;0.0.0.0/0&amp;quot; }</span>
<span class="c">#         ],</span>
<span class="c">#         &amp;quot;ranges&amp;quot;: [</span>
<span class="c">#           [ { &amp;quot;subnet&amp;quot;: &amp;quot;10.10.0.0/24&amp;quot; } ]</span>
<span class="c">#         ]</span>
<span class="c">#       },</span>
<span class="c">#       &amp;quot;mtu&amp;quot;: 1500</span>
<span class="c">#     },</span>
<span class="c">#     ...</span>
<span class="c">#     ]</span>
<span class="c">#    }</span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; node myk8s-worker &amp;lt;&amp;lt;</span>
<span class="c">#    ...</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> ip <span class="nt">-c</span> route<span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; &amp;gt;&amp;gt; node myk8s-control-plane &amp;lt;&amp;lt;</span>
<span class="c">#    default via &lt;span style="color:purple;"&gt;172.20.0.1 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:purple;"&gt;10.10.0.2 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;veth545bb56e &lt;/span&gt;scope host </span>
<span class="c">#    &lt;span style="color:purple;"&gt;10.10.0.3 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;veth184fcd53 &lt;/span&gt;scope host </span>
<span class="c">#    &lt;span style="color:purple;"&gt;10.10.0.4 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;vethc5dfe430 &lt;/span&gt;scope host </span>
<span class="c">#    &lt;span style="color:purple;"&gt;10.10.1.0/24 &lt;/span&gt;via &lt;span style="color:purple;"&gt;172.20.0.3 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:purple;"&gt;10.10.2.0/24 &lt;/span&gt;via &lt;span style="color:purple;"&gt;172.20.0.2 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:purple;"&gt;10.10.3.0/24 &lt;/span&gt;via &lt;span style="color:purple;"&gt;172.20.0.4 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.0.0/16 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;proto kernel scope link src &lt;span style="color:purple;"&gt;172.20.0.5 &lt;/span&gt;</span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; node myk8s-worker &amp;lt;&amp;lt;</span>
<span class="c">#    default via &lt;span style="color:purple;"&gt;172.20.0.1 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:purple;"&gt;10.10.0.0/24 &lt;/span&gt;via &lt;span style="color:purple;"&gt;172.20.0.5 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:purple;"&gt;10.10.1.0/24 &lt;/span&gt;via &lt;span style="color:purple;"&gt;172.20.0.3 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:purple;"&gt;10.10.2.0/24 &lt;/span&gt;via &lt;span style="color:purple;"&gt;172.20.0.2 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.0.0/16 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;proto kernel scope link src &lt;span style="color:purple;"&gt;172.20.0.4 &lt;/span&gt;</span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; node myk8s-worker2 &amp;lt;&amp;lt;</span>
<span class="c">#    default via &lt;span style="color:purple;"&gt;172.20.0.1 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:purple;"&gt;10.10.0.0/24 &lt;/span&gt;via &lt;span style="color:purple;"&gt;172.20.0.5 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:purple;"&gt;10.10.1.0/24 &lt;/span&gt;via &lt;span style="color:purple;"&gt;172.20.0.3 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:purple;"&gt;10.10.3.0/24 &lt;/span&gt;via &lt;span style="color:purple;"&gt;172.20.0.4 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.0.0/16 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;proto kernel scope link src &lt;span style="color:purple;"&gt;172.20.0.2 &lt;/span&gt;</span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; node myk8s-worker3 &amp;lt;&amp;lt;</span>
<span class="c">#    default via &lt;span style="color:purple;"&gt;172.20.0.1 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:purple;"&gt;10.10.0.0/24 &lt;/span&gt;via &lt;span style="color:purple;"&gt;172.20.0.5 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:purple;"&gt;10.10.2.0/24 &lt;/span&gt;via &lt;span style="color:purple;"&gt;172.20.0.2 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:purple;"&gt;10.10.3.0/24 &lt;/span&gt;via &lt;span style="color:purple;"&gt;172.20.0.4 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.0.0/16 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;proto kernel scope link src &lt;span style="color:purple;"&gt;172.20.0.3 &lt;/span&gt;</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> ip <span class="nt">-c</span> addr<span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; &amp;gt;&amp;gt; node myk8s-control-plane &amp;lt;&amp;lt;</span>
<span class="c">#    1: &lt;span style="color:teal;"&gt;lo: &lt;/span&gt;&amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</span>
<span class="c">#        link/loopback &lt;span style="color:olive;"&gt;00:00:00:00:00:00&lt;/span&gt; brd &lt;span style="color:olive;"&gt;00:00:00:00:00:00&lt;/span&gt;</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;127.0.0.1&lt;/span&gt;/8 scope host lo</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    2: &lt;span style="color:teal;"&gt;tunl0@NONE: &lt;/span&gt;&amp;lt;NOARP&amp;gt; mtu 1480 qdisc noop state &lt;span style="color:red;"&gt;DOWN &lt;/span&gt;group default qlen 1000</span>
<span class="c">#        link/ipip &lt;span style="color:olive;"&gt;0.0.0.0&lt;/span&gt; brd &lt;span style="color:olive;"&gt;0.0.0.0&lt;/span&gt;</span>
<span class="c">#    4: &lt;span style="color:teal;"&gt;veth545bb56e@if4: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state &lt;span style="color:green;"&gt;UP &lt;/span&gt;group default </span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;0e:8b:3c:4f:43:43&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt; link-netns cni-98b7b37a-bb7a-ea56-47c9-ce3a0b1fb08a</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;10.10.0.1&lt;/span&gt;/32 scope global veth545bb56e</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    5: &lt;span style="color:teal;"&gt;vethc5dfe430@if4: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state &lt;span style="color:green;"&gt;UP &lt;/span&gt;group default </span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;8a:70:dd:42:02:96&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt; link-netns cni-79ddddfd-6177-bbd6-5fdc-3f7f6bf07fdc</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;10.10.0.1&lt;/span&gt;/32 scope global vethc5dfe430</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    6: &lt;span style="color:teal;"&gt;veth184fcd53@if4: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state &lt;span style="color:green;"&gt;UP &lt;/span&gt;group default </span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;8a:92:74:11:f9:d9&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt; link-netns cni-5e4ecb7e-2120-f372-76cc-9a467c85159b</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;10.10.0.1&lt;/span&gt;/32 scope global veth184fcd53</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    28: &lt;span style="color:teal;"&gt;eth0@if29: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state &lt;span style="color:green;"&gt;UP &lt;/span&gt;group default </span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;02:42:ac:14:00:05&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt; link-netnsid 0</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;172.20.0.5&lt;/span&gt;/16 brd &lt;span style="color:purple;"&gt;172.20.255.255 &lt;/span&gt;scope global eth0</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; node myk8s-worker &amp;lt;&amp;lt;</span>
<span class="c">#    1: &lt;span style="color:teal;"&gt;lo: &lt;/span&gt;&amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</span>
<span class="c">#        link/loopback &lt;span style="color:olive;"&gt;00:00:00:00:00:00&lt;/span&gt; brd &lt;span style="color:olive;"&gt;00:00:00:00:00:00&lt;/span&gt;</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;127.0.0.1&lt;/span&gt;/8 scope host lo</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    2: &lt;span style="color:teal;"&gt;tunl0@NONE: &lt;/span&gt;&amp;lt;NOARP&amp;gt; mtu 1480 qdisc noop state &lt;span style="color:red;"&gt;DOWN &lt;/span&gt;group default qlen 1000</span>
<span class="c">#        link/ipip &lt;span style="color:olive;"&gt;0.0.0.0&lt;/span&gt; brd &lt;span style="color:olive;"&gt;0.0.0.0&lt;/span&gt;</span>
<span class="c">#    26: &lt;span style="color:teal;"&gt;eth0@if27: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state &lt;span style="color:green;"&gt;UP &lt;/span&gt;group default </span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;02:42:ac:14:00:04&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt; link-netnsid 0</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;172.20.0.4&lt;/span&gt;/16 brd &lt;span style="color:purple;"&gt;172.20.255.255 &lt;/span&gt;scope global eth0</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    ...</span>

<span class="c"># iptables 정보 확인</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>filter nat mangle raw <span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; IPTables Type : </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane  iptables <span class="nt">-t</span> <span class="nv">$i</span> <span class="nt">-S</span> <span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>filter nat mangle raw <span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; IPTables Type : </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker  iptables <span class="nt">-t</span> <span class="nv">$i</span> <span class="nt">-S</span> <span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>filter nat mangle raw <span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; IPTables Type : </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker2 iptables <span class="nt">-t</span> <span class="nv">$i</span> <span class="nt">-S</span> <span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>filter nat mangle raw <span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; IPTables Type : </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker3 iptables <span class="nt">-t</span> <span class="nv">$i</span> <span class="nt">-S</span> <span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>

<span class="c"># 각 노드 bash 접속</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane bash
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker bash
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker2 bash
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker3 bash
<span class="c"># ----------------------------------------</span>
<span class="nv">$ </span><span class="nb">exit</span>
<span class="c"># ----------------------------------------</span>

<span class="c"># kind 설치 시 kind 이름의 도커 브리지가 생성된다 : 172.20.0.0/16 대역</span>
<span class="nv">$ </span>docker network <span class="nb">ls</span>
<span class="c"># =&gt; NETWORK ID     NAME                     DRIVER    SCOPE</span>
<span class="c">#    a8d530305515   bridge                   bridge    local</span>
<span class="c">#    8204a0851463   host                     host      local</span>
<span class="c">#    3bbcc6aa8f38   kind                     bridge    local</span>

<span class="nv">$ </span>docker inspect kind
<span class="c"># =&gt; [</span>
<span class="c">#        {</span>
<span class="c">#            &amp;quot;Name&amp;quot;: &amp;quot;kind&amp;quot;,</span>
<span class="c">#            &amp;quot;Id&amp;quot;: &amp;quot;3bbcc6aa8f388f86f02478f41de1e4dd917e5812b6cf6257972e4af0bedf5021&amp;quot;,</span>
<span class="c">#            &amp;quot;Created&amp;quot;: &amp;quot;2020-01-01T11:37:09.195259833Z&amp;quot;,</span>
<span class="c">#            &amp;quot;Scope&amp;quot;: &amp;quot;local&amp;quot;,</span>
<span class="c">#            &amp;quot;Driver&amp;quot;: &amp;quot;bridge&amp;quot;,</span>
<span class="c">#            &amp;quot;IPAM&amp;quot;: {</span>
<span class="c">#                &amp;quot;Driver&amp;quot;: &amp;quot;default&amp;quot;,</span>
<span class="c">#                &amp;quot;Options&amp;quot;: {},</span>
<span class="c">#                &amp;quot;Config&amp;quot;: [</span>
<span class="c">#                    {</span>
<span class="c">#                        &amp;quot;Subnet&amp;quot;: &amp;quot;172.20.0.0/16&amp;quot;,</span>
<span class="c">#                        &amp;quot;Gateway&amp;quot;: &amp;quot;172.20.0.1&amp;quot;</span>
<span class="c">#                    }</span>
<span class="c">#                ]</span>
<span class="c">#            },</span>
<span class="c">#            &amp;quot;Internal&amp;quot;: false,</span>
<span class="c">#            &amp;quot;Attachable&amp;quot;: false,</span>
<span class="c">#            &amp;quot;Ingress&amp;quot;: false,</span>
<span class="c">#            &amp;quot;ConfigFrom&amp;quot;: {</span>
<span class="c">#                &amp;quot;Network&amp;quot;: &amp;quot;&amp;quot;</span>
<span class="c">#            },</span>
<span class="c">#            &amp;quot;ConfigOnly&amp;quot;: false,</span>
<span class="c">#            &amp;quot;Containers&amp;quot;: {</span>
<span class="c">#                &amp;quot;242777ad8f3c7009963155c3d7c4551e1407570d6986d9ef6346e6d33990e538&amp;quot;: {</span>
<span class="c">#                    &amp;quot;Name&amp;quot;: &amp;quot;myk8s-worker&amp;quot;,</span>
<span class="c">#                    &amp;quot;EndpointID&amp;quot;: &amp;quot;f6fb304fa38125ed1075d9c71b83559cff5066e71630c272e94311258021144e&amp;quot;,</span>
<span class="c">#                    &amp;quot;MacAddress&amp;quot;: &amp;quot;02:42:ac:14:00:04&amp;quot;,</span>
<span class="c">#                    &amp;quot;IPv4Address&amp;quot;: &amp;quot;172.20.0.4/16&amp;quot;,</span>
<span class="c">#                },</span>
<span class="c">#                &amp;quot;80988133cdfcfaafe520b35cec924b9fa87f26ea474102b833e92d7ca693fb2b&amp;quot;: {</span>
<span class="c">#                    &amp;quot;Name&amp;quot;: &amp;quot;myk8s-worker3&amp;quot;,</span>
<span class="c">#                    &amp;quot;EndpointID&amp;quot;: &amp;quot;42aec973b496fdc7b8ede07c11fd94fe35631216d3ecd54d2ab794849b834787&amp;quot;,</span>
<span class="c">#                    &amp;quot;MacAddress&amp;quot;: &amp;quot;02:42:ac:14:00:03&amp;quot;,</span>
<span class="c">#                    &amp;quot;IPv4Address&amp;quot;: &amp;quot;172.20.0.3/16&amp;quot;,</span>
<span class="c">#                },</span>
<span class="c">#                &amp;quot;83661e652fb1d34542b760209f670f330e25b1c51c8c0404e69d47eb9c79f407&amp;quot;: {</span>
<span class="c">#                    &amp;quot;Name&amp;quot;: &amp;quot;myk8s-control-plane&amp;quot;,</span>
<span class="c">#                    &amp;quot;EndpointID&amp;quot;: &amp;quot;d1e1efb2d90b7d8e9ce16b6274a62e7799d923681739dc8826c36c8b122d09c0&amp;quot;,</span>
<span class="c">#                    &amp;quot;MacAddress&amp;quot;: &amp;quot;02:42:ac:14:00:05&amp;quot;,</span>
<span class="c">#                    &amp;quot;IPv4Address&amp;quot;: &amp;quot;172.20.0.5/16&amp;quot;,</span>
<span class="c">#                },</span>
<span class="c">#                &amp;quot;f8022585c864bd53b31b84e22e2b4381da6c5b7a2ada1583f18136e7f8c6b3b9&amp;quot;: {</span>
<span class="c">#                    &amp;quot;Name&amp;quot;: &amp;quot;myk8s-worker2&amp;quot;,</span>
<span class="c">#                    &amp;quot;EndpointID&amp;quot;: &amp;quot;0866892fb0c2c1c8c2021d92a665a130a20aae5b76fbdc1549138da642a60883&amp;quot;,</span>
<span class="c">#                    &amp;quot;MacAddress&amp;quot;: &amp;quot;02:42:ac:14:00:02&amp;quot;,</span>
<span class="c">#                    &amp;quot;IPv4Address&amp;quot;: &amp;quot;172.20.0.2/16&amp;quot;,</span>
<span class="c">#                }</span>
<span class="c">#            },</span>
<span class="c">#            &amp;quot;Options&amp;quot;: {</span>
<span class="c">#                &amp;quot;com.docker.network.bridge.enable_ip_masquerade&amp;quot;: &amp;quot;true&amp;quot;,</span>
<span class="c">#                &amp;quot;com.docker.network.driver.mtu&amp;quot;: &amp;quot;1500&amp;quot;</span>
<span class="c">#            },</span>
<span class="c">#            &amp;quot;Labels&amp;quot;: {}</span>
<span class="c">#        }</span>
<span class="c">#    ]</span>

<span class="c"># arp scan 해두기</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane arp-scan <span class="nt">--interfac</span><span class="o">=</span>eth0 <span class="nt">--localnet</span>
<span class="c"># =&gt; Interface: eth0, type: EN10MB, MAC: 02:42:ac:14:00:05, IPv4: 172.20.0.5</span>
<span class="c">#    Starting arp-scan 1.10.0 with 65536 hosts (https://github.com/royhills/arp-scan)</span>
<span class="c">#    172.20.0.1      02:42:a0:b9:45:0f       (Unknown: locally administered)</span>
<span class="c">#    172.20.0.2      02:42:ac:14:00:02       (Unknown: locally administered)</span>
<span class="c">#    172.20.0.3      02:42:ac:14:00:03       (Unknown: locally administered)</span>
<span class="c">#    172.20.0.4      02:42:ac:14:00:04       (Unknown: locally administered)</span>

<span class="c"># mypc 컨테이너 기동 : kind 도커 브리지를 사용하고, 컨테이너 IP를 지정 없이 혹은 지정 해서 사용</span>
<span class="nv">$ </span>docker run <span class="nt">-d</span> <span class="nt">--rm</span> <span class="nt">--name</span> mypc <span class="nt">--network</span> kind <span class="nt">--ip</span> 172.20.0.100 nicolaka/netshoot <span class="nb">sleep </span>infinity <span class="c"># IP 지정 실행 시</span>
<span class="c"># =&gt; docker: Error response from daemon: Invalid address 172.20.0.100: It does not belong to any of this network's subnets.</span>
<span class="c"># IP 지정 실행 시 에러 발생 시 아래 처럼 IP 지정 없이 실행</span>
<span class="nv">$ </span>docker run <span class="nt">-d</span> <span class="nt">--rm</span> <span class="nt">--name</span> mypc <span class="nt">--network</span> kind nicolaka/netshoot <span class="nb">sleep </span>infinity <span class="c"># IP 지정 없이 실행 시</span>
<span class="c"># =&gt; 5863ee53a7334a4a524c8c965b2505237c43037ff33f435340b6c167e3484eb6</span>
<span class="nv">$ </span>docker ps
<span class="c"># =&gt; CONTAINER ID   IMAGE                  COMMAND                  CREATED          STATUS          PORTS                                                             NAMES</span>
<span class="c">#    5863ee53a733   nicolaka/netshoot      &amp;quot;sleep infinity&amp;quot;         15 seconds ago   Up 14 seconds                                                                     mypc</span>
<span class="c">#    ...</span>

<span class="c"># mypc2 컨테이너 기동 : kind 도커 브리지를 사용하고, 컨테이너 IP를 지정 없이 혹은 지정 해서 사용</span>
<span class="nv">$ </span>docker run <span class="nt">-d</span> <span class="nt">--rm</span> <span class="nt">--name</span> mypc2 <span class="nt">--network</span> kind <span class="nt">--ip</span> 172.20.0.200 nicolaka/netshoot <span class="nb">sleep </span>infinity <span class="c"># IP 지정 실행 시</span>
<span class="c"># =&gt; docker: Error response from daemon: Invalid address 172.20.0.200: It does not belong to any of this network's subnets.</span>
<span class="c"># IP 지정 실행 시 에러 발생 시 아래 처럼 IP 지정 없이 실행</span>
<span class="nv">$ </span>docker run <span class="nt">-d</span> <span class="nt">--rm</span> <span class="nt">--name</span> mypc2 <span class="nt">--network</span> kind nicolaka/netshoot <span class="nb">sleep </span>infinity <span class="c"># IP 지정 없이 실행 시</span>
<span class="c"># =&gt; 0d1d3bc32161bafcf5e188e4788553c88cd278d0a2e8dac02d42216e80a9985c</span>
<span class="nv">$ </span>docker ps
<span class="c"># =&gt; CONTAINER ID   IMAGE                  COMMAND                  CREATED              STATUS              PORTS                                                             NAMES</span>
<span class="c">#    0d1d3bc32161   nicolaka/netshoot      &amp;quot;sleep infinity&amp;quot;         9 seconds ago        Up 7 seconds                                                                          mypc2</span>
<span class="c">#    5863ee53a733   nicolaka/netshoot      &amp;quot;sleep infinity&amp;quot;         About a minute ago   Up About a minute                                                                     mypc</span>
<span class="c">#    ...</span>

<span class="c"># kind network 중 컨테이너(노드) IP(대역) 확인</span>
<span class="nv">$ </span>docker ps <span class="nt">-q</span> | xargs docker inspect <span class="nt">--format</span> <span class="s1">' '</span>
<span class="c"># =&gt; /myk8s-control-plane 172.20.0.5</span>
<span class="c">#    /myk8s-worker 172.20.0.4</span>
<span class="c">#    /myk8s-worker2 172.20.0.2</span>
<span class="c">#    /myk8s-worker3 172.20.0.3</span>
<span class="c">#    /mypc 172.20.0.6</span>
<span class="c">#    /mypc2 172.20.0.7</span>

<span class="c"># kube-ops-view 설치</span>
<span class="nv">$ </span>helm repo add geek-cookbook https://geek-cookbook.github.io/charts/
<span class="nv">$ </span>helm <span class="nb">install </span>kube-ops-view geek-cookbook/kube-ops-view <span class="nt">--version</span> 1.2.2 <span class="nt">--set</span> service.main.type<span class="o">=</span>NodePort,service.main.ports.http.nodePort<span class="o">=</span>30000 <span class="nt">--set</span> env.TZ<span class="o">=</span><span class="s2">"Asia/Seoul"</span> <span class="nt">--namespace</span> kube-system
<span class="c"># =&gt; NAME: kube-ops-view</span>
<span class="c">#    LAST DEPLOYED: Sun Jan  1 15:57:45 2020</span>
<span class="c">#    NAMESPACE: kube-system</span>
<span class="c">#    STATUS: deployed</span>
<span class="c">#    REVISION: 1</span>
<span class="c">#    TEST SUITE: None</span>
<span class="c">#    NOTES:</span>
<span class="c">#    1. Get the application URL by running these commands:</span>
<span class="c">#      export NODE_PORT=$(kubectl get --namespace kube-system -o jsonpath="{.spec.ports[0].nodePort}" services kube-ops-view)</span>
<span class="c">#      export NODE_IP=$(kubectl get nodes --namespace kube-system -o jsonpath="{.items[0].status.addresses[0].address}")</span>
<span class="c">#      echo http://$NODE_IP:$NODE_PORT</span>

<span class="c"># myk8s-control-plane 배치하기 위해서 nodeSelector, tolerations 설정</span>
<span class="nv">$ </span>kubectl <span class="nt">-n</span> kube-system edit deploy kube-ops-view
<span class="c"># =&gt; ---</span>
<span class="c">#    spec:</span>
<span class="c">#      ...</span>
<span class="c">#      template:</span>
<span class="c">#        ...</span>
<span class="c">#        spec:</span>
<span class="c">#          nodeSelector:</span>
<span class="c">#            mynode: control-plane</span>
<span class="c">#          tolerations:</span>
<span class="c">#          - key: "node-role.kubernetes.io/control-plane"</span>
<span class="c">#            operator: "Equal"</span>
<span class="c">#            effect: "NoSchedule"</span>
<span class="c">#    ---</span>

<span class="c"># 설치 확인</span>
<span class="nv">$ </span>kubectl <span class="nt">-n</span> kube-system get pod <span class="nt">-o</span> wide <span class="nt">-l</span> app.kubernetes.io/instance<span class="o">=</span>kube-ops-view
<span class="c"># =&gt; NAME                             READY   STATUS              RESTARTS   AGE   IP       NODE                  NOMINATED NODE   READINESS GATES</span>
<span class="c">#    kube-ops-view-58f96c464d-kp8l8   0/1     ContainerCreating   0          5s    &amp;lt;none&amp;gt;   &lt;span style="color: red;"&gt;myk8s-control-plane&lt;/span&gt;   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>

<span class="c"># kube-ops-view 접속 URL 확인 (1.5 , 2 배율) : macOS 사용자</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"KUBE-OPS-VIEW URL = http://localhost:30000/#scale=1.5"</span>
<span class="c"># =&gt; KUBE-OPS-VIEW URL = http://localhost:30000/#scale=1.5</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"KUBE-OPS-VIEW URL = http://localhost:30000/#scale=2"</span>

<span class="c"># kube-ops-view 접속 URL 확인 (1.5 , 2 배율) : Windows 사용자</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"KUBE-OPS-VIEW URL = http://192.168.50.10:30000/#scale=1.5"</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"KUBE-OPS-VIEW URL = http://192.168.50.10:30000/#scale=2"</span>

<span class="c"># kube-ops-view 접속 URL 확인 (1.5 , 2 배율) : AWS_EC2 사용자</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"KUBE-OPS-VIEW URL = http://</span><span class="si">$(</span>curl <span class="nt">-s</span> ipinfo.io/ip<span class="si">)</span><span class="s2">:30000/#scale=1.5"</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"KUBE-OPS-VIEW URL = http://</span><span class="si">$(</span>curl <span class="nt">-s</span> ipinfo.io/ip<span class="si">)</span><span class="s2">:30000/#scale=2"</span>
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w5/20241005_kans_w5_9.png" alt="img.png" class="image-center" />
<em class="image-caption">실습환경이 구축 완료된 kube-ops-view 화면</em></p>

<h5 id="프로메테우스-스택-설치">프로메테우스 스택 설치</h5>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#</span>
<span class="nv">$ </span>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
<span class="c"># =&gt; "prometheus-community" has been added to your repositories</span>

<span class="c"># 파라미터 파일 생성</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOT</span><span class="sh"> &gt; monitor-values.yaml
prometheus:
  service:
    type: NodePort
    nodePort: 30001

  prometheusSpec:
    podMonitorSelectorNilUsesHelmValues: false
    serviceMonitorSelectorNilUsesHelmValues: false
    nodeSelector:
      mynode: control-plane
    tolerations:
    - key: "node-role.kubernetes.io/control-plane"
      operator: "Equal"
      effect: "NoSchedule"


grafana:
  defaultDashboardsTimezone: Asia/Seoul
  adminPassword: kans1234

  service:
    type: NodePort
    nodePort: 30002
  nodeSelector:
    mynode: control-plane
  tolerations:
  - key: "node-role.kubernetes.io/control-plane"
    operator: "Equal"
    effect: "NoSchedule"

  #  sidecar:
  #    dashboards:
  #      enabled: true
  #  dashboards:
  #    default:
  #      custom-dashboard:
  #        gnetId: 20162  # MetalLB 대시보드 ID
  #        datasource: Prometheus  # 사용할 데이터소스 이름을 명시
  #        revision: 1    # 대시보드의 버전

defaultRules:
  create: false
alertmanager:
  enabled: false
</span><span class="no">EOT

</span><span class="c"># 배포</span>
<span class="nv">$ </span>kubectl create ns monitoring
<span class="c"># =&gt; namespace/monitoring created</span>
<span class="nv">$ </span>helm <span class="nb">install </span>kube-prometheus-stack prometheus-community/kube-prometheus-stack <span class="nt">--version</span> 62.3.0 <span class="nt">-f</span> monitor-values.yaml <span class="nt">--namespace</span> monitoring
<span class="c"># =&gt; NAME: kube-prometheus-stack</span>
<span class="c">#    LAST DEPLOYED: Sun Jan  1 16:16:32 2020</span>
<span class="c">#    NAMESPACE: monitoring</span>
<span class="c">#    STATUS: deployed</span>
<span class="c">#    REVISION: 1</span>
<span class="c">#    NOTES:</span>
<span class="c">#    kube-prometheus-stack has been installed. Check its status by running:</span>
<span class="c">#      kubectl --namespace monitoring get pods -l &amp;quot;release=kube-prometheus-stack&amp;quot;</span>
<span class="c">#    </span>
<span class="c">#    Visit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create &amp;amp; configure Alertmanager and Prometheus instances using the Operator.</span>

<span class="c"># 확인</span>
<span class="nv">$ </span>helm list <span class="nt">-n</span> monitoring
<span class="c"># =&gt; NAME                   NAMESPACE   REVISION  UPDATED                               STATUS    CHART                         APP VERSION</span>
<span class="c">#    kube-prometheus-stack  monitoring  1         2020-01-01 16:16:32.988771 +0900 KST  deployed  kube-prometheus-stack-62.3.0  v0.76.0    </span>

<span class="c"># Grafana 접속 계정 : admin / kans1234 : macOS 사용자</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"Prometheus URL = http://localhost:30001"</span>
<span class="c"># =&gt; Prometheus URL = http://localhost:30001</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"Grafana URL = http://localhost:30002"</span>
<span class="c"># =&gt; Grafana URL = http://localhost:30002</span>

<span class="c"># Grafana 접속 계정 : admin / kans1234 : Windows 사용자</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"Prometheus URL = http://192.168.50.10:30001"</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"Grafana URL = http://192.168.50.10:30002"</span>

<span class="c"># Grafana 접속 계정 : admin / kans1234 : AWS_EC2 사용자</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"Prometheus URL = http://</span><span class="si">$(</span>curl <span class="nt">-s</span> ipinfo.io/ip<span class="si">)</span><span class="s2">:30001"</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"Grafana URL = http://</span><span class="si">$(</span>curl <span class="nt">-s</span> ipinfo.io/ip<span class="si">)</span><span class="s2">:30002"</span>

<span class="c"># (참고) helm 삭제</span>
<span class="nv">$ </span>helm uninstall <span class="nt">-n</span> monitoring kube-prometheus-stack
</code></pre></div></div>

<ul>
  <li>그라파나 접속 후 MetalLB 대시보드 import
    <ul>
      <li>Dashboards &gt; Manage &gt; Import</li>
      <li>GnetId : 20162</li>
      <li>Datasource : Prometheus</li>
      <li>Import 버튼 클릭</li>
    </ul>
  </li>
  <li>그라파나 대시보드 확인
    <ul>
      <li>Home &gt; MetalLB 대시보드 선택
<img src="/assets/2024/kans-3th/w5/20241005_kans_w5_10.png" alt="img.png" class="image-center" /></li>
    </ul>
  </li>
</ul>

<h5 id="파드-생성">파드 생성</h5>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: webpod1
  labels:
    app: webpod
spec:
  nodeName: myk8s-worker
  containers:
  - name: container
    image: traefik/whoami
  terminationGracePeriodSeconds: 0
---
apiVersion: v1
kind: Pod
metadata:
  name: webpod2
  labels:
    app: webpod
spec:
  nodeName: myk8s-worker2
  containers:
  - name: container
    image: traefik/whoami
  terminationGracePeriodSeconds: 0
</span><span class="no">EOF
</span><span class="c"># =&gt; pod/webpod1 created</span>
<span class="c">#    pod/webpod2 created</span>

<span class="c"># 파드 정보 확인</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-owide</span>
<span class="c"># =&gt; NAME      READY   STATUS    RESTARTS   AGE   IP          NODE            NOMINATED NODE   READINESS GATES</span>
<span class="c">#    webpod1   1/1     Running   0          38s   10.10.3.2   myk8s-worker    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    webpod2   1/1     Running   0          38s   10.10.2.3   myk8s-worker2   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>

<span class="c"># 파드 IP주소를 변수에 지정</span>
<span class="nv">$ WPOD1</span><span class="o">=</span><span class="si">$(</span>kubectl get pod webpod1 <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.status.podIP}"</span><span class="si">)</span>
<span class="nv">$ WPOD2</span><span class="o">=</span><span class="si">$(</span>kubectl get pod webpod2 <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.status.podIP}"</span><span class="si">)</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$WPOD1</span> <span class="nv">$WPOD2</span>
<span class="c"># =&gt; 10.10.3.2 10.10.2.3</span>

<span class="c"># 접속 확인</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane  ping <span class="nt">-i</span> 1 <span class="nt">-W</span> 1 <span class="nt">-c</span> 1 <span class="nv">$WPOD1</span>
<span class="c"># =&gt; PING 10.10.3.2 (10.10.3.2) 56(84) bytes of data.</span>
<span class="c">#    64 bytes from 10.10.3.2: icmp_seq=1 ttl=63 time=0.082 ms</span>
<span class="c">#    </span>
<span class="c">#    --- 10.10.3.2 ping statistics ---</span>
<span class="c">#    1 packets transmitted, 1 received, 0% packet loss, time 0ms</span>
<span class="c">#    rtt min/avg/max/mdev = 0.082/0.082/0.082/0.000 ms</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane  ping <span class="nt">-i</span> 1 <span class="nt">-W</span> 1 <span class="nt">-c</span> 1 <span class="nv">$WPOD2</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane  curl <span class="nt">-s</span> <span class="nt">--connect-timeout</span> 1 <span class="nv">$WPOD1</span> | <span class="nb">grep </span>Hostname
<span class="c"># =&gt; Hostname: webpod1</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane  curl <span class="nt">-s</span> <span class="nt">--connect-timeout</span> 1 <span class="nv">$WPOD2</span> | <span class="nb">grep </span>Hostname
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane  curl <span class="nt">-s</span> <span class="nt">--connect-timeout</span> 1 <span class="nv">$WPOD1</span> | egrep <span class="s1">'Hostname|RemoteAddr|Host:'</span>
<span class="c"># =&gt; Hostname: webpod1</span>
<span class="c">#    RemoteAddr: 172.20.0.5:41896</span>
<span class="c">#    Host: 10.10.3.2</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane  curl <span class="nt">-s</span> <span class="nt">--connect-timeout</span> 1 <span class="nv">$WPOD2</span> | egrep <span class="s1">'Hostname|RemoteAddr|Host:'</span>
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w5/20241005_kans_w5_11.png" alt="img.png" /></p>

<h4 id="metallb---layer2-모드-실습">MetalLB - Layer2 모드 실습</h4>

<h5 id="metallb-설치">MetalLB 설치</h5>

<ul>
  <li>링크 : <a href="https://metallb.universe.tf/installation/">https://metallb.universe.tf/installation/</a></li>
  <li>설치 방법 : Kubernetes manifests, Kustomize, using Helm
    <ul>
      <li>kube-proxy가 ipvs 모드 사용시 <code class="language-plaintext highlighter-rouge">strictARP: true</code> 설정 필요</li>
    </ul>
  </li>
  <li>간단하게 manifests로 설치하겠습니다. - <a href="https://github.com/metallb/metallb/tree/main/config/manifests">링크</a></li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Kubernetes manifests 로 설치</span>
<span class="c"># kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.14.8/config/manifests/metallb-native.yaml</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> https://raw.githubusercontent.com/metallb/metallb/refs/heads/main/config/manifests/metallb-native-prometheus.yaml
<span class="c"># =&gt; namespace/metallb-system created</span>
<span class="c">#    ...</span>
<span class="c">#    serviceaccount/speaker created</span>
<span class="c">#    ...</span>
<span class="c">#    daemonset.apps/speaker created</span>
<span class="c">#    servicemonitor.monitoring.coreos.com/controller-monitor created</span>
<span class="c">#    servicemonitor.monitoring.coreos.com/speaker-monitor created</span>
<span class="c">#    validatingwebhookconfiguration.admissionregistration.k8s.io/metallb-webhook-configuration created</span>

<span class="c"># metallb crd 확인</span>
<span class="nv">$ </span>kubectl get crd | <span class="nb">grep </span>metallb
<span class="c"># =&gt; bfdprofiles.metallb.io                      2020-01-01T07:31:16Z</span>
<span class="c">#    bgpadvertisements.metallb.io                2020-01-01T07:31:16Z</span>
<span class="c">#    bgppeers.metallb.io                         2020-01-01T07:31:16Z</span>
<span class="c">#    communities.metallb.io                      2020-01-01T07:31:16Z</span>
<span class="c">#    ipaddresspools.metallb.io                   2020-01-01T07:31:17Z</span>
<span class="c">#    l2advertisements.metallb.io                 2020-01-01T07:31:17Z</span>
<span class="c">#    servicel2statuses.metallb.io                2020-01-01T07:31:17Z</span>

<span class="c"># 생성된 리소스 확인 : metallb-system 네임스페이스 생성, 파드(컨트롤러, 스피커) 생성, RBAC(서비스/파드/컨피그맵 조회 등등 권한들), SA 등</span>
<span class="nv">$ </span>kubectl get-all <span class="nt">-n</span> metallb-system <span class="c"># kubectl krew 플러그인 get-all 설치 후 사용 가능</span>
<span class="nv">$ </span>kubectl get all,configmap,secret,ep <span class="nt">-n</span> metallb-system
<span class="c"># =&gt; NAME                              READY   STATUS    RESTARTS      AGE</span>
<span class="c">#    pod/controller-679855f7d7-m8spp   2/2     Running   0             5m36s</span>
<span class="c">#    pod/speaker-dm26z                 2/2     Running   4 (91s ago)   5m36s</span>
<span class="c">#    pod/speaker-dr8kh                 2/2     Running   0             5m36s</span>
<span class="c">#    pod/speaker-pctt7                 2/2     Running   3 (90s ago)   5m36s</span>
<span class="c">#    pod/speaker-w69v6                 2/2     Running   4 (91s ago)   5m36s</span>
<span class="c">#    </span>
<span class="c">#    NAME                                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE</span>
<span class="c">#    service/controller-monitor-service   ClusterIP   None           &amp;lt;none&amp;gt;        9120/TCP   5m36s</span>
<span class="c">#    service/metallb-webhook-service      ClusterIP   10.200.1.191   &amp;lt;none&amp;gt;        443/TCP    5m36s</span>
<span class="c">#    service/speaker-monitor-service      ClusterIP   None           &amp;lt;none&amp;gt;        9120/TCP   5m36s</span>
<span class="c">#    </span>
<span class="c">#    NAME                     DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE</span>
<span class="c">#    daemonset.apps/speaker   4         4         4       4            4           kubernetes.io/os=linux   5m36s</span>
<span class="c">#    </span>
<span class="c">#    NAME                         READY   UP-TO-DATE   AVAILABLE   AGE</span>
<span class="c">#    deployment.apps/controller   1/1     1            1           5m36s</span>
<span class="c">#    </span>
<span class="c">#    NAME                                    DESIRED   CURRENT   READY   AGE</span>
<span class="c">#    replicaset.apps/controller-679855f7d7   1         1         1       5m36s</span>
<span class="c">#    </span>
<span class="c">#    NAME                          DATA   AGE</span>
<span class="c">#    configmap/kube-root-ca.crt    1      5m37s</span>
<span class="c">#    configmap/metallb-excludel2   1      5m36s</span>
<span class="c">#    </span>
<span class="c">#    NAME                          TYPE     DATA   AGE</span>
<span class="c">#    secret/memberlist             Opaque   1      5m18s</span>
<span class="c">#    secret/metallb-webhook-cert   Opaque   4      5m36s</span>
<span class="c">#    </span>
<span class="c">#    NAME                                   ENDPOINTS                                                     AGE</span>
<span class="c">#    endpoints/controller-monitor-service   10.10.1.3:9120                                                5m36s</span>
<span class="c">#    endpoints/metallb-webhook-service      10.10.1.3:9443                                                5m36s</span>
<span class="c">#    endpoints/speaker-monitor-service      172.20.0.2:9120,172.20.0.3:9120,172.20.0.4:9120 + 1 more...   5m36s</span>

<span class="c"># 파드 내에 kube-rbac-proxy 컨테이너는 프로메테우스 익스포터 역할 제공</span>
<span class="nv">$ </span>kubectl get pods <span class="nt">-n</span> metallb-system <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>metallb <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{range .items[*]}{.metadata.name}{':</span><span class="se">\n</span><span class="s2">'}{range .spec.containers[*]}{'  '}{.name}{' -&gt; '}{.image}{'</span><span class="se">\n</span><span class="s2">'}{end}{end}"</span>
<span class="c"># =&gt; controller-679855f7d7-m8spp:</span>
<span class="c">#      kube-rbac-proxy -&amp;gt; gcr.io/kubebuilder/kube-rbac-proxy:v0.12.0</span>
<span class="c">#      controller -&amp;gt; quay.io/metallb/controller:main</span>
<span class="c">#    speaker-dm26z:</span>
<span class="c">#      kube-rbac-proxy -&amp;gt; gcr.io/kubebuilder/kube-rbac-proxy:v0.12.0</span>
<span class="c">#      speaker -&amp;gt; quay.io/metallb/speaker:main</span>
<span class="c">#    speaker-dr8kh:</span>
<span class="c">#      kube-rbac-proxy -&amp;gt; gcr.io/kubebuilder/kube-rbac-proxy:v0.12.0</span>
<span class="c">#      speaker -&amp;gt; quay.io/metallb/speaker:main</span>
<span class="c">#    speaker-pctt7:</span>
<span class="c">#      kube-rbac-proxy -&amp;gt; gcr.io/kubebuilder/kube-rbac-proxy:v0.12.0</span>
<span class="c">#      speaker -&amp;gt; quay.io/metallb/speaker:main</span>
<span class="c">#    speaker-w69v6:</span>
<span class="c">#      kube-rbac-proxy -&amp;gt; gcr.io/kubebuilder/kube-rbac-proxy:v0.12.0</span>
<span class="c">#      speaker -&amp;gt; quay.io/metallb/speaker:main</span>

<span class="c">## metallb 컨트롤러는 디플로이먼트로 배포됨</span>
<span class="nv">$ </span>kubectl get ds,deploy <span class="nt">-n</span> metallb-system
<span class="c"># =&gt; NAME                     DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE</span>
<span class="c">#    daemonset.apps/speaker   4         4         4       4            4           kubernetes.io/os=linux   6m26s</span>
<span class="c">#    </span>
<span class="c">#    NAME                         READY   UP-TO-DATE   AVAILABLE   AGE</span>
<span class="c">#    deployment.apps/controller   1/1     1            1           6m26s</span>

<span class="c">## 데몬셋으로 배포되는 metallb 스피커 파드의 IP는 네트워크가 host 모드이므로 노드의 IP를 그대로 사용</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-n</span> metallb-system <span class="nt">-o</span> wide
<span class="c"># =&gt; NAME                          READY   STATUS    RESTARTS   AGE   IP           NODE                  NOMINATED NODE   READINESS GATES</span>
<span class="c">#    controller-679855f7d7-rg9pw   2/2     Running   0          22m   10.10.1.3    myk8s-worker3         &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    speaker-9njww                 2/2     Running   0          22m   172.20.0.3   myk8s-worker3         &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    speaker-lk9wt                 2/2     Running   0          22m   172.20.0.2   myk8s-worker2         &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    speaker-wz9w5                 2/2     Running   0          22m   172.20.0.5   myk8s-control-plane   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    speaker-zbwdq                 2/2     Running   0          22m   172.20.0.4   myk8s-worker          &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>

<span class="c"># (참고) 상세 정보 확인</span>
<span class="nv">$ </span>kubectl get sa,cm,secret <span class="nt">-n</span> metallb-system
<span class="nv">$ </span>kubectl describe role <span class="nt">-n</span> metallb-system
<span class="nv">$ </span>kubectl describe deploy controller <span class="nt">-n</span> metallb-system
<span class="nv">$ </span>kubectl describe ds speaker <span class="nt">-n</span> metallb-system
</code></pre></div></div>

<ul>
  <li>컨피그맵 생성 : 모드 및 서비스 대역 지정
    <ul>
      <li>서비스(External-IP) 대역을 노드가 속한 eth0의 대역이 아니여도 상관없습니다.
다만, 이 경우 GW 역할의 라우터에서 노드들로 라우팅 경로 지정 필요합니다.</li>
    </ul>
  </li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># kind 설치 시 kind 이름의 도커 브리지가 생성됩니다 : 172.20.0.0/16 대역</span>
<span class="nv">$ </span>docker network <span class="nb">ls</span>
<span class="c"># =&gt; NETWORK ID     NAME                     DRIVER    SCOPE</span>
<span class="c">#    a8d530305515   bridge                   bridge    local</span>
<span class="c">#    8204a0851463   host                     host      local</span>
<span class="c">#    3bbcc6aa8f38   kind                     bridge    local</span>
<span class="nv">$ </span>docker inspect kind
<span class="c"># kind network 중 컨테이너(노드) IP(대역) 확인 : 172.20.0.2~ 부터 할당되며, control-plane 이 꼭 172.20.0.2가 안될 수 도 있음</span>
<span class="nv">$ </span>docker ps <span class="nt">-q</span> | xargs docker inspect <span class="nt">--format</span> <span class="s1">' '</span>
<span class="c"># =&gt; /mypc2 172.20.0.7</span>
<span class="c">#    /mypc 172.20.0.6</span>
<span class="c">#    /myk8s-worker 172.20.0.4</span>
<span class="c">#    /myk8s-control-plane 172.20.0.5</span>
<span class="c">#    /myk8s-worker2 172.20.0.2</span>
<span class="c">#    /myk8s-worker3 172.20.0.3</span>

<span class="c"># IPAddressPool 생성 : LoadBalancer External IP로 사용할 IP 대역</span>
<span class="c">## MetalLB는 서비스를 위한 외부 IP 주소를 관리하고, 서비스가 생성될 때 해당 IP 주소를 동적으로 할당할 수 있습니다.</span>
<span class="nv">$ </span>kubectl explain ipaddresspools.metallb.io

<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: my-ippool
  namespace: metallb-system
spec:
  addresses:
  - 172.20.255.200-172.20.255.250
</span><span class="no">EOF
</span><span class="c"># =&gt; ipaddresspool.metallb.io/my-ippool unchanged</span>

<span class="nv">$ </span>kubectl get ipaddresspools <span class="nt">-n</span> metallb-system
<span class="c"># =&gt; NAME        AUTO ASSIGN   AVOID BUGGY IPS   ADDRESSES</span>
<span class="c">#    my-ippool   true          false             [&amp;quot;172.20.255.200-172.20.255.250&amp;quot;]</span>

<span class="c"># L2Advertisement 생성 : 설정한 IPpool을 기반으로 Layer2 모드로 LoadBalancer IP 사용 허용</span>
<span class="c">## Kubernetes 클러스터 내의 서비스가 외부 네트워크에 IP 주소를 광고하는 방식을 정의</span>

<span class="nv">$ </span>kubectl explain l2advertisements.metallb.io

<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: my-l2-advertise
  namespace: metallb-system
spec:
  ipAddressPools:
  - my-ippool
</span><span class="no">EOF
</span><span class="c"># =&gt; l2advertisement.metallb.io/my-l2-advertise created</span>

<span class="nv">$ </span>kubectl get l2advertisements <span class="nt">-n</span> metallb-system
<span class="c"># =&gt; NAME              IPADDRESSPOOLS   IPADDRESSPOOL SELECTORS   INTERFACES</span>
<span class="c">#    my-l2-advertise   [&amp;quot;my-ippool&amp;quot;]                              </span>
</code></pre></div></div>

<ul>
  <li>로그 확인</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># (옵션) metallb-speaker 파드 로그 확인</span>
<span class="nv">$ </span>kubectl logs <span class="nt">-n</span> metallb-system <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>metallb <span class="nt">-f</span>
<span class="nv">$ </span>kubectl logs <span class="nt">-n</span> metallb-system <span class="nt">-l</span> <span class="nv">component</span><span class="o">=</span>speaker <span class="nt">--since</span> 1h
<span class="nv">$ </span>kubectl logs <span class="nt">-n</span> metallb-system <span class="nt">-l</span> <span class="nv">component</span><span class="o">=</span>speaker <span class="nt">-f</span>

<span class="c"># (옵션) kubectl krew 플러그인 stern 설치 후 아래 명령 사용 가능</span>
<span class="nv">$ </span>kubectl stern <span class="nt">-n</span> metallb-system <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>metallb
<span class="nv">$ </span>kubectl stern <span class="nt">-n</span> metallb-system <span class="nt">-l</span> <span class="nv">component</span><span class="o">=</span>speaker <span class="nt">--since</span> 1h
<span class="nv">$ </span>kubectl stern <span class="nt">-n</span> metallb-system <span class="nt">-l</span> <span class="nv">component</span><span class="o">=</span>speaker <span class="c"># 기본 설정이 follow</span>
<span class="nv">$ </span>kubectl stern <span class="nt">-n</span> metallb-system speaker  <span class="c"># 매칭 사용 가능</span>
</code></pre></div></div>

<h5 id="서비스-생성-및-확인">서비스 생성 및 확인</h5>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: svc1
spec:
  ports:
    - name: svc1-webport
      port: 80
      targetPort: 80
  selector:
    app: webpod
  type: LoadBalancer  # 서비스 타입이 LoadBalancer
---
apiVersion: v1
kind: Service
metadata:
  name: svc2
spec:
  ports:
    - name: svc2-webport
      port: 80
      targetPort: 80
  selector:
    app: webpod
  type: LoadBalancer
---
apiVersion: v1
kind: Service
metadata:
  name: svc3
spec:
  ports:
    - name: svc3-webport
      port: 80
      targetPort: 80
  selector:
    app: webpod
  type: LoadBalancer
</span><span class="no">EOF
</span><span class="c"># =&gt; service/svc1 created</span>
<span class="c">#    service/svc2 created</span>
<span class="c">#    service/svc3 created</span>
</code></pre></div></div>

<h5 id="서비스-확인-및-리더-speaker-파드-확인">서비스 확인 및 리더 Speaker 파드 확인</h5>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># arp scan 해두기</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane arp-scan <span class="nt">--interfac</span><span class="o">=</span>eth0 <span class="nt">--localnet</span>
<span class="c"># =&gt; Interface: eth0, type: EN10MB, MAC: 02:42:ac:14:00:05, IPv4: 172.20.0.5</span>
<span class="c">#    Starting arp-scan 1.10.0 with 65536 hosts (https://github.com/royhills/arp-scan)</span>
<span class="c">#    172.20.0.1      02:42:1f:41:79:66       (Unknown: locally administered)</span>
<span class="c">#    172.20.0.2      02:42:ac:14:00:02       (Unknown: locally administered)</span>
<span class="c">#    172.20.0.3      02:42:ac:14:00:03       (Unknown: locally administered)</span>
<span class="c">#    172.20.0.4      02:42:ac:14:00:04       (Unknown: locally administered)</span>
<span class="c">#    172.20.0.6      02:42:ac:14:00:06       (Unknown: locally administered)</span>
<span class="c">#    172.20.0.7      02:42:ac:14:00:07       (Unknown: locally administered)</span>

<span class="c"># LoadBalancer 타입의 서비스 생성 확인 : EXTERNAL-IP가 서비스 마다 할당되며, 실습 환경에 따라 다를 수 있음</span>
<span class="c">## LoadBalancer 타입의 서비스는 NodePort 와 ClusterIP 를 포함함 - 'allocateLoadBalancerNodePorts : true' 기본값</span>
<span class="c">## ExternalIP 로 접속 시 사용하는 포트는 PORT(S) 의 앞에 있는 값을 사용 (아래의 경우는 TCP 80 임)</span>
<span class="c">## 만약 노드의 IP에 NodePort 로 접속 시 사용하는 포트는 PORT(S) 의 뒤에 있는 값을 사용 (아래는 30485 임)</span>
<span class="nv">$ </span>kubectl get service,ep
<span class="c"># =&gt; NAME                 TYPE           CLUSTER-IP     EXTERNAL-IP      PORT(S)        AGE</span>
<span class="c">#    service/kubernetes   ClusterIP      10.200.1.1     &amp;lt;none&amp;gt;           443/TCP        3h55m</span>
<span class="c">#    service/svc1         LoadBalancer   10.200.1.213   172.20.255.200   80:32145/TCP   128m</span>
<span class="c">#    service/svc2         LoadBalancer   10.200.1.59    172.20.255.201   80:32238/TCP   128m</span>
<span class="c">#    service/svc3         LoadBalancer   10.200.1.201   172.20.255.202   80:31593/TCP   128m</span>
<span class="c">#    </span>
<span class="c">#    NAME                   ENDPOINTS                   AGE</span>
<span class="c">#    endpoints/kubernetes   172.20.0.5:6443             3h55m</span>
<span class="c">#    endpoints/svc1         10.10.2.3:80,10.10.3.2:80   128m</span>
<span class="c">#    endpoints/svc2         10.10.2.3:80,10.10.3.2:80   128m</span>
<span class="c">#    endpoints/svc3         10.10.2.3:80,10.10.3.2:80   128m</span>

<span class="c"># LoadBalancer 타입은 기본적으로 NodePort를 포함 사용. NodePort는 ClusterIP를 포함 사용.</span>
<span class="c">## 클라우드사업자 LB Type이나 온프레미스환경 HW LB Type 경우 LB 사용 시 NodePort 미사용 설정 가능</span>
<span class="nv">$ </span>kubectl describe svc svc1
<span class="c"># =&gt; Name:                     svc1</span>
<span class="c">#    ...</span>
<span class="c">#    Annotations:              metallb.io/ip-allocated-from-pool: my-ippool</span>
<span class="c">#    Selector:                 app=webpod</span>
<span class="c">#    Type:                     LoadBalancer</span>
<span class="c">#    IP Family Policy:         SingleStack</span>
<span class="c">#    IP Families:              IPv4</span>
<span class="c">#    IP:                       10.200.1.213</span>
<span class="c">#    IPs:                      10.200.1.213</span>
<span class="c">#    LoadBalancer Ingress:     172.20.255.200 (VIP)</span>
<span class="c">#    Port:                     svc1-webport  80/TCP</span>
<span class="c">#    TargetPort:               80/TCP</span>
<span class="c">#    NodePort:                 svc1-webport  32145/TCP</span>
<span class="c">#    Endpoints:                10.10.3.2:80,10.10.2.3:80</span>
<span class="c">#    Session Affinity:         None</span>
<span class="c">#    External Traffic Policy:  Cluster</span>
<span class="c">#    Internal Traffic Policy:  Cluster</span>
<span class="c">#    Events:</span>
<span class="c">#      Type    Reason       Age    From                Message</span>
<span class="c">#      ----    ------       ----   ----                -------</span>
<span class="c">#      Normal  IPAllocated  3m19s  metallb-controller  Assigned IP [&amp;quot;172.20.255.200&amp;quot;]</span>
<span class="c">#      Normal  nodeAssigned  5m55s (x2 over 5m55s)  metallb-speaker  announcing from node &amp;quot;myk8s-worker&amp;quot; with protocol &amp;quot;layer2&amp;quot;</span>

<span class="c">## 아래 처럼 LB VIP 별로 이던 speaker 배포된 노드가 리더 역할을 하는지 확인 가능</span>
<span class="nv">$ </span>kubectl describe svc | <span class="nb">grep </span>Events: <span class="nt">-A5</span>
<span class="c"># =&gt; Events:                   &amp;lt;none&amp;gt;</span>
<span class="c">#</span>
<span class="c">#    Name:                     svc1</span>
<span class="c">#    Namespace:                default</span>
<span class="c">#    Labels:                   &amp;lt;none&amp;gt;</span>
<span class="c">#    --</span>
<span class="c">#    Events:</span>
<span class="c">#      Type    Reason       Age    From                Message</span>
<span class="c">#      ----    ------       ----   ----                -------</span>
<span class="c">#      Normal  IPAllocated  4m24s  metallb-controller  Assigned IP [&amp;quot;172.20.255.200&amp;quot;]</span>
<span class="c">#      Normal  nodeAssigned 6m42s (x2 over 6m42s)  metallb-speaker  announcing from node "myk8s-worker" with protocol "layer2"</span>
<span class="c">#    ...</span>
<span class="nv">$ </span>kubectl get svc svc1 <span class="nt">-o</span> json | jq
<span class="c"># =&gt; ...</span>
<span class="c">#      &amp;quot;spec&amp;quot;: {</span>
<span class="c">#        &amp;quot;allocateLoadBalancerNodePorts&amp;quot;: true,</span>
<span class="c">#      ...</span>
<span class="c">#      &amp;quot;status&amp;quot;: {</span>
<span class="c">#        &amp;quot;loadBalancer&amp;quot;: {</span>
<span class="c">#          &amp;quot;ingress&amp;quot;: [</span>
<span class="c">#            {</span>
<span class="c">#              &amp;quot;ip&amp;quot;: &amp;quot;172.20.255.200&amp;quot;,</span>
<span class="c">#              &amp;quot;ipMode&amp;quot;: &amp;quot;VIP&amp;quot;</span>
<span class="c">#    ...</span>

<span class="c"># metallb CRD인 servicel2status 로 상태 정보 확인</span>
<span class="nv">$ </span>kubectl explain servicel2status
<span class="nv">$ </span>kubectl get servicel2status <span class="nt">-n</span> metallb-system
<span class="c"># =&gt; NAME       ALLOCATED NODE   SERVICE NAME   SERVICE NAMESPACE</span>
<span class="c">#    l2-cm8sw   myk8s-worker     svc2           default</span>
<span class="c">#    l2-j6w4k   myk8s-worker     svc1           default</span>
<span class="c">#    l2-k5cdm   myk8s-worker3    svc3           default</span>
<span class="nv">$ </span>kubectl describe servicel2status <span class="nt">-n</span> metallb-system
<span class="nv">$ </span>kubectl get servicel2status <span class="nt">-n</span> metallb-system <span class="nt">-o</span> json <span class="nt">--watch</span> <span class="c"># watch 모드</span>

<span class="c"># 현재 SVC EXTERNAL-IP를 변수에 지정</span>
<span class="nv">$ SVC1EXIP</span><span class="o">=</span><span class="si">$(</span>kubectl get svc svc1 <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.status.loadBalancer.ingress[0].ip}'</span><span class="si">)</span>
<span class="nv">$ SVC2EXIP</span><span class="o">=</span><span class="si">$(</span>kubectl get svc svc2 <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.status.loadBalancer.ingress[0].ip}'</span><span class="si">)</span>
<span class="nv">$ SVC3EXIP</span><span class="o">=</span><span class="si">$(</span>kubectl get svc svc3 <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.status.loadBalancer.ingress[0].ip}'</span><span class="si">)</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$SVC1EXIP</span> <span class="nv">$SVC2EXIP</span> <span class="nv">$SVC3EXIP</span>
<span class="c"># =&gt; 172.20.255.200 172.20.255.201 172.20.255.202</span>

<span class="c"># mypc/mypc2 에서 현재 SVC EXTERNAL-IP를 담당하는 리더 Speaker 파드 찾는법 : arping 툴 사용</span>
<span class="c">## Unicast reply from 172.20.255.200: 해당 IP 주소에서 응답을 받았음을 의미합니다. </span>
<span class="c">## Sent 1 probes (1 broadcast(s)): 하나의 ARP 요청을 보냈고, 브로드캐스트 방식으로 요청을 전송했음을 나타냅니다.</span>
<span class="c">## Received 1 response(s): 하나의 응답을 수신했음을 나타냅니다.</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc arping <span class="nt">-I</span> eth0 <span class="nt">-f</span> <span class="nt">-c</span> 1 <span class="nv">$SVC1EXIP</span>
<span class="c"># =&gt; ARPING 172.20.255.200 from 172.20.0.6 eth0</span>
<span class="c">#    Unicast reply from 172.20.255.200 [02:42:AC:14:00:03]  1.139ms</span>
<span class="c">#    Sent 1 probes (1 broadcast(s))</span>
<span class="c">#    Received 1 response(s)</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc arping <span class="nt">-I</span> eth0 <span class="nt">-f</span> <span class="nt">-c</span> 1 <span class="nv">$SVC2EXIP</span>
<span class="c"># =&gt; ARPING 172.20.255.201 from 172.20.0.6 eth0</span>
<span class="c">#    Unicast reply from 172.20.255.201 [02:42:AC:14:00:02]  1.827ms</span>
<span class="c">#    ...</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc arping <span class="nt">-I</span> eth0 <span class="nt">-f</span> <span class="nt">-c</span> 1 <span class="nv">$SVC3EXIP</span>
<span class="c"># =&gt; ARPING 172.20.255.202 from 172.20.0.6 eth0</span>
<span class="c">#    Unicast reply from 172.20.255.202 [02:42:AC:14:00:04]  0.982ms</span>
<span class="c">#    ...</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="nv">$SVC1EXIP</span> <span class="nv">$SVC2EXIP</span> <span class="nv">$SVC3EXIP</span><span class="p">;</span> <span class="k">do </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc arping <span class="nt">-I</span> eth0 <span class="nt">-f</span> <span class="nt">-c</span> 1 <span class="nv">$i</span><span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; ARPING 172.20.255.200 from 172.20.0.6 eth0</span>
<span class="c">#    Unicast reply from 172.20.255.200 [02:42:AC:14:00:03]  1.016ms</span>
<span class="c">#    Sent 1 probes (1 broadcast(s))</span>
<span class="c">#    Received 1 response(s)</span>
<span class="c">#    ARPING 172.20.255.201 from 172.20.0.6 eth0</span>
<span class="c">#    Unicast reply from 172.20.255.201 [02:42:AC:14:00:02]  1.965ms</span>
<span class="c">#    ...</span>
<span class="c">#    ARPING 172.20.255.202 from 172.20.0.6 eth0</span>
<span class="c">#    Unicast reply from 172.20.255.202 [02:42:AC:14:00:04]  1.789ms</span>
<span class="c">#    ...</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc ip <span class="nt">-c</span> neigh
<span class="c"># =&gt; &lt;span style="color:purple;"&gt;172.20.0.1 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:1f:41:79:66 &lt;/span&gt;STALE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.0.4 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:04 &lt;/span&gt;STALE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.0.5 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:05 &lt;/span&gt;STALE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.255.200 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:03 &lt;/span&gt;REACHABLE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.255.201 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:02 &lt;/span&gt;REACHABLE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.255.202 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:04 &lt;/span&gt;REACHABLE </span>

<span class="c"># &lt;span style="color: green;"&gt;ping은 모두 패킷 100% 로스되면서 실패합니다.&lt;/span&gt;</span>
<span class="c"># &lt;span style="color: green;"&gt;서비스 port로만 열려있기때문에 ping은 실패하는것입니다.&lt;/span&gt;</span>
<span class="c"># &lt;span style="color: green;"&gt;여기서 ping을 하는 이유는 arp table을 생성하기 위함입니다.&lt;/span&gt;</span>
  
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc ping <span class="nt">-c</span> 1 <span class="nt">-w</span> 1 <span class="nt">-W</span> 1 <span class="nv">$SVC1EXIP</span>
<span class="c"># =&gt; PING 172.20.255.200 (172.20.255.200) 56(84) bytes of data.</span>
<span class="c">#    </span>
<span class="c">#    --- 172.20.255.200 ping statistics ---</span>
<span class="c">#    1 packets transmitted, 0 received, 100% packet loss, time 0ms</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc ping <span class="nt">-c</span> 1 <span class="nt">-w</span> 1 <span class="nt">-W</span> 1 <span class="nv">$SVC2EXIP</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc ping <span class="nt">-c</span> 1 <span class="nt">-w</span> 1 <span class="nt">-W</span> 1 <span class="nv">$SVC3EXIP</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="nv">$SVC1EXIP</span> <span class="nv">$SVC2EXIP</span> <span class="nv">$SVC3EXIP</span><span class="p">;</span> <span class="k">do </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc ping <span class="nt">-c</span> 1 <span class="nt">-w</span> 1 <span class="nt">-W</span> 1 <span class="nv">$i</span><span class="p">;</span> <span class="k">done</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>172.20.0.2 172.20.0.3 172.20.0.4 172.20.0.5<span class="p">;</span> <span class="k">do </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc ping <span class="nt">-c</span> 1 <span class="nt">-w</span> 1 <span class="nt">-W</span> 1 <span class="nv">$i</span><span class="p">;</span> <span class="k">done</span>

<span class="c"># mypc/mypc2 에서 arp 테이블 정보 확인 &gt;&gt; SVC IP별로 리더 파드(스피커) 역할의 노드를 확인!</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc ip <span class="nt">-c</span> neigh | <span class="nb">sort</span>
<span class="c"># =&gt; &lt;span style="color:purple;"&gt;172.20.0.1 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:1f:41:79:66 &lt;/span&gt;STALE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.0.2 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:02 &lt;/span&gt;REACHABLE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.0.3 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:03 &lt;/span&gt;REACHABLE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.0.4 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:04 &lt;/span&gt;REACHABLE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.0.5 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:05 &lt;/span&gt;REACHABLE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.255.200 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:03 &lt;/span&gt;REACHABLE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.255.201 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:02 &lt;/span&gt;REACHABLE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.255.202 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:04 &lt;/span&gt;REACHABLE </span>

<span class="nv">$ </span>kubectl get node <span class="nt">-owide</span> <span class="c"># mac 주소에 매칭되는 IP(노드) 찾기</span>
<span class="c"># =&gt; NAME                  STATUS   ROLES           AGE     VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                         KERNEL-VERSION     CONTAINER-RUNTIME</span>
<span class="c">#    myk8s-control-plane   Ready    control-plane   4h35m   v1.31.0   172.20.0.5    &amp;lt;none&amp;gt;        Debian GNU/Linux 12 (bookworm)   5.10.76-linuxkit   containerd://1.7.18</span>
<span class="c">#    myk8s-worker          Ready    &amp;lt;none&amp;gt;          4h34m   v1.31.0   172.20.0.4    &amp;lt;none&amp;gt;        Debian GNU/Linux 12 (bookworm)   5.10.76-linuxkit   containerd://1.7.18</span>
<span class="c">#    myk8s-worker2         Ready    &amp;lt;none&amp;gt;          4h34m   v1.31.0   172.20.0.2    &amp;lt;none&amp;gt;        Debian GNU/Linux 12 (bookworm)   5.10.76-linuxkit   containerd://1.7.18</span>
<span class="c">#    myk8s-worker3         Ready    &amp;lt;none&amp;gt;          4h34m   v1.31.0   172.20.0.3    &amp;lt;none&amp;gt;        Debian GNU/Linux 12 (bookworm)   5.10.76-linuxkit   containerd://1.7.18</span>

<span class="c"># (옵션) 노드에서 ARP 패킷 캡쳐 확인</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane tcpdump <span class="nt">-i</span> eth0 <span class="nt">-nn</span> arp
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker        tcpdump <span class="nt">-i</span> eth0 <span class="nt">-nn</span> arp
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker2       tcpdump <span class="nt">-i</span> eth0 <span class="nt">-nn</span> arp
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker3       tcpdump <span class="nt">-i</span> eth0 <span class="nt">-nn</span> arp

<span class="c"># (옵션) metallb-speaker 파드 로그 확인</span>
<span class="nv">$ </span>kubectl logs <span class="nt">-n</span> metallb-system <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>metallb <span class="nt">-f</span>
<span class="nv">$ </span>kubectl logs <span class="nt">-n</span> metallb-system <span class="nt">-l</span> <span class="nv">component</span><span class="o">=</span>speaker <span class="nt">--since</span> 1h
<span class="nv">$ </span>kubectl logs <span class="nt">-n</span> metallb-system <span class="nt">-l</span> <span class="nv">component</span><span class="o">=</span>speaker <span class="nt">-f</span>

<span class="c"># (옵션) kubectl krew 플러그인 stern 설치 후 아래 명령 사용 가능</span>
<span class="nv">$ </span>kubectl stern <span class="nt">-n</span> metallb-system <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>metallb
<span class="nv">$ </span>kubectl stern <span class="nt">-n</span> metallb-system <span class="nt">-l</span> <span class="nv">component</span><span class="o">=</span>speaker <span class="nt">--since</span> 1h
<span class="nv">$ </span>kubectl stern <span class="nt">-n</span> metallb-system <span class="nt">-l</span> <span class="nv">component</span><span class="o">=</span>speaker <span class="c"># 기본 설정이 follow</span>
<span class="nv">$ </span>kubectl stern <span class="nt">-n</span> metallb-system speaker  <span class="c"># 매칭 사용 가능</span>
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w5/20241005_kans_w5_12.png" alt="20241005_kans_w5_12.png" /></p>

<h5 id="서비스-접속-테스트">서비스 접속 테스트</h5>

<ul>
  <li>클러스터 외부에서 external ip와 port를 통해 k8s 클러스터 내부의 서비스에 접속해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 현재 SVC EXTERNAL-IP를 변수에 지정</span>
<span class="nv">$ SVC1EXIP</span><span class="o">=</span><span class="si">$(</span>kubectl get svc svc1 <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.status.loadBalancer.ingress[0].ip}'</span><span class="si">)</span>
<span class="nv">$ SVC2EXIP</span><span class="o">=</span><span class="si">$(</span>kubectl get svc svc2 <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.status.loadBalancer.ingress[0].ip}'</span><span class="si">)</span>
<span class="nv">$ SVC3EXIP</span><span class="o">=</span><span class="si">$(</span>kubectl get svc svc3 <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.status.loadBalancer.ingress[0].ip}'</span><span class="si">)</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$SVC1EXIP</span> <span class="nv">$SVC2EXIP</span> <span class="nv">$SVC3EXIP</span>
<span class="c"># =&gt; 172.20.255.200 172.20.255.201 172.20.255.202</span>

<span class="c"># mypc/mypc2 에서 접속 테스트</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc curl <span class="nt">-s</span> <span class="nv">$SVC1EXIP</span>
<span class="c"># =&gt; Hostname: webpod2</span>
<span class="c">#    IP: 127.0.0.1</span>
<span class="c">#    IP: 10.10.2.3</span>
<span class="c">#    RemoteAddr: 172.20.0.3:40816</span>
<span class="c">#    GET / HTTP/1.1</span>
<span class="c">#    Host: 172.20.255.200</span>
<span class="c">#    User-Agent: curl/8.7.1</span>
<span class="c">#    Accept: */*</span>

<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc curl <span class="nt">-s</span> <span class="nv">$SVC1EXIP</span> | <span class="nb">grep </span>Hostname
<span class="c"># =&gt; Hostname: webpod2</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="nv">$SVC1EXIP</span> <span class="nv">$SVC2EXIP</span> <span class="nv">$SVC3EXIP</span><span class="p">;</span> <span class="k">do </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc curl <span class="nt">-s</span> <span class="nv">$i</span> | <span class="nb">grep </span>Hostname <span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; Hostname: webpod1</span>
<span class="c">#    Hostname: webpod2</span>
<span class="c">#    Hostname: webpod2</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="nv">$SVC1EXIP</span> <span class="nv">$SVC2EXIP</span> <span class="nv">$SVC3EXIP</span><span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; Access Service External-IP : </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span> <span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> mypc curl <span class="nt">-s</span> <span class="nv">$i</span> | <span class="nb">grep </span>Hostname <span class="p">;</span> <span class="nb">echo</span> <span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; &amp;gt;&amp;gt; Access Service External-IP : 172.20.255.200 &amp;lt;&amp;lt;</span>
<span class="c">#    Hostname: webpod1</span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; Access Service External-IP : 172.20.255.201 &amp;lt;&amp;lt;</span>
<span class="c">#    Hostname: webpod2</span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; Access Service External-IP : 172.20.255.202 &amp;lt;&amp;lt;</span>
<span class="c">#    Hostname: webpod1    </span>

<span class="c">## RemoteAddr 주소는 어떻게 나오나요? 왜 그럴까요?</span>
<span class="c">##  NodePort 기본 동작과 동일하게 인입한 노드의 인터페이스로 SNAT 되어서 최종 파드로 전달되기 때문입니다.</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="nv">$SVC1EXIP</span> <span class="nv">$SVC2EXIP</span> <span class="nv">$SVC3EXIP</span><span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; Access Service External-IP : </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span> <span class="p">;</span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc curl <span class="nt">-s</span> <span class="nv">$i</span> | egrep <span class="s1">'Hostname|RemoteAddr|Host:'</span> <span class="p">;</span> <span class="nb">echo</span> <span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; &amp;gt;&amp;gt; Access Service External-IP : 172.20.255.200 &amp;lt;&amp;lt;</span>
<span class="c">#    Hostname: webpod2</span>
<span class="c">#    RemoteAddr: 172.20.0.3:23163</span>
<span class="c">#    Host: 172.20.255.200</span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; Access Service External-IP : 172.20.255.201 &amp;lt;&amp;lt;</span>
<span class="c">#    Hostname: webpod2</span>
<span class="c">#    RemoteAddr: 10.10.2.1:15401</span>
<span class="c">#    Host: 172.20.255.201</span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; Access Service External-IP : 172.20.255.202 &amp;lt;&amp;lt;</span>
<span class="c">#    Hostname: webpod2</span>
<span class="c">#    RemoteAddr: 172.20.0.4:12711</span>
<span class="c">#    Host: 172.20.255.202</span>

<span class="c"># 부하분산 접속됨</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc zsh <span class="nt">-c</span> <span class="s2">"for i in {1..100}; do curl -s </span><span class="nv">$SVC1EXIP</span><span class="s2"> | grep Hostname; done | sort | uniq -c | sort -nr"</span>
<span class="c"># =&gt;      54 Hostname: webpod2</span>
<span class="c">#         46 Hostname: webpod1</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc zsh <span class="nt">-c</span> <span class="s2">"for i in {1..100}; do curl -s </span><span class="nv">$SVC2EXIP</span><span class="s2"> | grep Hostname; done | sort | uniq -c | sort -nr"</span>
<span class="c"># =&gt;      56 Hostname: webpod1</span>
<span class="c">#         44 Hostname: webpod2</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc zsh <span class="nt">-c</span> <span class="s2">"for i in {1..100}; do curl -s </span><span class="nv">$SVC3EXIP</span><span class="s2"> | grep Hostname; done | sort | uniq -c | sort -nr"</span>
<span class="c"># =&gt;      53 Hostname: webpod1</span>
<span class="c">#         47 Hostname: webpod2</span>

<span class="c"># 지속적으로 반복 접속</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc zsh <span class="nt">-c</span> <span class="s2">"while true; do curl -s --connect-timeout 1 </span><span class="nv">$SVC1EXIP</span><span class="s2"> | egrep 'Hostname|RemoteAddr'; date '+%Y-%m-%d %H:%M:%S' ; echo ;  sleep 1; done"</span>
<span class="c"># =&gt; Hostname: webpod1</span>
<span class="c">#    RemoteAddr: 172.20.0.3:39516</span>
<span class="c">#    2024-01-01 11:22:10</span>
<span class="c">#    </span>
<span class="c">#    Hostname: webpod1</span>
<span class="c">#    RemoteAddr: 172.20.0.3:20966</span>
<span class="c">#    2024-01-01 11:22:11</span>
<span class="c">#    </span>
<span class="c">#    Hostname: webpod2</span>
<span class="c">#    RemoteAddr: 172.20.0.3:3638</span>
<span class="c">#    2024-01-01 11:22:12</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc zsh <span class="nt">-c</span> <span class="s2">"while true; do curl -s --connect-timeout 1 </span><span class="nv">$SVC2EXIP</span><span class="s2"> | egrep 'Hostname|RemoteAddr'; date '+%Y-%m-%d %H:%M:%S' ; echo ;  sleep 1; done"</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc zsh <span class="nt">-c</span> <span class="s2">"while true; do curl -s --connect-timeout 1 </span><span class="nv">$SVC3EXIP</span><span class="s2"> | egrep 'Hostname|RemoteAddr'; date '+%Y-%m-%d %H:%M:%S' ; echo ;  sleep 1; done"</span>

<span class="c"># LoadBalancer Type은 기본값으로 NodePort 포함. NodePort 서비스는 ClusterIP 를 포함</span>
<span class="c"># NodePort:PORT 및 CLUSTER-IP:PORT 로 접속 가능!</span>
<span class="nv">$ </span>kubectl get svc svc1
<span class="c"># =&gt; NAME   TYPE           CLUSTER-IP    EXTERNAL-IP      PORT(S)        AGE</span>
<span class="c">#    svc1   LoadBalancer   10.200.1.89   172.20.255.200   80:30613/TCP   22m</span>

<span class="c"># 컨트롤노드에서 각각 접속 확인 실행 해보자</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane curl <span class="nt">-s</span> 127.0.0.1:30613 <span class="c"># NodePort Type</span>
<span class="c"># =&gt; Hostname: webpod2</span>
<span class="c">#    IP: 127.0.0.1</span>
<span class="c">#    IP: 10.10.2.3</span>
<span class="c">#    RemoteAddr: 172.20.0.5:44387</span>
<span class="c">#    GET / HTTP/1.1</span>
<span class="c">#    Host: 127.0.0.1:30613</span>
<span class="c">#    User-Agent: curl/7.88.1</span>
<span class="c">#    Accept: */*    </span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane curl <span class="nt">-s</span> 10.200.1.89     <span class="c"># ClusterIP Tpye</span>
<span class="c"># =&gt; Hostname: webpod2</span>
<span class="c">#    IP: 127.0.0.1</span>
<span class="c">#    IP: 10.10.2.3</span>
<span class="c">#    RemoteAddr: 172.20.0.5:28647</span>
<span class="c">#    GET / HTTP/1.1</span>
<span class="c">#    Host: 10.200.1.89</span>
<span class="c">#    User-Agent: curl/7.88.1</span>
<span class="c">#    Accept: */*</span>
</code></pre></div></div>

<h5 id="failover-테스트">Failover 테스트</h5>

<p><img src="/assets/2024/kans-3th/w5/20241005_kans_w5_13.png" alt="img.png" /></p>

<ul>
  <li>위의 그림처럼 장애 발생전에 워커노드 1의 스피커 파드가 SVC1, SVC2 서비스의 리더 역할을 하고 있는 상태에서,
워커노드 1에 장애가 발생하면, 남아있는 스피커 파드들이 워커노드 1의 장애 상황을 인지하게 됩니다.</li>
  <li>이후 장애가 발생한 스피커 파드가 소유한 ExternalIP에 대해 리더파드를 다시 선출하고 GARP로 새로 선출된 리더파드의 MAC 주소를 전파합니다.</li>
  <li>다만 장애 발생으로 문제를 인식하는 시간과 ARP 정보가 전파되는 시간, 그리고 클라이언트의 ARP 캐시 갱신 시간 등을 
고려하면 20초~1분 이내의 장애 지속시간이 발생할 수 있습니다.</li>
  <li>현재 실습에서 SVC1 =&gt; worker node 3, SVC2 =&gt; worker node 2, SVC3 =&gt; worker node 1 에 배포되어 있는 상태에서
워커노드 중 1대를 중지하여 장애를 발생시키고, 장애시간을 확인해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 사전 준비</span>
<span class="c">## 지속적으로 반복 접속</span>
<span class="nv">$ SVC1EXIP</span><span class="o">=</span><span class="si">$(</span>kubectl get svc svc1 <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.status.loadBalancer.ingress[0].ip}'</span><span class="si">)</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc zsh <span class="nt">-c</span> <span class="s2">"while true; do curl -s --connect-timeout 1 </span><span class="nv">$SVC1EXIP</span><span class="s2"> | egrep 'Hostname|RemoteAddr'; date '+%Y-%m-%d %H:%M:%S' ; echo ;  sleep 1; done"</span>

<span class="c">## 상태 모니터링</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> kubectl get pod,svc,ep

<span class="c">## 실시간 로그 확인</span>
<span class="nv">$ </span>kubectl logs <span class="nt">-n</span> metallb-system <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>metallb <span class="nt">-f</span>
<span class="c"># 혹은</span>
<span class="nv">$ </span>kubectl stern <span class="nt">-n</span> metallb-system <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>metallb


<span class="c"># 장애 재연</span>
<span class="c">## 리더 Speaker 파드가 존재하는 노드(실제는 컨테이너)를 중지</span>
<span class="c"># $ docker stop &lt;svc1 번 리더 Speaker 파드가 존재하는 노드(실제는 컨테이너)&gt; --signal 9</span>
<span class="nv">$ </span>docker stop myk8s-worker <span class="nt">--signal</span> 9
<span class="c"># 혹은</span>
<span class="c"># $ docker stop &lt;svc1 번 리더 Speaker 파드가 존재하는 노드(실제는 컨테이너)&gt; --signal 15</span>
<span class="nv">$ </span>docker stop myk8s-worker <span class="nt">--signal</span> 15

<span class="nv">$ </span>docker ps <span class="nt">-a</span>
<span class="nv">$ </span>docker ps <span class="nt">-a</span> | <span class="nb">grep </span>worker<span class="err">$</span>
<span class="c"># =&gt; 242777ad8f3c   kindest/node:v1.31.0                 "/usr/local/bin/entr…"   6 hours ago     Exited (130) 7 minutes ago    myk8s-worker</span>

<span class="c">## 지속적으로 반복 접속 상태 모니터링</span>
<span class="c">### curl 연속 접속 시도 &gt;&gt; 대략 10초 이내에 정상 접근 되었지만, 20초까지는 불안정하게 접속이 되었다</span>
<span class="c">### 실제로는 다른 노드의 speaker 파드가 리더가 되고, 이후 다시 노드(컨테이너)가 정상화되면, 다시 리더 speaker 가 됨</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc zsh <span class="nt">-c</span> <span class="s2">"while true; do curl -s --connect-timeout 1 </span><span class="nv">$SVC1EXIP</span><span class="s2"> | egrep 'Hostname|RemoteAddr'; date '+%Y-%m-%d %H:%M:%S' ; echo ;  sleep 1; done"</span>
<span class="c"># =&gt; Hostname: webpod2</span>
<span class="c">#    RemoteAddr: 172.20.0.3:25432</span>
<span class="c">#    2024-10-05 12:04:30</span>
<span class="c">#    </span>
<span class="c">#    2024-10-05 12:04:32</span>
<span class="c">#    </span>
<span class="c">#    2024-10-05 12:04:34</span>
<span class="c">#    </span>
<span class="c">#    Hostname: webpod2</span>
<span class="c">#    RemoteAddr: 172.20.0.3:18511</span>
<span class="c">#    2024-10-05 12:04:35</span>
<span class="c">#    </span>
<span class="c">#    2024-10-05 12:04:37</span>
<span class="c">#    </span>
<span class="c">#    2024-10-05 12:04:39</span>
<span class="c">#    ...</span>

<span class="c"># 변경된 리더 Speaker 파드 확인</span>
<span class="c"># mypc/mypc2 에서 현재 SVC EXTERNAL-IP를 담당하는 리더 Speaker 파드 찾기</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="nv">$SVC1EXIP</span> <span class="nv">$SVC2EXIP</span> <span class="nv">$SVC3EXIP</span><span class="p">;</span> <span class="k">do </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc ping <span class="nt">-c</span> 1 <span class="nt">-w</span> 1 <span class="nt">-W</span> 1 <span class="nv">$i</span><span class="p">;</span> <span class="k">done</span>

<span class="c"># mypc/mypc2 에서 arp 테이블 정보 확인 &gt;&gt; SVC IP별로 리더 파드(스피커) 역할의 노드를 확인!</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc ip <span class="nt">-c</span> neigh | <span class="nb">sort</span>
<span class="c"># =&gt; &lt;span style="color:purple;"&gt;172.20.0.1 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:1f:41:79:66 &lt;/span&gt;STALE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.0.2 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:02 &lt;/span&gt;STALE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.0.3 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:03 &lt;/span&gt;STALE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.0.4 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:04 &lt;/span&gt;STALE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.0.5 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:05 &lt;/span&gt;STALE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.255.200 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:03 &lt;/span&gt;REACHABLE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.255.201 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:02 &lt;/span&gt;REACHABLE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.255.202 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:03 &lt;/span&gt;REACHABLE </span>
<span class="nv">$ </span>kubectl get node <span class="nt">-owide</span> <span class="c"># mac 주소에 매칭되는 IP(노드) 찾기</span>
<span class="c"># =&gt; NAME                  STATUS     ROLES           AGE     VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                         KERNEL-VERSION     CONTAINER-RUNTIME</span>
<span class="c">#    myk8s-control-plane   Ready      control-plane   5h47m   v1.31.0   172.20.0.5    &amp;lt;none&amp;gt;        Debian GNU/Linux 12 (bookworm)   5.10.76-linuxkit   containerd://1.7.18</span>
<span class="c">#    myk8s-worker          NotReady   &amp;lt;none&amp;gt;          5h47m   v1.31.0   172.20.0.4    &amp;lt;none&amp;gt;        Debian GNU/Linux 12 (bookworm)   5.10.76-linuxkit   containerd://1.7.18</span>
<span class="c">#    myk8s-worker2         Ready      &amp;lt;none&amp;gt;          5h47m   v1.31.0   172.20.0.2    &amp;lt;none&amp;gt;        Debian GNU/Linux 12 (bookworm)   5.10.76-linuxkit   containerd://1.7.18</span>
<span class="c">#    myk8s-worker3         Ready      &amp;lt;none&amp;gt;          5h47m   v1.31.0   172.20.0.3    &amp;lt;none&amp;gt;        Debian GNU/Linux 12 (bookworm)   5.10.76-linuxkit   containerd://1.7.18</span>

<span class="c"># &lt;span style="color: green;"&gt;원래 리더 Speaker 파드가 존재했던 myk8s-worker 노드가 아닌&lt;/span&gt; </span>
<span class="c"># &lt;span style="color: green;"&gt;myk8s-worker3 노드가 리더 Speaker 파드를 가지고 있음을 확인할 수 있습니다.&lt;/span&gt;</span>

<span class="c"># 장애 원복(노드 정상화)</span>
<span class="c">## 노드(실제 컨테이너) 정상화 </span>
<span class="c"># $ docker start &lt;svc1 번 리더 Speaker 파드가 존재하는 노드(실제는 컨테이너)&gt;</span>
<span class="nv">$ </span>docker start myk8s-worker

<span class="c"># 변경된 리더 Speaker 파드 확인</span>
<span class="c"># mypc/mypc2 에서 현재 SVC EXTERNAL-IP를 담당하는 리더 Speaker 파드 찾기</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="nv">$SVC1EXIP</span> <span class="nv">$SVC2EXIP</span> <span class="nv">$SVC3EXIP</span><span class="p">;</span> <span class="k">do </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc ping <span class="nt">-c</span> 1 <span class="nt">-w</span> 1 <span class="nt">-W</span> 1 <span class="nv">$i</span><span class="p">;</span> <span class="k">done</span>

<span class="c"># mypc/mypc2 에서 arp 테이블 정보 확인 &gt;&gt; SVC IP별로 리더 파드(스피커) 역할의 노드를 확인!</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc ip <span class="nt">-c</span> neigh | <span class="nb">sort</span>
<span class="c"># =&gt; &lt;span style="color:purple;"&gt;172.20.0.1 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:1f:41:79:66 &lt;/span&gt;STALE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.0.2 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:02 &lt;/span&gt;STALE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.0.3 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:03 &lt;/span&gt;STALE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.0.4 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:04 &lt;/span&gt;STALE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.0.5 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:05 &lt;/span&gt;STALE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.255.200 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:03 &lt;/span&gt;REACHABLE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.255.201 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:02 &lt;/span&gt;REACHABLE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.255.202 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:04 &lt;/span&gt;REACHABLE </span>
<span class="nv">$ </span>kubectl get node <span class="nt">-owide</span> <span class="c"># mac 주소에 매칭되는 IP(노드) 찾기</span>

<span class="c"># &lt;span style="color: green;"&gt;myk8s-worker를 복구하니 SVC3의 리더 스피커 파드가 다시 myk8s-worker가 되었습니다.&lt;/span&gt; </span>
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w5/20241005_kans_w5_14.png" alt="20241005_kans_w5_14.png" class="image-center" />
<em class="image-caption">장애발생후 복구 되기까지 실습 화면</em></p>

<h5 id="옵션-externaltrafficpolicy-local">(옵션) externalTrafficPolicy: Local</h5>

<ul>
  <li>LoadBalancer도 NodePort와 마찬가지로 externalTrafficPolicy 옵션을 사용할 수 있습니다.</li>
  <li>설정 방법</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl patch svc svc1 <span class="nt">-p</span> <span class="s1">'{"spec":{"externalTrafficPolicy": "Local"}}'</span>
<span class="nv">$ </span>kubectl patch svc svc2 <span class="nt">-p</span> <span class="s1">'{"spec":{"externalTrafficPolicy": "Local"}}'</span>
<span class="nv">$ </span>kubectl patch svc svc3 <span class="nt">-p</span> <span class="s1">'{"spec":{"externalTrafficPolicy": "Local"}}'</span>
</code></pre></div></div>

<ul>
  <li>클라이언트에서 서비스의 External IP로 접속시, 리더 스피커 노드에 위치한 애플리케이션 파드로만 접속이 되며, 클라이언트 IP가 보존됩니다.</li>
  <li>단점
    <ul>
      <li>부하분산이 되지 않아 비효율적입니다.</li>
      <li>리더 노드에 애플리케이션 파드가 없을 경우 서비스 접속이 불가능합니다.</li>
    </ul>
  </li>
  <li>따라서 MetalLB에서는 externalTrafficPolicy: Local 옵션을 사용하지 않는 것을 권장합니다.</li>
</ul>

<h4 id="metallb---bgp-모드">MetalLB - BGP 모드</h4>

<ul>
  <li>현재 실습환경이 KIND 여서 BGP 모드는 실습을 못해보는것 같습니다.</li>
  <li>향후에 baremetal이나 VM으로 구성된 클러스터에서 BGP 모드를 실습해 보고 이번에는 이론만 살펴보겠습니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w5/20241005_kans_w5_15.png" alt="img.png" /></p>

<ul>
  <li>BGP 모드에서는 ARP를 사용하지 않고, BGP 데몬을 사용하여, 클러스터 외부의 라우터에 External IP를 전파합니다.</li>
  <li>ARP 모드는 스피커 리더가 있는 노드로만 트래픽이 전달되었지만, BGP 모드에서는 ECMP를 지원하여 여러 노드에 서비스를 분산시킬 수 있습니다.</li>
  <li>BGP 패킷을 캡쳐해보면 아래와 같이 Service의 External IP를 전파하는 것을 확인할 수 있습니다.
<img src="/assets/2024/kans-3th/w5/20241005_kans_w5_16.png" alt="img.png" /></li>
  <li>이때는 <code class="language-plaintext highlighter-rouge">externalTrafficPolicy: Local</code> 의 사용을 적극 권장합니다.</li>
  <li>또한 Failover가 매우 빠르며 거의 무중단으로 서비스가 가능합니다.</li>
</ul>

<h5 id="bgp-모드-설정">BGP 모드 설정</h5>

<ul>
  <li>
    <p>MetalLB의 BGP 모드 설정은 ConfigMap을 통해 설정합니다.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl replace --force -f -
apiVersion: v1
kind: ConfigMap
metadata:
  namespace: metallb-system
  name: config
data:
  config: |
    peers:
    - peer-address: 192.168.10.254
      peer-asn: 64513
      my-asn: 64512
    address-pools:
    - name: default
      protocol: bgp
      avoid-buggy-ips: true
      addresses:
      - 172.20.1.0/24
</span><span class="no">EOF
</span></code></pre></div>    </div>
  </li>
  <li>
    <p>리눅스 라우터에 BGP 설정 예시</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>router bgp 64513
  bgp router-id 192.168.10.254
  maximum-paths 4
  network 10.1.1.0/24
  neighbor 192.168.10.10  remote-as 64512
  neighbor 192.168.10.101 remote-as 64512
  neighbor 192.168.10.102 remote-as 64512
  ...
</code></pre></div>    </div>
  </li>
</ul>

<hr />

<h2 id="externalip-서비스">ExternalIP 서비스</h2>

<ul>
  <li>ExternalIP 서비스는 NodePort 서비스와 유사하게 외부 IP를 제공하는 서비스입니다.</li>
  <li>ExternalIP 서비스는 특정 노드IP로 인입한 트래픽을 해당 노드의 파드로 전달해서 외부에서 접속할 수 있게 합니다.</li>
  <li>단 사용을 권장하고 있지는 않으며, 특별한 이유가 없다면 NodePort를 사용하는것이 좋습니다.</li>
</ul>

<table>
  <thead>
    <tr>
      <th>설정 항목</th>
      <th>설명</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>spec.externalIPs</td>
      <td>노드 IP 주소(ExternalIP)</td>
    </tr>
    <tr>
      <td>spec.ports[].port</td>
      <td>ExternalIP 와 ClusterIP 에서 수신할 포트 번호</td>
    </tr>
    <tr>
      <td>spec.ports[].targetPort</td>
      <td>목적지 컨테이너 포트 번호</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>실습</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl create -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deploy-echo
spec:
  replicas: 2
  selector:
    matchLabels:
      app: deploy-websrv
  template:
    metadata:
      labels:
        app: deploy-websrv
    spec:
      terminationGracePeriodSeconds: 0
      containers:
      - name: ndks-websrv
        image: k8s.gcr.io/echoserver:1.5
        ports:
        - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: svc-externalip
spec:
  type: ClusterIP
  externalIPs:
    - 192.168.10.101
    - 192.168.10.102
  ports:
    - name: svc-webport
      port: 9000
      targetPort: 8080
  selector:
    app: deploy-websrv
</span><span class="no">EOF
</span><span class="c"># =&gt; deployment.apps/deploy-echo created</span>
<span class="c">#    service/svc-externalip created</span>

<span class="c"># 확인 : ExternalIP 도 결국 ClusterIP를 사용(포함)</span>
<span class="nv">$ </span>kubectl get svc svc-externalip
<span class="c"># =&gt; NAME             TYPE        CLUSTER-IP    EXTERNAL-IP                     PORT(S)    AGE</span>
<span class="c">#    svc-externalip   ClusterIP   10.200.1.42   192.168.10.101,192.168.10.102   9000/TCP   14s</span>

<span class="nv">$ </span>kubectl describe svc svc-externalip
<span class="c"># =&gt; Name:                     svc-externalip</span>
<span class="c">#    Namespace:                default</span>
<span class="c">#    Labels:                   &amp;lt;none&amp;gt;</span>
<span class="c">#    Annotations:              &amp;lt;none&amp;gt;</span>
<span class="c">#    Selector:                 app=deploy-websrv</span>
<span class="c">#    Type:                     ClusterIP</span>
<span class="c">#    IP Family Policy:         SingleStack</span>
<span class="c">#    IP Families:              IPv4</span>
<span class="c">#    IP:                       10.200.1.42</span>
<span class="c">#    IPs:                      10.200.1.42</span>
<span class="c">#    External IPs:             192.168.10.101,192.168.10.102</span>
<span class="c">#    Port:                     svc-webport  9000/TCP</span>
<span class="c">#    TargetPort:               8080/TCP</span>
<span class="c">#    Endpoints:                10.10.1.3:8080,10.10.3.2:8080</span>
<span class="c">#    Session Affinity:         None</span>
<span class="c">#    External Traffic Policy:  Cluster</span>
<span class="c">#    Internal Traffic Policy:  Cluster</span>
<span class="c">#    Events:                   &amp;lt;none&amp;gt;</span>

<span class="c"># ExternalTrafficPolicy 설정이 없음</span>
<span class="nv">$ </span>kubectl get svc svc-externalip <span class="nt">-o</span> yaml
<span class="c"># =&gt; apiVersion: v1</span>
<span class="c">#    kind: Service</span>
<span class="c">#    metadata:</span>
<span class="c">#      creationTimestamp: &amp;quot;2024-10-05T12:45:06Z&amp;quot;</span>
<span class="c">#      name: svc-externalip</span>
<span class="c">#      namespace: default</span>
<span class="c">#      resourceVersion: &amp;quot;38088&amp;quot;</span>
<span class="c">#      uid: b64588f5-1589-4b9b-9652-b5979f8872a1</span>
<span class="c">#    spec:</span>
<span class="c">#      clusterIP: 10.200.1.42</span>
<span class="c">#      clusterIPs:</span>
<span class="c">#      - 10.200.1.42</span>
<span class="c">#      externalIPs:</span>
<span class="c">#      - 192.168.10.101</span>
<span class="c">#      - 192.168.10.102</span>
<span class="c">#      externalTrafficPolicy: Cluster</span>
<span class="c">#      internalTrafficPolicy: Cluster</span>
<span class="c">#      ipFamilies:</span>
<span class="c">#      - IPv4</span>
<span class="c">#      ipFamilyPolicy: SingleStack</span>
<span class="c">#      ports:</span>
<span class="c">#      - name: svc-webport</span>
<span class="c">#        port: 9000</span>
<span class="c">#        protocol: TCP</span>
<span class="c">#        targetPort: 8080</span>
<span class="c">#      selector:</span>
<span class="c">#        app: deploy-websrv</span>
<span class="c">#      sessionAffinity: None</span>
<span class="c">#      type: ClusterIP</span>
<span class="c">#    status:</span>
<span class="c">#      loadBalancer: {}</span>
</code></pre></div></div>

<hr />

<h2 id="ipvs-proxy-모드">IPVS Proxy 모드</h2>

<h3 id="ipvs-proxy-모드-소개">IPVS Proxy 모드 소개</h3>

<ul>
  <li>IPVS Proxy 모드는 지난주에 살펴보았던 <strong>kube-proxy의 모드중 하나</strong>로, 리눅스 <strong>커널의 IPVS 기능을 사용하여 로드밸런싱을 수행</strong>합니다.</li>
  <li>IPVS는 L4 레이어에서 동작하며, kube-proxy의 iptables 모드보다 <strong>성능이 우수</strong>하고, 대규모 클러스터에서 더 <strong>효율적으로 동작</strong>합니다.</li>
  <li>iptables이 비해 좀 더 높은 성능을 보여주며, 규칙 갯수도 줄일 수 있습니다.</li>
  <li>부하분산 알고리즘도 다음과 같이 다양하게 지원합니다.
    <ul>
      <li>라운드 로빈 (Round Robin) : 우선순위를 두지 않고 요청을 순차적으로 전달합니다.
        <ul>
          <li>가중치 라운드 로빈 (Weighted Round Robin) : 서버에 가중치를 부여하여 요청을 전달합니다.</li>
        </ul>
      </li>
      <li>최소 연결 (Least Connection) : 현재 연결 수가 가장 적은 서버로 요청을 전달합니다.
        <ul>
          <li>가중치 최소 연결 (Weighted Least Connection) : 서버에 가중치를 부여하여 연결 수가 가장 적은 서버로 요청을 전달합니다.</li>
          <li>지역성 기반 최소 연결 (Locality-Based Least Connection) : 클라이언트와 가까우면서 요청이 적은 서버로 요청을 전달합니다.</li>
        </ul>
      </li>
      <li>목적지 해싱 (Destination Hashing) : 요청의 목적지 IP 주소를 해싱하여 서버를 선택합니다.</li>
      <li>출발지 해싱 (Source Hashing) : 요청의 출발지 IP 주소를 해싱하여 서버를 선택합니다.</li>
      <li>최단 지연 (Shortest Expected Delay) : 서버의 응답 지연 시간을 고려하여 서버를 선택합니다.</li>
      <li>큐잉 방지 (Never Queue) : 연결이 없는 서버에 우선적으로 트래픽을 보내고, 모든 서버에 트래픽이 있으면 최단 지연 방식으로 트래픽을 보냅니다.</li>
    </ul>
  </li>
</ul>

<h3 id="ipvs-proxy-모드-실습">IPVS Proxy 모드 실습</h3>

<h4 id="실습환경-설정">실습환경 설정</h4>

<ul>
  <li>먼저 기존 실습 환경을 삭제합니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kind delete cluster <span class="nt">--name</span> myk8s
<span class="c"># =&gt; Deleting cluster "myk8s" ...</span>
<span class="c">#    Deleted nodes: ["myk8s-worker" "myk8s-control-plane" "myk8s-worker2" "myk8s-worker3"]</span>
</code></pre></div></div>

<ul>
  <li>실습환경은 KIND를 사용하며, KIND 클러스터에 IPVS Proxy 모드를 적용해보겠습니다.</li>
  <li>실습 환경 : K8S v1.31.0, CNI(Kindnet / Direct Routing mode),  IPVS proxy mode
    <ul>
      <li>노드(실제로는 컨테이너) 네트워크 대역 : 172.20.0.0/16</li>
      <li>파드 사용 네트워크 대역 : 10.10.0.0/16 ⇒ 각각 10.10.1.0/24, 10.10.2.0/24, 10.10.3.0/24, 10.10.4.0/24</li>
      <li>서비스 사용 네트워크 대역 : 10.200.1.0/24</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/2024/kans-3th/w5/20241005_kans_w5_18.png" alt="img.png" class="image-center" /></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 파일 작성</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOT</span><span class="sh">&gt; kind-svc-2w-ipvs.yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
featureGates:
  "InPlacePodVerticalScaling": true
  "MultiCIDRServiceAllocator": true
nodes:
- role: control-plane
  labels:
    mynode: control-plane
    topology.kubernetes.io/zone: ap-northeast-2a
  extraPortMappings:
  - containerPort: 30000
    hostPort: 30000
  - containerPort: 30001
    hostPort: 30001
  - containerPort: 30002
    hostPort: 30002
  - containerPort: 30003
    hostPort: 30003
  - containerPort: 30004
    hostPort: 30004
  kubeadmConfigPatches:
  - |
    kind: ClusterConfiguration
    apiServer:
      extraArgs:
        runtime-config: api/all=true
    controllerManager:
      extraArgs:
        bind-address: 0.0.0.0
    etcd:
      local:
        extraArgs:
          listen-metrics-urls: http://0.0.0.0:2381
    scheduler:
      extraArgs:
        bind-address: 0.0.0.0
  - |
    kind: KubeProxyConfiguration
    metricsBindAddress: 0.0.0.0
    ipvs:
      strictARP: true
- role: worker
  labels:
    mynode: worker1
    topology.kubernetes.io/zone: ap-northeast-2a
- role: worker
  labels:
    mynode: worker2
    topology.kubernetes.io/zone: ap-northeast-2b
- role: worker
  labels:
    mynode: worker3
    topology.kubernetes.io/zone: ap-northeast-2c
networking:
  podSubnet: 10.10.0.0/16
  serviceSubnet: 10.200.1.0/24
  kubeProxyMode: "ipvs"        
</span><span class="no">EOT

</span><span class="c"># k8s 클러스터 설치</span>
<span class="nv">$ </span>kind create cluster <span class="nt">--config</span> kind-svc-2w-ipvs.yaml <span class="nt">--name</span> myk8s <span class="nt">--image</span> kindest/node:v1.31.0
<span class="nv">$ </span>docker ps
<span class="c"># =&gt; CONTAINER ID   IMAGE                  COMMAND                  CREATED         STATUS         PORTS                                                             NAMES</span>
<span class="c">#    aa2f031f0959   kindest/node:v1.31.0   &amp;quot;/usr/local/bin/entr…&amp;quot;   2 minutes ago   Up 2 minutes                                                                     myk8s-worker3</span>
<span class="c">#    0a67b245f7ee   kindest/node:v1.31.0   &amp;quot;/usr/local/bin/entr…&amp;quot;   2 minutes ago   Up 2 minutes   0.0.0.0:30000-30004-&amp;gt;30000-30004/tcp, 127.0.0.1:49623-&amp;gt;6443/tcp   myk8s-control-plane</span>
<span class="c">#    4525aac3b06f   kindest/node:v1.31.0   &amp;quot;/usr/local/bin/entr…&amp;quot;   2 minutes ago   Up 2 minutes                                                                     myk8s-worker2</span>
<span class="c">#    0087fe52e3d2   kindest/node:v1.31.0   &amp;quot;/usr/local/bin/entr…&amp;quot;   2 minutes ago   Up 2 minutes                                                                     myk8s-worker</span>

<span class="c"># 노드에 기본 툴 설치</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane sh <span class="nt">-c</span> <span class="s1">'apt update &amp;&amp; apt install tree psmisc lsof wget bsdmainutils bridge-utils net-tools dnsutils ipset ipvsadm nfacct tcpdump ngrep iputils-ping arping git vim arp-scan -y'</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> sh <span class="nt">-c</span> <span class="s1">'apt update &amp;&amp; apt install tree psmisc lsof wget bsdmainutils bridge-utils net-tools dnsutils ipset ipvsadm nfacct tcpdump ngrep iputils-ping arping -y'</span><span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>

<span class="c"># kube-proxy configmap 확인</span>
<span class="nv">$ </span>kubectl describe cm <span class="nt">-n</span> kube-system kube-proxy
<span class="c"># =&gt; ...</span>
<span class="c">#    mode: ipvs</span>
<span class="c">#    ipvs: # 아래 각각 옵션 의미 조사해봅시다.</span>
<span class="c">#      excludeCIDRs: null   # IPVS에서 제외할 CIDR을 지정합니다. IPVS 룰을 정리할때 제외할 대역을 지정합니다.</span>
<span class="c">#      minSyncPeriod: 0s    # IPVS 룰을 동기화할 최소 주기를 지정합니다. 이 옵션을 사용하면 IPVS 룰을 동기화하는 주기를 제한할 수 있습니다.</span>
<span class="c">#      scheduler: ""        # IPVS 스케줄러는 IPVS가 사용할 로드밸런싱 알고리즘을 지정합니다.</span>
<span class="c">#      strictARP: true      # MetalLB 동작을 위해서 true 설정 변경 필요</span>
<span class="c">#      syncPeriod: 0s       # IPVS 룰을 동기화할 주기를 지정합니다. 이 옵션을 사용하면 IPVS 룰을 동기화하는 주기를 지정할 수 있습니다.</span>
<span class="c">#      tcpFinTimeout: 0s    # IPVS에서 TCP 연결이 종료된 후 FIN 상태를 유지하는 시간을 지정합니다.</span>
<span class="c">#      tcpTimeout: 0s       # IPVS에서 TCP 패킷을 처리하는 시간을 지정합니다.</span>
<span class="c">#      udpTimeout: 0s       # IPVS에서 UDP 패킷을 처리하는 시간을 지정합니다.</span>
<span class="c">#    ...</span>

<span class="c"># strictARP: true는 ARP 패킷을 보다 엄격하게 처리하겠다는 설정입니다.</span>
<span class="c">## IPVS 모드에서 strict ARP가 활성화되면, 노드의 인터페이스는 자신에게 할당된 IP 주소에 대해서만 ARP 응답을 보내게 됩니다. </span>
<span class="c">## 이는 IPVS로 로드밸런싱할 때 ARP 패킷이 잘못된 인터페이스로 전달되는 문제를 방지합니다.</span>
<span class="c">## 이 설정은 특히 클러스터 내에서 여러 노드가 동일한 IP를 갖는 VIP(Virtual IP)를 사용하는 경우 중요합니다.</span>

<span class="c"># 노드 별 네트워트 정보 확인 : kube-ipvs0 네트워크 인터페이스 확인</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> ip <span class="nt">-c</span> route<span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; &amp;gt;&amp;gt; node myk8s-control-plane &amp;lt;&amp;lt;</span>
<span class="c">#    default via &lt;span style="color:purple;"&gt;172.20.0.1 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:purple;"&gt;10.10.0.2 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;vethc61550c2 &lt;/span&gt;scope host </span>
<span class="c">#    &lt;span style="color:purple;"&gt;10.10.0.3 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;veth85b53091 &lt;/span&gt;scope host </span>
<span class="c">#    &lt;span style="color:purple;"&gt;10.10.0.4 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;veth8ad445fd &lt;/span&gt;scope host </span>
<span class="c">#    &lt;span style="color:purple;"&gt;10.10.1.0/24 &lt;/span&gt;via &lt;span style="color:purple;"&gt;172.20.0.3 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:purple;"&gt;10.10.2.0/24 &lt;/span&gt;via &lt;span style="color:purple;"&gt;172.20.0.2 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:purple;"&gt;10.10.3.0/24 &lt;/span&gt;via &lt;span style="color:purple;"&gt;172.20.0.4 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.0.0/16 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;proto kernel scope link src &lt;span style="color:purple;"&gt;172.20.0.5 &lt;/span&gt;</span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; node myk8s-worker &amp;lt;&amp;lt;</span>
<span class="c">#    default via &lt;span style="color:purple;"&gt;172.20.0.1 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:purple;"&gt;10.10.0.0/24 &lt;/span&gt;via &lt;span style="color:purple;"&gt;172.20.0.5 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:purple;"&gt;10.10.1.0/24 &lt;/span&gt;via &lt;span style="color:purple;"&gt;172.20.0.3 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:purple;"&gt;10.10.2.0/24 &lt;/span&gt;via &lt;span style="color:purple;"&gt;172.20.0.2 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.0.0/16 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;proto kernel scope link src &lt;span style="color:purple;"&gt;172.20.0.4 &lt;/span&gt;</span>
<span class="c">#    ...</span>

<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> ip <span class="nt">-c</span> addr<span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; &amp;gt;&amp;gt; node myk8s-control-plane &amp;lt;&amp;lt;</span>
<span class="c">#    1: &lt;span style="color:teal;"&gt;lo: &lt;/span&gt;&amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</span>
<span class="c">#        link/loopback &lt;span style="color:olive;"&gt;00:00:00:00:00:00&lt;/span&gt; brd &lt;span style="color:olive;"&gt;00:00:00:00:00:00&lt;/span&gt;</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;127.0.0.1&lt;/span&gt;/8 scope host lo</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    2: &lt;span style="color:teal;"&gt;tunl0@NONE: &lt;/span&gt;&amp;lt;NOARP&amp;gt; mtu 1480 qdisc noop state &lt;span style="color:red;"&gt;DOWN &lt;/span&gt;group default qlen 1000</span>
<span class="c">#        link/ipip &lt;span style="color:olive;"&gt;0.0.0.0&lt;/span&gt; brd &lt;span style="color:olive;"&gt;0.0.0.0&lt;/span&gt;</span>
<span class="c">#    4: &lt;span style="color:teal;"&gt;kube-ipvs0: &lt;/span&gt;&amp;lt;BROADCAST,NOARP&amp;gt; mtu 1500 qdisc noop state &lt;span style="color:red;"&gt;DOWN &lt;/span&gt;group default </span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;0a:91:66:a1:e6:bb&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt;</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;10.200.1.1&lt;/span&gt;/32 scope global kube-ipvs0</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;10.200.1.10&lt;/span&gt;/32 scope global kube-ipvs0</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    5: &lt;span style="color:teal;"&gt;vethc61550c2@if4: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state &lt;span style="color:green;"&gt;UP &lt;/span&gt;group default </span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;de:a6:86:25:bb:08&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt; link-netns cni-4751a95b-cc8c-ff23-9dd5-35e7f1a2223b</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;10.10.0.1&lt;/span&gt;/32 scope global vethc61550c2</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    6: &lt;span style="color:teal;"&gt;veth85b53091@if4: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state &lt;span style="color:green;"&gt;UP &lt;/span&gt;group default </span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;ea:36:d9:10:fc:2f&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt; link-netns cni-db21fe8b-f48c-f3fe-6c6a-b2f204eea0e5</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;10.10.0.1&lt;/span&gt;/32 scope global veth85b53091</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    7: &lt;span style="color:teal;"&gt;veth8ad445fd@if4: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state &lt;span style="color:green;"&gt;UP &lt;/span&gt;group default </span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;b6:bb:1b:44:13:eb&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt; link-netns cni-26749ed0-4863-eef5-640b-96f804e871ae</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;10.10.0.1&lt;/span&gt;/32 scope global veth8ad445fd</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    46: &lt;span style="color:teal;"&gt;eth0@if47: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state &lt;span style="color:green;"&gt;UP &lt;/span&gt;group default </span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;02:42:ac:14:00:05&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt; link-netnsid 0</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;172.20.0.5&lt;/span&gt;/16 brd &lt;span style="color:purple;"&gt;172.20.255.255 &lt;/span&gt;scope global eth0</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; node myk8s-worker &amp;lt;&amp;lt;</span>
<span class="c">#    1: &lt;span style="color:teal;"&gt;lo: &lt;/span&gt;&amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</span>
<span class="c">#        link/loopback &lt;span style="color:olive;"&gt;00:00:00:00:00:00&lt;/span&gt; brd &lt;span style="color:olive;"&gt;00:00:00:00:00:00&lt;/span&gt;</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;127.0.0.1&lt;/span&gt;/8 scope host lo</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    2: &lt;span style="color:teal;"&gt;tunl0@NONE: &lt;/span&gt;&amp;lt;NOARP&amp;gt; mtu 1480 qdisc noop state &lt;span style="color:red;"&gt;DOWN &lt;/span&gt;group default qlen 1000</span>
<span class="c">#        link/ipip &lt;span style="color:olive;"&gt;0.0.0.0&lt;/span&gt; brd &lt;span style="color:olive;"&gt;0.0.0.0&lt;/span&gt;</span>
<span class="c">#    4: &lt;span style="color:teal;"&gt;kube-ipvs0: &lt;/span&gt;&amp;lt;BROADCAST,NOARP&amp;gt; mtu 1500 qdisc noop state &lt;span style="color:red;"&gt;DOWN &lt;/span&gt;group default </span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;fe:2c:45:76:c8:8d&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt;</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;10.200.1.1&lt;/span&gt;/32 scope global kube-ipvs0</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;10.200.1.10&lt;/span&gt;/32 scope global kube-ipvs0</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    44: &lt;span style="color:teal;"&gt;eth0@if45: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state &lt;span style="color:green;"&gt;UP &lt;/span&gt;group default </span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;02:42:ac:14:00:04&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt; link-netnsid 0</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;172.20.0.4&lt;/span&gt;/16 brd &lt;span style="color:purple;"&gt;172.20.255.255 &lt;/span&gt;scope global eth0</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    ...</span>

<span class="c"># &lt;span style="color: green;"&gt;👉 노드별로 kube-ipvs0 인터페이스가 생성되었으며, IP 주소가 할당되어 있습니다.&lt;/span&gt;</span>

<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> ip <span class="nt">-br</span> <span class="nt">-c</span> addr show kube-ipvs0<span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; &amp;gt;&amp;gt; node myk8s-control-plane &amp;lt;&amp;lt;</span>
<span class="c">#    &lt;span style="color:teal;"&gt;kube-ipvs0       &lt;/span&gt;&lt;span style="color:red;"&gt;DOWN           &lt;/span&gt;&lt;span style="color:purple;"&gt;10.200.1.1&lt;/span&gt;/32 &lt;span style="color:purple;"&gt;10.200.1.10&lt;/span&gt;/32 </span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; node myk8s-worker &amp;lt;&amp;lt;</span>
<span class="c">#    &lt;span style="color:teal;"&gt;kube-ipvs0       &lt;/span&gt;&lt;span style="color:red;"&gt;DOWN           &lt;/span&gt;&lt;span style="color:purple;"&gt;10.200.1.1&lt;/span&gt;/32 &lt;span style="color:purple;"&gt;10.200.1.10&lt;/span&gt;/32 </span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; node myk8s-worker2 &amp;lt;&amp;lt;</span>
<span class="c">#    &lt;span style="color:teal;"&gt;kube-ipvs0       &lt;/span&gt;&lt;span style="color:red;"&gt;DOWN           &lt;/span&gt;&lt;span style="color:purple;"&gt;10.200.1.1&lt;/span&gt;/32 &lt;span style="color:purple;"&gt;10.200.1.10&lt;/span&gt;/32 </span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; node myk8s-worker3 &amp;lt;&amp;lt;</span>
<span class="c">#    &lt;span style="color:teal;"&gt;kube-ipvs0       &lt;/span&gt;&lt;span style="color:red;"&gt;DOWN           &lt;/span&gt;&lt;span style="color:purple;"&gt;10.200.1.1&lt;/span&gt;/32 &lt;span style="color:purple;"&gt;10.200.1.10&lt;/span&gt;/32 </span>

<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> ip <span class="nt">-d</span> <span class="nt">-c</span> addr show kube-ipvs0<span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; &amp;gt;&amp;gt; node myk8s-control-plane &amp;lt;&amp;lt;</span>
<span class="c">#    4: &lt;span style="color:teal;"&gt;kube-ipvs0: &lt;/span&gt;&amp;lt;BROADCAST,NOARP&amp;gt; mtu 1500 qdisc noop state &lt;span style="color:red;"&gt;DOWN &lt;/span&gt;group default </span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;0a:91:66:a1:e6:bb&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt; promiscuity 0 minmtu 0 maxmtu 0 </span>
<span class="c">#        dummy numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535 </span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;10.200.1.1&lt;/span&gt;/32 scope global kube-ipvs0</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;10.200.1.10&lt;/span&gt;/32 scope global kube-ipvs0</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; node myk8s-worker &amp;lt;&amp;lt;</span>
<span class="c">#    4: &lt;span style="color:teal;"&gt;kube-ipvs0: &lt;/span&gt;&amp;lt;BROADCAST,NOARP&amp;gt; mtu 1500 qdisc noop state &lt;span style="color:red;"&gt;DOWN &lt;/span&gt;group default </span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;fe:2c:45:76:c8:8d&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt; promiscuity 0 minmtu 0 maxmtu 0 </span>
<span class="c">#        dummy numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535 </span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;10.200.1.1&lt;/span&gt;/32 scope global kube-ipvs0</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;10.200.1.10&lt;/span&gt;/32 scope global kube-ipvs0</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; node myk8s-worker2 &amp;lt;&amp;lt;</span>
<span class="c">#    4: &lt;span style="color:teal;"&gt;kube-ipvs0: &lt;/span&gt;&amp;lt;BROADCAST,NOARP&amp;gt; mtu 1500 qdisc noop state &lt;span style="color:red;"&gt;DOWN &lt;/span&gt;group default </span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;5a:26:49:54:18:21&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt; promiscuity 0 minmtu 0 maxmtu 0 </span>
<span class="c">#        dummy numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535 </span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;10.200.1.1&lt;/span&gt;/32 scope global kube-ipvs0</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;10.200.1.10&lt;/span&gt;/32 scope global kube-ipvs0</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; node myk8s-worker3 &amp;lt;&amp;lt;</span>
<span class="c">#    4: &lt;span style="color:teal;"&gt;kube-ipvs0: &lt;/span&gt;&amp;lt;BROADCAST,NOARP&amp;gt; mtu 1500 qdisc noop state &lt;span style="color:red;"&gt;DOWN &lt;/span&gt;group default </span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;fa:0c:2d:44:52:23&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt; promiscuity 0 minmtu 0 maxmtu 0 </span>
<span class="c">#        dummy numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535 </span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;10.200.1.1&lt;/span&gt;/32 scope global kube-ipvs0</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;10.200.1.10&lt;/span&gt;/32 scope global kube-ipvs0</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>

<span class="c"># kube-ipvs0 에 할당된 IP(기본 IP + 보조 IP들) 정보 확인 </span>
<span class="nv">$ </span>kubectl get svc,ep <span class="nt">-A</span>
<span class="c"># =&gt; NAMESPACE     NAME                 TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)                  AGE</span>
<span class="c">#    default       service/kubernetes   ClusterIP   10.200.1.1    &amp;lt;none&amp;gt;        443/TCP                  31m</span>
<span class="c">#    kube-system   service/kube-dns     ClusterIP   10.200.1.10   &amp;lt;none&amp;gt;        53/UDP,53/TCP,9153/TCP   31m</span>
<span class="c">#    </span>
<span class="c">#    NAMESPACE     NAME                   ENDPOINTS                                            AGE</span>
<span class="c">#    default       endpoints/kubernetes   172.20.0.5:6443                                      31m</span>
<span class="c">#    kube-system   endpoints/kube-dns     10.10.0.3:53,10.10.0.4:53,10.10.0.3:53 + 3 more...   31m</span>

<span class="c"># ipvsadm 툴로 부하분산 되는 정보 확인 : 서비스의 IP와 서비스에 연동되어 있는 파드의 IP 를 확인</span>
<span class="c">## Service IP(VIP) 처리를 ipvs 에서 담당 -&gt; 이를 통해 iptables 에 체인/정책이 상당 수준 줄어듬</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> ipvsadm <span class="nt">-Ln</span> <span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; &amp;gt;&amp;gt; node myk8s-control-plane &amp;lt;&amp;lt;</span>
<span class="c">#    IP Virtual Server version 1.2.1 (size=4096)</span>
<span class="c">#    Prot LocalAddress:Port Scheduler Flags</span>
<span class="c">#      -&amp;gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn</span>
<span class="c">#    TCP  10.200.1.1:443 rr</span>
<span class="c">#      -&amp;gt; 172.20.0.5:6443              Masq    1      3          0         </span>
<span class="c">#    TCP  10.200.1.10:53 rr</span>
<span class="c">#      -&amp;gt; 10.10.0.3:53                 Masq    1      0          0         </span>
<span class="c">#      -&amp;gt; 10.10.0.4:53                 Masq    1      0          0         </span>
<span class="c">#    TCP  10.200.1.10:9153 rr</span>
<span class="c">#      -&amp;gt; 10.10.0.3:9153               Masq    1      0          0         </span>
<span class="c">#      -&amp;gt; 10.10.0.4:9153               Masq    1      0          0         </span>
<span class="c">#    UDP  10.200.1.10:53 rr</span>
<span class="c">#      -&amp;gt; 10.10.0.3:53                 Masq    1      0          0         </span>
<span class="c">#      -&amp;gt; 10.10.0.4:53                 Masq    1      0          0         </span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; node myk8s-worker &amp;lt;&amp;lt;</span>
<span class="c">#    IP Virtual Server version 1.2.1 (size=4096)</span>
<span class="c">#    Prot LocalAddress:Port Scheduler Flags</span>
<span class="c">#      -&amp;gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn</span>
<span class="c">#    TCP  10.200.1.1:443 rr</span>
<span class="c">#      -&amp;gt; 172.20.0.5:6443              Masq    1      0          0         </span>
<span class="c">#    TCP  10.200.1.10:53 rr</span>
<span class="c">#      -&amp;gt; 10.10.0.3:53                 Masq    1      0          0         </span>
<span class="c">#      -&amp;gt; 10.10.0.4:53                 Masq    1      0          0         </span>
<span class="c">#    TCP  10.200.1.10:9153 rr</span>
<span class="c">#      -&amp;gt; 10.10.0.3:9153               Masq    1      0          0         </span>
<span class="c">#      -&amp;gt; 10.10.0.4:9153               Masq    1      0          0         </span>
<span class="c">#    UDP  10.200.1.10:53 rr</span>
<span class="c">#      -&amp;gt; 10.10.0.3:53                 Masq    1      0          0         </span>
<span class="c">#      -&amp;gt; 10.10.0.4:53                 Masq    1      0          0         </span>
<span class="c">#    ...</span>

<span class="c">## IPSET 확인</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker ipset <span class="nt">-h</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker ipset <span class="nt">-L</span>

<span class="c"># iptables 정보 확인 : 정책 갯수를 iptables proxy 모드와 비교해보자</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>filter nat mangle raw <span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; IPTables Type : </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane  iptables <span class="nt">-t</span> <span class="nv">$i</span> <span class="nt">-S</span> <span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>filter nat mangle raw <span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; IPTables Type : </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker  iptables <span class="nt">-t</span> <span class="nv">$i</span> <span class="nt">-S</span> <span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>filter nat mangle raw <span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; IPTables Type : </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker2 iptables <span class="nt">-t</span> <span class="nv">$i</span> <span class="nt">-S</span> <span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>filter nat mangle raw <span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; IPTables Type : </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker3 iptables <span class="nt">-t</span> <span class="nv">$i</span> <span class="nt">-S</span> <span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>

<span class="c"># 각 노드 bash 접속</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane bash
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker bash
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker2 bash
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker3 bash
<span class="nt">----------------------------------------</span>
<span class="nv">$ </span><span class="nb">exit</span>
<span class="nt">----------------------------------------</span>

<span class="c"># mypc 컨테이너 기동 : kind 도커 브리지를 사용하고, 컨테이너 IP를 직접 지정 혹은 IP 지정 없이 배포</span>
<span class="nv">$ </span>docker run <span class="nt">-d</span> <span class="nt">--rm</span> <span class="nt">--name</span> mypc <span class="nt">--network</span> kind <span class="nt">--ip</span> 172.20.0.100 nicolaka/netshoot <span class="nb">sleep </span>infinity
<span class="c"># 혹은</span>
<span class="nv">$ </span>docker run <span class="nt">-d</span> <span class="nt">--rm</span> <span class="nt">--name</span> mypc <span class="nt">--network</span> kind nicolaka/netshoot <span class="nb">sleep </span>infinity

<span class="nv">$ </span>docker ps
<span class="c"># =&gt; CONTAINER ID   IMAGE                  COMMAND                  CREATED          STATUS          PORTS                                                             NAMES</span>
<span class="c">#    16b541ee953e   nicolaka/netshoot      &amp;quot;sleep infinity&amp;quot;         31 seconds ago   Up 31 seconds                                                                     mypc</span>
<span class="c">#    aa2f031f0959   kindest/node:v1.31.0   &amp;quot;/usr/local/bin/entr…&amp;quot;   34 minutes ago   Up 34 minutes                                                                     myk8s-worker3</span>
<span class="c">#    0a67b245f7ee   kindest/node:v1.31.0   &amp;quot;/usr/local/bin/entr…&amp;quot;   34 minutes ago   Up 34 minutes   0.0.0.0:30000-30004-&amp;gt;30000-30004/tcp, 127.0.0.1:49623-&amp;gt;6443/tcp   myk8s-control-plane</span>
<span class="c">#    4525aac3b06f   kindest/node:v1.31.0   &amp;quot;/usr/local/bin/entr…&amp;quot;   34 minutes ago   Up 34 minutes                                                                     myk8s-worker2</span>
<span class="c">#    0087fe52e3d2   kindest/node:v1.31.0   &amp;quot;/usr/local/bin/entr…&amp;quot;   34 minutes ago   Up 34 minutes                                                                     myk8s-worker</span>
</code></pre></div></div>

<h5 id="ipvs-정보-확인">IPVS 정보 확인</h5>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># kube-proxy 로그 확인 :  기본값 부하분산 스케줄러(RoundRobin = RR)</span>
<span class="nv">$ </span>kubectl stern <span class="nt">-n</span> kube-system <span class="nt">-l</span> k8s-app<span class="o">=</span>kube-proxy <span class="nt">--since</span> 2h | egrep <span class="s1">'(ipvs|IPVS)'</span>
<span class="c"># =&gt; ...</span>
<span class="c">#    kube-proxy-24z49 kube-proxy I1005 15:10:04.041490       1 server_linux.go:230] "Using ipvs Proxier"</span>
<span class="c">#    kube-proxy-24z49 kube-proxy I1005 15:10:04.048394       1 proxier.go:364] "IPVS scheduler not specified, use rr by default" ipFamily="IPv4"</span>
<span class="c">#    kube-proxy-24z49 kube-proxy I1005 15:10:04.048529       1 proxier.go:364] "IPVS scheduler not specified, use rr by default" ipFamily="IPv6"</span>

<span class="c"># 기본 모드 정보 확인</span>
<span class="nv">$ </span>kubectl get cm <span class="nt">-n</span> kube-system kube-proxy <span class="nt">-o</span> yaml | egrep <span class="s1">'mode|strictARP|scheduler'</span>
<span class="c"># =&gt;       scheduler: &amp;quot;&amp;quot;</span>
<span class="c">#          strictARP: true</span>
<span class="c">#        mode: ipvs</span>

<span class="c"># ipvsadm 툴로 부하분산 되는 정보 확인 : RR 부하분산 스케줄러 확인</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> ipvsadm <span class="nt">-Ln</span> <span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; &amp;gt;&amp;gt; node myk8s-control-plane &amp;lt;&amp;lt;</span>
<span class="c">#    IP Virtual Server version 1.2.1 (size=4096)</span>
<span class="c">#    Prot LocalAddress:Port Scheduler Flags</span>
<span class="c">#      -&amp;gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn</span>
<span class="c">#    TCP  10.200.1.1:443 rr</span>
<span class="c">#      -&amp;gt; 172.20.0.5:6443              Masq    1      3          0         </span>
<span class="c">#    ...</span>

<span class="c"># 커널 파라미터 확인</span>
<span class="c"># (심화 옵션) strictARP - 링크 설정(유사한)이유</span>
<span class="c"># --ipvs-strict-arp : Enable strict ARP by setting arp_ignore to 1 and arp_announce to 2</span>
<span class="c"># arp_ignore : ARP request 를 받았을때 응답 여부 - 0(ARP 요청 도착시, any Interface 있으면 응답), 1(ARP 요청을 받은 Interface 가 해당 IP일때만 응답)</span>
<span class="c"># arp_announce : ARP request 를 보낼 때 'ARP Sender IP 주소'에 지정 값 - 0(sender IP로 시스템의 any IP 가능), 2(sender IP로 실제 전송하는 Interface 에 IP를 사용)</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker tree /proc/sys/net/ipv4/conf/kube-ipvs0
<span class="c"># =&gt; /proc/sys/net/ipv4/conf/kube-ipvs0</span>
<span class="c">#    |-- ...</span>
<span class="c">#    |-- arp_accept</span>
<span class="c">#    |-- arp_announce</span>
<span class="c">#    `-- ...</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker <span class="nb">cat</span> /proc/sys/net/ipv4/conf/kube-ipvs0/arp_ignore
<span class="c"># =&gt; 0</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker <span class="nb">cat</span> /proc/sys/net/ipv4/conf/kube-ipvs0/arp_announce
<span class="c"># =&gt; 0</span>

<span class="c"># all 은 모든 인터페이스에 영항을 줌, 단 all 과 interface 값이 다를때 우선순위는 커널 파라미터 별로 다르다 - 링크</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker sysctl net.ipv4.conf.all.arp_ignore
<span class="c"># =&gt; net.ipv4.conf.all.arp_ignore = 1</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker sysctl net.ipv4.conf.all.arp_announce
<span class="c"># =&gt; net.ipv4.conf.all.arp_announce = 2</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker sysctl net.ipv4.conf.kube-ipvs0.arp_ignore
<span class="c"># =&gt; net.ipv4.conf.kube-ipvs0.arp_ignore = 0</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker sysctl net.ipv4.conf.kube-ipvs0.arp_announce
<span class="c"># =&gt; net.ipv4.conf.kube-ipvs0.arp_announce = 0</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker sysctl <span class="nt">-a</span> | <span class="nb">grep </span>arp_ignore
<span class="c"># =&gt; net.ipv4.conf.all.arp_ignore = 1</span>
<span class="c">#    net.ipv4.conf.default.arp_ignore = 0</span>
<span class="c">#    net.ipv4.conf.eth0.arp_ignore = 0</span>
<span class="c">#    net.ipv4.conf.ip6tnl0.arp_ignore = 0</span>
<span class="c">#    net.ipv4.conf.kube-ipvs0.arp_ignore = 0</span>
<span class="c">#    net.ipv4.conf.lo.arp_ignore = 0</span>
<span class="c">#    net.ipv4.conf.tunl0.arp_ignore = 0</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker sysctl <span class="nt">-a</span> | <span class="nb">grep </span>arp_announce
<span class="c"># =&gt; net.ipv4.conf.all.arp_announce = 2</span>
<span class="c">#    net.ipv4.conf.default.arp_announce = 0</span>
<span class="c">#    net.ipv4.conf.eth0.arp_announce = 0</span>
<span class="c">#    net.ipv4.conf.ip6tnl0.arp_announce = 0</span>
<span class="c">#    net.ipv4.conf.kube-ipvs0.arp_announce = 0</span>
<span class="c">#    net.ipv4.conf.lo.arp_announce = 0</span>
<span class="c">#    net.ipv4.conf.tunl0.arp_announce = 0</span>

<span class="c"># IPSET 확인</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker ipset <span class="nt">-h</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker ipset <span class="nt">-L</span>
</code></pre></div></div>

<h5 id="목적지backend-파드pod-생성--3podyaml">목적지(backend) 파드(Pod) 생성 : 3pod.yaml</h5>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOT</span><span class="sh">&gt; 3pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: webpod1
  labels:
    app: webpod
spec:
  nodeName: myk8s-worker
  containers:
  - name: container
    image: traefik/whoami
  terminationGracePeriodSeconds: 0
---
apiVersion: v1
kind: Pod
metadata:
  name: webpod2
  labels:
    app: webpod
spec:
  nodeName: myk8s-worker2
  containers:
  - name: container
    image: traefik/whoami
  terminationGracePeriodSeconds: 0
---
apiVersion: v1
kind: Pod
metadata:
  name: webpod3
  labels:
    app: webpod
spec:
  nodeName: myk8s-worker3
  containers:
  - name: container
    image: traefik/whoami
  terminationGracePeriodSeconds: 0
</span><span class="no">EOT
</span></code></pre></div></div>

<h5 id="클라이언트testpod-생성--netpodyaml">클라이언트(TestPod) 생성 : netpod.yaml</h5>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOT</span><span class="sh">&gt; netpod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: net-pod
spec:
  nodeName: myk8s-control-plane
  containers:
  - name: netshoot-pod
    image: nicolaka/netshoot
    command: ["tail"]
    args: ["-f", "/dev/null"]
  terminationGracePeriodSeconds: 0
</span><span class="no">EOT
</span></code></pre></div></div>

<h5 id="서비스clusterip-생성--svc-clusteripyaml">서비스(ClusterIP) 생성 : svc-clusterip.yaml</h5>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOT</span><span class="sh">&gt; svc-clusterip.yaml
apiVersion: v1
kind: Service
metadata:
  name: svc-clusterip
spec:
  ports:
    - name: svc-webport
      port: 9000        # 서비스 IP 에 접속 시 사용하는 포트 port 를 의미
      targetPort: 80    # 타킷 targetPort 는 서비스를 통해서 목적지 파드로 접속 시 해당 파드로 접속하는 포트를 의미
  selector:
    app: webpod         # 셀렉터 아래 app:webpod 레이블이 설정되어 있는 파드들은 해당 서비스에 연동됨
  type: ClusterIP       # 서비스 타입
</span><span class="no">EOT
</span></code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w5/20241005_kans_w5_19.png" alt="img.png" /></p>

<h5 id="생성-및-확인--ipvs-proxy-모드">생성 및 확인 : IPVS Proxy 모드</h5>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 생성</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> 3pod.yaml,netpod.yaml,svc-clusterip.yaml
<span class="c"># =&gt; pod/webpod1 created</span>
<span class="c">#    pod/webpod2 created</span>
<span class="c">#    pod/webpod3 created</span>
<span class="c">#    pod/net-pod created</span>
<span class="c">#    service/svc-clusterip created</span>

<span class="c"># 파드와 서비스 사용 네트워크 대역 정보 확인 </span>
<span class="nv">$ </span>kubectl cluster-info dump | <span class="nb">grep</span> <span class="nt">-m</span> 2 <span class="nt">-E</span> <span class="s2">"cluster-cidr|service-cluster-ip-range"</span>
<span class="c"># =&gt;                             &amp;quot;--service-cluster-ip-range=10.200.1.0/24&amp;quot;,</span>
<span class="c">#                                &amp;quot;--cluster-cidr=10.10.0.0/16&amp;quot;,</span>

<span class="c"># 확인</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-owide</span>
<span class="c"># =&gt; NAME      READY   STATUS    RESTARTS   AGE   IP          NODE                  NOMINATED NODE   READINESS GATES</span>
<span class="c">#    net-pod   1/1     Running   0          36s   10.10.0.5   myk8s-control-plane   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    webpod1   1/1     Running   0          36s   10.10.3.2   myk8s-worker          &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    webpod2   1/1     Running   0          36s   10.10.1.2   myk8s-worker2         &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    webpod3   1/1     Running   0          36s   10.10.2.2   myk8s-worker3         &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="nv">$ </span>kubectl get svc svc-clusterip
<span class="c"># =&gt; NAME            TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)    AGE</span>
<span class="c">#    svc-clusterip   ClusterIP   10.200.1.17   &amp;lt;none&amp;gt;        9000/TCP   44s</span>
<span class="nv">$ </span>kubectl describe svc svc-clusterip
<span class="nv">$ </span>kubectl get endpoints svc-clusterip
<span class="c"># =&gt; NAME            ENDPOINTS                                AGE</span>
<span class="c">#    svc-clusterip   10.10.1.2:80,10.10.2.2:80,10.10.3.2:80   55s</span>
<span class="nv">$ </span>kubectl get endpointslices <span class="nt">-l</span> kubernetes.io/service-name<span class="o">=</span>svc-clusterip
<span class="c"># =&gt; NAME                  ADDRESSTYPE   PORTS   ENDPOINTS                       AGE</span>
<span class="c">#    svc-clusterip-scf9k   IPv4          80      10.10.2.2,10.10.1.2,10.10.3.2   63s</span>

<span class="c"># 노드 별 네트워트 정보 확인 : kube-ipvs0 네트워크 인터페이스 확인</span>
<span class="c">## ClusterIP 생성 시 kube-ipvs0 인터페이스에 ClusterIP 가 할당되는 것을 확인</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> ip <span class="nt">-c</span> addr<span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> ip <span class="nt">-br</span> <span class="nt">-c</span> addr show kube-ipvs0<span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> ip <span class="nt">-d</span> <span class="nt">-c</span> addr show kube-ipvs0<span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>

<span class="c"># 변수 지정</span>
<span class="nv">$ CIP</span><span class="o">=</span><span class="si">$(</span>kubectl get svc svc-clusterip <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.spec.clusterIP}"</span><span class="si">)</span>
<span class="nv">$ CPORT</span><span class="o">=</span><span class="si">$(</span>kubectl get svc svc-clusterip <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.spec.ports[0].port}"</span><span class="si">)</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$CIP</span> <span class="nv">$CPORT</span>
<span class="c"># =&gt; 10.200.1.17 9000</span>

<span class="c"># ipvsadm 툴로 부하분산 되는 정보 확인</span>
<span class="c">## 10.200.1.216(TCP 9000) 인입 시 3곳의 목적지로 라운드로빈(rr)로 부하분산하여 전달됨을 확인 : 모든 노드에서 동일한 IPVS 분산 설정 정보 확인</span>
<span class="c">## 3곳의 목적지는 각각 서비스에 연동된 목적지 파드 3개이며, 전달 시 출발지 IP는 마스커레이딩 변환 처리</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane ipvsadm <span class="nt">-Ln</span> <span class="nt">-t</span> <span class="nv">$CIP</span>:<span class="nv">$CPORT</span>
<span class="c"># =&gt; Prot LocalAddress:Port Scheduler Flags</span>
<span class="c">#      -&amp;gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn</span>
<span class="c">#    TCP  10.200.1.17:9000 rr</span>
<span class="c">#      -&amp;gt; 10.10.1.2:80                 Masq    1      0          0         </span>
<span class="c">#      -&amp;gt; 10.10.2.2:80                 Masq    1      0          0         </span>
<span class="c">#      -&amp;gt; 10.10.3.2:80                 Masq    1      0          0         </span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> ipvsadm <span class="nt">-Ln</span> <span class="nt">-t</span> <span class="nv">$CIP</span>:<span class="nv">$CPORT</span> <span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; &amp;gt;&amp;gt; node myk8s-control-plane &amp;lt;&amp;lt;</span>
<span class="c">#    Prot LocalAddress:Port Scheduler Flags</span>
<span class="c">#      -&amp;gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn</span>
<span class="c">#    TCP  10.200.1.17:9000 rr</span>
<span class="c">#      -&amp;gt; 10.10.1.2:80                 Masq    1      0          0         </span>
<span class="c">#      -&amp;gt; 10.10.2.2:80                 Masq    1      0          0         </span>
<span class="c">#      -&amp;gt; 10.10.3.2:80                 Masq    1      0          0         </span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; node myk8s-worker &amp;lt;&amp;lt;</span>
<span class="c">#    Prot LocalAddress:Port Scheduler Flags</span>
<span class="c">#      -&amp;gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn</span>
<span class="c">#    TCP  10.200.1.17:9000 rr</span>
<span class="c">#      -&amp;gt; 10.10.1.2:80                 Masq    1      0          0         </span>
<span class="c">#      -&amp;gt; 10.10.2.2:80                 Masq    1      0          0         </span>
<span class="c">#      -&amp;gt; 10.10.3.2:80                 Masq    1      0          0         </span>
<span class="c">#    ...</span>

<span class="c"># ipvsadm 툴로 부하분산 되는 현재 연결 정보 확인 : 추가로 --rate 도 있음</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane ipvsadm <span class="nt">-Ln</span> <span class="nt">-t</span> <span class="nv">$CIP</span>:<span class="nv">$CPORT</span> <span class="nt">--stats</span>
<span class="c"># =&gt; Prot LocalAddress:Port               Conns   InPkts  OutPkts  InBytes OutBytes</span>
<span class="c">#      -&amp;gt; RemoteAddress:Port</span>
<span class="c">#    TCP  10.200.1.17:9000                    0        0        0        0        0</span>
<span class="c">#      -&amp;gt; 10.10.1.2:80                        0        0        0        0        0</span>
<span class="c">#      -&amp;gt; 10.10.2.2:80                        0        0        0        0        0</span>
<span class="c">#      -&amp;gt; 10.10.3.2:80                        0        0        0        0        0</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> ipvsadm <span class="nt">-Ln</span> <span class="nt">-t</span> <span class="nv">$CIP</span>:<span class="nv">$CPORT</span> <span class="nt">--stats</span> <span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; &amp;gt;&amp;gt; node myk8s-control-plane &amp;lt;&amp;lt;</span>
<span class="c">#    Prot LocalAddress:Port               Conns   InPkts  OutPkts  InBytes OutBytes</span>
<span class="c">#      -&amp;gt; RemoteAddress:Port</span>
<span class="c">#    TCP  10.200.1.17:9000                    0        0        0        0        0</span>
<span class="c">#      -&amp;gt; 10.10.1.2:80                        0        0        0        0        0</span>
<span class="c">#      -&amp;gt; 10.10.2.2:80                        0        0        0        0        0</span>
<span class="c">#      -&amp;gt; 10.10.3.2:80                        0        0        0        0        0</span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; node myk8s-worker &amp;lt;&amp;lt;</span>
<span class="c">#    Prot LocalAddress:Port               Conns   InPkts  OutPkts  InBytes OutBytes</span>
<span class="c">#      -&amp;gt; RemoteAddress:Port</span>
<span class="c">#    TCP  10.200.1.17:9000                    0        0        0        0        0</span>
<span class="c">#      -&amp;gt; 10.10.1.2:80                        0        0        0        0        0</span>
<span class="c">#      -&amp;gt; 10.10.2.2:80                        0        0        0        0        0</span>
<span class="c">#      -&amp;gt; 10.10.3.2:80                        0        0        0        0        0</span>
<span class="c">#    ...</span>

<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane ipvsadm <span class="nt">-Ln</span> <span class="nt">-t</span> <span class="nv">$CIP</span>:<span class="nv">$CPORT</span> <span class="nt">--rate</span>
<span class="c"># =&gt; Prot LocalAddress:Port                 CPS    InPPS   OutPPS    InBPS   OutBPS</span>
<span class="c">#      -&amp;gt; RemoteAddress:Port</span>
<span class="c">#    TCP  10.200.1.17:9000                    0        0        0        0        0</span>
<span class="c">#      -&amp;gt; 10.10.1.2:80                        0        0        0        0        0</span>
<span class="c">#      -&amp;gt; 10.10.2.2:80                        0        0        0        0        0</span>
<span class="c">#      -&amp;gt; 10.10.3.2:80                        0        0        0        0        0</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> ipvsadm <span class="nt">-Ln</span> <span class="nt">-t</span> <span class="nv">$CIP</span>:<span class="nv">$CPORT</span> <span class="nt">--rate</span> <span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; &amp;gt;&amp;gt; node myk8s-control-plane &amp;lt;&amp;lt;</span>
<span class="c">#    Prot LocalAddress:Port                 CPS    InPPS   OutPPS    InBPS   OutBPS</span>
<span class="c">#      -&amp;gt; RemoteAddress:Port</span>
<span class="c">#    TCP  10.200.1.17:9000                    0        0        0        0        0</span>
<span class="c">#      -&amp;gt; 10.10.1.2:80                        0        0        0        0        0</span>
<span class="c">#      -&amp;gt; 10.10.2.2:80                        0        0        0        0        0</span>
<span class="c">#      -&amp;gt; 10.10.3.2:80                        0        0        0        0        0</span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; node myk8s-worker &amp;lt;&amp;lt;</span>
<span class="c">#    Prot LocalAddress:Port                 CPS    InPPS   OutPPS    InBPS   OutBPS</span>
<span class="c">#      -&amp;gt; RemoteAddress:Port</span>
<span class="c">#    TCP  10.200.1.17:9000                    0        0        0        0        0</span>
<span class="c">#      -&amp;gt; 10.10.1.2:80                        0        0        0        0        0</span>
<span class="c">#      -&amp;gt; 10.10.2.2:80                        0        0        0        0        0</span>
<span class="c">#      -&amp;gt; 10.10.3.2:80                        0        0        0        0        0</span>
<span class="c">#    ...</span>

<span class="c"># iptables 규칙 확인 : ipset list 를 활용</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane iptables <span class="nt">-t</span> nat <span class="nt">-S</span> | <span class="nb">grep </span>KUBE-CLUSTER-IP
<span class="c"># =&gt; -A KUBE-SERVICES ! -s 10.10.0.0/16 -m comment --comment &amp;quot;Kubernetes service cluster ip + port for masquerade purpose&amp;quot; -m set --match-set KUBE-CLUSTER-IP dst,dst -j KUBE-MARK-MASQ</span>
<span class="c">#    -A KUBE-SERVICES -m set --match-set KUBE-CLUSTER-IP dst,dst -j ACCEPT</span>

<span class="c"># ipset list 정보를 확인 : KUBE-CLUSTER-IP 이름은 아래 6개의 IP:Port 조합을 지칭</span>
<span class="c"># 예를 들면 ipset list 를 사용하지 않을 경우 6개의 iptables 규칙이 필요하지만, ipset 사용 시 1개의 규칙으로 가능</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane ipset list KUBE-CLUSTER-IP
<span class="c"># =&gt; Name: KUBE-CLUSTER-IP</span>
<span class="c">#    Type: hash:ip,port</span>
<span class="c">#    Revision: 5</span>
<span class="c">#    Header: family inet hashsize 1024 maxelem 65536</span>
<span class="c">#    Size in memory: 512</span>
<span class="c">#    References: 3</span>
<span class="c">#    Number of entries: 5</span>
<span class="c">#    Members:</span>
<span class="c">#    10.200.1.1,tcp:443</span>
<span class="c">#    10.200.1.10,tcp:9153</span>
<span class="c">#    10.200.1.10,tcp:53</span>
<span class="c">#    10.200.1.17,tcp:9000</span>
<span class="c">#    10.200.1.10,udp:53</span>
</code></pre></div></div>

<h4 id="ipvs-정보-확인-및-서비스-접속-확인">IPVS 정보 확인 및 서비스 접속 확인</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> ipvsadm <span class="nt">-Ln</span> <span class="nt">-t</span> <span class="nv">$CIP</span>:<span class="nv">$CPORT</span> <span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; &amp;gt;&amp;gt; node myk8s-control-plane &amp;lt;&amp;lt;</span>
<span class="c">#    Prot LocalAddress:Port Scheduler Flags</span>
<span class="c">#      -&amp;gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn</span>
<span class="c">#    TCP  10.200.1.17:9000 rr</span>
<span class="c">#      -&amp;gt; 10.10.1.2:80                 Masq    1      0          0         </span>
<span class="c">#      -&amp;gt; 10.10.2.2:80                 Masq    1      0          0         </span>
<span class="c">#      -&amp;gt; 10.10.3.2:80                 Masq    1      0          0         </span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; node myk8s-worker &amp;lt;&amp;lt;</span>
<span class="c">#    Prot LocalAddress:Port Scheduler Flags</span>
<span class="c">#      -&amp;gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn</span>
<span class="c">#    TCP  10.200.1.17:9000 rr</span>
<span class="c">#      -&amp;gt; 10.10.1.2:80                 Masq    1      0          0         </span>
<span class="c">#      -&amp;gt; 10.10.2.2:80                 Masq    1      0          0         </span>
<span class="c">#      -&amp;gt; 10.10.3.2:80                 Masq    1      0          0         </span>
<span class="c">#    ...</span>

<span class="c"># 변수 지정</span>
<span class="nv">$ CIP</span><span class="o">=</span><span class="si">$(</span>kubectl get svc svc-clusterip <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.spec.clusterIP}"</span><span class="si">)</span>
<span class="nv">$ CPORT</span><span class="o">=</span><span class="si">$(</span>kubectl get svc svc-clusterip <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.spec.ports[0].port}"</span><span class="si">)</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$CIP</span> <span class="nv">$CPORT</span>
<span class="c"># =&gt; 10.200.1.17 9000</span>

<span class="c"># 컨트롤플레인 노드에서 ipvsadm 모니터링 실행 : ClusterIP 접속 시 아래 처럼 연결 정보 확인됨</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s2">"docker exec -it myk8s-control-plane ipvsadm -Ln -t </span><span class="nv">$CIP</span><span class="s2">:</span><span class="nv">$CPORT</span><span class="s2"> --stats; echo; docker exec -it myk8s-control-plane ipvsadm -Ln -t </span><span class="nv">$CIP</span><span class="s2">:</span><span class="nv">$CPORT</span><span class="s2"> --rate"</span>

<span class="c"># --------------------------</span>

<span class="c"># 서비스 IP 변수 지정 : svc-clusterip 의 ClusterIP주소</span>
<span class="nv">$ SVC1</span><span class="o">=</span><span class="si">$(</span>kubectl get svc svc-clusterip <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.spec.clusterIP<span class="o">}</span><span class="si">)</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$SVC1</span>
<span class="c"># =&gt; 10.200.1.17</span>

<span class="c"># TCP 80,9000 포트별 접속 확인 : 출력 정보 의미 확인</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> curl <span class="nt">-s</span> <span class="nt">--connect-timeout</span> 1 <span class="nv">$SVC1</span>:9000
<span class="c"># =&gt; Hostname: webpod2</span>
<span class="c">#    IP: 127.0.0.1</span>
<span class="c">#    IP: ::1</span>
<span class="c">#    IP: 10.10.1.2</span>
<span class="c">#    IP: fe80::3009:36ff:fe8f:d5a</span>
<span class="c">#    RemoteAddr: 10.10.0.5:58980</span>
<span class="c">#    GET / HTTP/1.1</span>
<span class="c">#    Host: 10.200.1.17:9000</span>
<span class="c">#    User-Agent: curl/8.7.1</span>
<span class="c">#    Accept: */*</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> curl <span class="nt">-s</span> <span class="nt">--connect-timeout</span> 1 <span class="nv">$SVC1</span>:9000 | <span class="nb">grep </span>Hostname
<span class="c"># =&gt; Hostname: webpod3</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> curl <span class="nt">-s</span> <span class="nt">--connect-timeout</span> 1 <span class="nv">$SVC1</span>:9000 | <span class="nb">grep </span>Hostname
<span class="c"># =&gt; Hostname: webpod1</span>

<span class="c"># 서비스(ClusterIP) 부하분산 접속 확인 : 부하분산 비률 확인</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> zsh <span class="nt">-c</span> <span class="s2">"for i in {1..10};   do curl -s </span><span class="nv">$SVC1</span><span class="s2">:9000 | grep Hostname; done | sort | uniq -c | sort -nr"</span>
<span class="c"># =&gt;       4 Hostname: webpod3</span>
<span class="c">#          3 Hostname: webpod2</span>
<span class="c">#          3 Hostname: webpod1</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> zsh <span class="nt">-c</span> <span class="s2">"for i in {1..100};  do curl -s </span><span class="nv">$SVC1</span><span class="s2">:9000 | grep Hostname; done | sort | uniq -c | sort -nr"</span>
<span class="c"># =&gt;      34 Hostname: webpod1</span>
<span class="c">#         33 Hostname: webpod3</span>
<span class="c">#         33 Hostname: webpod2</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> zsh <span class="nt">-c</span> <span class="s2">"for i in {1..1000}; do curl -s </span><span class="nv">$SVC1</span><span class="s2">:9000 | grep Hostname; done | sort | uniq -c | sort -nr"</span>
<span class="c"># =&gt;     334 Hostname: webpod2</span>
<span class="c">#        333 Hostname: webpod3</span>
<span class="c">#        333 Hostname: webpod1</span>
<span class="c"># 혹은</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> zsh <span class="nt">-c</span> <span class="s2">"for i in {1..100};   do curl -s </span><span class="nv">$SVC1</span><span class="s2">:9000 | grep Hostname; sleep 1; done"</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> zsh <span class="nt">-c</span> <span class="s2">"for i in {1..100};   do curl -s </span><span class="nv">$SVC1</span><span class="s2">:9000 | grep Hostname; sleep 0.1; done"</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> zsh <span class="nt">-c</span> <span class="s2">"for i in {1..10000}; do curl -s </span><span class="nv">$SVC1</span><span class="s2">:9000 | grep Hostname; sleep 0.01; done"</span>

<span class="c"># 반복 접속</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> zsh <span class="nt">-c</span> <span class="s2">"while true; do curl -s --connect-timeout 1 </span><span class="nv">$SVC1</span><span class="s2">:9000 | egrep 'Hostname|RemoteAddr|Host:'; date '+%Y-%m-%d %H:%M:%S' ; echo '--------------' ;  sleep 1; done"</span>
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w5/20241005_kans_w5_20.png" alt="img.png" class="image-center" />
<em class="image-caption">IPVS Proxy 모드 : 부하분산 확인</em></p>

<ul>
  <li>IPVS는 기존의 iptables의 부하분산보다 더 균등하게 부하분산을 수행함을 확인 할 수 있었습니다.</li>
</ul>

<hr />

<h2 id="마치며">마치며</h2>

<p>이번 주에는 LoadBalancer, LoadBalancer를 온프레미스에서 사용하기 위한 MetalLB, ClusterIP, IPVS Proxy 모드에 대해 알아보았습니다.
온프레미스 K8S에서 서비스 유형을 LoadBalancer로 했을때 ExternalIP가 할당 되지 않은 이유를 이제야 알았습니다. 
단순히 쓰기만 해왔던 기술의 원리와 이유를 알게되니 뿌듯합니다. 
아직 알아야 할 것이 산더미이고 지금 이순간에도 새로운 기술들이 개발된다니 또다시 첩첩산중이라는것을 느낍니다.</p>

<p>IPVS는 아직 모르는 부분이 많지만, 실무에 적용해보고 싶은 기술입니다. 네트워크 부하때문에 CPU가 높아지는 경우가 많은데, 
이를 해결할 수 있는 방법인것 같아 유용할것 같습니다.</p>

<p>정말 매운맛의 스터디이지만 많은 것을 배우고 있습니다.
다음 주에는 드디어 기다리던 GatewayAPI를 스터디 합니다. 기대가 됩니다. :)</p>]]></content><author><name></name></author><category term="kans" /><category term="kubernetes," /><category term="network," /><category term="linux" /><summary type="html"><![CDATA[이번 주에는 LoadBalancer 서비스와 MetalLB, 그리고 kube-proxy의 모드중 하나인 IPVS에 대해 알아보겠습니다.]]></summary></entry><entry><title type="html">[KANS 3기] K8S Service : ClusterIP, NodePort</title><link href="https://sweetlittlebird.github.io/posts/2024-09-27-KANS-Study-Week4/" rel="alternate" type="text/html" title="[KANS 3기] K8S Service : ClusterIP, NodePort" /><published>2024-09-27T01:00:18+09:00</published><updated>2024-09-27T01:00:18+09:00</updated><id>https://sweetlittlebird.github.io/posts/KANS%20Study%20-%20Week4</id><content type="html" xml:base="https://sweetlittlebird.github.io/posts/2024-09-27-KANS-Study-Week4/"><![CDATA[<h2 id="들어가며">들어가며</h2>

<p>지난주에 이어 이번주에는 Kubernetes의 Service, 그 중에 ClusterIP, NodePort에 대해 알아보겠습니다.
KANS 3기 4주차 스터디를 시작하겠습니다.</p>

<hr />

<h2 id="k8s-service">K8S Service</h2>

<p>Kubernetes의 Service는 개별 Pod에 접근하기 위한 추상화된 방법을 제공합니다.
Pod는 생성될 때마다 IP가 동적으로 할당되기 때문에 Pod의 IP를 직접 사용하는 것은 좋은 방법이 아닙니다.
Service는 Pod의 IP를 추상화하여 Pod에 접근할 수 있도록 해줍니다.</p>

<h3 id="service의-탄생-배경">Service의 탄생 배경</h3>

<p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_4.png" alt="img.png" /></p>

<p>위의 그림과 같이 하나의 파드의 엔드포인트를 다른 파드 (또는 외부)에서 사용할때, 해당 파드의 IP로 지정을 하면, 파드가 재실행 될 때 IP가 변경되어 접속이 안 되서 장애가 발생하는 현상이 생깁니다.</p>

<p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_5.png" alt="img.png" /></p>

<p>그래서 고정된 IP의 서비스를 만들고 서비스의 IP로 접속시 파드가 재실행되어도 안정적으로 접속할 수 있도록 하기위해서 만들어졌습니다.</p>

<p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_6.png" alt="img.png" /></p>

<p>서비스는 또한 부하분산의 기능도 할 수 있습니다. 위의 그림과 같이 파드가 여러개일때 서비스 IP로 접속시 각 파드들에 부하를 분산시킬 수 있게 됩니다.</p>

<h3 id="k8s-service-종류">K8S Service 종류</h3>

<h4 id="clusterip">ClusterIP</h4>

<p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_1.png" alt="img.png" class="w-80 image-center" /></p>

<ul>
  <li>동일한 애플리케이션을 실행하는 여러 Pod에 접속을 용이하기 위해 사용합니다.</li>
  <li>ClusterIP는 Cluster 내부에서만 접근이 가능하며 외부에서는 접근이 불가능합니다.</li>
  <li>iptables 의 NAT 기능을 이용하여 Pod에 접근하며, 동일한 iptables 분산룰을 각 노드에 적용합니다.</li>
</ul>

<h4 id="nodeport">NodePort</h4>

<p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_2.png" alt="img.png" class="w-80 image-center" /></p>

<ul>
  <li>NodePort는 ClusterIP와 같이 Cluster 내부에서 접근이 가능하며, 외부에서도 접근이 가능합니다.</li>
  <li>NodePort도 ClusterIP와 같이 iptables의 NAT 기능을 이용하여 Pod에 접근하며, 각 노드에 NodePort를 할당합니다.</li>
  <li>외부에서는 NodePort를 통해 각 노드에 접근 할 수 있습니다.</li>
</ul>

<h4 id="loadbalancer">LoadBalancer</h4>

<p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_3.png" alt="img.png" class="w-80 image-center" /></p>

<ul>
  <li>LoadBalancer도 외부에서 접근이 가능하며, 클라우드 서비스에서 제공하는 LoadBalancer를 사용합니다. (AWS의 경우 ELB(Elastic Load Balancer)가 사용됩니다.)</li>
  <li>온프레미스 환경에서도 MetalLB와 같은 LoadBalancer를 사용할 수 있습니다.</li>
</ul>

<h3 id="서비스의-구조">서비스의 구조</h3>

<p>서비스를 선언시 <code class="language-plaintext highlighter-rouge">port</code>와 <code class="language-plaintext highlighter-rouge">targetPort</code>, 그리고 <code class="language-plaintext highlighter-rouge">label</code> <code class="language-plaintext highlighter-rouge">selector</code> 를 사용합니다. 각각의 역할은 다음과 같습니다.</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">port</code> : 서비스가 listen 할 포트를 지정합니다.</li>
  <li><code class="language-plaintext highlighter-rouge">targetPort</code> : 대상 파드의 port를 지정합니다.</li>
  <li><code class="language-plaintext highlighter-rouge">label selector</code>  : 대상 파드를 특정합니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_7.png" alt="img.png" /></p>

<h3 id="kube-proxy-모드">kube-proxy 모드</h3>

<ul>
  <li>kube-proxy는 서비스 통신 동작에 대한 설정을 관리합니다. 데몬셋으로 배포되어 모든 노드에 파드가 생성됩니다.</li>
  <li>kube-proxy 모드의 종류는 userspace proxy 모드, iptables proxy 모드, ipvs proxy 모드, nftables proxy 모드 등이 있습니다.</li>
</ul>

<h4 id="userspace-proxy-모드">userspace proxy 모드</h4>

<p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_8.png" alt="img.png" /></p>

<ul>
  <li>기초적인 모드이며 사용자 영역의 kube-proxy를 통해 NIC1으로 들어온 패킷을 NIC2로 전달하여 목적 파드로 전달합니다.</li>
  <li>이렇게 하는 과정에서 커널영역(netfilter)과 사용자영역(kube-proxy)를 오가는 과정에서 스위칭에 의한 오버헤드가 발생하는 단점이 있습니다.</li>
</ul>

<h4 id="iptables-proxy-모드">iptables proxy 모드</h4>

<p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_9.png" alt="img.png" /></p>

<ul>
  <li>쿠버네티스 설치시 기본 모드이며, kube-proxy는  트래픽 전달에 직접 관여하지는 않고, iptables 규칙을 관리하는 역할을 합니다.</li>
  <li>iptables proxy 모드는 트래픽 전달 과정에서 kube-proxy를 경유하지 않고, 커널 영역과 사용자 영역 전환이 필요하지 않아서, 유저스페이스 proxy 모드에 비해 오버헤드가 줄어듭니다.</li>
  <li>단점으로는 iptables 규칙이 많아 질 경우 모든 규칙 평가 하는데 지연이 발생할 수 있습니다.</li>
  <li>또한 장애시 모든 규칙을 확인하기 어려워 장애 처리에 불리합니다.</li>
</ul>

<h4 id="ipvs-proxy-모드">ipvs proxy 모드</h4>

<p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_10.png" alt="img.png" /></p>

<ul>
  <li>ipvs proxy 모드는 지금까지의 모드 중 가장 효율적인 모드입니다. IPVS(IP Virtual Server)는 넷필터에서 동작하는 Layer 4 로드밸런서입니다. iptables 보다 더 높은 성능 처리를 보여주고, 규칙 갯수를 줄일 수 있습니다. 또한 다양한 부하분산 알고리즘을 제공합니다.</li>
</ul>

<h4 id="nftables-proxy-모드">nftables proxy 모드</h4>
<ul>
  <li>nftables 는 iptables를 대체하기 위해 개발된 패킷 필터링 프레임워크로, iptables 보다 더 유연하고 강력한 규칙 설정을 제공합니다.</li>
  <li>하지만 아직  실험적으로 개발중인 단계로 실무에서는 ipvs proxy 모드를 권장합니다.</li>
</ul>

<h4 id="ebpf-모드--xdp">eBPF 모드 + XDP</h4>

<p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_11.png" alt="img.png" class="w-90 image-center" /></p>

<ul>
  <li>앞에서 알아보았던 모든 모드들이 netfilter 기반인데 반해, eBPF 모드 +  XDP 는 netfilter 전 단계에서 트래픽 라우팅을 처리하여 훨씬 효율 적입니다. calico나 cilium을 사용하여서 eBPF 모드를 사용할 수 있습니다.</li>
</ul>

<h3 id="실습">실습</h3>

<h4 id="실습환경-구축">실습환경 구축</h4>

<ul>
  <li>이번 실습은 실습환경 구축의 용이성을 위해서 kind를 이용하여 실습하였습니다.</li>
  <li>실습 환경 구축은 다음과 같이 진행 하였습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># kind 클러스터 정의 파일 생성</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOT</span><span class="sh">&gt; kind-svc-w3.yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
featureGates:
  "InPlacePodVerticalScaling": true
  "MultiCIDRServiceAllocator": true
nodes:
- role: control-plane
  labels:
    mynode: control-plane
  extraPortMappings:
  - containerPort: 30000
    hostPort: 30000
  - containerPort: 30001
    hostPort: 30001
  - containerPort: 30002
    hostPort: 30002
  kubeadmConfigPatches:
  - |
    kind: ClusterConfiguration
    apiServer:
      extraArgs:
        runtime-config: api/all=true
- role: worker
  labels:
    mynode: worker1
- role: worker
  labels:
    mynode: worker2
- role: worker
  labels:
    mynode: worker3
networking:
  podSubnet: 10.10.0.0/16
  serviceSubnet: 10.200.1.0/24
</span><span class="no">EOT

</span><span class="c"># k8s 클러스터 설치</span>
<span class="nv">$ </span>kind create cluster <span class="nt">--config</span> kind-svc-w3.yaml <span class="nt">--name</span> myk8s <span class="nt">--image</span> kindest/node:v1.31.0
<span class="c"># =&gt; Creating cluster "myk8s" ...</span>
<span class="c">#     ✓ Ensuring node image (kindest/node:v1.31.0) 🖼</span>
<span class="c">#     ✓ Preparing nodes 📦 📦 📦 📦</span>
<span class="c">#     ✓ Writing configuration 📜</span>
<span class="c">#     ✓ Starting control-plane 🕹️</span>
<span class="c">#     ✓ Installing CNI 🔌</span>
<span class="c">#     ✓ Installing StorageClass 💾</span>
<span class="c">#     ✓ Joining worker nodes 🚜</span>
<span class="c">#    Set kubectl context to "kind-myk8s"</span>
<span class="c">#    You can now use your cluster with:</span>
<span class="c">#    </span>
<span class="c">#    kubectl cluster-info --context kind-myk8s</span>

<span class="nv">$ </span>docker ps
<span class="c"># =&gt; CONTAINER ID   IMAGE                                COMMAND                  CREATED          STATUS                 PORTS                                                            NAMES</span>
<span class="c">#    1b7e6b646e48   kindest/node:v1.31.0                 "/usr/local/bin/entr…"   18 minutes ago   Up 18 minutes                                                                           myk8s-worker</span>
<span class="c">#    5406c013a571   kindest/node:v1.31.0                 "/usr/local/bin/entr…"   18 minutes ago   Up 18 minutes          0.0.0.0:30000-30002-&gt;30000-30002/tcp, 127.0.0.1:43315-&gt;6443/tcp  myk8s-control-plane</span>
<span class="c">#    4134657c5a70   kindest/node:v1.31.0                 "/usr/local/bin/entr…"   18 minutes ago   Up 18 minutes                                                                           myk8s-worker3</span>
<span class="c">#    6caf2b177502   kindest/node:v1.31.0                 "/usr/local/bin/entr…"   18 minutes ago   Up 18 minutes                                                                           myk8s-worker2</span>

<span class="c"># 노드에 기본 툴 설치</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane sh <span class="nt">-c</span> <span class="s1">'apt update &amp;&amp; apt install tree psmisc lsof wget bridge-utils net-tools ipset ipvsadm nfacct tcpdump ngrep iputils-ping arping git vim arp-scan -y'</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> sh <span class="nt">-c</span> <span class="s1">'apt update &amp;&amp; apt install tree psmisc lsof wget bridge-utils net-tools ipset ipvsadm nfacct tcpdump ngrep iputils-ping arping -y'</span><span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>

<span class="c"># k8s v1.31.0 버전 확인</span>
<span class="nv">$ </span>kubectl get node
<span class="c"># =&gt; NAME                  STATUS   ROLES           AGE   VERSION</span>
<span class="c">#    myk8s-control-plane   Ready    control-plane   40m   v1.31.0</span>
<span class="c">#    myk8s-worker          Ready    &lt;none&gt;          40m   v1.31.0</span>
<span class="c">#    myk8s-worker2         Ready    &lt;none&gt;          40m   v1.31.0</span>
<span class="c">#    myk8s-worker3         Ready    &lt;none&gt;          40m   v1.31.0</span>

<span class="c"># 노드 labels 확인</span>
<span class="nv">$ </span>kubectl get nodes <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.items[*].metadata.labels}"</span> | <span class="nb">grep </span>mynode
<span class="nv">$ </span>kubectl get nodes <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.items[*].metadata.labels}"</span> | jq | <span class="nb">grep </span>mynode
<span class="c"># =&gt;   "mynode": "control-plane",</span>
<span class="c">#      "mynode": "worker1"</span>
<span class="c">#      "mynode": "worker2"</span>
<span class="c">#      "mynode": "worker3"</span>

<span class="c"># kind network 중 컨테이너(노드) IP(대역) 확인 : 172.18.0.2~ 부터 할당되며, control-plane 이 꼭 172.18.0.2가 안될 수 도 있음</span>
<span class="nv">$ </span>docker ps <span class="nt">-q</span> | xargs docker inspect <span class="nt">--format</span> <span class="s1">' '</span>
<span class="c"># =&gt; /myk8s-control-plane 172.23.0.2</span>
<span class="c">#    /myk8s-worker 172.23.0.4</span>
<span class="c">#    /myk8s-worker2 172.23.0.5</span>
<span class="c">#    /myk8s-worker3 172.23.0.3</span>
    
<span class="c"># 파드CIDR 과 Service 대역 확인 : CNI는 kindnet 사용</span>
<span class="nv">$ </span>kubectl get cm <span class="nt">-n</span> kube-system kubeadm-config <span class="nt">-oyaml</span> | <span class="nb">grep</span> <span class="nt">-i</span> subnet
<span class="c"># =&gt;       podSubnet: 10.10.0.0/16</span>
<span class="c">#          serviceSubnet: 10.200.1.0/24</span>
<span class="nv">$ </span>kubectl cluster-info dump | <span class="nb">grep</span> <span class="nt">-m</span> 2 <span class="nt">-E</span> <span class="s2">"cluster-cidr|service-cluster-ip-range"</span>
<span class="c"># =&gt;                             "--service-cluster-ip-range=10.200.1.0/24",</span>
<span class="c">#                                "--cluster-cidr=10.10.0.0/16",</span>

<span class="c"># feature-gates 확인 : https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/</span>
<span class="nv">$ </span>kubectl describe pod <span class="nt">-n</span> kube-system | <span class="nb">grep </span>feature-gates
<span class="c"># =&gt;       --feature-gates=InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true</span>
<span class="nv">$ </span>kubectl describe pod <span class="nt">-n</span> kube-system | <span class="nb">grep </span>runtime-config
<span class="c"># =&gt;       --runtime-config=api/all=true</span>

<span class="c"># MultiCIDRServiceAllocator : https://kubernetes.io/docs/tasks/network/extend-service-ip-ranges/</span>
<span class="nv">$ </span>kubectl get servicecidr
<span class="c"># =&gt; NAME         CIDRS           AGE</span>
<span class="c">#    kubernetes   10.200.1.0/24   62m</span>

<span class="c"># 노드마다 할당된 dedicated subnet (podCIDR) 확인</span>
<span class="nv">$ </span>kubectl get nodes <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.items[*].spec.podCIDR}"</span>
<span class="c"># =&gt; 10.10.0.0/24 10.10.4.0/24 10.10.1.0/24 10.10.2.0/24</span>

<span class="c"># kube-proxy configmap 확인</span>
<span class="nv">$ </span>kubectl describe cm <span class="nt">-n</span> kube-system kube-proxy
<span class="c"># =&gt; ...</span>
<span class="c">#    iptables:</span>
<span class="c">#      localhostNodePorts: null</span>
<span class="c">#      masqueradeAll: false</span>
<span class="c">#      masqueradeBit: null</span>
<span class="c">#      minSyncPeriod: 1s</span>
<span class="c">#      syncPeriod: 0s</span>
<span class="c">#    mode: iptables</span>
<span class="c">#    ...</span>

<span class="c"># kube-proxy가 iptables 모드로 동작중임을 확인할 수 있습니다.</span>

<span class="c"># 노드 별 네트워트 정보 확인 : CNI는 kindnet 사용</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> <span class="nb">ls</span> /opt/cni/bin/<span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> <span class="nb">cat</span> /etc/cni/net.d/10-kindnet.conflist<span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> ip <span class="nt">-c</span> route<span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> ip <span class="nt">-c</span> addr<span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> ip <span class="nt">-c</span> <span class="nt">-4</span> addr show dev eth0<span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>

<span class="c"># iptables 정보 확인</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>filter nat mangle raw <span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; IPTables Type : </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane  iptables <span class="nt">-t</span> <span class="nv">$i</span> <span class="nt">-S</span> <span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>filter nat mangle raw <span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; IPTables Type : </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker  iptables <span class="nt">-t</span> <span class="nv">$i</span> <span class="nt">-S</span> <span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>filter nat mangle raw <span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; IPTables Type : </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker2 iptables <span class="nt">-t</span> <span class="nv">$i</span> <span class="nt">-S</span> <span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>filter nat mangle raw <span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; IPTables Type : </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker3 iptables <span class="nt">-t</span> <span class="nv">$i</span> <span class="nt">-S</span> <span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>

<span class="c"># 각 노드 bash 접속</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane bash
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker bash
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker2 bash
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker3 bash
<span class="nt">----------------------------------------</span>
<span class="nv">$ </span><span class="nb">exit</span>
<span class="nt">----------------------------------------</span>

<span class="c"># kind 설치 시 kind 이름의 도커 브리지가 생성됩니다. : 172.18.0.0/16 대역</span>
<span class="nv">$ </span>docker network <span class="nb">ls</span>
<span class="c"># =&gt; NETWORK ID     NAME                           DRIVER    SCOPE</span>
<span class="c">#    ...</span>
<span class="c">#    1c5d73657215   kind                           bridge    local</span>

<span class="nv">$ </span>docker inspect kind
<span class="c"># =&gt; [</span>
<span class="c">#        {</span>
<span class="c">#            "Name": "kind",</span>
<span class="c">#            ...</span>
<span class="c">#            "IPAM": {</span>
<span class="c">#                ...</span>
<span class="c">#                "Config": [</span>
<span class="c">#                    {</span>
<span class="c">#                        "Subnet": "172.23.0.0/16",</span>
<span class="c">#                        "Gateway": "172.23.0.1"</span>
<span class="c">#                    }</span>
<span class="c">#                ]</span>
<span class="c">#            },</span>
<span class="c">#            ...</span>
<span class="c">#            "Containers": {</span>
<span class="c">#                "1b7e6b646e4867591b5dd2a3bb4fcd2223dfcfd36dc08d86c8efc8fdc2112462": {</span>
<span class="c">#                    "Name": "myk8s-worker",</span>
<span class="c">#                    "IPv4Address": "172.23.0.4/16",</span>
<span class="c">#                },</span>
<span class="c">#                "4134657c5a7049d20944c2f80d3a3183a91a70107a47be72888e5c5fa972312a": {</span>
<span class="c">#                    "Name": "myk8s-worker3",</span>
<span class="c">#                    "IPv4Address": "172.23.0.3/16",</span>
<span class="c">#                },</span>
<span class="c">#                "5406c013a57167caf9a94ee9e89e550899a6efed9386f35548f03d2f670e8196": {</span>
<span class="c">#                    "Name": "myk8s-control-plane",</span>
<span class="c">#                    "IPv4Address": "172.23.0.2/16",</span>
<span class="c">#                },</span>
<span class="c">#                "6caf2b177502b92eccd4353ae3f4b3ac2da2949fc840225a02c9e83e1d24b09a": {</span>
<span class="c">#                    "Name": "myk8s-worker2",</span>
<span class="c">#                    "IPv4Address": "172.23.0.5/16",</span>
<span class="c">#                }</span>
<span class="c">#            },</span>
<span class="c">#            ...</span>
<span class="c">#        }</span>
<span class="c">#    ]</span>

<span class="c"># arp scan 해두기</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane arp-scan <span class="nt">--interfac</span><span class="o">=</span>eth0 <span class="nt">--localnet</span>
<span class="c"># =&gt; Interface: eth0, type: EN10MB, MAC: 02:42:ac:17:00:02, IPv4: 172.23.0.2</span>
<span class="c">#    Starting arp-scan 1.10.0 with 65536 hosts (https://github.com/royhills/arp-scan)</span>
<span class="c">#    172.23.0.1	02:42:a4:3f:b3:d9	(Unknown: locally administered)</span>
<span class="c">#    172.23.0.3	02:42:ac:17:00:03	(Unknown: locally administered)</span>
<span class="c">#    172.23.0.4	02:42:ac:17:00:04	(Unknown: locally administered)</span>
<span class="c">#    172.23.0.5	02:42:ac:17:00:05	(Unknown: locally administered)</span>

<span class="c"># mypc 컨테이너 기동 : kind 도커 브리지를 사용하고, 컨테이너 IP를 직접 지정</span>
<span class="nv">$ </span>docker run <span class="nt">-d</span> <span class="nt">--rm</span> <span class="nt">--name</span> mypc <span class="nt">--network</span> kind <span class="nt">--ip</span> 172.23.0.100 nicolaka/netshoot <span class="nb">sleep </span>infinity
<span class="nv">$ </span>docker ps
<span class="c">## 만약 kind 네트워크 대역이 다를 경우 위 IP 지정이 실패할 수 있으니, 그냥 IP 지정 없이 mypc 컨테이너 기동 할 것</span>
<span class="c">## docker run -d --rm --name mypc --network kind nicolaka/netshoot sleep infinity</span>

<span class="c"># 통신 확인</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc ping <span class="nt">-c</span> 1 172.23.0.1
<span class="c"># =&gt; PING 172.23.0.1 (172.23.0.1) 56(84) bytes of data.</span>
<span class="c">#    64 bytes from 172.23.0.1: icmp_seq=1 ttl=64 time=0.154 ms</span>
<span class="c">#    </span>
<span class="c">#    --- 172.23.0.1 ping statistics ---</span>
<span class="c">#    1 packets transmitted, 1 received, 0% packet loss, time 0ms</span>
<span class="c">#    rtt min/avg/max/mdev = 0.154/0.154/0.154/0.000 ms</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..5<span class="o">}</span> <span class="p">;</span> <span class="k">do </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc ping <span class="nt">-c</span> 1 172.23.0.<span class="nv">$i</span><span class="p">;</span> <span class="k">done</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc zsh
<span class="nt">-------------</span>
<span class="nv">$ </span>ifconfig
<span class="c"># =&gt; eth0      Link encap:Ethernet  HWaddr 02:42:AC:17:00:06  </span>
<span class="c">#              inet addr:172.23.0.6  Bcast:172.23.255.255  Mask:255.255.0.0</span>
<span class="c">#    ...</span>
<span class="nv">$ </span>ping <span class="nt">-c</span> 1 172.23.0.2
<span class="c"># =&gt; PING 172.23.0.2 (172.23.0.2) 56(84) bytes of data.</span>
<span class="c">#    64 bytes from 172.23.0.2: icmp_seq=1 ttl=64 time=0.258 ms</span>
<span class="c">#    </span>
<span class="c">#    --- 172.23.0.2 ping statistics ---</span>
<span class="c">#    1 packets transmitted, 1 received, 0% packet loss, time 0ms</span>
<span class="c">#    rtt min/avg/max/mdev = 0.258/0.258/0.258/0.000 ms</span>
<span class="nv">$ </span><span class="nb">exit</span>
<span class="nt">-------------</span>

<span class="c"># kube-ops-view 설치</span>
<span class="nv">$ </span>helm repo add geek-cookbook https://geek-cookbook.github.io/charts/
<span class="c"># =&gt; "geek-cookbook" has been added to your repositories</span>
<span class="nv">$ </span>helm <span class="nb">install </span>kube-ops-view geek-cookbook/kube-ops-view <span class="nt">--version</span> 1.2.2 <span class="nt">--set</span> service.main.type<span class="o">=</span>NodePort,service.main.ports.http.nodePort<span class="o">=</span>30000 <span class="nt">--set</span> env.TZ<span class="o">=</span><span class="s2">"Asia/Seoul"</span> <span class="nt">--namespace</span> kube-system
<span class="c"># =&gt; NAME: kube-ops-view</span>
<span class="c">#    ...</span>
<span class="c">#    1. Get the application URL by running these commands:</span>
<span class="c">#      export NODE_PORT=$(kubectl get --namespace kube-system -o jsonpath="{.spec.ports[0].nodePort}" services kube-ops-view)</span>
<span class="c">#      export NODE_IP=$(kubectl get nodes --namespace kube-system -o jsonpath="{.items[0].status.addresses[0].address}")</span>
<span class="c">#      echo http://$NODE_IP:$NODE_PORT</span>

<span class="c"># myk8s-control-plane 배치</span>
<span class="nv">$ </span>kubectl <span class="nt">-n</span> kube-system edit deploy kube-ops-view
<span class="nt">---</span>
spec:
  ...
  template:
    ...
    spec:
      nodeSelector:
        mynode: control-plane
      tolerations:
      - key: <span class="s2">"node-role.kubernetes.io/control-plane"</span>
        operator: <span class="s2">"Equal"</span>
        effect: <span class="s2">"NoSchedule"</span>
<span class="nt">---</span>

<span class="c"># 설치 확인</span>
<span class="nv">$ </span>kubectl <span class="nt">-n</span> kube-system get pod <span class="nt">-o</span> wide <span class="nt">-l</span> app.kubernetes.io/instance<span class="o">=</span>kube-ops-view
<span class="c"># =&gt; NAME                             READY   STATUS    RESTARTS   AGE   IP          NODE                  NOMINATED NODE   READINESS GATES</span>
<span class="c">#    kube-ops-view-58f96c464d-t5t68   1/1     Running   0          30s   10.10.0.5   myk8s-control-plane   &lt;none&gt;           &lt;none&gt;</span>

<span class="c"># kube-ops-view 접속 URL 확인 (1.5 , 2 배율) : macOS 사용자</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"KUBE-OPS-VIEW URL = http://localhost:30000/#scale=1.5"</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"KUBE-OPS-VIEW URL = http://localhost:30000/#scale=2"</span>

<span class="c"># kube-ops-view 접속 URL 확인 (1.5 , 2 배율) : Windows 사용자</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"KUBE-OPS-VIEW URL = http://192.168.50.10:30000/#scale=1.5"</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"KUBE-OPS-VIEW URL = http://192.168.50.10:30000/#scale=2"</span>

</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_12.png" alt="img.png" /></p>

<h4 id="clusterip-실습">ClusterIP 실습</h4>

<ul>
  <li>앞에서 알아본 ClusterIP 타입에 대해 실습해 보겠습니다.</li>
  <li>다음의 사항들을 살펴볼 것입니다.
    <ul>
      <li>ClusterIP의 서비스의 경우 클러스터 내부에서만 접근이 가능한 특성이 있습니다.</li>
      <li>IP로도 접속할 수 있지만 도메인 명으로도 접속이 가능합니다.</li>
      <li>서비스 타입(ClusterIP)을 생성하면 apiserver ⇒ (kubelet) ⇒ kube-proxy ⇒ iptables 에 rule 이 생성 됩니다.</li>
      <li>모든 노드(컨트롤 플레인 포함) 에 iptables rule이 설정 되므로, 파드에서 접속 시 해당 노드에 존재하는 iptables rule 에 의해 분산 접속됩니다.</li>
    </ul>
  </li>
  <li>실습 구성
    <ul>
      <li>
        <p>목적지(backend) 파드 (pod) 생성 : 3pod.yml</p>

        <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c1"># 3pod.yml</span>
  <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
  <span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
  <span class="na">metadata</span><span class="pi">:</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">webpod1</span>
    <span class="na">labels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">webpod</span>
  <span class="na">spec</span><span class="pi">:</span>
    <span class="na">nodeName</span><span class="pi">:</span> <span class="s">myk8s-worker</span>
    <span class="na">containers</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">container</span>
      <span class="na">image</span><span class="pi">:</span> <span class="s">traefik/whoami</span>
    <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
  <span class="s">---</span>
  <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
  <span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
  <span class="na">metadata</span><span class="pi">:</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">webpod2</span>
    <span class="na">labels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">webpod</span>
  <span class="na">spec</span><span class="pi">:</span>
    <span class="na">nodeName</span><span class="pi">:</span> <span class="s">myk8s-worker2</span>
    <span class="na">containers</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">container</span>
      <span class="na">image</span><span class="pi">:</span> <span class="s">traefik/whoami</span>
    <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
  <span class="s">---</span>
  <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
  <span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
  <span class="na">metadata</span><span class="pi">:</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">webpod3</span>
    <span class="na">labels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">webpod</span>
  <span class="na">spec</span><span class="pi">:</span>
    <span class="na">nodeName</span><span class="pi">:</span> <span class="s">myk8s-worker3</span>
    <span class="na">containers</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">container</span>
      <span class="na">image</span><span class="pi">:</span> <span class="s">traefik/whoami</span>
    <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>클라이언트 생성 : netpod.yml</p>

        <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c1"># netpod.yml</span>
  <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
  <span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
  <span class="na">metadata</span><span class="pi">:</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">net-pod</span>
  <span class="na">spec</span><span class="pi">:</span>
    <span class="na">nodeName</span><span class="pi">:</span> <span class="s">myk8s-control-plane</span>
    <span class="na">containers</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">netshoot-pod</span>
      <span class="na">image</span><span class="pi">:</span> <span class="s">nicolaka/netshoot</span>
      <span class="na">command</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">tail"</span><span class="pi">]</span>
      <span class="na">args</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">-f"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">/dev/null"</span><span class="pi">]</span>
    <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>서비스(ClusterIP) 생성 : svc-clusterip.yml</p>

        <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c1"># svc-clusterip.yml</span>
  <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
  <span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
  <span class="na">metadata</span><span class="pi">:</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">svc-clusterip</span>
  <span class="na">spec</span><span class="pi">:</span>
    <span class="na">ports</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">svc-webport</span>
        <span class="na">port</span><span class="pi">:</span> <span class="m">9000</span>        <span class="c1"># 서비스 IP 에 접속 시 사용하는 포트 port 를 의미</span>
        <span class="na">targetPort</span><span class="pi">:</span> <span class="m">80</span>    <span class="c1"># 타킷 targetPort 는 서비스를 통해서 목적지 파드로 접속 시 해당 파드로 접속하는 포트를 의미</span>
    <span class="na">selector</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">webpod</span>         <span class="c1"># 셀렉터 아래 app:webpod 레이블이 설정되어 있는 파드들은 해당 서비스에 연동됨</span>
    <span class="na">type</span><span class="pi">:</span> <span class="s">ClusterIP</span>       <span class="c1"># 서비스 타입</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>생성 및 확인</p>

        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c"># 모니터링</span>
  <span class="nv">$ </span><span class="k">**</span>watch <span class="nt">-d</span> <span class="s1">'kubectl get pod -owide ;echo; kubectl get svc,ep svc-clusterip'</span><span class="k">**</span>
      
  <span class="c"># 생성</span>
  <span class="nv">$ </span>kubectl apply <span class="nt">-f</span> 3pod.yml,netpod.yml,svc-clusterip.yml
  <span class="c"># =&gt; pod/webpod1 created</span>
  <span class="c">#    pod/webpod2 created</span>
  <span class="c">#    pod/webpod3 created</span>
  <span class="c">#    pod/net-pod created</span>
  <span class="c">#    service/svc-clusterip created</span>
      
  <span class="c"># 파드와 서비스 사용 네트워크 대역 정보 확인 </span>
  <span class="nv">$ </span>kubectl cluster-info dump | <span class="nb">grep</span> <span class="nt">-m</span> 2 <span class="nt">-E</span> <span class="s2">"cluster-cidr|service-cluster-ip-range"</span>
  <span class="c"># =&gt; "--service-cluster-ip-range=10.200.1.0/24",</span>
  <span class="c">#    "--cluster-cidr=10.10.0.0/16",</span>
      
  <span class="c"># 확인</span>
  <span class="nv">$ </span>kubectl get pod <span class="nt">-owide</span>
  <span class="c"># =&gt; NAME      READY   STATUS    RESTARTS   AGE    IP          NODE                  NOMINATED NODE   READINESS GATES</span>
  <span class="c">#    net-pod   1/1     Running   0          2m8s   10.10.0.7   myk8s-control-plane   &lt;none&gt;           &lt;none&gt;</span>
  <span class="c">#    webpod1   1/1     Running   0          2m8s   10.10.4.3   myk8s-worker          &lt;none&gt;           &lt;none&gt;</span>
  <span class="c">#    webpod2   1/1     Running   0          2m8s   10.10.1.4   myk8s-worker2         &lt;none&gt;           &lt;none&gt;</span>
  <span class="c">#    webpod3   1/1     Running   0          2m8s   10.10.2.3   myk8s-worker3         &lt;none&gt;           &lt;none&gt;</span>
  <span class="nv">$ </span>kubectl get svc svc-clusterip
  <span class="c"># =&gt; NAME            TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)    AGE</span>
  <span class="c">#    svc-clusterip   ClusterIP   10.200.1.96   &lt;none&gt;        9000/TCP   2m15s</span>
      
  <span class="c"># spec.ports.port 와 spec.ports.targetPort 가 어떤 의미인지 꼭 이해하자!</span>
  <span class="nv">$ </span>kubectl describe svc svc-clusterip
  <span class="c"># =&gt; Name:              svc-clusterip</span>
  <span class="c">#    Namespace:         default</span>
  <span class="c">#    Labels:            &lt;none&gt;</span>
  <span class="c">#    Annotations:       &lt;none&gt;</span>
  <span class="c">#    Selector:          app=webpod</span>
  <span class="c">#    Type:              ClusterIP</span>
  <span class="c">#    IP Family Policy:  SingleStack</span>
  <span class="c">#    IP Families:       IPv4</span>
  <span class="c">#    IP:                10.200.1.96</span>
  <span class="c">#    IPs:               10.200.1.96</span>
  <span class="c">#    Port:              svc-webport  9000/TCP                    # service의 listening port</span>
  <span class="c">#    TargetPort:        80/TCP                                   # pod의 실제 port</span>
  <span class="c">#    Endpoints:         10.10.1.4:80,10.10.2.3:80,10.10.4.3:80   # pod의 ip:port 목록</span>
  <span class="c">#    Session Affinity:  None</span>
  <span class="c">#    Events:            &lt;none&gt;</span>
      
  <span class="c"># 서비스 생성 시 엔드포인트를 자동으로 생성, 물론 수동으로 설정 생성도 가능</span>
  <span class="nv">$ </span>kubectl get endpoints svc-clusterip
  <span class="c"># =&gt; NAME            ENDPOINTS                                AGE</span>
  <span class="c">#    svc-clusterip   10.10.1.4:80,10.10.2.3:80,10.10.4.3:80   3m32s</span>
  <span class="nv">$ </span>kubectl get endpointslices <span class="nt">-l</span> kubernetes.io/service-name<span class="o">=</span>svc-clusterip
  <span class="c"># =&gt; NAME                  ADDRESSTYPE   PORTS   ENDPOINTS                       AGE</span>
  <span class="c">#    svc-clusterip-xxvws   IPv4          80      10.10.4.3,10.10.1.4,10.10.2.3   3m39s</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
</ul>

<p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_13.png" alt="img.png" /></p>

<ul>
  <li>서비스 (ClusterIP) 접속 확인
    <ul>
      <li>
        <p>클라이언트 (TestPod)의 Shell 에 접속하여 서비스(ClusterIP) 부하분산 접속 확인</p>

        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c"># webpod 파드의 IP 를 출력</span>
  <span class="nv">$ </span>kubectl get pod <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>webpod <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.items[*].status.podIP}"</span>
  <span class="c"># =&gt; 10.10.4.3 10.10.1.4 10.10.2.3</span>
      
  <span class="c"># webpod 파드의 IP를 변수에 지정</span>
  <span class="nv">$ WEBPOD1</span><span class="o">=</span><span class="si">$(</span>kubectl get pod webpod1 <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.status.podIP<span class="o">}</span><span class="si">)</span>
  <span class="nv">$ WEBPOD2</span><span class="o">=</span><span class="si">$(</span>kubectl get pod webpod2 <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.status.podIP<span class="o">}</span><span class="si">)</span>
  <span class="nv">$ WEBPOD3</span><span class="o">=</span><span class="si">$(</span>kubectl get pod webpod3 <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.status.podIP<span class="o">}</span><span class="si">)</span>
  <span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$WEBPOD1</span> <span class="nv">$WEBPOD2</span> <span class="nv">$WEBPOD3</span>
  <span class="c"># =&gt; 10.10.4.3 10.10.1.4 10.10.2.3</span>
      
  <span class="c"># net-pod 파드에서 webpod 파드의 IP로 직접 curl 로 반복 접속</span>
  <span class="nv">$ </span><span class="k">for </span>pod <span class="k">in</span> <span class="nv">$WEBPOD1</span> <span class="nv">$WEBPOD2</span> <span class="nv">$WEBPOD3</span><span class="p">;</span> <span class="k">do </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> curl <span class="nt">-s</span> <span class="nv">$pod</span><span class="p">;</span> <span class="k">done</span>
  <span class="c"># =&gt; Hostname: webpod1</span>
  <span class="c">#    IP: 10.10.4.3</span>
  <span class="c">#    RemoteAddr: 10.10.0.7:56374</span>
  <span class="c">#    GET / HTTP/1.1</span>
  <span class="c">#    Host: 10.10.4.3</span>
  <span class="c">#    User-Agent: curl/8.7.1</span>
  <span class="c">#    Accept: */*</span>
  <span class="c">#    </span>
  <span class="c">#    Hostname: webpod2</span>
  <span class="c">#    ...</span>
  <span class="c">#    Hostname: webpod3</span>
  <span class="c">#    ...</span>
  <span class="nv">$ </span><span class="k">for </span>pod <span class="k">in</span> <span class="nv">$WEBPOD1</span> <span class="nv">$WEBPOD2</span> <span class="nv">$WEBPOD3</span><span class="p">;</span> <span class="k">do </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> curl <span class="nt">-s</span> <span class="nv">$pod</span> | <span class="nb">grep </span>Hostname<span class="p">;</span> <span class="k">done</span>
  <span class="c"># =&gt; Hostname: webpod1</span>
  <span class="c">#    Hostname: webpod2</span>
  <span class="c">#    Hostname: webpod3</span>
  <span class="nv">$ </span><span class="k">for </span>pod <span class="k">in</span> <span class="nv">$WEBPOD1</span> <span class="nv">$WEBPOD2</span> <span class="nv">$WEBPOD3</span><span class="p">;</span> <span class="k">do </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> curl <span class="nt">-s</span> <span class="nv">$pod</span> | <span class="nb">grep </span>Host<span class="p">;</span> <span class="k">done</span>
  <span class="c"># =&gt; Hostname: webpod1</span>
  <span class="c">#    Host: 10.10.4.3</span>
  <span class="c">#    Hostname: webpod2</span>
  <span class="c">#    Host: 10.10.1.4</span>
  <span class="c">#    Hostname: webpod3</span>
  <span class="c">#    Host: 10.10.2.3</span>
  <span class="nv">$ </span><span class="k">for </span>pod <span class="k">in</span> <span class="nv">$WEBPOD1</span> <span class="nv">$WEBPOD2</span> <span class="nv">$WEBPOD3</span><span class="p">;</span> <span class="k">do </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> curl <span class="nt">-s</span> <span class="nv">$pod</span> | egrep <span class="s1">'Host|RemoteAddr'</span><span class="p">;</span> <span class="k">done</span>
  <span class="c"># =&gt; Hostname: webpod1</span>
  <span class="c">#    RemoteAddr: 10.10.0.7:36382</span>
  <span class="c">#    Host: 10.10.4.3</span>
  <span class="c">#    Hostname: webpod2</span>
  <span class="c">#    RemoteAddr: 10.10.0.7:52122</span>
  <span class="c">#    Host: 10.10.1.4</span>
  <span class="c">#    Hostname: webpod3</span>
  <span class="c">#    RemoteAddr: 10.10.0.7:55962</span>
  <span class="c">#    Host: 10.10.2.3</span>
      
  <span class="c"># 서비스 IP 변수 지정 : svc-clusterip 의 ClusterIP주소</span>
  <span class="nv">$ SVC1</span><span class="o">=</span><span class="si">$(</span>kubectl get svc svc-clusterip <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.spec.clusterIP<span class="o">}</span><span class="si">)</span>
  <span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$SVC1</span>
  <span class="c"># =&gt; 10.200.1.96</span>
      
  <span class="c"># 위 서비스 생성 시 kube-proxy 에 의해서 iptables 규칙이 모든 노드에 추가됨 </span>
  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane iptables <span class="nt">-t</span> nat <span class="nt">-S</span> | <span class="nb">grep</span> <span class="nv">$SVC1</span>
  <span class="c"># =&gt; -A KUBE-SERVICES -d 10.200.1.96/32 -p tcp -m comment --comment "default/svc-clusterip:svc-webport cluster IP" -m tcp --dport 9000 -j KUBE-SVC-KBDEBIL6IU6WL7RF</span>
  <span class="c">#    -A KUBE-SVC-KBDEBIL6IU6WL7RF ! -s 10.10.0.0/16 -d 10.200.1.96/32 -p tcp -m comment --comment "default/svc-clusterip:svc-webport cluster IP" -m tcp --dport 9000 -j KUBE-MARK-MASQ</span>
  <span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> iptables <span class="nt">-t</span> nat <span class="nt">-S</span> | <span class="nb">grep</span> <span class="nv">$SVC1</span><span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
  <span class="c"># =&gt; &gt;&gt; node myk8s-control-plane &lt;&lt;</span>
  <span class="c">#    -A KUBE-SERVICES -d 10.200.1.96/32 -p tcp -m comment --comment "default/svc-clusterip:svc-webport cluster IP" -m tcp --dport 9000 -j KUBE-SVC-KBDEBIL6IU6WL7RF</span>
  <span class="c">#    -A KUBE-SVC-KBDEBIL6IU6WL7RF ! -s 10.10.0.0/16 -d 10.200.1.96/32 -p tcp -m comment --comment "default/svc-clusterip:svc-webport cluster IP" -m tcp --dport 9000 -j KUBE-MARK-MASQ</span>
  <span class="c">#    </span>
  <span class="c">#    &gt;&gt; node myk8s-worker &lt;&lt;</span>
  <span class="c">#    -A KUBE-SERVICES -d 10.200.1.96/32 -p tcp -m comment --comment "default/svc-clusterip:svc-webport cluster IP" -m tcp --dport 9000 -j KUBE-SVC-KBDEBIL6IU6WL7RF</span>
  <span class="c">#    -A KUBE-SVC-KBDEBIL6IU6WL7RF ! -s 10.10.0.0/16 -d 10.200.1.96/32 -p tcp -m comment --comment "default/svc-clusterip:svc-webport cluster IP" -m tcp --dport 9000 -j KUBE-MARK-MASQ</span>
  <span class="c">#    </span>
  <span class="c">#    &gt;&gt; node myk8s-worker2 &lt;&lt;</span>
  <span class="c">#    -A KUBE-SERVICES -d 10.200.1.96/32 -p tcp -m comment --comment "default/svc-clusterip:svc-webport cluster IP" -m tcp --dport 9000 -j KUBE-SVC-KBDEBIL6IU6WL7RF</span>
  <span class="c">#    -A KUBE-SVC-KBDEBIL6IU6WL7RF ! -s 10.10.0.0/16 -d 10.200.1.96/32 -p tcp -m comment --comment "default/svc-clusterip:svc-webport cluster IP" -m tcp --dport 9000 -j KUBE-MARK-MASQ</span>
  <span class="c">#    </span>
  <span class="c">#    &gt;&gt; node myk8s-worker3 &lt;&lt;</span>
  <span class="c">#    -A KUBE-SERVICES -d 10.200.1.96/32 -p tcp -m comment --comment "default/svc-clusterip:svc-webport cluster IP" -m tcp --dport 9000 -j KUBE-SVC-KBDEBIL6IU6WL7RF</span>
  <span class="c">#    -A KUBE-SVC-KBDEBIL6IU6WL7RF ! -s 10.10.0.0/16 -d 10.200.1.96/32 -p tcp -m comment --comment "default/svc-clusterip:svc-webport cluster IP" -m tcp --dport 9000 -j KUBE-MARK-MASQ</span>
      
  <span class="c">## (참고) ss 툴로 tcp listen 정보에는 없음 , 별도 /32 host 라우팅 추가 없음 -&gt; 즉, iptables rule 에 의해서 처리됨을 확인</span>
  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane ss <span class="nt">-tnlp</span>
  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane ip <span class="nt">-c</span> route
      
  <span class="c"># TCP 80,9000 포트별 접속 확인 : 출력 정보 의미 확인</span>
  <span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> curl <span class="nt">-s</span> <span class="nt">--connect-timeout</span> 1 <span class="nv">$SVC1</span>:80
  <span class="c"># =&gt; (공백)</span>
  <span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> curl <span class="nt">-s</span> <span class="nt">--connect-timeout</span> 1 <span class="nv">$SVC1</span>:9000
  <span class="c"># =&gt; Hostname: webpod2</span>
  <span class="c">#    ...</span>
  <span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> curl <span class="nt">-s</span> <span class="nt">--connect-timeout</span> 1 <span class="nv">$SVC1</span>:9000 | <span class="nb">grep </span>Hostname
  <span class="c"># =&gt; Hostname: webpod3</span>
  <span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> curl <span class="nt">-s</span> <span class="nt">--connect-timeout</span> 1 <span class="nv">$SVC1</span>:9000 | <span class="nb">grep </span>Hostname
  <span class="c"># =&gt; Hostname: webpod1 </span>
      
  <span class="c"># curl로 접속했을때 컨테이너의 포트인 targetPort 80으로는 접속이 안 되고 port 9000로는 접속이 됩니다.</span>
  <span class="c"># 또한 접속시마다 각 pod에 부하가 분산되어 HostName: 이 변경됨을 확인할 수 있습니다.</span>
      
  <span class="c"># 서비스(ClusterIP) 부하분산 접속 확인</span>
  <span class="c">## for 문을 이용하여 SVC1 IP 로 100번 접속을 시도 후 출력되는 내용 중 반복되는 내용의 갯수 출력</span>
  <span class="c">## 반복해서 실행을 해보면, SVC1 IP로 curl 접속 시 3개의 파드로 대략 33% 정도로 부하분산 접속됨을 확인</span>
  <span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> zsh <span class="nt">-c</span> <span class="s2">"for i in {1..10};   do curl -s </span><span class="nv">$SVC1</span><span class="s2">:9000 | grep Hostname; done | sort | uniq -c | sort -nr"</span>
  <span class="c"># =&gt;       4 Hostname: webpod3</span>
  <span class="c">#          4 Hostname: webpod2</span>
  <span class="c">#          2 Hostname: webpod1</span>
  <span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> zsh <span class="nt">-c</span> <span class="s2">"for i in {1..100};  do curl -s </span><span class="nv">$SVC1</span><span class="s2">:9000 | grep Hostname; done | sort | uniq -c | sort -nr"</span>
  <span class="c"># =&gt;      38 Hostname: webpod3</span>
  <span class="c">#         35 Hostname: webpod1</span>
  <span class="c">#         27 Hostname: webpod2</span>
  <span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> zsh <span class="nt">-c</span> <span class="s2">"for i in {1..1000}; do curl -s </span><span class="nv">$SVC1</span><span class="s2">:9000 | grep Hostname; done | sort | uniq -c | sort -nr"</span>
  <span class="c"># =&gt;     346 Hostname: webpod2</span>
  <span class="c">#        336 Hostname: webpod1</span>
  <span class="c">#        318 Hostname: webpod3</span>
  <span class="c"># 혹은</span>
  <span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> zsh <span class="nt">-c</span> <span class="s2">"for i in {1..100};   do curl -s </span><span class="nv">$SVC1</span><span class="s2">:9000 | grep Hostname; sleep 1; done"</span>
  <span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> zsh <span class="nt">-c</span> <span class="s2">"for i in {1..100};   do curl -s </span><span class="nv">$SVC1</span><span class="s2">:9000 | grep Hostname; sleep 0.1; done"</span>
  <span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> zsh <span class="nt">-c</span> <span class="s2">"for i in {1..10000}; do curl -s </span><span class="nv">$SVC1</span><span class="s2">:9000 | grep Hostname; sleep 0.01; done"</span>
      
  <span class="c"># conntrack 확인</span>
  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane bash
  <span class="nt">----------------------------------------</span>
  <span class="nv">$ </span>conntrack <span class="nt">-h</span>
  <span class="nv">$ </span>conntrack <span class="nt">-E</span>
  <span class="c"># =&gt;     [NEW] tcp      6 120 SYN_SENT src=172.23.0.2 dst=172.23.0.2 sport=45466 dport=6443 [UNREPLIED] src=172.23.0.2 dst=172.23.0.2 sport=6443 dport=45466</span>
  <span class="c">#     [UPDATE] tcp      6 60 SYN_RECV src=172.23.0.2 dst=172.23.0.2 sport=45466 dport=6443 src=172.23.0.2 dst=172.23.0.2 sport=6443 dport=45466</span>
  <span class="nv">$ </span>conntrack <span class="nt">-C</span>
  <span class="c"># =&gt; 2763</span>
  <span class="nv">$ </span>conntrack <span class="nt">-S</span>
  <span class="c"># =&gt; cpu=0           found=0 invalid=0 insert=0 insert_failed=0 drop=0 early_drop=0 error=0 search_restart=3 clash_resolve=0 chaintoolong=0</span>
  <span class="c">#    cpu=1           found=0 invalid=0 insert=0 insert_failed=0 drop=0 early_drop=0 error=0 search_restart=0 clash_resolve=0 chaintoolong=0</span>
  <span class="nv">$ </span>conntrack <span class="nt">-L</span> <span class="nt">--src</span> 10.200.0.7 <span class="c"># net-pod IP</span>
  <span class="c"># =&gt; tcp      6 93 TIME_WAIT src=10.10.0.7 dst=10.200.1.96 sport=37008 dport=9000 src=10.10.2.3 dst=10.10.0.7 sport=80 dport=37008 [ASSURED] mark=0 use=1</span>
  <span class="c">#    tcp      6 93 TIME_WAIT src=10.10.0.7 dst=10.200.1.96 sport=36584 dport=9000 src=10.10.2.3 dst=10.10.0.7 sport=80 dport=36584 [ASSURED] mark=0 use=1</span>
  <span class="nv">$ SVC1</span><span class="o">=</span><span class="si">$(</span>kubectl get svc svc-clusterip <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.spec.clusterIP<span class="o">}</span><span class="si">)</span>
  <span class="nv">$ </span>conntrack <span class="nt">-L</span> <span class="nt">--dst</span> <span class="nv">$SVC1</span>     <span class="c"># service ClusterIP</span>
  <span class="c"># =&gt; tcp      6 31 TIME_WAIT src=10.10.0.7 dst=10.200.1.96 sport=37008 dport=9000 src=10.10.2.3 dst=10.10.0.7 sport=80 dport=37008 [ASSURED] mark=0 use=1</span>
  <span class="c">#    tcp      6 31 TIME_WAIT src=10.10.0.7 dst=10.200.1.96 sport=36584 dport=9000 src=10.10.2.3 dst=10.10.0.7 sport=80 dport=36584 [ASSURED] mark=0 use=1</span>
  <span class="nv">$ </span><span class="nb">exit</span>
  <span class="nt">----------------------------------------</span>
      
  <span class="c"># (참고) Link layer 에서 동작하는 ebtables</span>
  <span class="nv">$ </span>ebtables <span class="nt">-L</span>
  <span class="c"># =&gt; Bridge table: filter</span>
  <span class="c">#    Bridge chain: INPUT, entries: 0, policy: ACCEPT</span>
  <span class="c">#    Bridge chain: FORWARD, entries: 0, policy: ACCEPT</span>
  <span class="c">#    Bridge chain: OUTPUT, entries: 0, policy: ACCEPT</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>각 워커 노드에서 패킷  덤프 확인</p>

        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c"># 방안1 : 1대 혹은 3대 bash 진입 후 tcpdump 해둘 것</span>
  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker bash
  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker2 bash
  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker3 bash
  <span class="nt">----------------------------------</span>
  <span class="c"># nic 정보 확인</span>
  <span class="nv">$ </span>ip <span class="nt">-c</span> <span class="nb">link</span>
  <span class="c"># =&gt; &lt;&lt;myk8s-worker&gt;&gt;</span>
  <span class="c">#    3: veth9a888981@if2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default</span>
  <span class="c">#        link/ether 26:a5:d2:44:f4:b4 brd ff:ff:ff:ff:ff:ff link-netns cni-0b7e59c3-3920-ce1f-874a-9e228adf3b72</span>
  <span class="c">#    134: eth0@if135: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default</span>
  <span class="c">#        link/ether 02:42:ac:17:00:04 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span>
  <span class="c">#    </span>
  <span class="c">#    &lt;&lt;myk8s-worker2&gt;&gt;</span>
  <span class="c">#    4: veth570fce87@if2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default</span>
  <span class="c">#        link/ether a6:8a:d8:a3:f4:ac brd ff:ff:ff:ff:ff:ff link-netns cni-8dcb2d16-f339-2571-03d2-d6b0f850878d</span>
  <span class="c">#    136: eth0@if137: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default</span>
  <span class="c">#        link/ether 02:42:ac:17:00:05 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span>
  <span class="c">#    </span>
  <span class="c">#    &lt;&lt;myk8s-worker3&gt;&gt;</span>
  <span class="c">#    3: veth2e19df47@if2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default</span>
  <span class="c">#        link/ether da:5e:a5:14:62:7b brd ff:ff:ff:ff:ff:ff link-netns cni-1f44bebd-9ebf-6af4-7888-945649a2b5c8</span>
  <span class="c">#    132: eth0@if133: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default</span>
  <span class="c">#        link/ether 02:42:ac:17:00:03 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span>
      
  <span class="nv">$ </span>ip <span class="nt">-c</span> route
  <span class="c"># =&gt; &lt;&lt;myk8s-worker&gt;&gt;</span>
  <span class="c">#    default via 172.23.0.1 dev eth0</span>
  <span class="c">#    10.10.0.0/24 via 172.23.0.2 dev eth0</span>
  <span class="c">#    10.10.1.0/24 via 172.23.0.5 dev eth0</span>
  <span class="c">#    10.10.2.0/24 via 172.23.0.3 dev eth0</span>
  <span class="c">#    10.10.4.3 dev veth9a888981 scope host</span>
  <span class="c">#    172.23.0.0/16 dev eth0 proto kernel scope link src 172.23.0.4</span>
  <span class="c">#</span>
  <span class="c">#    &lt;&lt;myk8s-worker2&gt;&gt;</span>
  <span class="c">#    default via 172.23.0.1 dev eth0</span>
  <span class="c">#    10.10.0.0/24 via 172.23.0.2 dev eth0</span>
  <span class="c">#    10.10.1.4 dev veth570fce87 scope host</span>
  <span class="c">#    10.10.2.0/24 via 172.23.0.3 dev eth0</span>
  <span class="c">#    10.10.4.0/24 via 172.23.0.4 dev eth0</span>
  <span class="c">#    172.23.0.0/16 dev eth0 proto kernel scope link src 172.23.0.5</span>
  <span class="c">#    </span>
  <span class="c">#    &lt;&lt;myk8s-worker3&gt;&gt;</span>
  <span class="c">#    default via 172.23.0.1 dev eth0</span>
  <span class="c">#    10.10.0.0/24 via 172.23.0.2 dev eth0</span>
  <span class="c">#    10.10.1.0/24 via 172.23.0.5 dev eth0</span>
  <span class="c">#    10.10.2.3 dev veth2e19df47 scope host</span>
  <span class="c">#    10.10.4.0/24 via 172.23.0.4 dev eth0</span>
  <span class="c">#    172.23.0.0/16 dev eth0 proto kernel scope link src 172.23.0.3</span>
      
  <span class="nv">$ </span>ip <span class="nt">-c</span> addr
  <span class="c"># =&gt; &lt;&lt;myk8s-worker&gt;&gt;</span>
  <span class="c">#    3: veth9a888981@if2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span>
  <span class="c">#        link/ether 26:a5:d2:44:f4:b4 brd ff:ff:ff:ff:ff:ff link-netns cni-0b7e59c3-3920-ce1f-874a-9e228adf3b72</span>
  <span class="c">#        inet 10.10.4.1/32 scope global veth9a888981</span>
  <span class="c">#           valid_lft forever preferred_lft forever</span>
  <span class="c">#    134: eth0@if135: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span>
  <span class="c">#        link/ether 02:42:ac:17:00:04 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span>
  <span class="c">#        inet 172.23.0.4/16 brd 172.23.255.255 scope global eth0</span>
  <span class="c">#           valid_lft forever preferred_lft forever</span>
  <span class="c">#    </span>
  <span class="c">#    &lt;&lt;myk8s-worker2&gt;&gt;</span>
  <span class="c">#    4: veth570fce87@if2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span>
  <span class="c">#        link/ether a6:8a:d8:a3:f4:ac brd ff:ff:ff:ff:ff:ff link-netns cni-8dcb2d16-f339-2571-03d2-d6b0f850878d</span>
  <span class="c">#        inet 10.10.1.1/32 scope global veth570fce87</span>
  <span class="c">#           valid_lft forever preferred_lft forever</span>
  <span class="c">#    136: eth0@if137: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span>
  <span class="c">#        link/ether 02:42:ac:17:00:05 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span>
  <span class="c">#        inet 172.23.0.5/16 brd 172.23.255.255 scope global eth0</span>
  <span class="c">#           valid_lft forever preferred_lft forever</span>
  <span class="c">#    </span>
  <span class="c">#    &lt;&lt;myk8s-worker3&gt;&gt;</span>
  <span class="c">#    3: veth2e19df47@if2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span>
  <span class="c">#        link/ether da:5e:a5:14:62:7b brd ff:ff:ff:ff:ff:ff link-netns cni-1f44bebd-9ebf-6af4-7888-945649a2b5c8</span>
  <span class="c">#        inet 10.10.2.1/32 scope global veth2e19df47</span>
  <span class="c">#           valid_lft forever preferred_lft forever</span>
  <span class="c">#    132: eth0@if133: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span>
  <span class="c">#        link/ether 02:42:ac:17:00:03 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span>
  <span class="c">#        inet 172.23.0.3/16 brd 172.23.255.255 scope global eth0</span>
  <span class="c">#           valid_lft forever preferred_lft forever</span>
      
  <span class="c"># tcpdump/ngrep : eth0 &gt;&gt; tcp 9000 포트 트래픽은 왜 없을까? iptables rule 동작 그림을 한번 더 확인하고 이해해보자</span>
  <span class="c">## ngrep 네트워크 패킷 분석기 활용해보기 : 특정 url 호출에 대해서만 필터 등 깔끔하게 볼 수 있음 - 링크</span>
  <span class="nv">$ </span>tcpdump <span class="nt">-i</span> eth0 tcp port 80 <span class="nt">-nnq</span>
  <span class="nv">$ </span>tcpdump <span class="nt">-i</span> eth0 tcp port 80 <span class="nt">-w</span> /root/svc1-1.pcap
  <span class="nv">$ </span>tcpdump <span class="nt">-i</span> eth0 tcp port 9000 <span class="nt">-nnq</span>
  <span class="nv">$ </span>ngrep <span class="nt">-tW</span> byline <span class="nt">-d</span> eth0 <span class="s1">''</span> <span class="s1">'tcp port 80'</span>
      
  <span class="c"># tcpdump/ngrep : vethX</span>
  <span class="c"># $ VETH1=&lt;각자 자신의 veth 이름&gt;</span>
  <span class="nv">$ VETH1</span><span class="o">=</span>veth9a888981
  <span class="nv">$ </span>tcpdump <span class="nt">-i</span> <span class="nv">$VETH1</span> tcp port 80 <span class="nt">-nn</span>
  <span class="nv">$ </span>tcpdump <span class="nt">-i</span> <span class="nv">$VETH1</span> tcp port 80 <span class="nt">-w</span> /root/svc1-2.pcap
  <span class="nv">$ </span>tcpdump <span class="nt">-i</span> <span class="nv">$VETH1</span> tcp port 9000 <span class="nt">-nn</span>
  <span class="nv">$ </span>ngrep <span class="nt">-tW</span> byline <span class="nt">-d</span> <span class="nv">$VETH1</span> <span class="s1">''</span> <span class="s1">'tcp port 80'</span>
      
  <span class="nv">$ </span><span class="nb">exit</span>
  <span class="nt">----------------------------------</span>
      
  <span class="c"># 방안2 : kind 노드 컨테이너 bash 직접 접속하지 않고 호스트에서 tcpdump 하기</span>
  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker tcpdump <span class="nt">-i</span> eth0 tcp port 80 <span class="nt">-nnq</span>
  <span class="nv">$ VETH1</span><span class="o">=</span>&lt;각자 자신의 veth 이름&gt; docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker ip <span class="nt">-c</span> route
  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker tcpdump <span class="nt">-i</span> <span class="nv">$VETH1</span> tcp port 80 <span class="nt">-nnq</span>
      
  <span class="c"># 호스트PC에 pcap 파일 복사 &gt;&gt; wireshark 에서 분석</span>
  <span class="nv">$ </span>docker <span class="nb">cp </span>myk8s-worker:/root/svc1-1.pcap <span class="nb">.</span>
  <span class="nv">$ </span>docker <span class="nb">cp </span>myk8s-worker:/root/svc1-2.pcap <span class="nb">.</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>net-pod 포드에 접속 후 10개 curl 요청</p>

        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> zsh
<span class="nt">----------------------------------</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..10<span class="o">}</span><span class="p">;</span>   <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$SVC1</span>:9000 | <span class="nb">grep </span>Hostname<span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
<span class="c"># =&gt;       4 Hostname: webpod3</span>
<span class="c">#          3 Hostname: webpod2</span>
<span class="c">#          3 Hostname: webpod1</span>
    
<span class="nv">$ </span><span class="nb">exit</span>
<span class="nt">----------------------------------</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>각각 net-pod와 워커 노드들의 패킷캡쳐파일(*.pcap)를 받아서 와이어샤크로 확인해보겠습니다.</p>
        <ul>
          <li>
            <p>net-pod(10.10.0.7)에서 서비스:9000 (IP:10.200.1.96)으로 요청된 패킷이 DNAT 되어 k8s-worker의 webpod1:80 (IP:10.10.4.3)으로 전달되고, 응답은 그 반대로 전달 되는 것을 확인 할 수 있습니다.</p>

            <p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_14.png" alt="img.png" /></p>
          </li>
          <li>
            <p>또한 Stastics 메뉴의→ Flow Graph 기능을 통해 패킷의 흐름을 확인할 수 있었습니다.</p>

            <p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_15.png" alt="img.png" /></p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>iptables 정책 확인
    <ul>
      <li>kubernetes에서 service는 다음의 iptables 과정을 거칩니다.
        <ul>
          <li>(1) PREROUTING ⇒ (2) KUBE-SERVICES ⇒ (3) KUBE-SVC-YYY ⇒ (4) KUBE-SEP-#파드1, KUBE-SEP-#파드2, KUBE-SEP-#파드3</li>
          <li>그림으로 나타내면 다음과 같습니다.</li>
        </ul>

        <p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_16.png" alt="img.png" /></p>

        <ul>
          <li>
            <p>각각에 대하여 iptables 룰을 확인해보겠습니다.</p>

            <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c"># 컨트롤플레인에서 확인하겠습니다.</span>
  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane bash
  <span class="nt">----------------------------------------</span>
        
  <span class="c"># iptables 확인</span>
  <span class="nv">$ </span>iptables <span class="nt">-t</span> filter <span class="nt">-S</span>
  <span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span>
  <span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span> | <span class="nb">wc</span> <span class="nt">-l</span>
  <span class="c"># =&gt; 97</span>
  <span class="nv">$ </span>iptables <span class="nt">-t</span> mangle <span class="nt">-S</span>
        
  <span class="c"># iptables 상세 확인 - 매칭 패킷 카운트, 인터페이스 정보 등 포함</span>
  <span class="nv">$ </span>iptables <span class="nt">-nvL</span> <span class="nt">-t</span> filter
  <span class="nv">$ </span>iptables <span class="nt">-nvL</span> <span class="nt">-t</span> nat
  <span class="nv">$ </span>iptables <span class="nt">-nvL</span> <span class="nt">-t</span> mangle
        
  <span class="c"># rule 갯수 확인</span>
  <span class="nv">$ </span>iptables <span class="nt">-nvL</span> <span class="nt">-t</span> filter | <span class="nb">wc</span> <span class="nt">-l</span>
  <span class="c"># =&gt; 47</span>
  <span class="nv">$ </span>iptables <span class="nt">-nvL</span> <span class="nt">-t</span> nat | <span class="nb">wc</span> <span class="nt">-l</span>
  <span class="c"># =&gt; 158</span>
        
  <span class="c"># 규칙 패킷 바이트 카운트 초기화</span>
  <span class="nv">$ </span>iptables <span class="nt">-t</span> filter <span class="nt">--zero</span><span class="p">;</span> iptables <span class="nt">-t</span> nat <span class="nt">--zero</span><span class="p">;</span> iptables <span class="nt">-t</span> mangle <span class="nt">--zero</span>
        
  <span class="c"># 정책 확인 : 아래 정책 내용은 핵심적인 룰(rule)만 표시했습니다!</span>
  <span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-nvL</span>
  <span class="c"># =&gt; Chain PREROUTING (policy ACCEPT 121 packets, 7260 bytes) &lt;&lt;1. PREROUTING&gt;&gt;</span>
  <span class="c">#     pkts bytes target     prot opt in     out     source               destination         </span>
  <span class="c">#      121  7260 KUBE-SERVICES  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service portals */</span>
  <span class="c">#    ...</span>
  <span class="c">#    </span>
  <span class="c">#    Chain INPUT (policy ACCEPT 121 packets, 7260 bytes)</span>
  <span class="c">#    </span>
  <span class="c">#    Chain OUTPUT (policy ACCEPT 392 packets, 23520 bytes)</span>
  <span class="c">#     pkts bytes target     prot opt in     out     source               destination         </span>
  <span class="c">#      392 23520 KUBE-SERVICES  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service portals */</span>
  <span class="c">#    ...</span>
  <span class="c">#    </span>
  <span class="c">#    Chain KUBE-MARK-MASQ (18 references)</span>
  <span class="c">#     pkts bytes target     prot opt in     out     source               destination         </span>
  <span class="c">#        0     0 MARK       0    --  *      *       0.0.0.0/0            0.0.0.0/0            MARK or 0x4000</span>
  <span class="c">#    </span>
  <span class="c">#    Chain KUBE-SERVICES (2 references) &lt;&lt;2. SERVICES&gt;&gt;</span>
  <span class="c">#     pkts bytes target     prot opt in     out     source               destination         </span>
  <span class="c">#        0     0 KUBE-SVC-KBDEBIL6IU6WL7RF  6    --  *      *       0.0.0.0/0            10.200.1.96          /* default/svc-clusterip:svc-webport cluster IP */ tcp dpt:9000</span>
  <span class="c">#    ...</span>
  <span class="c">#    </span>
  <span class="c">#    Chain KUBE-SVC-KBDEBIL6IU6WL7RF (1 references) &lt;&lt;3. KUBE-SVC-YYY&gt;&gt;</span>
  <span class="c">#     pkts bytes target     prot opt in     out     source               destination         </span>
  <span class="c">#        0     0 KUBE-MARK-MASQ  6    --  *      *      !10.10.0.0/16         10.200.1.96          /* default/svc-clusterip:svc-webport cluster IP */ tcp dpt:9000</span>
  <span class="c">#        0     0 KUBE-SEP-T7YVH2JOMUTQFUDU  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/svc-clusterip:svc-webport -&gt; 10.10.1.4:80 */ statistic mode random probability 0.33333333349</span>
  <span class="c">#        0     0 KUBE-SEP-SZHENXPAXVOCHRDA  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/svc-clusterip:svc-webport -&gt; 10.10.2.3:80 */ statistic mode random probability 0.50000000000</span>
  <span class="c">#        0     0 KUBE-SEP-X47GKN7LA32LZ4H7  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/svc-clusterip:svc-webport -&gt; 10.10.4.3:80 */</span>
  <span class="c">#    </span>
  <span class="c">#    Chain KUBE-SEP-X47GKN7LA32LZ4H7 (1 references) &lt;&lt;4. KUBE-SEP-#WEBPOD1&gt;&gt;</span>
  <span class="c">#     pkts bytes target     prot opt in     out     source               destination         </span>
  <span class="c">#        0     0 KUBE-MARK-MASQ  0    --  *      *       10.10.4.3            0.0.0.0/0            /* default/svc-clusterip:svc-webport */</span>
  <span class="c">#        0     0 DNAT       6    --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/svc-clusterip:svc-webport */ tcp to:10.10.4.3:80</span>
  <span class="c">#    </span>
  <span class="c">#    Chain KUBE-SEP-T7YVH2JOMUTQFUDU (1 references) &lt;&lt;4. KUBE-SEP-#WEBPOD2&gt;&gt;</span>
  <span class="c">#     pkts bytes target     prot opt in     out     source               destination         </span>
  <span class="c">#        0     0 KUBE-MARK-MASQ  0    --  *      *       10.10.1.4            0.0.0.0/0            /* default/svc-clusterip:svc-webport */</span>
  <span class="c">#        0     0 DNAT       6    --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/svc-clusterip:svc-webport */ tcp to:10.10.1.4:80</span>
  <span class="c">#</span>
  <span class="c">#    Chain KUBE-SEP-SZHENXPAXVOCHRDA (1 references) &lt;&lt;4. KUBE-SEP-#WEBPOD3&gt;&gt;</span>
  <span class="c">#     pkts bytes target     prot opt in     out     source               destination         </span>
  <span class="c">#        0     0 KUBE-MARK-MASQ  0    --  *      *       10.10.2.3            0.0.0.0/0            /* default/svc-clusterip:svc-webport */</span>
  <span class="c">#        0     0 DNAT       6    --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/svc-clusterip:svc-webport */ tcp to:10.10.2.3:80</span>
        
  <span class="nv">$ </span>iptables <span class="nt">-v</span> <span class="nt">--numeric</span> <span class="nt">--table</span> nat <span class="nt">--list</span> PREROUTING | column <span class="nt">-t</span>
  <span class="c"># =&gt; Chain  PREROUTING  (policy        ACCEPT  777  packets,  46620  bytes)</span>
  <span class="c">#    pkts   bytes       target         prot    opt  in        out    source     destination</span>
  <span class="c">#    777    46620       KUBE-SERVICES  0       --   *         *      0.0.0.0/0  0.0.0.0/0    /*  kubernetes  service  portals  */</span>
  <span class="c">#    0      0           DOCKER_OUTPUT  0       --   *         *      0.0.0.0/0  172.23.0.1</span>
        
  <span class="nv">$ </span>iptables <span class="nt">-v</span> <span class="nt">--numeric</span> <span class="nt">--table</span> nat <span class="nt">--list</span> KUBE-SERVICES | column
  <span class="c"># 바로 아래 룰(rule)에 의해서 서비스(ClusterIP)를 인지하고 처리를 합니다</span>
  <span class="c"># =&gt; Chain  KUBE-SERVICES  (2                         references)</span>
  <span class="c">#    pkts   bytes          target                     prot         opt  in  out  source     destination</span>
  <span class="c">#    0      0              KUBE-SVC-KBDEBIL6IU6WL7RF  6            --   *   *    0.0.0.0/0  10.200.1.96   /*  default/svc-clusterip:svc-webport  cluster  IP          */     tcp   dpt:9000</span>
        
  <span class="nv">$ </span>iptables <span class="nt">-v</span> <span class="nt">--numeric</span> <span class="nt">--table</span> nat <span class="nt">--list</span> KUBE-SVC-KBDEBIL6IU6WL7RF | column
  <span class="c"># =&gt; Chain  KUBE-SVC-KBDEBIL6IU6WL7RF  (1                         references)</span>
  <span class="c">#    pkts   bytes                      target                     prot         opt  in  out  source         destination</span>
  <span class="c">#    0      0                          KUBE-SEP-T7YVH2JOMUTQFUDU  0            --   *   *    0.0.0.0/0      0.0.0.0/0    /*  default/svc-clusterip:svc-webport  -&gt;       10.10.1.4:80  */  statistic  mode      random  probability  0.33333333349</span>
  <span class="c">#    0      0                          KUBE-SEP-SZHENXPAXVOCHRDA  0            --   *   *    0.0.0.0/0      0.0.0.0/0    /*  default/svc-clusterip:svc-webport  -&gt;       10.10.2.3:80  */  statistic  mode      random  probability  0.50000000000</span>
  <span class="c">#    0      0                          KUBE-SEP-X47GKN7LA32LZ4H7  0            --   *   *    0.0.0.0/0      0.0.0.0/0    /*  default/svc-clusterip:svc-webport  -&gt;       10.10.4.3:80  */</span>
        
  <span class="c"># 패킷 전달 수를 확인 하기 위해 watch를 겁니다.</span>
  <span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s1">'iptables -v --numeric --table nat --list KUBE-SVC-KBDEBIL6IU6WL7RF'</span>
        
  <span class="c"># control-plane 에서 테스트 패킷을 보냅니다.</span>
  <span class="nv">$ SVC1</span><span class="o">=</span><span class="si">$(</span>kubectl get svc svc-clusterip <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.spec.clusterIP<span class="o">}</span><span class="si">)</span>
  <span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> zsh <span class="nt">-c</span> <span class="s2">"for i in {1..100};   do curl -s </span><span class="nv">$SVC1</span><span class="s2">:9000 | grep Hostname; sleep 1; done"</span>
</code></pre></div>            </div>

            <p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_17.png" alt="img.png" /></p>
          </li>
          <li>
            <p>iptables에서  카운트가 증가함을 확인 할 수 있습니다.</p>

            <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># SVC-### 에서 랜덤 확률(대략 33%)로 SEP(Service EndPoint)인 각각 파드 IP로 DNAT 됩니다!</span>
<span class="c">## 첫번째 룰에 일치 확률은 33% 이고, 매칭되지 않을 경우 아래 2개 남을때는 룰 일치 확률은 50%가 됩니다. 이것도 매칭되지 않으면 마지막 룰로 100% 일치됩니다</span>
<span class="c">#    Chain KUBE-SVC-KBDEBIL6IU6WL7RF (1 references)</span>
<span class="c">#     pkts bytes target     prot opt in     out     source               destination</span>
<span class="c">#        0     0 KUBE-MARK-MASQ  6    --  *      *      !10.10.0.0/16         10.200.1.96          /* default/svc-clusterip:svc-webport cluster IP */ tcp dpt:9000</span>
<span class="c">#       41  2460 KUBE-SEP-T7YVH2JOMUTQFUDU  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/svc-clusterip:svc-webport -&gt; 10.10.1.4:80 */ statistic mode random probability 0.33333333349</span>
<span class="c">#       47  2820 KUBE-SEP-SZHENXPAXVOCHRDA  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/svc-clusterip:svc-webport -&gt; 10.10.2.3:80 */ statistic mode random probability 0.50000000000</span>
<span class="c">#       45  2700 KUBE-SEP-X47GKN7LA32LZ4H7  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/svc-clusterip:svc-webport -&gt; 10.10.4.3:80 */</span>
      
<span class="c"># $ iptables -v --numeric --table nat --list KUBE-SEP-&lt;각자 값 입력&gt;</span>
<span class="nv">$ </span>iptables <span class="nt">-v</span> <span class="nt">--numeric</span> <span class="nt">--table</span> nat <span class="nt">--list</span> KUBE-SEP-T7YVH2JOMUTQFUDU
<span class="c"># =&gt; Chain  KUBE-SEP-T7YVH2JOMUTQFUDU  (1              references)</span>
<span class="c">#    pkts   bytes                      target          prot         opt  in  out  source     destination</span>
<span class="c">#    49     2940                       DNAT            6            --   *   *    0.0.0.0/0  0.0.0.0/0    /*  default/svc-clusterip:svc-webport  */  tcp  to:10.10.1.4:80</span>
      
<span class="nv">$ </span>iptables <span class="nt">-v</span> <span class="nt">--numeric</span> <span class="nt">--table</span> nat <span class="nt">--list</span> KUBE-SEP-SZHENXPAXVOCHRDA  | column <span class="nt">-t</span>
<span class="c"># =&gt; Chain  KUBE-SEP-SZHENXPAXVOCHRDA  (1              references)</span>
<span class="c">#    pkts   bytes                      target          prot         opt  in  out  source     destination</span>
<span class="c">#    0      0                          KUBE-MARK-MASQ  0            --   *   *    10.10.2.3  0.0.0.0/0    /*  default/svc-clusterip:svc-webport  */</span>
<span class="c">#    56     3360                       DNAT            6            --   *   *    0.0.0.0/0  0.0.0.0/0    /*  default/svc-clusterip:svc-webport  */  tcp  to:10.10.2.3:80</span>
      
<span class="nv">$ </span>iptables <span class="nt">-v</span> <span class="nt">--numeric</span> <span class="nt">--table</span> nat <span class="nt">--list</span> KUBE-SEP-X47GKN7LA32LZ4H7  | column <span class="nt">-t</span>
<span class="c"># =&gt; Chain  KUBE-SEP-X47GKN7LA32LZ4H7  (1              references)</span>
<span class="c">#    pkts   bytes                      target          prot         opt  in  out  source     destination</span>
<span class="c">#    0      0                          KUBE-MARK-MASQ  0            --   *   *    10.10.4.3  0.0.0.0/0    /*  default/svc-clusterip:svc-webport  */</span>
<span class="c">#    48     2880                       DNAT            6            --   *   *    0.0.0.0/0  0.0.0.0/0    /*  default/svc-clusterip:svc-webport  */  tcp  to:10.10.4.3:80</span>
      
<span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">--zero</span>
<span class="nv">$ </span>iptables <span class="nt">-v</span> <span class="nt">--numeric</span> <span class="nt">--table</span> nat <span class="nt">--list</span> POSTROUTING | column<span class="p">;</span> <span class="nb">echo</span> <span class="p">;</span> iptables <span class="nt">-v</span> <span class="nt">--numeric</span> <span class="nt">--table</span> nat <span class="nt">--list</span> KUBE-POSTROUTING | column
<span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s1">'iptables -v --numeric --table nat --list POSTROUTING; echo ; iptables -v --numeric --table nat --list KUBE-POSTROUTING'</span>
<span class="c"># POSTROUTE(nat) : 0x4000 마킹 되어 있지 않으니 RETURN 되고 그냥 빠져나가서 SNAT 되지 않는다!</span>
<span class="c"># =&gt; Chain  POSTROUTING  (policy             ACCEPT  0    packets,  0    bytes)</span>
<span class="c">#    pkts   bytes        target              prot    opt  in        out  source     destination</span>
<span class="c">#    0      0            KUBE-POSTROUTING    0       --   *         *    0.0.0.0/0  0.0.0.0/0    /*        kubernetes  postrouting  rules   */</span>
<span class="c">#    0      0            DOCKER_POSTROUTING  0       --   *         *    0.0.0.0/0  172.23.0.1</span>
<span class="c">#    0      0            KIND-MASQ-AGENT     0       --   *         *    0.0.0.0/0  0.0.0.0/0    ADDRTYPE  match       dst-type     !LOCAL  /*  kind-masq-agent:  ensure  nat  POSTROUTING  directs  all  non-LOCAL  destination  traffic  to  our  custom  KIND-MASQ-AGENT  chain  */</span>
<span class="c"># =&gt; Chain  KUBE-POSTROUTING  (1          references)</span>
<span class="c">#    pkts   bytes             target      prot         opt  in  out  source     destination</span>
<span class="c">#    0      0                 RETURN      0            --   *   *    0.0.0.0/0  0.0.0.0/0    mark  match       !        0x4000/0x4000</span>
<span class="c">#    0      0                 MARK        0            --   *   *    0.0.0.0/0  0.0.0.0/0    MARK  xor         0x4000</span>
<span class="c">#    0      0                 MASQUERADE  0            --   *   *    0.0.0.0/0  0.0.0.0/0    /*    kubernetes  service  traffic        requiring  SNAT  */  random-fully</span>
      
<span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span> | <span class="nb">grep </span>KUBE-POSTROUTING
<span class="c"># =&gt; -N KUBE-POSTROUTING</span>
<span class="c">#    -A POSTROUTING -m comment --comment "kubernetes postrouting rules" -j KUBE-POSTROUTING</span>
<span class="c">#    -A KUBE-POSTROUTING -m mark ! --mark 0x4000/0x4000 -j RETURN</span>
<span class="c">#    -A KUBE-POSTROUTING -j MARK --set-xmark 0x4000/0x0</span>
<span class="c">#    -A KUBE-POSTROUTING -m comment --comment "kubernetes service traffic requiring SNAT" -j MASQUERADE --random-fully</span>
      
<span class="nv">$ </span><span class="nb">exit</span>
<span class="nt">----------------------------------------</span>
      
<span class="c"># 위 서비스 생성 시 kube-proxy 에 의해서 iptables 규칙이 모든 노드에 추가됨을 한번 더 확인</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane iptables <span class="nt">-v</span> <span class="nt">--numeric</span> <span class="nt">--table</span> nat <span class="nt">--list</span> KUBE-SVC-KBDEBIL6IU6WL7RF
<span class="c"># =&gt; Chain KUBE-SVC-KBDEBIL6IU6WL7RF (1 references)</span>
<span class="c">#     pkts bytes target     prot opt in     out     source               destination         </span>
<span class="c">#        0     0 KUBE-MARK-MASQ  6    --  *      *      !10.10.0.0/16         10.200.1.96          /* default/svc-clusterip:svc-webport cluster IP */ tcp dpt:9000</span>
<span class="c">#        0     0 KUBE-SEP-T7YVH2JOMUTQFUDU  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/svc-clusterip:svc-webport -&gt; 10.10.1.4:80 */ statistic mode random probability 0.33333333349</span>
<span class="c">#        0     0 KUBE-SEP-SZHENXPAXVOCHRDA  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/svc-clusterip:svc-webport -&gt; 10.10.2.3:80 */ statistic mode random probability 0.50000000000</span>
<span class="c">#        0     0 KUBE-SEP-X47GKN7LA32LZ4H7  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/svc-clusterip:svc-webport -&gt; 10.10.4.3:80 */</span>
      
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> iptables <span class="nt">-v</span> <span class="nt">--numeric</span> <span class="nt">--table</span> nat <span class="nt">--list</span> KUBE-SVC-KBDEBIL6IU6WL7RF<span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; &gt;&gt; node myk8s-control-plane &lt;&lt;</span>
<span class="c">#    Chain KUBE-SVC-KBDEBIL6IU6WL7RF (1 references)</span>
<span class="c">#     pkts bytes target     prot opt in     out     source               destination         </span>
<span class="c">#        0     0 KUBE-MARK-MASQ  6    --  *      *      !10.10.0.0/16         10.200.1.96          /* default/svc-clusterip:svc-webport cluster IP */ tcp dpt:9000</span>
<span class="c">#        0     0 KUBE-SEP-T7YVH2JOMUTQFUDU  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/svc-clusterip:svc-webport -&gt; 10.10.1.4:80 */ statistic mode random probability 0.33333333349</span>
<span class="c">#        0     0 KUBE-SEP-SZHENXPAXVOCHRDA  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/svc-clusterip:svc-webport -&gt; 10.10.2.3:80 */ statistic mode random probability 0.50000000000</span>
<span class="c">#        0     0 KUBE-SEP-X47GKN7LA32LZ4H7  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/svc-clusterip:svc-webport -&gt; 10.10.4.3:80 */</span>
<span class="c">#    </span>
<span class="c">#    &gt;&gt; node myk8s-worker &lt;&lt;</span>
<span class="c">#    Chain KUBE-SVC-KBDEBIL6IU6WL7RF (1 references)</span>
<span class="c">#     pkts bytes target     prot opt in     out     source               destination         </span>
<span class="c">#        0     0 KUBE-MARK-MASQ  6    --  *      *      !10.10.0.0/16         10.200.1.96          /* default/svc-clusterip:svc-webport cluster IP */ tcp dpt:9000</span>
<span class="c">#        0     0 KUBE-SEP-T7YVH2JOMUTQFUDU  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/svc-clusterip:svc-webport -&gt; 10.10.1.4:80 */ statistic mode random probability 0.33333333349</span>
<span class="c">#        0     0 KUBE-SEP-SZHENXPAXVOCHRDA  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/svc-clusterip:svc-webport -&gt; 10.10.2.3:80 */ statistic mode random probability 0.50000000000</span>
<span class="c">#        0     0 KUBE-SEP-X47GKN7LA32LZ4H7  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/svc-clusterip:svc-webport -&gt; 10.10.4.3:80 */</span>
<span class="c">#    </span>
<span class="c">#    &gt;&gt; node myk8s-worker2 &lt;&lt;</span>
<span class="c">#    Chain KUBE-SVC-KBDEBIL6IU6WL7RF (1 references)</span>
<span class="c">#     pkts bytes target     prot opt in     out     source               destination         </span>
<span class="c">#        0     0 KUBE-MARK-MASQ  6    --  *      *      !10.10.0.0/16         10.200.1.96          /* default/svc-clusterip:svc-webport cluster IP */ tcp dpt:9000</span>
<span class="c">#        0     0 KUBE-SEP-T7YVH2JOMUTQFUDU  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/svc-clusterip:svc-webport -&gt; 10.10.1.4:80 */ statistic mode random probability 0.33333333349</span>
<span class="c">#        0     0 KUBE-SEP-SZHENXPAXVOCHRDA  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/svc-clusterip:svc-webport -&gt; 10.10.2.3:80 */ statistic mode random probability 0.50000000000</span>
<span class="c">#        0     0 KUBE-SEP-X47GKN7LA32LZ4H7  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/svc-clusterip:svc-webport -&gt; 10.10.4.3:80 */</span>
<span class="c">#    </span>
<span class="c">#    &gt;&gt; node myk8s-worker3 &lt;&lt;</span>
<span class="c">#    Chain KUBE-SVC-KBDEBIL6IU6WL7RF (1 references)</span>
<span class="c">#     pkts bytes target     prot opt in     out     source               destination         </span>
<span class="c">#        0     0 KUBE-MARK-MASQ  6    --  *      *      !10.10.0.0/16         10.200.1.96          /* default/svc-clusterip:svc-webport cluster IP */ tcp dpt:9000</span>
<span class="c">#        0     0 KUBE-SEP-T7YVH2JOMUTQFUDU  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/svc-clusterip:svc-webport -&gt; 10.10.1.4:80 */ statistic mode random probability 0.33333333349</span>
<span class="c">#        0     0 KUBE-SEP-SZHENXPAXVOCHRDA  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/svc-clusterip:svc-webport -&gt; 10.10.2.3:80 */ statistic mode random probability 0.50000000000</span>
<span class="c">#        0     0 KUBE-SEP-X47GKN7LA32LZ4H7  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/svc-clusterip:svc-webport -&gt; 10.10.4.3:80 */</span>
</code></pre></div>            </div>
          </li>
          <li>
            <p>동일한 iptables 룰이 각 노드에 있는 것을 확인할 수 있습니다.</p>
          </li>
        </ul>
      </li>
      <li>파드 1개에 장애를 발생시켜서 장애시 동작을 확인해보겠습니다.
        <ul>
          <li>
            <p>동작 확인을 위한 모니터링</p>

            <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c"># 터미널1 &gt;&gt; ENDPOINTS 변화를 잘 확인해보자!</span>
  <span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s1">'kubectl get pod -owide;echo; kubectl get svc,ep svc-clusterip;echo; kubectl get endpointslices -l kubernetes.io/service-name=svc-clusterip'</span>
        
  <span class="c"># 터미널2</span>
  <span class="nv">$ SVC1</span><span class="o">=</span><span class="si">$(</span>kubectl get svc svc-clusterip <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.spec.clusterIP<span class="o">}</span><span class="si">)</span>
  <span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> zsh <span class="nt">-c</span> <span class="s2">"while true; do curl -s --connect-timeout 1 </span><span class="nv">$SVC1</span><span class="s2">:9000 | egrep 'Hostname|IP: 10'; date '+%Y-%m-%d %H:%M:%S' ; echo ;  sleep 1; done"</span>
  <span class="c"># 혹은</span>
  <span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> zsh <span class="nt">-c</span> <span class="s2">"for i in {1..100};  do curl -s </span><span class="nv">$SVC1</span><span class="s2">:9000 | grep Hostname; done | sort | uniq -c | sort -nr"</span>
</code></pre></div>            </div>
          </li>
          <li>
            <p>파드 1개 삭제 후 확인</p>

            <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c"># (방안1) 파드3번 삭제 &gt;&gt; 서비스의 엔드포인트가 어떻게 변경되는지 확인 하자!, 지속적인 curl 접속 결과 확인!, for 문 실행 시 결과 확인!, 절체 시간(순단) 확인!</span>
  <span class="nv">$ </span>kubectl delete pod webpod3
        
  <span class="c"># (방안1) 결과 확인 후 다시 파드 3번 생성 &gt;&gt; 서비스 디스커버리!</span>
  <span class="nv">$ </span>kubectl apply <span class="nt">-f</span> 3pod.yaml
        
  <span class="nt">---------------------------------</span>
  <span class="c"># (방안2) 파드3번에 레이블 삭제</span>
  <span class="nv">$ </span>kubectl get pod <span class="nt">--show-labels</span>
        
  <span class="c">## 레이블(라벨)의 키값 바로 뒤에 하이픈(-) 입력 시 해당 레이블 삭제됨! &gt;&gt; 레이블과 셀렉터는 쿠버네티스 환경에서 매우 많이 활용된다!</span>
  <span class="nv">$ </span>kubectl label pod webpod3 app-
  <span class="nv">$ </span>kubectl get pod <span class="nt">--show-labels</span>
        
  <span class="c"># (방안2) 결과 확인 후 파드3번에 다시 레이블 생성</span>
  <span class="nv">$ </span>kubectl label pod webpod3 <span class="nv">app</span><span class="o">=</span>webpod
</code></pre></div>            </div>

            <ul>
              <li>
                <p>파드 삭제 전</p>

                <p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_18.png" alt="img.png" /></p>
              </li>
              <li>
                <p>파드 삭제 후</p>

                <p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_19.png" alt="img.png" /></p>
              </li>
              <li>
                <p>파드 다시 생성 후</p>

                <p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_20.png" alt="img.png" /></p>
              </li>
              <li>
                <p>레이블 삭제 후</p>

                <p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_21.png" alt="img.png" /></p>
              </li>
              <li>
                <p>레이블 복구 후</p>

                <p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_22.png" alt="img.png" /></p>
              </li>
            </ul>
          </li>
          <li>
            <p>파드가 삭제되고 복구 됨에 따라 서비스 엔드포인트에서 삭제되고, label selector 에 따라서도 엔드포인트에서 삭제되고 복구됨을 확인할 수 있었습니다.</p>
          </li>
        </ul>
      </li>
      <li>sessionAffinity: ClientIP
        <ul>
          <li><code class="language-plaintext highlighter-rouge">sessionAffinity: ClientIP</code> : 클라이언트가 접속한 목적지(파드)에 고정적인 접속을 지원하게 할 수 있습니다.</li>
          <li>
            <p>기본적으로 서비스는 파드에 랜덤으로 부하를 분산하지만 <code class="language-plaintext highlighter-rouge">sessionAffinity: ClientIP</code>를 통해 동일한 파드에 접속하도록 강제 할 수 있습니다.</p>

            <p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_23.png" alt="img.png" /></p>
          </li>
          <li>
            <p>설정 및 파드 접속 확인</p>

            <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c"># 기본 정보 확인</span>
  <span class="nv">$ </span>kubectl get svc svc-clusterip <span class="nt">-o</span> yaml
  <span class="nv">$ </span>kubectl get svc svc-clusterip <span class="nt">-o</span> yaml | <span class="nb">grep </span>sessionAffinity
  <span class="c"># =&gt;   sessionAffinity: None</span>
        
  <span class="c"># 반복 접속</span>
  <span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> zsh <span class="nt">-c</span> <span class="s2">"while true; do curl -s --connect-timeout 1 </span><span class="nv">$SVC1</span><span class="s2">:9000 | egrep 'Hostname|IP: 10|Remote'; date '+%Y-%m-%d %H:%M:%S' ; echo ;  sleep 1; done"</span>
  <span class="c"># =&gt; Hostname: webpod2</span>
  <span class="c">#    IP: 10.10.1.4</span>
  <span class="c">#    RemoteAddr: 10.10.0.7:57246</span>
  <span class="c">#    2024-09-01 12:25:49</span>
  <span class="c">#    </span>
  <span class="c">#    Hostname: webpod1</span>
  <span class="c">#    IP: 10.10.4.3</span>
  <span class="c">#    RemoteAddr: 10.10.0.7:57250</span>
  <span class="c">#    2024-09-01 12:25:50</span>
  <span class="c">#    </span>
  <span class="c">#    Hostname: webpod3</span>
  <span class="c">#    IP: 10.10.2.6</span>
  <span class="c">#    RemoteAddr: 10.10.0.7:57252</span>
  <span class="c">#    2024-09-01 12:25:51</span>
        
  <span class="c"># 현재는 랜덤으로 접속 됩니다.</span>
        
  <span class="c"># sessionAffinity: ClientIP 설정 변경</span>
  <span class="nv">$ </span>kubectl patch svc svc-clusterip <span class="nt">-p</span> <span class="s1">'{"spec":{"sessionAffinity":"ClientIP"}}'</span>
  <span class="c"># =&gt; service/svc-clusterip patched</span>
  <span class="c"># 혹은</span>
  <span class="c">## $ kubectl get svc svc-clusterip -o yaml | sed -e "s/sessionAffinity: None/sessionAffinity: ClientIP/" | kubectl apply -f -</span>
        
  <span class="c">#</span>
  <span class="nv">$ </span>kubectl get svc svc-clusterip <span class="nt">-o</span> yaml
  <span class="c"># =&gt; ...</span>
  <span class="c">#      sessionAffinity: ClientIP</span>
  <span class="c">#      sessionAffinityConfig:</span>
  <span class="c">#        clientIP:</span>
  <span class="c">#          timeoutSeconds: 10800</span>
  <span class="c">#    ...</span>
        
  <span class="c"># 클라이언트(TestPod) Shell 실행</span>
  <span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> zsh <span class="nt">-c</span> <span class="s2">"for i in {1..100};  do curl -s </span><span class="nv">$SVC1</span><span class="s2">:9000 | grep Hostname; done | sort | uniq -c | sort -nr"</span>
  <span class="c"># =&gt; 100 Hostname: webpod2</span>
  <span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> zsh <span class="nt">-c</span> <span class="s2">"for i in {1..1000}; do curl -s </span><span class="nv">$SVC1</span><span class="s2">:9000 | grep Hostname; done | sort | uniq -c | sort -nr"</span>
  <span class="c"># =&gt; 1000 Hostname: webpod2</span>
</code></pre></div>            </div>

            <ul>
              <li>sessionAffinity: ClientIP를 하면 spec.sessionAffinityConfig.clientIP.timeoutSeconds 시간동안 서비스를 통해 접속 되는 파드가 고정됨을 확인할 수 있었습니다.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>이상과 같이 ClusterIP 타입의 서비스를 확인해보았습니다.</li>
  <li>ClusterIP 타입의 서비스는 다음과 같은 단점이 있다고 합니다.
    <ul>
      <li>클러스터 외부에서는 서비스(ClusterIP)로 접속이 불가능합니다. ⇒ <strong>NodePort</strong> 타입으로 외부에서 접속 가능</li>
      <li>IPtables 는 파드에 대한 헬스체크 기능이 없어서 문제 있는 파드에 연결이 되는 경우가 있습니다. ⇒ 서비스 사용, 파드에 Readiness Probe 설정으로 파드 문제 시 서비스의 엔드포인트에서 제거되게 하자! ← 이 정도면 충분한가? 혹시 부족한 점이 없을까?</li>
      <li>서비스에 연동된 파드 갯수 퍼센트(%)로 <strong>랜덤 분산</strong> 방식, <strong>세션어피니티</strong> 이외에 <strong>다른 분산 방식 불가능합니다.</strong> ⇒ <strong>IPVS</strong> 경우 다양한 분산 방식(알고리즘) 가능
        <ul>
          <li>목적지 파드 다수가 있는 환경에서, 출발지 파드와 목적지 파드가 동일한 노드에 배치되어 있어도, 랜덤 분산으로 다른 노드에 목적지 파드로 연결 가능</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h4 id="nodeport-실습">NodePort 실습</h4>

<ul>
  <li>NodePort는 ClusterIP와 다르게 클러스터 외부에서도 접속 할 수 있습니다.</li>
  <li>컨트롤플레인을 포함한 모든 노드에 iptables rule이 적용되므로, 모든 노드에 NodePort로 접속시 iptables rule에 의해서 분산 접속이 됩니다.</li>
  <li>Node의 모든 Local IP (loopback을 포함한 각 호스트의 interface의 IP) 사용 가능하고 Local IP 지정도 가능합니다.</li>
  <li>쿠버네티스의 NodePort는 기본 30000~32767 포트에서 랜덤으로 지정됩니다.
    <ul>
      <li>
        <p>랜덤 포트 범위를 바꾸려면 다음과 같이  <code class="language-plaintext highlighter-rouge">/etc/kubernetes/manifests/kube-apiserver.yaml</code> 파일을 수정하여 kube-apiserver 의 파라메터에 <code class="language-plaintext highlighter-rouge">--service-node-port-range=시작포트-종료포트</code>를 변경하면됩니다. <a href="https://blog.frec.kr/cloud/modify_nodeport_range/">참고</a></p>

        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c"># /etc/kubernetes/manifests/kube-apiserver.yaml</span>
      
  ...
  spec:
    containers:
    - <span class="nb">command</span>:
      - kube-apiserver
      - <span class="nt">--authorization-mode</span><span class="o">=</span>Node,RBAC
      ...
      - <span class="nt">--service-node-port-range</span><span class="o">=</span>30000-50000
  ...
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>실습 구성
    <ul>
      <li>
        <p>목적지(backend) 디플로이먼트 파일 생성 : echo-deploy.yml</p>

        <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
  <span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
  <span class="na">metadata</span><span class="pi">:</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">deploy-echo</span>
  <span class="na">spec</span><span class="pi">:</span>
    <span class="na">replicas</span><span class="pi">:</span> <span class="m">3</span>
    <span class="na">selector</span><span class="pi">:</span>
      <span class="na">matchLabels</span><span class="pi">:</span>
        <span class="na">app</span><span class="pi">:</span> <span class="s">deploy-websrv</span>
    <span class="na">template</span><span class="pi">:</span>
      <span class="na">metadata</span><span class="pi">:</span>
        <span class="na">labels</span><span class="pi">:</span>
          <span class="na">app</span><span class="pi">:</span> <span class="s">deploy-websrv</span>
      <span class="na">spec</span><span class="pi">:</span>
        <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
        <span class="na">containers</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">kans-websrv</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">mendhak/http-https-echo</span>
          <span class="na">ports</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">8080</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>서비스(NodePort) 파일 생성 : svc-nodeport.yml</p>

        <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
  <span class="na">kind</span><span class="pi">:</span> <span class="err">*</span><span class="nv">*Service</span><span class="err">**</span>
  <span class="na">metadata</span><span class="pi">:</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">svc-nodeport</span>
  <span class="na">spec</span><span class="pi">:</span>
    <span class="na">ports</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">svc-webport</span>
        <span class="na">port</span><span class="pi">:</span> <span class="m">9000</span>        <span class="c1"># 서비스 ClusterIP 에 접속 시 사용하는 포트 port 를 의미</span>
        <span class="na">targetPort</span><span class="pi">:</span> <span class="m">8080</span>  <span class="c1"># 타킷 targetPort 는 서비스를 통해서 목적지 파드로 접속 시 해당 파드로 접속하는 포트를 의미</span>
    <span class="na">selector</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">deploy-websrv</span>
    <span class="na">**type</span><span class="pi">:</span> <span class="s">NodePort**</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>생성 및 확인</p>

        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c"># 생성</span>
  <span class="nv">$ </span>kubectl apply <span class="nt">-f</span> echo-deploy.yml,svc-nodeport.yml
  <span class="c"># =&gt; deployment.apps/deploy-echo created</span>
  <span class="c">#    service/svc-nodeport created</span>
      
  <span class="c"># 모니터링</span>
  <span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s1">'kubectl get pod -owide;echo; kubectl get svc,ep svc-nodeport'</span>
      
  <span class="c"># 확인</span>
  <span class="nv">$ </span>kubectl get deploy,pod <span class="nt">-o</span> wide
  <span class="c"># =&gt; NAME                          READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS    IMAGES                    SELECTOR</span>
  <span class="c">#    deployment.apps/deploy-echo   3/3     3            3           49s   kans-websrv   mendhak/http-https-echo   app=deploy-websrv</span>
  <span class="c">#    </span>
  <span class="c">#    NAME                               READY   STATUS    RESTARTS   AGE    IP          NODE                  NOMINATED NODE   READINESS GATES</span>
  <span class="c">#    pod/deploy-echo-5c689d5454-dxf2t   1/1     Running   0          49s    10.10.4.4   myk8s-worker          &lt;none&gt;           &lt;none&gt;</span>
  <span class="c">#    pod/deploy-echo-5c689d5454-rbgcp   1/1     Running   0          49s    10.10.1.5   myk8s-worker2         &lt;none&gt;           &lt;none&gt;</span>
  <span class="c">#    pod/deploy-echo-5c689d5454-wppr8   1/1     Running   0          49s    10.10.2.7   myk8s-worker3         &lt;none&gt;           &lt;none&gt;</span>
      
  <span class="c"># 아래 31791은 서비스(NodePort) 정보!</span>
  <span class="nv">$ </span>kubectl get svc svc-nodeport
  <span class="c"># =&gt; NAME           TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE</span>
  <span class="c">#    svc-nodeport   NodePort   10.200.1.169   &lt;none&gt;        9000:31791/TCP   69s</span>
      
  <span class="nv">$ </span>kubectl get endpoints svc-nodeport
  <span class="c"># =&gt; NAME           ENDPOINTS                                      AGE</span>
  <span class="c">#    svc-nodeport   10.10.1.5:8080,10.10.2.7:8080,10.10.4.4:8080   85s</span>
      
  <span class="c"># Port , TargetPort , NodePort 각각의 차이점의 의미를 알자!</span>
  <span class="nv">$ </span>kubectl describe svc svc-nodeport
  <span class="c"># =&gt; Name:                     svc-nodeport</span>
  <span class="c">#    Namespace:                default</span>
  <span class="c">#    Labels:                   &lt;none&gt;</span>
  <span class="c">#    Annotations:              &lt;none&gt;</span>
  <span class="c">#    Selector:                 app=deploy-websrv</span>
  <span class="c">#    Type:                     NodePort</span>
  <span class="c">#    IP Family Policy:         SingleStack</span>
  <span class="c">#    IP Families:              IPv4</span>
  <span class="c">#    IP:                       10.200.1.169</span>
  <span class="c">#    IPs:                      10.200.1.169</span>
  <span class="c">#    Port:                     svc-webport  9000/TCP     &lt;&lt;ClusterIP와 동일하게 동작하는 클러스터 내부에서 사용하는 포트&gt;&gt;</span>
  <span class="c">#    TargetPort:               8080/TCP                  &lt;&lt;파드의 컨테이너의 포트&gt;&gt;</span>
  <span class="c">#    NodePort:                 svc-webport  31791/TCP    &lt;&lt;각 Node에서 Listening 하는 nodePort&gt;&gt;</span>
  <span class="c">#    Endpoints:                10.10.1.5:8080,10.10.2.7:8080,10.10.4.4:8080    &lt;&lt;Port Forwarding 대상이 되는 파드의 엔드포인트 파드IP:파드Port&gt;&gt;</span>
  <span class="c">#    Session Affinity:         None</span>
  <span class="c">#    External Traffic Policy:  Cluster &lt;&lt;부하 분산방식&gt;&gt;</span>
  <span class="c">#    Events:                   &lt;none&gt;</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>2.3. 서비스 접속 확인
    <ul>
      <li>
        <p>NodePort의 서비스 접속을 통한 통신의 흐름</p>

        <p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_24.png" alt="img.png" /></p>

        <ul>
          <li>Client 가상 머신(192.168.10.200)에서 컨트롤 플레인 IP(192.168.10.10)의 nodePort 접속을 시도합니다.</li>
          <li>nodePort는 서비스(NodePort) 생성시에 할당된 랜덤포트가 사용 됩니다.</li>
          <li>컨트롤 플레인의 iptables의 NAT 테이블의 규칙과 매칭되어 목적지 IP와 목적지 Port는 변환 됩니다. 목적지 IP는 app=deploy-websrv 레이블을 가지고 있는 파드 3개가 대상이 되며, 랜덤 부하분산이 선택됩니다.</li>
        </ul>
      </li>
      <li>
        <p>실습을 통해 위의 과정을 확인해보겠습니다. 단 현재 실습환경에서는 컨트롤 플레인에는 파드가 없으므로 위의 설명과는 다르게 워커노드의 파드를 접속하는것으로 실습하겠습니다.</p>

        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c"># NodePort 확인 : 아래 NodePort 는 범위내 랜덤 할당으로 실습 환경마다 다릅니다</span>
  <span class="nv">$ </span>kubectl get service svc-nodeport <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.spec.ports[0].nodePort}'</span>
  <span class="c"># =&gt; 31791</span>
      
  <span class="c"># NodePort 를 변수에 지정</span>
  <span class="nv">$ NPORT</span><span class="o">=</span><span class="si">$(</span>kubectl get service svc-nodeport <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.spec.ports[0].nodePort}'</span><span class="si">)</span>
  <span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$NPORT</span>
  <span class="c"># =&gt; 31791</span>
      
  <span class="c"># 현재 k8s 버전에서는 포트 Listen 되지 않고, iptables rules 처리됨</span>
  <span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> ss <span class="nt">-tlnp</span><span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
  <span class="c"># =&gt; &gt;&gt; node myk8s-control-plane &lt;&lt;</span>
  <span class="c">#    State  Recv-Q Send-Q Local Address:Port  Peer Address:PortProcess                                  </span>
  <span class="c">#    LISTEN 0      4096       127.0.0.1:2381       0.0.0.0:*    users:(("etcd",pid=710,fd=15))          </span>
  <span class="c">#    ... nodePort인 31791를 LISTEN하는 건이 없음</span>
  <span class="c">#</span>
  <span class="c">#    &gt;&gt; node myk8s-worker &lt;&lt;</span>
  <span class="c">#    State  Recv-Q Send-Q Local Address:Port  Peer Address:PortProcess                              </span>
  <span class="c">#    LISTEN 0      4096      127.0.0.11:35033      0.0.0.0:*                                        </span>
  <span class="c">#    ... nodePort인 31791를 LISTEN하는 건이 없음</span>
  <span class="c">#    </span>
  <span class="c">#    &gt;&gt; node myk8s-worker2 &lt;&lt;</span>
  <span class="c">#    State  Recv-Q Send-Q Local Address:Port  Peer Address:PortProcess                              </span>
  <span class="c">#    LISTEN 0      4096      127.0.0.11:45927      0.0.0.0:*                                        </span>
  <span class="c">#    ... nodePort인 31791를 LISTEN하는 건이 없음</span>
  <span class="c">#    </span>
  <span class="c">#    &gt;&gt; node myk8s-worker3 &lt;&lt;</span>
  <span class="c">#    State  Recv-Q Send-Q Local Address:Port  Peer Address:PortProcess                              </span>
  <span class="c">#    LISTEN 0      4096       127.0.0.1:10248      0.0.0.0:*    users:(("kubelet",pid=262,fd=19))   </span>
  <span class="c">#    ... nodePort인 31791를 LISTEN하는 건이 없음</span>
      
  <span class="c">## (참고) 아래처럼 예전 k8s 환경에서 Service(NodePort) 생성 시, TCP Port Listen 되었었음</span>
  <span class="c"># $ root@k8s-m:~# ss -4tlnp | egrep "(Process|$NPORT)"</span>
  <span class="c"># State     Recv-Q    Send-Q        Local Address:Port        Peer Address:Port   Process</span>
  <span class="c"># LISTEN    0         4096                0.0.0.0:30466            0.0.0.0:*       users:(("kube-proxy",pid=8661,fd=10))</span>
      
  <span class="c"># 파드 로그 실시간 확인 (웹 파드에 접속자의 IP가 출력)</span>
  <span class="nv">$ </span>kubectl logs <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>deploy-websrv <span class="nt">-f</span>
  <span class="c"># =&gt; Listening on ports 8080 for http, and 8443 for https.</span>
  <span class="c">#    ...</span>
  <span class="c">#    ::ffff:172.23.0.2 - - [26/Sep/2024:04:35:00 +0000] "GET / HTTP/1.1" 200 396 "-" "curl/7.88.1"</span>
  <span class="c">#    ...</span>
      
  <span class="c"># 외부 클라이언트(mypc 컨테이너)에서 접속 시도를 해보자</span>
      
  <span class="c"># 노드의 IP와 NodePort를 변수에 지정</span>
  <span class="c">## CNODE=&lt;컨트롤플레인노드의 IP주소&gt;</span>
  <span class="c">## NODE1=&lt;노드1의 IP주소&gt;</span>
  <span class="c">## NODE2=&lt;노드2의 IP주소&gt;</span>
  <span class="c">## NODE3=&lt;노드3의 IP주소&gt;</span>
  <span class="nv">$ CNODE</span><span class="o">=</span>172.23.0.2
  <span class="nv">$ NODE1</span><span class="o">=</span>172.23.0.4
  <span class="nv">$ NODE2</span><span class="o">=</span>172.23.0.5
  <span class="nv">$ NODE3</span><span class="o">=</span>172.23.0.3
      
  <span class="nv">$ NPORT</span><span class="o">=</span><span class="si">$(</span>kubectl get service svc-nodeport <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.spec.ports[0].nodePort}'</span><span class="si">)</span>
  <span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$NPORT</span>
      
  <span class="c"># 서비스(NodePort) 부하분산 접속 확인</span>
  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc curl <span class="nt">-s</span> <span class="nv">$CNODE</span>:<span class="nv">$NPORT</span> | jq <span class="c"># headers.host 주소는 왜 그런거죠?</span>
  <span class="c"># =&gt; {</span>
  <span class="c">#      "path": "/",</span>
  <span class="c">#      "headers": {</span>
  <span class="c">#        "host": "172.23.0.2:31791",  &lt;&lt;여기의 headers.host는 요청하는 url의 주소인데, 우리가 $CNODE(컨트롤플레인의 IP)의 url로 접속했기 때문입니다.&gt;&gt;</span>
  <span class="c">#        "user-agent": "curl/8.7.1",</span>
  <span class="c">#        "accept": "*/*"</span>
  <span class="c">#      },</span>
  <span class="c">#      "method": "GET",</span>
  <span class="c">#      "body": "",</span>
  <span class="c">#      "fresh": false,</span>
  <span class="c">#      "hostname": "172.23.0.2",   &lt;&lt;이 hostname과&gt;&gt;</span>
  <span class="c">#      "ip": "::ffff:172.23.0.2",  &lt;&lt;이 ip는 접속하는 클라이언트의 ip인데 부하분산 과정에서 목적지가 Local Pod가 아닌 경우 Node IP로 POSTROUTING(SNAT) 되기 때문입니다.&gt;&gt;</span>
  <span class="c">#      "ips": [],</span>
  <span class="c">#      "protocol": "http",</span>
  <span class="c">#      "query": {},</span>
  <span class="c">#      "subdomains": [],</span>
  <span class="c">#      "xhr": false,</span>
  <span class="c">#      "os": {</span>
  <span class="c">#        "hostname": "deploy-echo-5c689d5454-dxf2t"</span>
  <span class="c">#      },</span>
  <span class="c">#      "connection": {}</span>
  <span class="c">#    }</span>
      
  <span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="nv">$CNODE</span> <span class="nv">$NODE1</span> <span class="nv">$NODE2</span> <span class="nv">$NODE3</span> <span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> mypc curl <span class="nt">-s</span> <span class="nv">$i</span>:<span class="nv">$NPORT</span><span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
  <span class="c"># =&gt; &gt;&gt; node 172.23.0.2 &lt;&lt;</span>
  <span class="c">#    {</span>
  <span class="c">#      "headers": {</span>
  <span class="c">#        "host": "172.23.0.2:31791",</span>
  <span class="c">#        ...</span>
  <span class="c">#      },</span>
  <span class="c">#      ...</span>
  <span class="c">#      "hostname": "172.23.0.2",</span>
  <span class="c">#      "ip": "::ffff:172.23.0.2",</span>
  <span class="c">#      ...</span>
  <span class="c">#      "os": {</span>
  <span class="c">#        "hostname": "deploy-echo-5c689d5454-dxf2t" </span>
  <span class="c">#      }</span>
  <span class="c">#    }</span>
  <span class="c">#    &gt;&gt; node 172.23.0.4 &lt;&lt;</span>
  <span class="c">#    {</span>
  <span class="c">#      "headers": {</span>
  <span class="c">#        "host": "172.23.0.4:31791",</span>
  <span class="c">#        ...</span>
  <span class="c">#      },</span>
  <span class="c">#      ...</span>
  <span class="c">#      "hostname": "172.23.0.4",</span>
  <span class="c">#      "ip": "::ffff:10.10.4.1",</span>
  <span class="c">#      ...</span>
  <span class="c">#      "os": {</span>
  <span class="c">#        "hostname": "deploy-echo-5c689d5454-dxf2t"</span>
  <span class="c">#      }</span>
  <span class="c">#    }</span>
  <span class="c">#    &gt;&gt; node 172.23.0.5 &lt;&lt;</span>
  <span class="c">#    {</span>
  <span class="c">#      "headers": {</span>
  <span class="c">#        "host": "172.23.0.5:31791",</span>
  <span class="c">#        ...</span>
  <span class="c">#      },</span>
  <span class="c">#      ...</span>
  <span class="c">#      "hostname": "172.23.0.5",</span>
  <span class="c">#      "ip": "::ffff:10.10.1.1",</span>
  <span class="c">#      ...</span>
  <span class="c">#      "os": {</span>
  <span class="c">#        "hostname": "deploy-echo-5c689d5454-rbgcp"</span>
  <span class="c">#      }</span>
  <span class="c">#    }</span>
  <span class="c">#    &gt;&gt; node 172.23.0.3 &lt;&lt;</span>
  <span class="c">#    {</span>
  <span class="c">#      "headers": {</span>
  <span class="c">#        "host": "172.23.0.3:31791",</span>
  <span class="c">#        ...</span>
  <span class="c">#      },</span>
  <span class="c">#      ...</span>
  <span class="c">#      "hostname": "172.23.0.3",</span>
  <span class="c">#      "ip": "::ffff:10.10.2.1",</span>
  <span class="c">#      ...</span>
  <span class="c">#      "os": {</span>
  <span class="c">#        "hostname": "deploy-echo-5c689d5454-wppr8"</span>
  <span class="c">#      }</span>
  <span class="c">#    }</span>
      
  <span class="c"># 컨트롤플레인 노드에는 목적지 파드가 없는데도, 접속을 받아줍니다! 이유는 서비스(nodePort)의 endpoint로 로드밸런싱 되기 때문입니다.</span>
  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc zsh <span class="nt">-c</span> <span class="s2">"for i in {1..100}; do curl -s </span><span class="nv">$CNODE</span><span class="s2">:</span><span class="nv">$NPORT</span><span class="s2"> | grep hostname; done | sort | uniq -c | sort -nr"</span>
  <span class="c"># =&gt;     100   "hostname": "172.23.0.2",</span>
  <span class="c">#         37     "hostname": "deploy-echo-5c689d5454-wppr8"</span>
  <span class="c">#         33     "hostname": "deploy-echo-5c689d5454-rbgcp"</span>
  <span class="c">#         30     "hostname": "deploy-echo-5c689d5454-dxf2t"</span>
  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc zsh <span class="nt">-c</span> <span class="s2">"for i in {1..100}; do curl -s </span><span class="nv">$NODE1</span><span class="s2">:</span><span class="nv">$NPORT</span><span class="s2"> | grep hostname; done | sort | uniq -c | sort -nr"</span>
  <span class="c"># =&gt;     100   "hostname": "172.23.0.4",</span>
  <span class="c">#         40     "hostname": "deploy-echo-5c689d5454-wppr8"</span>
  <span class="c">#         32     "hostname": "deploy-echo-5c689d5454-dxf2t"</span>
  <span class="c">#         28     "hostname": "deploy-echo-5c689d5454-rbgcp"$ docker exec -it mypc zsh -c "for i in {1..100}; do curl -s $NODE2:$NPORT | grep hostname; done | sort | uniq -c | sort -nr"</span>
  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc zsh <span class="nt">-c</span> <span class="s2">"for i in {1..100}; do curl -s </span><span class="nv">$NODE3</span><span class="s2">:</span><span class="nv">$NPORT</span><span class="s2"> | grep hostname; done | sort | uniq -c | sort -nr"</span>
  <span class="c"># =&gt;     100   "hostname": "172.23.0.3",</span>
  <span class="c">#         43     "hostname": "deploy-echo-5c689d5454-wppr8"</span>
  <span class="c">#         34     "hostname": "deploy-echo-5c689d5454-dxf2t"</span>
  <span class="c">#         23     "hostname": "deploy-echo-5c689d5454-rbgcp"</span>
  <span class="c"># 아래 반복 접속 실행 해두자</span>
  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc zsh <span class="nt">-c</span> <span class="s2">"while true; do curl -s --connect-timeout 1 </span><span class="nv">$CNODE</span><span class="s2">:</span><span class="nv">$NPORT</span><span class="s2"> | grep hostname; date '+%Y-%m-%d %H:%M:%S' ; echo ;  sleep 1; done"</span>
      
  <span class="c"># NodePort 서비스는 ClusterIP 를 포함</span>
  <span class="c"># CLUSTER-IP:PORT 로 접속 가능! &lt;- 컨트롤노드에서 아래 실행 해보자</span>
  <span class="nv">$ </span>kubectl get svc svc-nodeport
  <span class="c"># =&gt; NAME           TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE</span>
  <span class="c">#    svc-nodeport   NodePort   10.200.1.169   &lt;none&gt;        9000:31791/TCP   51m</span>
      
  <span class="nv">$ CIP</span><span class="o">=</span><span class="si">$(</span>kubectl get service svc-nodeport <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.spec.clusterIP}"</span><span class="si">)</span>
  <span class="nv">$ CIPPORT</span><span class="o">=</span><span class="si">$(</span>kubectl get service svc-nodeport <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.spec.ports[0].port}"</span><span class="si">)</span>
  <span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$CIP</span> <span class="nv">$CIPPORT</span>
  <span class="c"># =&gt; 10.200.1.169 9000</span>
  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane curl <span class="nt">-s</span> <span class="nv">$CIP</span>:<span class="nv">$CIPPORT</span> | jq
  <span class="c"># =&gt; {</span>
  <span class="c">#      "path": "/",</span>
  <span class="c">#      "headers": {</span>
  <span class="c">#        "host": "10.200.1.169:9000",</span>
  <span class="c">#        "user-agent": "curl/7.88.1",</span>
  <span class="c">#        "accept": "*/*"</span>
  <span class="c">#      },</span>
  <span class="c">#      "method": "GET",</span>
  <span class="c">#      "body": "",</span>
  <span class="c">#      "fresh": false,</span>
  <span class="c">#      "hostname": "10.200.1.169",</span>
  <span class="c">#      "ip": "::ffff:172.23.0.2",</span>
  <span class="c">#      "ips": [],</span>
  <span class="c">#      "protocol": "http",</span>
  <span class="c">#      "query": {},</span>
  <span class="c">#      "subdomains": [],</span>
  <span class="c">#      "xhr": false,</span>
  <span class="c">#      "os": {</span>
  <span class="c">#        "hostname": "deploy-echo-5c689d5454-rbgcp"</span>
  <span class="c">#      },</span>
  <span class="c">#      "connection": {}</span>
  <span class="c">#    }</span>
      
  <span class="c"># mypc에서 CLUSTER-IP:PORT 로 접속 가능할까?</span>
  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc curl <span class="nt">-s</span> <span class="nv">$CIP</span>:<span class="nv">$CIPPORT</span>
  <span class="c"># =&gt; (에러)</span>
      
  <span class="c"># mypc에서 cluster ip port로의 접속은 불가능합니다. mypc는 kubernetes 클러스터 내부에 있지 않기 때문입니다.</span>
      
  <span class="c"># (옵션) 노드에서 Network Connection</span>
  <span class="nv">$ </span>conntrack <span class="nt">-E</span>
  <span class="c"># =&gt;     [NEW] tcp      6 120 SYN_SENT src=172.23.0.2 dst=10.10.4.4 sport=36907 dport=8080 [UNREPLIED] src=10.10.4.4 dst=172.23.0.2 sport=8080 dport=36907</span>
  <span class="c">#     [UPDATE] tcp      6 60 SYN_RECV src=172.23.0.2 dst=10.10.4.4 sport=36907 dport=8080 src=10.10.4.4 dst=172.23.0.2 sport=8080 dport=36907</span>
  <span class="c">#     [UPDATE] tcp      6 86400 ESTABLISHED src=172.23.0.2 dst=10.10.4.4 sport=36907 dport=8080 src=10.10.4.4 dst=172.23.0.2 sport=8080 dport=36907 [ASSURED]</span>
  <span class="c">#     [UPDATE] tcp      6 120 FIN_WAIT src=172.23.0.2 dst=10.10.4.4 sport=36907 dport=8080 src=10.10.4.4 dst=172.23.0.2 sport=8080 dport=36907 [ASSURED]</span>
  <span class="c">#     [UPDATE] tcp      6 30 LAST_ACK src=172.23.0.2 dst=10.10.4.4 sport=36907 dport=8080 src=10.10.4.4 dst=172.23.0.2 sport=8080 dport=36907 [ASSURED]</span>
  <span class="c">#     [UPDATE] tcp      6 120 TIME_WAIT src=172.23.0.2 dst=10.10.4.4 sport=36907 dport=8080 src=10.10.4.4 dst=172.23.0.2 sport=8080 dport=36907 [ASSURED]</span>
  <span class="c">#      ...</span>
  <span class="c"># SNAT나 빠른 iptables 룰 처리등을 위해 접속 정보가 추적됨을 알 수 있습니다.</span>
      
  <span class="nv">$ </span>conntrack <span class="nt">-L</span> <span class="nt">--any-nat</span>
</code></pre></div>        </div>
      </li>
      <li>파드에서 바라본 클라이언트의 주소가 실제 클라이언트가 아닌 node의 ip로 표시되는데 그 이유를 살펴보겠습니다.
        <ul>
          <li>
            <p>컨트롤 플레인에서 iptables의 nat 테이블의 KUBE-POSTROUTING 룰을 확인하면 다음과 같습니다.</p>

            <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="nv">$ </span>iptables <span class="nt">-v</span> <span class="nt">--numeric</span> <span class="nt">--table</span> nat <span class="nt">--list</span> 
  <span class="c"># =&gt; Chain POSTROUTING (policy ACCEPT 5813 packets, 349K bytes)</span>
  <span class="c">#     pkts bytes target     prot opt in     out     source               destination         </span>
  <span class="c">#    37925 2276K KUBE-POSTROUTING  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes postrouting rules */</span>
  <span class="c">#    ...</span>
  <span class="c">#    Chain KUBE-POSTROUTING (1 references)</span>
  <span class="c">#     pkts bytes target     prot opt in     out     source               destination</span>
  <span class="c">#     5343  321K RETURN     0    --  *      *       0.0.0.0/0            0.0.0.0/0            mark match ! 0x4000/0x4000</span>
  <span class="c">#     1265 75900 MARK       0    --  *      *       0.0.0.0/0            0.0.0.0/0            MARK xor 0x4000</span>
  <span class="c">#     1265 75900 MASQUERADE  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service traffic requiring SNAT */ random-fully</span>
</code></pre></div>            </div>

            <p>확인 결과 POSTROUTING시 KUBE-POSTROUTING을 통해 SNAT 되고 있음을 알 수 있습니다.</p>
          </li>
        </ul>
      </li>
      <li>
        <p>외부 클라이언트 → 서비스(NodePort) 접속 시 : 3개의 목적지(backend) 파드로 <strong>랜덤 부하 분산</strong> 접속됨을 확인해보겠습니다.</p>

        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$CNODE</span>:<span class="nv">$NPORT</span> | <span class="nb">grep hostname</span><span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
  <span class="c"># =&gt;    100   "hostname": "172.23.0.2",</span>
  <span class="c">#        42     "hostname": "deploy-echo-5c689d5454-wppr8"</span>
  <span class="c">#        31     "hostname": "deploy-echo-5c689d5454-dxf2t"</span>
  <span class="c">#        27     "hostname": "deploy-echo-5c689d5454-rbgcp"</span>
      
  <span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$NODE1</span>:<span class="nv">$NPORT</span> | <span class="nb">grep hostname</span><span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
  <span class="c"># =&gt;     100   "hostname": "172.23.0.4",</span>
  <span class="c">#         37     "hostname": "deploy-echo-5c689d5454-dxf2t"</span>
  <span class="c">#         34     "hostname": "deploy-echo-5c689d5454-rbgcp"</span>
  <span class="c">#         29     "hostname": "deploy-echo-5c689d5454-wppr8"</span>
      
  <span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$NODE2</span>:<span class="nv">$NPORT</span> | <span class="nb">grep hostname</span><span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
  <span class="c"># =&gt;     100   "hostname": "172.23.0.5",</span>
  <span class="c">#         41     "hostname": "deploy-echo-5c689d5454-wppr8"</span>
  <span class="c">#         32     "hostname": "deploy-echo-5c689d5454-rbgcp"</span>
  <span class="c">#         27     "hostname": "deploy-echo-5c689d5454-dxf2t"</span>
      
  <span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$NODE3</span>:<span class="nv">$NPORT</span> | <span class="nb">grep hostname</span><span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
  <span class="c"># =&gt;     100   "hostname": "172.23.0.3",</span>
  <span class="c">#         39     "hostname": "deploy-echo-5c689d5454-dxf2t"</span>
  <span class="c">#         34     "hostname": "deploy-echo-5c689d5454-wppr8"</span>
  <span class="c">#         27     "hostname": "deploy-echo-5c689d5454-rbgcp"</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>웹 파드에서  log를 통해 접속자의 IP 확인시 외부 클라이언트 IP가 아닌, 노드의 IP로 SNAT 되어서 접속됨을 확인할 수 있습니다.</p>

        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="nv">$ </span>kubectl logs <span class="nt">-f</span> deploy-echo-5c689d5454-dxf2t | <span class="nb">grep </span>HTTP
  <span class="c"># =&gt; ::ffff:172.23.0.3 - - [01/Sep/2024:05:28:36 +0000] "GET / HTTP/1.1" 200 398 "-" "curl/7.88.1"</span>
  <span class="c">#    ::ffff:172.23.0.3 - - [01/Sep/2024:05:28:36 +0000] "GET / HTTP/1.1" 200 398 "-" "curl/7.88.1"</span>
  <span class="c">#    ::ffff:172.23.0.3 - - [01/Sep/2024:05:28:36 +0000] "GET / HTTP/1.1" 200 398 "-" "curl/7.88.1"</span>
  <span class="c">#    ::ffff:172.23.0.3 - - [01/Sep/2024:05:28:36 +0000] "GET / HTTP/1.1" 200 398 "-" "curl/7.88.1"</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>2.4.  IPTABLES 정책 확인
    <ul>
      <li>
        <p>iptables 정책 적용 순서는 다음과 같습니다.</p>

        <p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_25.png" alt="img.png" /></p>

        <ul>
          <li>PREROUTING → KUBE-SERVICES → KUBE-NODEPORTS → <strong>KUBE-EXT-#(MARK)</strong> → KUBE-SVC-# → KUBE-SEP-#  ⇒ KUBE-POSTROUTING (MASQUERADE) <strong>**</strong></li>
          <li><code class="language-plaintext highlighter-rouge">KUBE-EXT-#(MARK)</code> 규칙 과정이 추가됨을 확인할 수 있습니다.</li>
        </ul>
      </li>
      <li>기본 규칙은 ClusterIP 서비스 동작 규칙과 거의 같으며 차이점은 KUBE-NODEPORTS, KUBE-MARK-MASK, KUBE-POSTROUTING 체인이  다릅니다. 핵심 내용은 NodePort에 매칭시 마킹 후 출발지 IP를 해당 노드에 있는 네트워크 IP로 변환(MASQUERADE : SNAT)하여 목적지 파드로 전달합니다.</li>
      <li>
        <p>실습을 통해 iptables 정책에 대해 확인해보겠습니다.  컨트롤플레인에서 실습을 진행하겠습니다.</p>

        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane bash
  <span class="nt">----------------------------------------</span>
      
  <span class="c"># 패킷 카운트 초기화</span>
  <span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">--zero</span>
      
  <span class="c"># PREROUTING 정보 확인</span>
  <span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span> | <span class="nb">grep </span>PREROUTING
  <span class="c"># =&gt; -P PREROUTING ACCEPT</span>
  <span class="c">#    -A PREROUTING -m comment --comment "kubernetes service portals" -j KUBE-SERVICES</span>
  <span class="c">#    -A PREROUTING -d 172.23.0.1/32 -j DOCKER_OUTPUT</span>
      
  <span class="c"># 외부 클라이언트가 노드IP:NodePort 로 접속하기 때문에 --dst-type LOCAL 에 매칭되어서 -j KUBE-NODEPORTS 로 점프!</span>
  <span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span> | <span class="nb">grep </span>KUBE-SERVICES
  <span class="c"># =&gt; ...</span>
  <span class="c">#    -A KUBE-SERVICES -m comment --comment "kubernetes service nodeports; NOTE: this must be the last rule in this chain" -m addrtype --dst-type LOCAL -j KUBE-NODEPORTS</span>
      
  <span class="c"># KUBE-NODEPORTS 에서 KUBE-EXT-# 로 점프!</span>
  <span class="c">## -m nfacct --nfacct-name localhost_nps_accepted_pkts 추가됨 : 패킷 flow 카운팅 - 카운트 이름 지정 </span>
  <span class="nv">$ NPORT</span><span class="o">=</span><span class="si">$(</span>kubectl get service svc-nodeport <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.spec.ports[0].nodePort}'</span><span class="si">)</span>
  <span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$NPORT</span>
  <span class="c"># =&gt; 31791</span>
      
  <span class="c"># $ iptables -t nat -S | grep KUBE-NODEPORTS | grep &lt;NodePort&gt;</span>
  <span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span> | <span class="nb">grep </span>KUBE-NODEPORTS | <span class="nb">grep</span> <span class="nv">$NPORT</span>
  <span class="c"># =&gt; -A KUBE-NODEPORTS -p tcp -m comment --comment "default/svc-nodeport:svc-webport" -m tcp --dport 31791 -j KUBE-EXT-VTR7MTHHNMFZ3OFS</span>
      
  <span class="c"># (참고) nfacct 확인</span>
  <span class="nv">$ </span>nfacct list
  <span class="c">## nfacct flush # 초기화</span>
      
  <span class="c">## KUBE-EXT-# 에서 'KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000' 마킹 및 KUBE-SVC-# 로 점프!</span>
  <span class="c"># docker exec -it mypc zsh -c "while true; do curl -s --connect-timeout 1 $CNODE:$NPORT | grep hostname; date '+%Y-%m-%d %H:%M:%S' ; echo ;  sleep 1; done" 반복 접속 후 아래 확인</span>
  <span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s1">'iptables -v --numeric --table nat --list KUBE-EXT-VTR7MTHHNMFZ3OFS'</span>
  <span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span> | <span class="nb">grep</span> <span class="s2">"A KUBE-EXT-VTR7MTHHNMFZ3OFS"</span>
  <span class="c"># =&gt; -A KUBE-EXT-VTR7MTHHNMFZ3OFS -m comment --comment "masquerade traffic for default/svc-nodeport:svc-webport external destinations" -j KUBE-MARK-MASQ</span>
  <span class="c">#    -A KUBE-EXT-VTR7MTHHNMFZ3OFS -j KUBE-SVC-VTR7MTHHNMFZ3OFS</span>
      
  <span class="c"># iptables -t nat -S | grep "A KUBE-MARK-MASQ" | sed -e 's/^/#    /' -e '1s/^#    /# =&gt; /'</span>
  <span class="c"># =&gt; -A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000        # 0x4000/0x4000으로 마킹하는 룰</span>
      
  <span class="c"># KUBE-SVC-# 이후 과정은 Cluster-IP 와 동일! : 3개의 파드로 DNAT 되어서 전달</span>
  <span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span> | <span class="nb">grep</span> <span class="s2">"A KUBE-SVC-VTR7MTHHNMFZ3OFS -"</span>
  <span class="c"># =&gt; -A KUBE-SVC-VTR7MTHHNMFZ3OFS -m comment --comment "default/svc-nodeport:svc-webport -&gt; 10.10.1.5:8080" -m statistic --mode random --probability 0.33333333349 -j KUBE-SEP-SESYGQFRQSLJQZ6Q</span>
  <span class="c">#    -A KUBE-SVC-VTR7MTHHNMFZ3OFS -m comment --comment "default/svc-nodeport:svc-webport -&gt; 10.10.2.7:8080" -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-FBJG45W6XHLV2NA6</span>
  <span class="c">#    -A KUBE-SVC-VTR7MTHHNMFZ3OFS -m comment --comment "default/svc-nodeport:svc-webport -&gt; 10.10.4.4:8080" -j KUBE-SEP-GEQNJ6BO5AOHB6LH</span>
      
  <span class="c"># POSTROUTING 정보 확인</span>
  <span class="c"># 마킹되어 있어서 출발지IP를 접속한 노드의 IP 로 SNAT(MASQUERADE) 처리함! , 최초 출발지Port는 랜덤Port 로 변경</span>
  <span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span> | <span class="nb">grep</span> <span class="s2">"A KUBE-POSTROUTING"</span>
  <span class="c"># =&gt; -A KUBE-POSTROUTING -m mark ! --mark 0x4000/0x4000 -j RETURN   # 0x4000/0x4000 되어 있으니 여기에 매칭되지 않고 아래 Rule로 내려감</span>
  <span class="c">#    -A KUBE-POSTROUTING -j MARK --set-xmark 0x4000/0x0</span>
  <span class="c">#    -A KUBE-POSTROUTING -m comment --comment "kubernetes service traffic requiring SNAT" -j MASQUERADE --random-fully</span>
      
  <span class="c"># docker exec -it mypc zsh -c "while true; do curl -s --connect-timeout 1 $CNODE:$NPORT | grep hostname; date '+%Y-%m-%d %H:%M:%S' ; echo ;  sleep 1; done" 반복 접속 후 아래 확인</span>
  <span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s1">'iptables -v --numeric --table nat --list KUBE-POSTROUTING;echo;iptables -v --numeric --table nat --list POSTROUTING'</span>
      
  <span class="nv">$ </span><span class="nb">exit</span>
  <span class="nt">----------------------------------------</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>서비스 (NodePort) 생성 시 kube-proxy에 의해서 iptables 규칙이 모든 노드에 추가되는지 확인해보겠습니다.</p>

        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c">#</span>
  <span class="nv">$ NPORT</span><span class="o">=</span><span class="si">$(</span>kubectl get service svc-nodeport <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.spec.ports[0].nodePort}'</span><span class="si">)</span>
  <span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$NPORT</span> 
  <span class="c"># =&gt; 31791</span>
  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane iptables <span class="nt">-t</span> nat <span class="nt">-S</span> | <span class="nb">grep </span>KUBE-NODEPORTS | <span class="nb">grep</span> <span class="nv">$NPORT</span>
  <span class="c"># =&gt; -A KUBE-NODEPORTS -p tcp -m comment --comment "default/svc-nodeport:svc-webport" -m tcp --dport 31791 -j KUBE-EXT-VTR7MTHHNMFZ3OFS</span>
      
  <span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> iptables <span class="nt">-t</span> nat <span class="nt">-S</span> | <span class="nb">grep </span>KUBE-NODEPORTS | <span class="nb">grep</span> <span class="nv">$NPORT</span><span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
  <span class="c"># =&gt; &gt;&gt; node myk8s-control-plane &lt;&lt;</span>
  <span class="c">#    -A KUBE-NODEPORTS -p tcp -m comment --comment "default/svc-nodeport:svc-webport" -m tcp --dport 31791 -j KUBE-EXT-VTR7MTHHNMFZ3OFS</span>
  <span class="c">#    </span>
  <span class="c">#    &gt;&gt; node myk8s-worker &lt;&lt;</span>
  <span class="c">#    -A KUBE-NODEPORTS -p tcp -m comment --comment "default/svc-nodeport:svc-webport" -m tcp --dport 31791 -j KUBE-EXT-VTR7MTHHNMFZ3OFS</span>
  <span class="c">#    </span>
  <span class="c">#    &gt;&gt; node myk8s-worker2 &lt;&lt;</span>
  <span class="c">#    -A KUBE-NODEPORTS -p tcp -m comment --comment "default/svc-nodeport:svc-webport" -m tcp --dport 31791 -j KUBE-EXT-VTR7MTHHNMFZ3OFS</span>
  <span class="c">#    </span>
  <span class="c">#    &gt;&gt; node myk8s-worker3 &lt;&lt;</span>
  <span class="c">#    -A KUBE-NODEPORTS -p tcp -m comment --comment "default/svc-nodeport:svc-webport" -m tcp --dport 31791 -j KUBE-EXT-VTR7MTHHNMFZ3OFS</span>
</code></pre></div>        </div>
      </li>
      <li>iptables 룰이 모든 노드에 추가되어있음을 확인 할 수 있습니다.</li>
    </ul>
  </li>
  <li>2.5. externalTrafficPolicy  설정
    <ul>
      <li>
        <p><code class="language-plaintext highlighter-rouge">externalTrafficPolicy: Local</code> : 앞에서 실습한 바와같이 서비스가 바라보는 파드에 접속시 클라이언트 IP가 node의 IP로 접속됩니다. 이때 <code class="language-plaintext highlighter-rouge">externalTrafficPolicy: Local</code> 를 하면 <strong>해당 노드에 배치된 파드로만 접속되면서</strong>, SNAT가 되지않아 <strong>외부 클라이언트 IP가 보존</strong>됩니다.</p>

        <p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_26.png" alt="img.png" /></p>

        <ul>
          <li>이전까지는 같은 iptables 룰이 모든 노드에 적용 되었지만, 노드 자신의 파드로만 가는 룰만 있어서 각각 조금씩 다른 룰이 적용되게 됩니다.</li>
        </ul>

        <p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_27.png" alt="img.png" /></p>

        <ul>
          <li>만약 노드에 해당하는 파드가 없으면 위의 그림과 같이 연결이 실패하게되니 사용에 주의가 필요합니다.</li>
        </ul>
      </li>
      <li>
        <p><code class="language-plaintext highlighter-rouge">externalTrafficPolicy: Local</code> 설정 시의 통신 흐름을 좀 더 자세히 알아보겠습니다.</p>

        <p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_28.png" alt="img.png" /></p>

        <ul>
          <li>클라이언트에서 파드가 배포되어있는 워커노드1에 NodePort로 접속합니다.</li>
          <li>워커노드1의 IPTABLES의 nat 테이블 규칙과 매칭되어 목적지 IP와 목적지 Port는 변환 되지만, SNAT 되지 않고 바로 파드로 전달되므로 클라이언트의 IP가 파드에 그대로 전달 됩니다.</li>
        </ul>
      </li>
      <li>
        <p>설정 및 파드 접속 확인</p>

        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c"># 기본 정보 확인</span>
  <span class="nv">$ </span>kubectl get svc svc-nodeport <span class="nt">-o</span> json | <span class="nb">grep</span> <span class="s1">'TrafficPolicy"'</span>
  <span class="c"># =&gt;         "externalTrafficPolicy": "Cluster",</span>
  <span class="c">#            "internalTrafficPolicy": "Cluster",</span>
      
  <span class="c"># 기존 통신 연결 정보(conntrack) 제거 후 아래 실습 진행하자! : (모든 노드에서) conntrack -F</span>
  <span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> conntrack <span class="nt">-F</span><span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
  <span class="c"># =&gt; &gt;&gt; node myk8s-control-plane &lt;&lt;</span>
  <span class="c">#    conntrack v1.4.7 (conntrack-tools): connection tracking table has been emptied.</span>
  <span class="c">#    </span>
  <span class="c">#    &gt;&gt; node myk8s-worker &lt;&lt;</span>
  <span class="c">#    conntrack v1.4.7 (conntrack-tools): connection tracking table has been emptied.</span>
  <span class="c">#    </span>
  <span class="c">#    &gt;&gt; node myk8s-worker2 &lt;&lt;</span>
  <span class="c">#    conntrack v1.4.7 (conntrack-tools): connection tracking table has been emptied.</span>
  <span class="c">#    </span>
  <span class="c">#    &gt;&gt; node myk8s-worker3 &lt;&lt;</span>
  <span class="c">#    conntrack v1.4.7 (conntrack-tools): connection tracking table has been emptied.</span>
  <span class="c">#    </span>
  <span class="nv">$ </span>kubectl delete <span class="nt">-f</span> svc-nodeport.yml
  <span class="c"># =&gt; service "svc-nodeport" deleted</span>
  <span class="nv">$ </span>kubectl apply <span class="nt">-f</span> svc-nodeport.yml
  <span class="c"># =&gt; service/svc-nodeport created</span>
      
  <span class="c"># externalTrafficPolicy: local 설정 변경</span>
  <span class="nv">$ </span>kubectl patch svc svc-nodeport <span class="nt">-p</span> <span class="s1">'{"spec":{"externalTrafficPolicy": "Local"}}'</span>
  <span class="c"># =&gt; service/svc-nodeport patched</span>
  <span class="nv">$ </span>kubectl get svc svc-nodeport <span class="nt">-o</span> json | <span class="nb">grep</span> <span class="s1">'TrafficPolicy"'</span>
  <span class="c"># =&gt;         "externalTrafficPolicy": "Local",</span>
  <span class="c">#            "internalTrafficPolicy": "Cluster",</span>
      
  <span class="c"># 파드 3개를 2개로 줄입니다.</span>
  <span class="nv">$ </span>kubectl scale deployment deploy-echo <span class="nt">--replicas</span><span class="o">=</span>2
  <span class="c"># =&gt; deployment.apps/deploy-echo scaled</span>
</code></pre></div>        </div>

        <p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_29.png" alt="img.png" /></p>

        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c"># 파드 존재하는 노드 정보 확인</span>
  <span class="nv">$ </span>kubectl get pod <span class="nt">-owide</span>
  <span class="c"># =&gt; NAME                           READY   STATUS    RESTARTS   AGE   IP          NODE                  NOMINATED NODE   READINESS GATES</span>
  <span class="c">#    deploy-echo-5c689d5454-24cql   1/1     Running   0          30s   10.10.4.5   myk8s-worker          &lt;none&gt;           &lt;none&gt;</span>
  <span class="c">#    deploy-echo-5c689d5454-2kgfj   1/1     Running   0          30s   10.10.1.6   myk8s-worker2         &lt;none&gt;           &lt;none&gt;</span>
  <span class="c">#    net-pod                        1/1     Running   0          46h   10.10.0.7   myk8s-control-plane   &lt;none&gt;           &lt;none&gt;</span>
      
  <span class="c"># 파드 로그 실시간 확인 (웹 파드에 접속자의 IP가 출력)</span>
  <span class="nv">$ </span>kubectl logs <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>deploy-websrv <span class="nt">-f</span>
      
  <span class="c"># 외부 클라이언트(mypc)에서 접속 시도</span>
      
  <span class="c"># 노드의 IP와 NodePort를 변수에 지정</span>
  <span class="c">## CNODE=&lt;컨트롤플레인노드의 IP주소&gt;</span>
  <span class="c">## NODE1=&lt;노드1의 IP주소&gt;</span>
  <span class="c">## NODE2=&lt;노드2의 IP주소&gt;</span>
  <span class="c">## NODE3=&lt;노드3의 IP주소&gt;</span>
  <span class="nv">$ CNODE</span><span class="o">=</span>172.23.0.2
  <span class="nv">$ NODE1</span><span class="o">=</span>172.23.0.4
  <span class="nv">$ NODE2</span><span class="o">=</span>172.23.0.5
  <span class="nv">$ NODE3</span><span class="o">=</span>172.23.0.3
      
  <span class="c">## NodePort 를 변수에 지정</span>
  <span class="nv">$ NPORT</span><span class="o">=</span><span class="si">$(</span>kubectl get service svc-nodeport <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.spec.ports[0].nodePort}'</span><span class="si">)</span>
  <span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$NPORT</span>
  <span class="c"># =&gt; 31177</span>
      
  <span class="c"># 서비스(NodePort) 부하분산 접속 확인 : 파드가 존재하지 않는 노드로는 접속 실패!, 파드가 존재하는 노드는 접속 성공 및 클라이언트 IP 확인!</span>
  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc curl <span class="nt">-s</span> <span class="nt">--connect-timeout</span> 1 <span class="nv">$CNODE</span>:<span class="nv">$NPORT</span> | jq
  <span class="c"># =&gt; (공백)</span>
  <span class="c"># 컨트롤 플레인에는 파드가 없으므로 결과가 없습니다.</span>
      
  <span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="nv">$CNODE</span> <span class="nv">$NODE1</span> <span class="nv">$NODE2</span> <span class="nv">$NODE3</span> <span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> mypc curl <span class="nt">-s</span> <span class="nt">--connect-timeout</span> 1 <span class="nv">$i</span>:<span class="nv">$NPORT</span><span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
  <span class="c"># =&gt; &gt;&gt; node 172.23.0.2 &lt;&lt;</span>
  <span class="c">#    </span>
  <span class="c">#    &gt;&gt; node 172.23.0.4 &lt;&lt;</span>
  <span class="c">#    {</span>
  <span class="c">#      "path": "/",</span>
  <span class="c">#      "headers": {</span>
  <span class="c">#        "host": "172.23.0.4:31177",</span>
  <span class="c">#        ...</span>
  <span class="c">#      },</span>
  <span class="c">#      ...</span>
  <span class="c">#      "hostname": "172.23.0.4",</span>
  <span class="c">#      "ip": "::ffff:172.23.0.6",</span>
  <span class="c">#      ...</span>
  <span class="c">#      "os": {</span>
  <span class="c">#        "hostname": "deploy-echo-5c689d5454-24cql"</span>
  <span class="c">#      }</span>
  <span class="c">#    }</span>
  <span class="c">#    &gt;&gt; node 172.23.0.5 &lt;&lt;</span>
  <span class="c">#    {</span>
  <span class="c">#      "headers": {</span>
  <span class="c">#        "host": "172.23.0.5:31177",</span>
  <span class="c">#        ...</span>
  <span class="c">#      },</span>
      
  <span class="c">#      "hostname": "172.23.0.5",</span>
  <span class="c">#      "ip": "::ffff:172.23.0.6",</span>
  <span class="c">#      ...</span>
  <span class="c">#      "os": {</span>
  <span class="c">#        "hostname": "deploy-echo-5c689d5454-2kgfj"</span>
  <span class="c">#      }</span>
  <span class="c">#    }</span>
  <span class="c">#    &gt;&gt; node 172.23.0.3 &lt;&lt;</span>
  <span class="c">#    </span>
      
  <span class="c"># 목적지 파드가 배치되지 않은 노드는 접속이 어떻게? 왜 그런가?</span>
  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc zsh <span class="nt">-c</span> <span class="s2">"for i in {1..100}; do curl -s </span><span class="nv">$CNODE</span><span class="s2">:</span><span class="nv">$NPORT</span><span class="s2"> | grep hostname; done | sort | uniq -c | sort -nr"</span>
  <span class="c"># =&gt; </span>
  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc zsh <span class="nt">-c</span> <span class="s2">"for i in {1..100}; do curl -s </span><span class="nv">$NODE1</span><span class="s2">:</span><span class="nv">$NPORT</span><span class="s2"> | grep hostname; done | sort | uniq -c | sort -nr"</span>
  <span class="c"># =&gt;     100   "hostname": "172.23.0.4",</span>
  <span class="c">#        100     "hostname": "deploy-echo-5c689d5454-24cql"</span>
  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc zsh <span class="nt">-c</span> <span class="s2">"for i in {1..100}; do curl -s </span><span class="nv">$NODE2</span><span class="s2">:</span><span class="nv">$NPORT</span><span class="s2"> | grep hostname; done | sort | uniq -c | sort -nr"</span>
  <span class="c"># =&gt;     100   "hostname": "172.23.0.5",</span>
  <span class="c">#        100     "hostname": "deploy-echo-5c689d5454-2kgfj"</span>
  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc zsh <span class="nt">-c</span> <span class="s2">"for i in {1..100}; do curl -s </span><span class="nv">$NODE3</span><span class="s2">:</span><span class="nv">$NPORT</span><span class="s2"> | grep hostname; done | sort | uniq -c | sort -nr"</span>
  <span class="c"># =&gt; </span>
  <span class="c"># 목적지 파드가 배치되지 않은 노드는 응답이 없어 타임아웃이 됩니다. 그 이유는 externalTrafficPolicy: Local여서 노드포트로 온 패킷이, local pod로 전달하려고 하는데</span>
  <span class="c"># local pod가 없기 때문입니다.</span>
      
  <span class="c"># 아래 반복 접속 실행 해두자</span>
  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc zsh <span class="nt">-c</span> <span class="s2">"while true; do curl -s --connect-timeout 1 </span><span class="nv">$NODE2</span><span class="s2">:</span><span class="nv">$NPORT</span><span class="s2"> | grep hostname; date '+%Y-%m-%d %H:%M:%S' ; echo ;  sleep 1; done"</span>
      
  <span class="c"># (옵션) 노드에서 Network Connection</span>
  <span class="nv">$ </span>conntrack <span class="nt">-E</span>
  <span class="nv">$ </span>conntrack <span class="nt">-L</span> <span class="nt">--any-nat</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>외부 클라이언트 → 각각 워커 노드 1,2 접속시 각각 노드의 파드로만 접속 됩니다.</p>

        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c"># 호스트에서 실행</span>
  <span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="nv">$CNODE</span> <span class="nv">$NODE1</span> <span class="nv">$NODE2</span> <span class="nv">$NODE3</span> <span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> curl <span class="nt">-s</span> <span class="nt">--connect-timeout</span> 1 <span class="nv">$i</span>:<span class="nv">$NPORT</span><span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
  <span class="c"># =&gt; &gt;&gt; node 172.23.0.2 &lt;&lt;</span>
  <span class="c">#    </span>
  <span class="c">#    &gt;&gt; node 172.23.0.4 &lt;&lt;</span>
  <span class="c">#    {</span>
  <span class="c">#      "headers": {</span>
  <span class="c">#        "host": "172.23.0.4:31177",</span>
  <span class="c">#      },</span>
  <span class="c">#      "hostname": "172.23.0.4",</span>
  <span class="c">#      "ip": "::ffff:172.23.0.1",</span>
  <span class="c">#      "os": {</span>
  <span class="c">#        "hostname": "deploy-echo-5c689d5454-24cql"</span>
  <span class="c">#      }</span>
  <span class="c">#    }</span>
  <span class="c">#    &gt;&gt; node 172.23.0.5 &lt;&lt;</span>
  <span class="c">#    {</span>
  <span class="c">#      "headers": {</span>
  <span class="c">#        "host": "172.23.0.5:31177",</span>
  <span class="c">#      },</span>
  <span class="c">#      "hostname": "172.23.0.5",</span>
  <span class="c">#      "ip": "::ffff:172.23.0.1",</span>
  <span class="c">#      "os": {</span>
  <span class="c">#        "hostname": "deploy-echo-5c689d5454-2kgfj"</span>
  <span class="c">#      }</span>
  <span class="c">#    }</span>
  <span class="c">#    &gt;&gt; node 172.23.0.3 &lt;&lt;    </span>
    
  <span class="c"># 다른 터미널에서 로그 표시</span>
  <span class="nv">$ </span>kubectl logs <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>deploy-websrv <span class="nt">-f</span> | <span class="nb">grep </span>HTTP
  ::ffff:172.23.0.1 - - <span class="o">[</span>26/Sep/2024:07:33:09 +0000] <span class="s2">"GET / HTTP/1.1"</span> 200 397 <span class="s2">"-"</span> <span class="s2">"curl/8.7.1"</span>
  ::ffff:172.23.0.1 - - <span class="o">[</span>26/Sep/2024:07:33:10 +0000] <span class="s2">"GET / HTTP/1.1"</span> 200 397 <span class="s2">"-"</span> <span class="s2">"curl/8.7.1"</span>
  ::ffff:172.23.0.1 - - <span class="o">[</span>26/Sep/2024:07:33:26 +0000] <span class="s2">"GET / HTTP/1.1"</span> 200 397 <span class="s2">"-"</span> <span class="s2">"curl/8.7.1"</span>
  ::ffff:172.23.0.1 - - <span class="o">[</span>26/Sep/2024:07:33:26 +0000] <span class="s2">"GET / HTTP/1.1"</span> 200 397 <span class="s2">"-"</span> <span class="s2">"curl/8.7.1"</span>
</code></pre></div>        </div>
        <ul>
          <li><code class="language-plaintext highlighter-rouge">kubectl logs -l app=deploy-websrv -f</code>로 확인시 외부 클라이언트인 172.23.0.1이 보존되는 것을 확인 할 수 있습니다.</li>
        </ul>
      </li>
      <li>이렇게 동작하는 이유를 iptables 룰을 통해 확인해보겠습니다.
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 컨트롤플레인 노드 - iptables 분석 &lt;&lt; 정책 확인 : 아래 정책 내용은 핵심적인 룰(rule)만 표시했습니다!</span>
<span class="c"># (예시) 파드가 배포되어 있는 노드1에서 확인했습니다</span>
    
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker bash
<span class="nt">---------------------------------------</span>
    
<span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span>
<span class="c"># $ iptables -t nat -S | grep &lt;NodePort&gt;</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span> | <span class="nb">grep </span>31177
<span class="c"># =&gt; -A KUBE-NODEPORTS -p tcp -m comment --comment "default/svc-nodeport:svc-webport" -m tcp --dport 31177 -j KUBE-EXT-VTR7MTHHNMFZ3OFS</span>
    
<span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span> | <span class="nb">grep</span> <span class="s1">'A KUBE-EXT-VTR7MTHHNMFZ3OFS'</span>
<span class="c"># =&gt; -A KUBE-EXT-VTR7MTHHNMFZ3OFS -s 10.10.0.0/16 -m comment --comment "pod traffic for default/svc-nodeport:svc-webport external destinations" -j KUBE-SVC-VTR7MTHHNMFZ3OFS</span>
<span class="c">#    -A KUBE-EXT-VTR7MTHHNMFZ3OFS -m comment --comment "masquerade LOCAL traffic for default/svc-nodeport:svc-webport external destinations" -m addrtype --src-type LOCAL -j KUBE-MARK-MASQ</span>
<span class="c">#    -A KUBE-EXT-VTR7MTHHNMFZ3OFS -m comment --comment "route LOCAL traffic for default/svc-nodeport:svc-webport external destinations" -m addrtype --src-type LOCAL -j KUBE-SVC-VTR7MTHHNMFZ3OFS</span>
<span class="c">#    -A KUBE-EXT-VTR7MTHHNMFZ3OFS -j KUBE-SVL-VTR7MTHHNMFZ3OFS</span>
    
<span class="c"># 실습 환경에서는 아래처럼 2개의 파드 중 자신의 노드에 생성된 파드 1개만 DNAT 연결됨</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span> | <span class="nb">grep</span> <span class="s1">'A KUBE-SVL-VTR7MTHHNMFZ3OFS'</span>
<span class="c"># =&gt; -A KUBE-SVL-VTR7MTHHNMFZ3OFS -m comment --comment "default/svc-nodeport:svc-webport -&gt; 10.10.4.5:8080" -j KUBE-SEP-COBCKEECYTEF2ZXK</span>
    
<span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span> | <span class="nb">grep</span> <span class="s1">'A KUBE-SEP-COBCKEECYTEF2ZXK'</span>
<span class="c"># =&gt; -A KUBE-SEP-COBCKEECYTEF2ZXK -s 10.10.4.5/32 -m comment --comment "default/svc-nodeport:svc-webport" -j KUBE-MARK-MASQ</span>
<span class="c">#    -A KUBE-SEP-COBCKEECYTEF2ZXK -p tcp -m comment --comment "default/svc-nodeport:svc-webport" -m tcp -j DNAT --to-destination 10.10.4.5:8080</span>
    
<span class="nv">$ </span><span class="nb">exit</span>
<span class="c"># ---------------------------------------</span>
</code></pre></div>        </div>
        <ul>
          <li>정책을 확인해보면 <code class="language-plaintext highlighter-rouge">externalTrafficPolicy: Local</code> 설정 전에는 MASQUERADE로 SNAT 되었지만, 설정 후에는 DNAT으로 바로 전달되는 것을 확인할 수 있습니다.</li>
          <li>SNAT 되지 않았기 때문에 클라이언트의 IP가 그대로 전달되어 파드에서 확인할 수 있습니다.</li>
        </ul>
      </li>
      <li>서비스(NodePort, externalTrafficPolicy: Local) 생성 시 iptables 규칙(KUBE-SVL-#)이 모든 노드에 추가되는지 확인해보겠습니다.
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 컨트롤 플레인에는 파드가 없으므로 결과가 없습니다.</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane iptables <span class="nt">-t</span> nat <span class="nt">-S</span> | <span class="nb">grep</span> <span class="s1">'A KUBE-SVL-VTR7MTHHNMFZ3OFS'</span>
<span class="c"># =&gt; (공백)</span>
    
<span class="c"># 각 노드에 확인해보겠습니다.</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> iptables <span class="nt">-t</span> nat <span class="nt">-S</span> | <span class="nb">grep</span> <span class="s1">'A KUBE-SVL-VTR7MTHHNMFZ3OFS'</span><span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; &gt;&gt; node myk8s-control-plane &lt;&lt;</span>
<span class="c">#    </span>
<span class="c">#    &gt;&gt; node myk8s-worker &lt;&lt;</span>
<span class="c">#    -A KUBE-SVL-VTR7MTHHNMFZ3OFS -m comment --comment "default/svc-nodeport:svc-webport -&gt; 10.10.4.5:8080" -j KUBE-SEP-COBCKEECYTEF2ZXK</span>
<span class="c">#    </span>
<span class="c">#    &gt;&gt; node myk8s-worker2 &lt;&lt;</span>
<span class="c">#    -A KUBE-SVL-VTR7MTHHNMFZ3OFS -m comment --comment "default/svc-nodeport:svc-webport -&gt; 10.10.1.6:8080" -j KUBE-SEP-ABUS75FNO53OAK6G</span>
<span class="c">#    </span>
<span class="c">#    &gt;&gt; node myk8s-worker3 &lt;&lt;</span>
</code></pre></div>        </div>
        <ul>
          <li>파드가 있는 worker, worker2 노드에만 iptables 규칙이 추가되어 있음을 확인할 수 있습니다.</li>
        </ul>
      </li>
      <li>NodePort의 부족한 점
        <ul>
          <li>외부에서 노드의 IP와 포트로 직접 접속이 필요합니다.</li>
          <li>따라서 내부망이 외부에 공개(라우팅 가능)되어 보안에 취약합니다.
            <ul>
              <li>=&gt; <strong>LoadBalancer 서비스</strong> 타입으로 외부 공개 최소화 가능</li>
            </ul>
          </li>
          <li>클라이언트 IP 보존을 위해서, <code class="language-plaintext highlighter-rouge">externalTrafficPolicy: local</code>를 사용하면 파드가 없는 노드 IP로 NodePort 접속 시 실패하게 됩니다.
            <ul>
              <li>=&gt; <strong>LoadBalancer 서비스</strong>에서 헬스체크(Probe) 로 대응 가능</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h4 id="파드간-속도-측정">파드간 속도 측정</h4>

<ul>
  <li>이번 실습에서는 iperf3를 사용해서 파드간 속도를 측정해보겠습니다.</li>
  <li>iperf3는 네트워크 대역폭을 측정하는 도구로, 서버와 클라이언트로 나뉘어 서버는 대역폭을 제공하고 클라이언트는 대역폭을 측정합니다. TCP와 UDP, SCTP를 지원합니다.</li>
  <li>iperf3의 기본 사용법을 살펴 보겠습니다.
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># iperf3 설치 </span>
<span class="c"># macOS 인 경우</span>
<span class="nv">$ </span>brew <span class="nb">install </span>iperf3
<span class="c"># ubuntu 등 debian 계열인 경우 </span>
<span class="nv">$ </span><span class="nb">sudo </span>apt <span class="nb">install </span>iperf3 <span class="nt">-y</span>
  
<span class="c"># iperf3 테스트 1 : TCP 5201, 측정시간 10초</span>
<span class="nv">$ </span>iperf3 <span class="nt">-s</span> <span class="c"># 서버모드 실행</span>
<span class="c"># =&gt; -----------------------------------------------------------</span>
<span class="c">#    Server listening on 5201</span>
<span class="c">#    -----------------------------------------------------------</span>
<span class="c">#    Accepted connection from 127.0.0.1, port 40142</span>
<span class="c">#    [  5] local 127.0.0.1 port 5201 connected to 127.0.0.1 port 40154</span>
<span class="c">#    [ ID] Interval           Transfer     Bitrate</span>
<span class="c">#    [  5]   0.00-1.00   sec  7.11 GBytes  61.1 Gbits/sec</span>
<span class="c">#    [  5]   1.00-2.00   sec  8.03 GBytes  68.9 Gbits/sec</span>
<span class="c">#    [  5]   2.00-3.00   sec  7.53 GBytes  64.7 Gbits/sec</span>
<span class="c">#    [  5]   3.00-4.00   sec  7.73 GBytes  66.4 Gbits/sec</span>
<span class="c">#    [  5]   4.00-5.00   sec  7.64 GBytes  65.6 Gbits/sec</span>
<span class="c">#    [  5]   5.00-6.00   sec  7.89 GBytes  67.8 Gbits/sec</span>
<span class="c">#    [  5]   6.00-7.00   sec  7.95 GBytes  68.3 Gbits/sec</span>
<span class="c">#    [  5]   7.00-8.00   sec  7.78 GBytes  66.9 Gbits/sec</span>
<span class="c">#    [  5]   8.00-9.00   sec  7.91 GBytes  67.9 Gbits/sec</span>
<span class="c">#    [  5]   9.00-10.00  sec  7.64 GBytes  65.6 Gbits/sec</span>
<span class="c">#    [  5]  10.00-10.05  sec   384 MBytes  66.0 Gbits/sec</span>
<span class="c">#    - - - - - - - - - - - - - - - - - - - - - - - - -</span>
<span class="c">#    [ ID] Interval           Transfer     Bitrate</span>
<span class="c">#    [  5]   0.00-10.05  sec  77.6 GBytes  66.3 Gbits/sec                  receiver</span>
<span class="nv">$ </span>iperf3 <span class="nt">-c</span> 127.0.0.1 <span class="c"># 다른 터미널에서 클라이언트모드 실행</span>
<span class="c"># =&gt; Connecting to host 127.0.0.1, port 5201</span>
<span class="c">#    [  5] local 127.0.0.1 port 40154 connected to 127.0.0.1 port 5201</span>
<span class="c">#    [ ID] Interval           Transfer     Bitrate         Retr  Cwnd</span>
<span class="c">#    [  5]   0.00-1.00   sec  7.48 GBytes  64.2 Gbits/sec    8   2.69 MBytes</span>
<span class="c">#    ...</span>
<span class="c">#    [  5]   9.00-10.00  sec  7.72 GBytes  66.3 Gbits/sec    1   3.06 MBytes</span>
<span class="c">#    - - - - - - - - - - - - - - - - - - - - - - - - -</span>
<span class="c">#    [ ID] Interval           Transfer     Bitrate         Retr</span>
<span class="c">#    [  5]   0.00-10.00  sec  77.6 GBytes  66.6 Gbits/sec   53             sender</span>
<span class="c">#    [  5]   0.00-10.05  sec  77.6 GBytes  66.3 Gbits/sec                  receiver</span>
  
<span class="c"># iperf3 테스트 2 : TCP 80, 측정시간 5초</span>
<span class="nv">$ </span>iperf3 <span class="nt">-s</span> <span class="nt">-p</span> 80
<span class="nv">$ </span>iperf3 <span class="nt">-c</span> 127.0.0.1 <span class="nt">-p</span> 80 <span class="nt">-t</span> 5
  
<span class="c"># iperf3 테스트 3 : UDP 사용, 역방향 모드(-R)</span>
<span class="nv">$ </span>iperf3 <span class="nt">-s</span> 
<span class="nv">$ </span>iperf3 <span class="nt">-c</span> 127.0.0.1 <span class="nt">-u</span> <span class="nt">-b</span> 100G
  
<span class="c"># iperf3 테스트 4 : 역방향 모드(-R) =&gt; 서버에서 클라이언트로 전송할때 속도를 측정합니다.  </span>
<span class="nv">$ </span>iperf3 <span class="nt">-s</span> 
<span class="nv">$ </span>iperf3 <span class="nt">-c</span> 127.0.0.1 <span class="nt">-R</span>
  
<span class="c"># iperf3 테스트 5 : 쌍방향 모드(-R)</span>
<span class="nv">$ </span>iperf3 <span class="nt">-s</span> 
<span class="nv">$ </span>iperf3 <span class="nt">-c</span> 127.0.0.1 <span class="nt">--bidir</span>
  
<span class="c"># iperf3 테스트 6 : TCP 다중 스트림(30개), -P(number of parallel client streams to run)</span>
<span class="nv">$ </span>iperf3 <span class="nt">-s</span> 
<span class="nv">$ </span>iperf3 <span class="nt">-c</span> 127.0.0.1 <span class="nt">-P</span> 2 <span class="nt">-t</span> 30
</code></pre></div>    </div>
  </li>
  <li>쿠버네티스 환경에서 속도 측정 테스트해보겠습니다.
    <ul>
      <li>테스트 환경 배포
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 배포</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> https://raw.githubusercontent.com/gasida/PKOS/main/aews/k8s-iperf3.yaml
    
<span class="c"># 확인 : 서버와 클라이언트가 다른 워커노드에 배포되었는지 확인</span>
<span class="nv">$ </span>kubectl get deploy,svc,pod <span class="nt">-owide</span>
<span class="c"># =&gt; NAME                            READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS      IMAGES                    SELECTOR</span>
<span class="c">#    deployment.apps/iperf3-client   0/1     1            0           5s    iperf3-client   networkstatic/iperf3      app=iperf3-client</span>
<span class="c">#    deployment.apps/iperf3-server   0/1     1            0           5s    iperf3-server   networkstatic/iperf3      app=iperf3-server</span>
<span class="c">#    </span>
<span class="c">#    NAME                    TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)             AGE    SELECTOR</span>
<span class="c">#    service/iperf3-server   ClusterIP   10.200.1.166   &lt;none&gt;        5201/TCP,5201/UDP   5s     app=iperf3-server</span>
<span class="c">#    </span>
<span class="c">#    NAME                                 READY   STATUS              RESTARTS   AGE    IP          NODE                  NOMINATED NODE   READINESS GATES</span>
<span class="c">#    pod/iperf3-client-598b85fd6b-tq5xg   0/1     ContainerCreating   0          5s     &lt;none&gt;      myk8s-worker3         &lt;none&gt;           &lt;none&gt;</span>
<span class="c">#    pod/iperf3-server-688df6d56f-hlhrm   0/1     ContainerCreating   0          5s     &lt;none&gt;      myk8s-worker          &lt;none&gt;           &lt;none&gt;</span>
    
<span class="c"># 서버 파드 로그 확인 : 기본 5201 포트 Listen</span>
<span class="nv">$ </span>kubectl logs <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>iperf3-server <span class="nt">-f</span>
</code></pre></div>        </div>
      </li>
      <li>TCP 5201, 측정시간 5초
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 클라이언트 파드에서 아래 명령 실행</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deploy/iperf3-client <span class="nt">--</span> iperf3 <span class="nt">-c</span> iperf3-server <span class="nt">-t</span> 5
<span class="c"># =&gt; Connecting to host iperf3-server, port 5201</span>
<span class="c">#    [  5] local 10.10.2.9 port 54972 connected to 10.200.1.166 port 5201</span>
<span class="c">#    [ ID] Interval           Transfer     Bitrate         Retr  Cwnd</span>
<span class="c">#    [  5]   0.00-1.00   sec  4.68 GBytes  40.2 Gbits/sec  3333   1.07 MBytes</span>
<span class="c">#    [  5]   1.00-2.00   sec  4.87 GBytes  41.8 Gbits/sec  1293   1.09 MBytes</span>
<span class="c">#    [  5]   2.00-3.00   sec  4.86 GBytes  41.8 Gbits/sec  1020   1.11 MBytes</span>
<span class="c">#    [  5]   3.00-4.00   sec  4.85 GBytes  41.7 Gbits/sec  590   1.21 MBytes</span>
<span class="c">#    [  5]   4.00-5.00   sec  4.93 GBytes  42.4 Gbits/sec  988   1.27 MBytes</span>
<span class="c">#    - - - - - - - - - - - - - - - - - - - - - - - - -</span>
<span class="c">#    [ ID] Interval           Transfer     Bitrate         Retr</span>
<span class="c">#    [  5]   0.00-5.00   sec  24.2 GBytes  41.6 Gbits/sec  7224             sender</span>
<span class="c">#    [  5]   0.00-5.00   sec  24.2 GBytes  41.6 Gbits/sec                  receiver</span>
    
<span class="c"># 서버 파드 로그 확인 : 기본 5201 포트 Listen</span>
<span class="nv">$ </span>kubectl logs <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>iperf3-server <span class="nt">-f</span>
<span class="c"># =&gt; -----------------------------------------------------------</span>
<span class="c">#    Server listening on 5201 (test #1)</span>
<span class="c">#    -----------------------------------------------------------</span>
<span class="c">#    Accepted connection from 10.10.2.9, port 54962</span>
<span class="c">#    [  5] local 10.10.4.6 port 5201 connected to 10.10.2.9 port 54972</span>
<span class="c">#    [ ID] Interval           Transfer     Bitrate</span>
<span class="c">#    [  5]   0.00-1.00   sec  4.67 GBytes  40.1 Gbits/sec</span>
<span class="c">#    [  5]   1.00-2.00   sec  4.87 GBytes  41.8 Gbits/sec</span>
<span class="c">#    [  5]   2.00-3.00   sec  4.86 GBytes  41.8 Gbits/sec</span>
<span class="c">#    [  5]   3.00-4.00   sec  4.85 GBytes  41.7 Gbits/sec</span>
<span class="c">#    [  5]   4.00-5.00   sec  4.93 GBytes  42.4 Gbits/sec</span>
<span class="c">#    [  5]   5.00-5.00   sec   384 KBytes  41.4 Gbits/sec</span>
<span class="c">#    - - - - - - - - - - - - - - - - - - - - - - - - -</span>
<span class="c">#    [ ID] Interval           Transfer     Bitrate</span>
<span class="c">#    [  5]   0.00-5.00   sec  24.2 GBytes  41.6 Gbits/sec                  receiver</span>
</code></pre></div>        </div>
      </li>
      <li>UDP 사용, 역방향 모드(-R)
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 클라이언트 파드에서 아래 명령 실행</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deploy/iperf3-client <span class="nt">--</span> iperf3 <span class="nt">-c</span> iperf3-server <span class="nt">-u</span> <span class="nt">-b</span> 20G
<span class="c"># =&gt; Connecting to host iperf3-server, port 5201</span>
<span class="c">#    [  5] local 10.10.2.9 port 41928 connected to 10.200.1.166 port 5201</span>
<span class="c">#    [ ID] Interval           Transfer     Bitrate         Total Datagrams</span>
<span class="c">#    [  5]   0.00-1.00   sec   161 MBytes  1.35 Gbits/sec  116453</span>
<span class="c">#    [  5]   1.00-2.00   sec   187 MBytes  1.57 Gbits/sec  135745</span>
<span class="c">#    [  5]   2.00-3.00   sec   163 MBytes  1.36 Gbits/sec  117693</span>
<span class="c">#    [  5]   3.00-4.00   sec   220 MBytes  1.84 Gbits/sec  159109</span>
<span class="c">#    [  5]   4.00-5.00   sec   168 MBytes  1.41 Gbits/sec  121705</span>
<span class="c">#    [  5]   5.00-6.00   sec   183 MBytes  1.54 Gbits/sec  132730</span>
<span class="c">#    [  5]   6.00-7.00   sec   184 MBytes  1.54 Gbits/sec  133267</span>
<span class="c">#    [  5]   7.00-8.00   sec   158 MBytes  1.32 Gbits/sec  114073</span>
<span class="c">#    [  5]   8.00-9.00   sec   171 MBytes  1.44 Gbits/sec  124005</span>
<span class="c">#    [  5]   9.00-10.00  sec   160 MBytes  1.35 Gbits/sec  116175</span>
<span class="c">#    - - - - - - - - - - - - - - - - - - - - - - - - -</span>
<span class="c">#    [ ID] Interval           Transfer     Bitrate         Jitter    Lost/Total Datagrams</span>
<span class="c">#    [  5]   0.00-10.00  sec  1.71 GBytes  1.47 Gbits/sec  0.000 ms  0/1270955 (0%)  sender</span>
<span class="c">#    [  5]   0.00-10.00  sec  1.68 GBytes  1.44 Gbits/sec  0.008 ms  28438/1270955 (2.2%)  receiver</span>
    
<span class="c"># 서버 파드 로그 확인 : 기본 5201 포트 Listen</span>
<span class="nv">$ </span>kubectl logs <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>iperf3-server <span class="nt">-f</span>
<span class="c"># =&gt; -----------------------------------------------------------</span>
<span class="c">#    Server listening on 5201 (test #3)</span>
<span class="c">#    -----------------------------------------------------------</span>
<span class="c">#    Accepted connection from 10.10.2.9, port 48546</span>
<span class="c">#    [  5] local 10.10.4.6 port 5201 connected to 10.10.2.9 port 41928</span>
<span class="c">#    [ ID] Interval           Transfer     Bitrate         Jitter    Lost/Total Datagrams</span>
<span class="c">#    [  5]   0.00-1.00   sec   158 MBytes  1.33 Gbits/sec  0.011 ms  2000/116449 (1.7%)</span>
<span class="c">#    [  5]   1.00-2.00   sec   180 MBytes  1.51 Gbits/sec  0.009 ms  5401/135743 (4%)</span>
<span class="c">#    [  5]   2.00-3.00   sec   161 MBytes  1.35 Gbits/sec  0.011 ms  1241/117696 (1.1%)</span>
<span class="c">#    [  5]   3.00-4.00   sec   215 MBytes  1.81 Gbits/sec  0.009 ms  3132/159105 (2%)</span>
<span class="c">#    [  5]   4.00-5.00   sec   165 MBytes  1.39 Gbits/sec  0.007 ms  2073/121704 (1.7%)</span>
<span class="c">#    [  5]   5.00-6.00   sec   179 MBytes  1.51 Gbits/sec  0.008 ms  2758/132731 (2.1%)</span>
<span class="c">#    [  5]   6.00-7.00   sec   181 MBytes  1.52 Gbits/sec  0.009 ms  2397/133243 (1.8%)</span>
<span class="c">#    [  5]   7.00-8.00   sec   153 MBytes  1.28 Gbits/sec  0.007 ms  3612/114097 (3.2%)</span>
<span class="c">#    [  5]   8.00-9.00   sec   166 MBytes  1.39 Gbits/sec  0.009 ms  3707/124005 (3%)</span>
<span class="c">#    [  5]   9.00-10.00  sec   158 MBytes  1.32 Gbits/sec  0.009 ms  2117/116179 (1.8%)</span>
<span class="c">#    [  5]  10.00-10.00  sec  4.24 KBytes   656 Mbits/sec  0.008 ms  0/3 (0%)</span>
<span class="c">#    - - - - - - - - - - - - - - - - - - - - - - - - -</span>
<span class="c">#    [ ID] Interval           Transfer     Bitrate         Jitter    Lost/Total Datagrams</span>
<span class="c">#    [  5]   0.00-10.00  sec  1.68 GBytes  1.44 Gbits/sec  0.008 ms  28438/1270955 (2.2%)  receiver</span>
</code></pre></div>        </div>
      </li>
      <li>TCP, 쌍방향 모드(-R)
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 클라이언트 파드에서 아래 명령 실행</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deploy/iperf3-client <span class="nt">--</span> iperf3 <span class="nt">-c</span> iperf3-server <span class="nt">-t</span> 5 <span class="nt">--bidir</span>
<span class="c"># =&gt; Connecting to host iperf3-server, port 5201</span>
<span class="c">#    [  5] local 10.10.2.9 port 59852 connected to 10.200.1.166 port 5201</span>
<span class="c">#    [  7] local 10.10.2.9 port 59860 connected to 10.200.1.166 port 5201</span>
<span class="c">#    [ ID][Role] Interval           Transfer     Bitrate         Retr  Cwnd</span>
<span class="c">#    [  5][TX-C]   0.00-1.00   sec  3.85 GBytes  33.0 Gbits/sec  2249   1.55 MBytes</span>
<span class="c">#    [  7][RX-C]   0.00-1.00   sec   553 MBytes  4.64 Gbits/sec</span>
<span class="c">#    [  5][TX-C]   1.00-2.00   sec  1.77 GBytes  15.2 Gbits/sec  3105   1.07 MBytes</span>
<span class="c">#    [  7][RX-C]   1.00-2.00   sec  2.73 GBytes  23.4 Gbits/sec</span>
<span class="c">#    [  5][TX-C]   2.00-3.00   sec  1.64 GBytes  14.1 Gbits/sec  639    850 KBytes</span>
<span class="c">#    [  7][RX-C]   2.00-3.00   sec  2.93 GBytes  25.2 Gbits/sec</span>
<span class="c">#    [  5][TX-C]   3.00-4.00   sec  2.07 GBytes  17.8 Gbits/sec    0    853 KBytes</span>
<span class="c">#    [  7][RX-C]   3.00-4.00   sec  2.48 GBytes  21.3 Gbits/sec</span>
<span class="c">#    [  5][TX-C]   4.00-5.00   sec  1.22 GBytes  10.5 Gbits/sec    2    877 KBytes</span>
<span class="c">#    [  7][RX-C]   4.00-5.00   sec  3.25 GBytes  27.9 Gbits/sec</span>
<span class="c">#    - - - - - - - - - - - - - - - - - - - - - - - - -</span>
<span class="c">#    [ ID][Role] Interval           Transfer     Bitrate         Retr</span>
<span class="c">#    [  5][TX-C]   0.00-5.00   sec  10.5 GBytes  18.1 Gbits/sec  5995             sender</span>
<span class="c">#    [  5][TX-C]   0.00-5.00   sec  10.5 GBytes  18.1 Gbits/sec                  receiver</span>
<span class="c">#    [  7][RX-C]   0.00-5.00   sec  11.9 GBytes  20.5 Gbits/sec  9201             sender</span>
<span class="c">#    [  7][RX-C]   0.00-5.00   sec  11.9 GBytes  20.5 Gbits/sec                  receiver</span>
    
<span class="c"># 서버 파드 로그 확인 : 기본 5201 포트 Listen</span>
<span class="nv">$ </span>kubectl logs <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>iperf3-server <span class="nt">-f</span>
<span class="c"># =&gt; -----------------------------------------------------------</span>
<span class="c">#    Server listening on 5201 (test #2)</span>
<span class="c">#    -----------------------------------------------------------</span>
<span class="c">#    Accepted connection from 10.10.2.9, port 59836</span>
<span class="c">#    [  5] local 10.10.4.6 port 5201 connected to 10.10.2.9 port 59852</span>
<span class="c">#    [  8] local 10.10.4.6 port 5201 connected to 10.10.2.9 port 59860</span>
<span class="c">#    [ ID][Role] Interval           Transfer     Bitrate         Retr  Cwnd</span>
<span class="c">#    [  5][RX-S]   0.00-1.00   sec  3.85 GBytes  33.0 Gbits/sec</span>
<span class="c">#    [  8][TX-S]   0.00-1.00   sec   561 MBytes  4.70 Gbits/sec   59   1.02 MBytes</span>
<span class="c">#    [  5][RX-S]   1.00-2.00   sec  1.77 GBytes  15.2 Gbits/sec</span>
<span class="c">#    [  8][TX-S]   1.00-2.00   sec  2.73 GBytes  23.5 Gbits/sec  2468   1.09 MBytes</span>
<span class="c">#    [  5][RX-S]   2.00-3.00   sec  1.63 GBytes  14.0 Gbits/sec</span>
<span class="c">#    [  8][TX-S]   2.00-3.00   sec  2.92 GBytes  25.1 Gbits/sec  3327   1.10 MBytes</span>
<span class="c">#    [  5][RX-S]   3.00-4.00   sec  2.08 GBytes  17.9 Gbits/sec</span>
<span class="c">#    [  8][TX-S]   3.00-4.00   sec  2.49 GBytes  21.4 Gbits/sec  2315   1.13 MBytes</span>
<span class="c">#    [  5][RX-S]   4.00-5.00   sec  1.22 GBytes  10.5 Gbits/sec</span>
<span class="c">#    [  8][TX-S]   4.00-5.00   sec  3.25 GBytes  27.9 Gbits/sec  1032   1.16 MBytes</span>
<span class="c">#    [  5][RX-S]   5.00-5.00   sec   768 KBytes  27.6 Gbits/sec</span>
<span class="c">#    [  8][TX-S]   5.00-5.00   sec  1.25 MBytes  41.3 Gbits/sec    0   1.16 MBytes</span>
<span class="c">#    - - - - - - - - - - - - - - - - - - - - - - - - -</span>
<span class="c">#    [ ID][Role] Interval           Transfer     Bitrate         Retr</span>
<span class="c">#    [  5][RX-S]   0.00-5.00   sec  10.5 GBytes  18.1 Gbits/sec                  receiver</span>
<span class="c">#    [  8][TX-S]   0.00-5.00   sec  11.9 GBytes  20.5 Gbits/sec  9201             sender</span>
</code></pre></div>        </div>
      </li>
      <li>TCP 다중 스트림(30개), -P(number of parallel client streams to run)
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 클라이언트 파드에서 아래 명령 실행</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deploy/iperf3-client <span class="nt">--</span> iperf3 <span class="nt">-c</span> iperf3-server <span class="nt">-t</span> 10 <span class="nt">-P</span> 2
<span class="c"># =&gt; [  5] local 10.10.2.9 port 41976 connected to 10.200.1.166 port 5201</span>
<span class="c">#    [  7] local 10.10.2.9 port 41982 connected to 10.200.1.166 port 5201</span>
<span class="c">#    [ ID] Interval           Transfer     Bitrate         Retr  Cwnd</span>
<span class="c">#    [  5]   0.00-1.00   sec  2.87 GBytes  24.7 Gbits/sec  822    570 KBytes</span>
<span class="c">#    [  7]   0.00-1.00   sec  2.88 GBytes  24.7 Gbits/sec  159    576 KBytes</span>
<span class="c">#    [SUM]   0.00-1.00   sec  5.75 GBytes  49.4 Gbits/sec  981</span>
<span class="c">#    - - - - - - - - - - - - - - - - - - - - - - - - -</span>
<span class="c">#    ...</span>
<span class="c">#    - - - - - - - - - - - - - - - - - - - - - - - - -</span>
<span class="c">#    [ ID] Interval           Transfer     Bitrate         Retr</span>
<span class="c">#    [  5]   0.00-10.00  sec  29.2 GBytes  25.1 Gbits/sec  3825             sender</span>
<span class="c">#    [  5]   0.00-10.00  sec  29.2 GBytes  25.1 Gbits/sec                  receiver</span>
<span class="c">#    [  7]   0.00-10.00  sec  29.2 GBytes  25.1 Gbits/sec  2063             sender</span>
<span class="c">#    [  7]   0.00-10.00  sec  29.2 GBytes  25.0 Gbits/sec                  receiver</span>
<span class="c">#    [SUM]   0.00-10.00  sec  58.3 GBytes  50.1 Gbits/sec  5888             sender</span>
<span class="c">#    [SUM]   0.00-10.00  sec  58.3 GBytes  50.1 Gbits/sec                  receiver</span>
    
<span class="c"># 서버 파드 로그 확인 : 기본 5201 포트 Listen</span>
<span class="nv">$ </span>kubectl logs <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>iperf3-server <span class="nt">-f</span>
<span class="c"># =&gt; -----------------------------------------------------------</span>
<span class="c">#    Server listening on 5201 (test #4)</span>
<span class="c">#    -----------------------------------------------------------</span>
<span class="c">#    Accepted connection from 10.10.2.9, port 41962</span>
<span class="c">#    [  5] local 10.10.4.6 port 5201 connected to 10.10.2.9 port 41976</span>
<span class="c">#    [  8] local 10.10.4.6 port 5201 connected to 10.10.2.9 port 41982</span>
<span class="c">#    [ ID] Interval           Transfer     Bitrate</span>
<span class="c">#    [  5]   0.00-1.00   sec  2.87 GBytes  24.6 Gbits/sec</span>
<span class="c">#    [  8]   0.00-1.00   sec  2.87 GBytes  24.7 Gbits/sec</span>
<span class="c">#    [SUM]   0.00-1.00   sec  5.74 GBytes  49.3 Gbits/sec</span>
<span class="c">#    - - - - - - - - - - - - - - - - - - - - - - - - -</span>
<span class="c">#    ...</span>
<span class="c">#    - - - - - - - - - - - - - - - - - - - - - - - - -</span>
<span class="c">#    [ ID] Interval           Transfer     Bitrate</span>
<span class="c">#    [  5]   0.00-10.00  sec  29.2 GBytes  25.1 Gbits/sec                  receiver</span>
<span class="c">#    [  8]   0.00-10.00  sec  29.2 GBytes  25.0 Gbits/sec                  receiver</span>
<span class="c">#    [SUM]   0.00-10.00  sec  58.3 GBytes  50.1 Gbits/sec                  receiver</span>
</code></pre></div>        </div>
      </li>
      <li>실습결과 <code class="language-plaintext highlighter-rouge">iperf3 -c 127.0.0.1 -t 5</code>로 측정하였을때는 호스트에서는 67.1 Gbits/sec 였던것에 반해, 쿠버네티스를 통하면 41.6 Gbits/sec로 측정됩니다.
        <ul>
          <li>쿠버네티스도 로컬호스트에서 docker로 실행되는데 kube-proxy, iptables 포워딩 등의 오버헤드로 인해 발생하는것 같습니다.</li>
        </ul>
      </li>
      <li>UDP의 경우에도 <code class="language-plaintext highlighter-rouge">iperf3 -c 127.0.0.1 -u -b 20G</code>로 측정했을때 호스트에서는 20.0 Gbits/sec가 나오는데, 쿠버네티스를 통하면 1.41 Gbits/sec로 측정됩니다.
        <ul>
          <li>UDP는 더 오버헤드가 심한데 원인을 찾아봐야 할것 같습니다.</li>
        </ul>
      </li>
      <li>이번 실습을 통해 다양한 네트워크 CNI, 설정등을 변경해가며 최적의 설정을 찾아보는 방법을 배워보았습니다.</li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="마치며">마치며</h2>

<p>실습을 할수록 점점 더 iptables과 친숙해지는것 같습니다.
눈에 익은게 많아지고는 있지만, nftables라던지 ipvs라던지, eBPF라던지 아직 갈길이 멉니다. 😅</p>

<p>새삼스레 스터디를 진행하시는 가시다님을 비롯해서 조력자 분들도 정말 대단하다는 생각이 듭니다.
그리고 내용들 및 그림들이 가시다님이 집필하신 책에서 많이 가져왔습니다.
책이 출판되면 꼭 구매해서 읽어보겠습니다! 
이제 스터디도 중반을 향해 달려가고 있습니다. 남은 날들도 스터디에서 생존할 수 있기를 바랍니다. :pray:</p>]]></content><author><name></name></author><category term="kans" /><category term="kubernetes," /><category term="network," /><category term="linux" /><summary type="html"><![CDATA[지난주에 이어 이번주에는 Kubernetes의 Service, 그 중에 ClusterIP, NodePort에 대해 알아보겠습니다.]]></summary></entry><entry><title type="html">[KANS 3기] K8S Calico CNI</title><link href="https://sweetlittlebird.github.io/posts/2024-09-22-KANS-Study-Week3/" rel="alternate" type="text/html" title="[KANS 3기] K8S Calico CNI" /><published>2024-09-22T02:00:18+09:00</published><updated>2024-09-22T02:00:18+09:00</updated><id>https://sweetlittlebird.github.io/posts/KANS%20Study%20-%20Week3</id><content type="html" xml:base="https://sweetlittlebird.github.io/posts/2024-09-22-KANS-Study-Week3/"><![CDATA[<h2 id="들어가며">들어가며</h2>

<p>지난주에 이어 이번주에는 Calico CNI와 Calico Network Mode에 대해 알아보겠습니다.
KANS 3기 3주차 스터디를 시작하겠습니다.</p>

<hr />

<h2 id="calico-cni">Calico CNI</h2>

<h3 id="calico-소개">Calico 소개</h3>

<h4 id="calico란">Calico란?</h4>

<p>Calico CNI는 Kubernetes 클러스터에서 네트워크를 관리하는 CNI(Container Network Interface) 플러그인 중 하나로
Kubernetes와 non-Kubernetes/legacy 네트워크를 연결하는 역할을 합니다.
특징으로는 L3/L4 네트워크를 제공하며, BGP 프로토콜을 사용하여 라우팅을 수행합니다.
(모드에 따라 BGP를 사용하지 않을 수도 있습니다.)</p>

<h3 id="calico-설치">Calico 설치</h3>

<ul>
  <li>컨트롤플레인에서 <code class="language-plaintext highlighter-rouge">kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.28.1/manifests/calico.yaml</code> 명령어를 실행하여
Calico CNI를 설치할 수 있습니다. 이때 실습환경에 맞추기 위해 <code class="language-plaintext highlighter-rouge">CALICO_IPV4POOL_BLOCK_SIZE</code>를 “24”로 설정해야 합니다.</li>
  <li>
    <p>해당 부분이 적용된 yaml 파일인 <a href="https://raw.githubusercontent.com/gasida/KANS/main/kans3/calico-kans.yaml">calico-kans.yaml</a>를 사용하여 설치하였습니다.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 모니터링</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s1">'kubectl get pod -A -owide'</span>
  
<span class="c"># 컨트롤플레인(k8s-m)에서 calico cni 설치 실행</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> https://raw.githubusercontent.com/projectcalico/calico/v3.28.1/manifests/calico.yaml
<span class="c"># 기본 yaml 에 4946줄 이동 후 아래 내용 추가 해둠</span>
<span class="c">##            # Block size to use for the IPv4 POOL created at startup. Block size for IPv4 should be in the range 20-32. default 24</span>
<span class="c">##            - name: CALICO_IPV4POOL_BLOCK_SIZE</span>
<span class="c">##              value: "24"</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> https://raw.githubusercontent.com/gasida/KANS/main/kans3/calico-kans.yaml
<span class="c"># =&gt; poddisruptionbudget.policy/calico-kube-controllers created</span>
<span class="c">#    serviceaccount/calico-kube-controllers created</span>
<span class="c">#    serviceaccount/calico-node created</span>
<span class="c">#    serviceaccount/calico-cni-plugin created</span>
<span class="c">#    ...</span>
  
<span class="c"># 설치 확인</span>
<span class="nv">$ </span>tree /opt/cni/bin/
<span class="c"># =&gt; /opt/cni/bin/</span>
<span class="c">#    ├── bandwidth</span>
<span class="c">#    ├── bridge</span>
<span class="c">#    ├── calico</span>
<span class="c">#    ├── calico-ipam</span>
<span class="c">#    ├── dhcp</span>
<span class="c">#    ├── dummy</span>
<span class="c">#    ...</span>
<span class="nv">$ </span><span class="nb">ls</span> <span class="nt">-l</span> /opt/cni/bin/
<span class="nv">$ </span>ip <span class="nt">-c</span> route
<span class="c"># =&gt; ...</span>
<span class="c">#    172.16.34.0/24 via 192.168.20.100 dev tunl0 proto bird onlink</span>
<span class="c">#    blackhole 172.16.116.0/24 proto bird</span>
<span class="c">#    172.16.158.0/24 via 192.168.10.101 dev tunl0 proto bird onlink</span>
<span class="c">#    172.16.184.0/24 via 192.168.10.102 dev tunl0 proto bird onlink</span>
<span class="c">#    ...</span>
<span class="nv">$ </span>ip <span class="nt">-c</span> addr
<span class="nv">$ </span>iptables <span class="nt">-t</span> filter <span class="nt">-L</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-L</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> filter <span class="nt">-L</span> | <span class="nb">wc</span> <span class="nt">-l</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-L</span> | <span class="nb">wc</span> <span class="nt">-l</span>
  
<span class="c"># calicoctl 설치</span>
<span class="nv">$ </span>curl <span class="nt">-L</span> https://github.com/projectcalico/calico/releases/download/v3.28.1/calicoctl-linux-amd64 <span class="nt">-o</span> calicoctl
<span class="nv">$ </span><span class="nb">chmod</span> +x calicoctl <span class="o">&amp;&amp;</span> <span class="nb">mv </span>calicoctl /usr/bin
<span class="nv">$ </span>calicoctl version
  
<span class="c"># CNI 설치 후 파드 상태 확인</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-A</span> <span class="nt">-o</span> wide
<span class="c"># =&gt; NAMESPACE     NAME                                       READY   STATUS    RESTARTS      AGE     IP               NODE     NOMINATED NODE   READINESS GATES</span>
<span class="c">#    kube-system   calico-kube-controllers-77d59654f4-wbzth   1/1     Running   0             4m22s   172.16.34.2      k8s-w0   &lt;none&gt;           &lt;none&gt;</span>
<span class="c">#    kube-system   calico-node-545hj                          1/1     Running   0             4m22s   192.168.20.100   k8s-w0   &lt;none&gt;           &lt;none&gt;</span>
<span class="c">#    kube-system   calico-node-p5xpt                          1/1     Running   0             4m22s   192.168.10.10    k8s-m    &lt;none&gt;           &lt;none&gt;</span>
<span class="c">#    kube-system   calico-node-rmzvb                          1/1     Running   0             4m22s   192.168.10.101   k8s-w1   &lt;none&gt;           &lt;none&gt;</span>
<span class="c">#    kube-system   calico-node-sd9x8                          1/1     Running   0             4m22s   192.168.10.102   k8s-w2   &lt;none&gt;           &lt;none&gt;</span>
<span class="c">#    ...</span>
</code></pre></div>    </div>
  </li>
  <li>설치 확인을 했을때 위와 같이 정상적으로 설치가 되었다면 Calico CNI가 정상적으로 동작하고 있는 것입니다.</li>
  <li><code class="language-plaintext highlighter-rouge">ip -c route</code> 명령어를 통해 Bird 라우팅 테이블에 Calico CNI가 적용된 것을 확인할 수 있습니다.
그 중에서 <code class="language-plaintext highlighter-rouge">blackhole</code>은 해당 명령어를 실행하는 노드를 의미합니다.</li>
  <li>실습을 위해서 metrics-server를 설치하고, <code class="language-plaintext highlighter-rouge">kubectl top node</code> 명령어를 통해 노드의 리소스 사용량을 확인해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># metrics-server 설치</span>
<span class="nv">$ </span>helm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server/
<span class="c"># =&gt; "metrics-server" has been added to your repositories</span>
<span class="nv">$ </span>helm upgrade <span class="nt">--install</span> metrics-server metrics-server/metrics-server <span class="nt">--set</span> <span class="s1">'args[0]=--kubelet-insecure-tls'</span> <span class="nt">-n</span> kube-system
<span class="c"># =&gt; Release "metrics-server" does not exist. Installing it now.</span>
<span class="c">#    ...</span>
<span class="c">#      Chart version: 3.12.1</span>
<span class="c">#      App version:   0.7.1</span>
<span class="c">#      Image tag:     registry.k8s.io/metrics-server/metrics-server:v0.7.1</span>
<span class="nv">$ </span>kubectl get all <span class="nt">-n</span> kube-system <span class="nt">-l</span> app.kubernetes.io/instance<span class="o">=</span>metrics-server
<span class="nv">$ </span>kubectl get apiservices |egrep <span class="s1">'(AVAILABLE|metrics)'</span>
<span class="c"># =&gt; NAME                                   SERVICE                      AVAILABLE                  AGE</span>
<span class="c">#    v1beta1.metrics.k8s.io                 kube-system/metrics-server   False (MissingEndpoints)   16s</span>

<span class="c"># 확인</span>
<span class="nv">$ </span>kubectl top node  <span class="c"># 노드 리소스 사용량 확인</span>
<span class="c"># =&gt; NAME     CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%</span>
<span class="c">#    k8s-m    218m         5%     1131Mi          29%</span>
<span class="c">#    k8s-w0   95m          2%     807Mi           43%</span>
<span class="c">#    k8s-w1   77m          1%     768Mi           41%</span>
<span class="c">#    k8s-w2   62m          1%     806Mi           43%</span>
<span class="nv">$ </span>kubectl top pod <span class="nt">-A</span> <span class="nt">--sort-by</span><span class="o">=</span><span class="s1">'cpu'</span>    <span class="c"># 파드 리소스 사용량을 CPU 사용량 순으로 정렬해서 확인</span>
<span class="nv">$ </span>kubectl top pod <span class="nt">-A</span> <span class="nt">--sort-by</span><span class="o">=</span><span class="s1">'memory'</span> <span class="c"># 파드 리소스 사용량을 Memory 사용량 순으로 정렬해서 확인</span>

<span class="c"># (참고) 삭제</span>
<span class="nv">$ </span>helm uninstall <span class="nt">-n</span> kube-system metrics-server
</code></pre></div></div>

<h3 id="calico-cni-구성요소">Calico CNI 구성요소</h3>

<p><img src="/assets/2024/kans-3th/w3/20240921_kans_w3_1.png" alt="img.png" class="image-center" />
<em class="image-caption">출처: 추가예정</em></p>
<ul>
  <li><strong>Calico Datastore</strong> : Calico의 구성 정보를 저장하는 데이터베이스입니다. Kubernetes API 서버(기본값) 또는 etcd에 저장됩니다.</li>
  <li><strong>Bird</strong> : 오픈소스 라우팅 데몬으로, Calico CNI에서 라우팅을 수행합니다.
Bird는 BGP 프로토콜을 이용한 라우팅 데몬으로
Calico나 Kubernetes에서만 사용되는것이 아닌 일반적인 라우팅 데몬입니다.
노드의 파드 네트워크 대역을 BGP 라우팅 프로토콜을 통해서 광고(advertise)합니다.</li>
  <li><strong>Felix</strong> : Bird를 통해 배포된 라우팅 정보를 수신하여, 노드의 파드 네트워크 대역을 호스트의 라우팅 테이블에
업데이트 하는 역할을 합니다. 또한 Iptables 등 방화벽 규칙 설정 관리를 합니다.</li>
  <li><strong>Confd</strong> : Calico 구성 정보를 관리하는 데몬으로, BGP 설정등으로 Calico 데이터 저장소에 변경이 발생하면
Bird의 설정 파일을 만들고, 변경된 설정 파일을 반영하게 합니다.</li>
  <li><strong>CNI IPAM Plugin</strong> : Calico가 제공하는 IPAM(IP Address Management) 플러그인으로
Calico CNI에서 IP 주소를 할당하는 역할을 합니다. (Flannel CNI의 경우 기본 IPAM인 host-local IPAM을 사용합니다.)</li>
  <li><strong>calico-kube-controllers</strong> : Calico의 동작을 감시 및 제어하는 컨트롤러입니다.</li>
  <li><strong>Typha</strong> : Calico의 성능을 향상시키기 위한 컴포넌트로, Calico의 데이터베이스에 대한 읽기 전용 요청을 처리합니다.
워커노드 수가 많지 않은 경우 생략해도 무방합니다.</li>
  <li><strong>calicoctl</strong> : Calico를 제어할 수 있는 CLI 인터페이스로, datastore에 접근하여 Calico의 구성 정보를 관리할 수 있습니다.</li>
</ul>

<h4 id="calico-구성요소-확인">Calico 구성요소 확인</h4>

<ul>
  <li>
    <p>설치된 구성요소를 명령들을 사용해서 살펴보겠습니다.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 버전 확인 - 링크</span>
<span class="c">## kdd 의미는 쿠버네티스 API 를 데이터저장소로 사용 : k8s API datastore(kdd)</span>
<span class="nv">$ </span>calicoctl version
<span class="c"># =&gt; Client Version:    v3.28.1</span>
<span class="c">#    ...</span>
  
<span class="c"># calico 관련 정보 확인</span>
<span class="nv">$ </span>kubectl get daemonset <span class="nt">-n</span> kube-system
<span class="c"># =&gt; NAME          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE</span>
<span class="c">#    calico-node   4         4         4       4            4           kubernetes.io/os=linux   26m</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-n</span> kube-system <span class="nt">-l</span> k8s-app<span class="o">=</span>calico-node <span class="nt">-owide</span>
<span class="c"># =&gt; NAME                READY   STATUS    RESTARTS   AGE   IP               NODE     NOMINATED NODE   READINESS GATES</span>
<span class="c">#    calico-node-545hj   1/1     Running   0          26m   192.168.20.100   k8s-w0   &lt;none&gt;           &lt;none&gt;</span>
<span class="c">#    calico-node-p5xpt   1/1     Running   0          26m   192.168.10.10    k8s-m    &lt;none&gt;           &lt;none&gt;</span>
<span class="c">#    calico-node-rmzvb   1/1     Running   0          26m   192.168.10.101   k8s-w1   &lt;none&gt;           &lt;none&gt;</span>
<span class="c">#    calico-node-sd9x8   1/1     Running   0          26m   192.168.10.102   k8s-w2   &lt;none&gt;           &lt;none&gt;</span>
<span class="c"># calico-node 는 데몬셋으로 모든 노드에 배포되어 있음을 확인할 수 있습니다.</span>
  
<span class="c"># calico-kube-controllers 정보 확인</span>
<span class="nv">$ </span>kubectl get deploy <span class="nt">-n</span> kube-system calico-kube-controllers
<span class="c"># =&gt; NAME                      READY   UP-TO-DATE   AVAILABLE   AGE</span>
<span class="c">#    calico-kube-controllers   1/1     1            1           27m</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-n</span> kube-system <span class="nt">-l</span> k8s-app<span class="o">=</span>calico-kube-controllers <span class="nt">-owide</span>
<span class="c"># =&gt; NAME                                       READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES</span>
<span class="c">#    calico-kube-controllers-77d59654f4-wbzth   1/1     Running   0          28m   172.16.34.2   k8s-w0   &lt;none&gt;           &lt;none&gt;</span>
  
<span class="c"># 칼리코 IPAM 정보 확인 : 칼리코 CNI 를 사용한 파드가 생성된 노드에 podCIDR 네트워크 대역 확인 - 링크</span>
<span class="nv">$ </span>calicoctl ipam show
<span class="c"># =&gt; +----------+---------------+-----------+------------+--------------+</span>
<span class="c">#    | GROUPING |     CIDR      | IPS TOTAL | IPS IN USE |   IPS FREE   |</span>
<span class="c">#    +----------+---------------+-----------+------------+--------------+</span>
<span class="c">#    | IP Pool  | 172.16.0.0/16 |     65536 | 8 (0%)     | 65528 (100%) |</span>
<span class="c">#    +----------+---------------+-----------+------------+--------------+</span>
  
<span class="c"># Block 는 각 노드에 할당된 podCIDR 정보</span>
<span class="nv">$ </span>calicoctl ipam show <span class="nt">--show-blocks</span>
<span class="c"># =&gt; +----------+-----------------+-----------+------------+--------------+</span>
<span class="c">#    | GROUPING |      CIDR       | IPS TOTAL | IPS IN USE |   IPS FREE   |</span>
<span class="c">#    +----------+-----------------+-----------+------------+--------------+</span>
<span class="c">#    | IP Pool  | 172.16.0.0/16   |     65536 | 8 (0%)     | 65528 (100%) |</span>
<span class="c">#    | Block    | 172.16.116.0/24 |       256 | 1 (0%)     | 255 (100%)   |</span>
<span class="c">#    | Block    | 172.16.158.0/24 |       256 | 2 (1%)     | 254 (99%)    |</span>
<span class="c">#    | Block    | 172.16.184.0/24 |       256 | 1 (0%)     | 255 (100%)   |</span>
<span class="c">#    | Block    | 172.16.34.0/24  |       256 | 4 (2%)     | 252 (98%)    |</span>
<span class="c">#    +----------+-----------------+-----------+------------+--------------+</span>
<span class="nv">$ </span>calicoctl ipam show <span class="nt">--show-borrowed</span>
<span class="nv">$ </span>calicoctl ipam show <span class="nt">--show-configuration</span>
  
<span class="c"># host-local IPAM 정보 확인 : k8s-m 노드의 podCIDR 은 host-local 대신 칼리코 IPAM 를 사용함</span>
  
<span class="c"># 워커 노드마다 할당된 dedicated subnet (podCIDR) 확인</span>
<span class="nv">$ </span>kubectl get nodes <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.items[*].spec.podCIDR}'</span> <span class="p">;</span><span class="nb">echo</span>
<span class="c"># =&gt; 172.16.0.0/24 172.16.1.0/24 172.16.2.0/24 172.16.4.0/24</span>
  
<span class="c"># 컨트롤플레인의 pod에 할당되는 podCIDR 확인</span>
<span class="nv">$ </span>kubectl get node k8s-m <span class="nt">-o</span> json | jq <span class="s1">'.spec.podCIDR'</span>
<span class="c"># =&gt; "172.16.0.0/24"</span>
  
<span class="c"># CNI Plugin 정보 확인 - 링크</span>
<span class="nv">$ </span>tree /etc/cni/net.d/
<span class="c"># =&gt; /etc/cni/net.d/</span>
<span class="c">#    ├── 10-calico.conflist</span>
<span class="c">#    └── calico-kubeconfig</span>
<span class="nv">$ </span><span class="nb">cat</span> /etc/cni/net.d/10-calico.conflist | jq
<span class="c"># =&gt; ...</span>
<span class="c">#    "datastore_type": "kubernetes", # 칼리코 데이터저장소는 쿠버네티스 API 를 사용</span>
<span class="c">#    "ipam": { </span>
<span class="c">#      "type": "calico-ipam" # IPAM 은 칼리코 자체 IPAM 을 사용</span>
<span class="c">#    },</span>
<span class="c">#    ...</span>
  
<span class="c"># calicoctl node 정보 확인 : Bird 데몬(BGP)을 통한 BGP 네이버 연결 정보(bgp peer 는 노드의 IP로 연결) - 링크</span>
<span class="nv">$ </span>calicoctl node status
<span class="c"># =&gt; Calico process is running.</span>
<span class="c">#    </span>
<span class="c">#    IPv4 BGP status</span>
<span class="c">#    +----------------+-------------------+-------+----------+-------------+</span>
<span class="c">#    |  PEER ADDRESS  |     PEER TYPE     | STATE |  SINCE   |    INFO     |</span>
<span class="c">#    +----------------+-------------------+-------+----------+-------------+</span>
<span class="c">#    | 192.168.20.100 | node-to-node mesh | up    | 14:13:02 | Established |</span>
<span class="c">#    | 192.168.10.101 | node-to-node mesh | up    | 14:13:31 | Established |</span>
<span class="c">#    | 192.168.10.102 | node-to-node mesh | up    | 14:12:44 | Established |</span>
<span class="c">#    +----------------+-------------------+-------+----------+-------------+</span>
<span class="nv">$ </span>calicoctl node checksystem
<span class="c"># =&gt; Checking kernel version...</span>
<span class="c">#       5.15.0-119-generic  					OK</span>
<span class="c">#    Checking kernel modules...</span>
<span class="c">#       xt_mark             					OK</span>
  
<span class="c"># ippool 정보 확인 : 클러스터가 사용하는 IP 대역 정보와 칼리코 모드 정보 확인</span>
<span class="nv">$ </span>calicoctl get ippool <span class="nt">-o</span> wide
<span class="c"># =&gt; NAME                  CIDR            NAT    IPIPMODE   VXLANMODE   DISABLED   DISABLEBGPEXPORT   SELECTOR</span>
<span class="c">#    default-ipv4-ippool   172.16.0.0/16   true   Always     Never       false      false              all()</span>
  
<span class="c"># 파드와 서비스 사용 네트워크 대역 정보 확인 </span>
<span class="nv">$ </span>kubectl cluster-info dump | <span class="nb">grep</span> <span class="nt">-m</span> 2 <span class="nt">-E</span> <span class="s2">"cluster-cidr|service-cluster-ip-range"</span>  
<span class="c"># =&gt; "--service-cluster-ip-range=10.200.1.0/24",</span>
<span class="c">#    "--cluster-cidr=172.16.0.0/16",</span>
                              
<span class="nv">$ </span>kubectl get cm <span class="nt">-n</span> kube-system kubeadm-config <span class="nt">-oyaml</span> | <span class="nb">grep</span> <span class="nt">-i</span> subnet
<span class="c"># =&gt; podSubnet: 172.16.0.0/16</span>
<span class="c">#    serviceSubnet: 10.200.1.0/24</span>
   
<span class="c"># calico endpoint (파드)의 정보 확인 : WORKLOAD 는 파드 이름이며, 어떤 노드에 배포되었고 IP 와 cali 인터페이스와 연결됨을 확인</span>
<span class="nv">$ </span>calicoctl get workloadEndpoint
<span class="nv">$ </span>calicoctl get workloadEndpoint <span class="nt">-A</span>
<span class="nv">$ </span>calicoctl get workloadEndpoint <span class="nt">-o</span> wide <span class="nt">-A</span>
<span class="c"># =&gt; NAMESPACE     NAME                                                            WORKLOAD                                   NODE     NETWORKS          INTERFACE         PROFILES                                                  NATS</span>
<span class="c">#    kube-system   k8s--w0-k8s-calico--kube--controllers--77d59654f4--wbzth-eth0   calico-kube-controllers-77d59654f4-wbzth   k8s-w0   172.16.34.5/32    cali544ab3155a5   kns.kube-system,ksa.kube-system.calico-kube-controllers</span>
<span class="c">#    kube-system   k8s--w0-k8s-coredns--55cb58b774--7qvtv-eth0                     coredns-55cb58b774-7qvtv                   k8s-w0   172.16.34.6/32    cali6be4c908feb   kns.kube-system,ksa.kube-system.coredns</span>
<span class="c">#    kube-system   k8s--w0-k8s-coredns--55cb58b774--8q4f6-eth0                     coredns-55cb58b774-8q4f6                   k8s-w0   172.16.34.4/32    cali23a9e6edc85   kns.kube-system,ksa.kube-system.coredns</span>
<span class="c">#    kube-system   k8s--w1-k8s-metrics--server--68cfccbdf6--wjjs2-eth0             metrics-server-68cfccbdf6-wjjs2            k8s-w1   172.16.158.2/32   cali2789c1b51d6   kns.kube-system,ksa.kube-system.metrics-server</span>
  
<span class="c"># 노드에서 컨테이너(프로세스) 확인</span>
<span class="nv">$ </span>ps axf 
<span class="c"># =&gt;    1325 ?        Sl     0:02 /usr/bin/containerd-shim-runc-v2 -namespace k8s.io -id 4b2a9892a9147b1a3b73b4864bec5270f18da7d8393b82f8863dc8a6cee8ac0c -address /run/containerd/conta</span>
<span class="c">#       1352 ?        Ss     0:00  \_ /pause</span>
<span class="c">#       1762 ?        Ss     0:00  \_ /usr/local/bin/runsvdir -P /etc/service/enabled</span>
<span class="c">#       1838 ?        Ss     0:00      \_ runsv confd</span>
<span class="c">#       1853 ?        Sl     0:00      |   \_ calico-node -confd</span>
<span class="c">#       1839 ?        Ss     0:00      \_ runsv bird</span>
<span class="c">#       2032 ?        S      0:00      |   \_ bird -R -s /var/run/calico/bird.ctl -d -c /etc/calico/confd/config/bird.cfg</span>
<span class="c">#       1840 ?        Ss     0:00      \_ runsv node-status-reporter</span>
<span class="c">#       1847 ?        Sl     0:00      |   \_ calico-node -status-reporter</span>
<span class="c">#       1841 ?        Ss     0:00      \_ runsv monitor-addresses</span>
<span class="c">#       1851 ?        Sl     0:00      |   \_ calico-node -monitor-addresses</span>
<span class="c">#       1842 ?        Ss     0:00      \_ runsv felix</span>
<span class="c">#       1849 ?        Sl     0:27      |   \_ calico-node -felix</span>
<span class="c">#       1843 ?        Ss     0:00      \_ runsv allocate-tunnel-addrs</span>
<span class="c">#       1846 ?        Sl     0:00      |   \_ calico-node -allocate-tunnel-addrs</span>
<span class="c">#       1844 ?        Ss     0:00      \_ runsv cni</span>
<span class="c">#       1848 ?        Sl     0:00      |   \_ calico-node -monitor-token</span>
<span class="c">#       1845 ?        Ss     0:00      \_ runsv bird6</span>
<span class="c">#       2033 ?        S      0:00          \_ bird6 -R -s /var/run/calico/bird6.ctl -d -c /etc/calico/confd/config/bird6.cfg</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p><strong>felix</strong> : Host의 Network Interface, Routing Table, Iptables를 관리합니다.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Calico의 Felix를 통해 설정된 iptables 규칙 설정 확인</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> filter <span class="nt">-S</span> | <span class="nb">grep </span>cali
<span class="c"># =&gt; -N cali-FORWARD</span>
<span class="c">#    -N cali-INPUT</span>
<span class="c">#    -N cali-OUTPUT</span>
<span class="c">#    -N cali-cidr-block</span>
<span class="c">#    -N cali-from-hep-forward</span>
<span class="c">#    -N cali-from-host-endpoint</span>
<span class="c">#    -N cali-from-wl-dispatch</span>
<span class="c">#    -N cali-to-hep-forward</span>
<span class="c">#    -N cali-to-host-endpoint</span>
<span class="c">#    -N cali-to-wl-dispatch</span>
<span class="c">#    -N cali-wl-to-host</span>
<span class="c">#    -A INPUT -m comment --comment "cali:Cz_u1IQiXIMmKD4c" -j cali-INPUT</span>
<span class="c">#    -A FORWARD -m comment --comment "cali:wUHhoiAYhphO9Mso" -j cali-FORWARD</span>
<span class="c">#    -A FORWARD -m comment --comment "cali:S93hcgKJrXEqnTfs" -m comment --comment "Policy explicitly accepted packet." -j ACCEPT</span>
<span class="c">#    -A FORWARD -m comment --comment "cali:mp77cMpurHhyjLrM" -j MARK --set-xmark 0x10000/0x10000</span>
<span class="c">#    -A OUTPUT -m comment --comment "cali:tVnHkvAo15HuiPy0" -j cali-OUTPUT</span>
<span class="c">#    -A cali-FORWARD -m comment --comment "cali:vjrMJCRpqwy5oRoX" -j MARK --set-xmark 0x0/0xe0000</span>
<span class="c">#    -A cali-FORWARD -m comment --comment "cali:A_sPAO0mcxbT9mOV" -j cali-from-hep-forward</span>
<span class="c">#    -A cali-FORWARD -i cali+ -m comment --comment "cali:8ZoYfO5HKXWbB3pk" -j cali-from-wl-dispatch</span>
<span class="c">#    -A cali-FORWARD -o cali+ -m comment --comment "cali:jdEuaPBe14V2hutn" -j cali-to-wl-dispatch</span>
<span class="c">#    -A cali-FORWARD -m comment --comment "cali:12bc6HljsMKsmfr-" -j cali-to-hep-forward</span>
<span class="c">#    -A cali-FORWARD -m comment --comment "cali:NOSxoaGx8OIstr1z" -j cali-cidr-block</span>
<span class="c">#    -A cali-INPUT -p ipencap -m comment --comment "cali:PajejrV4aFdkZojI" -m comment --comment "Allow IPIP packets from Calico hosts" -m set --match-set cali40all-hosts-net src -m addrtype --dst-type LOCAL -j ACCEPT</span>
<span class="c">#    -A cali-INPUT -p ipencap -m comment --comment "cali:_wjq-Yrma8Ly1Svo" -m comment --comment "Drop IPIP packets from non-Calico hosts" -j DROP</span>
<span class="c">#    -A cali-INPUT -i cali+ -m comment --comment "cali:8TZGxLWh_Eiz66wc" -g cali-wl-to-host</span>
<span class="c">#    -A cali-INPUT -m comment --comment "cali:6McIeIDvPdL6PE1T" -j ACCEPT</span>
<span class="c">#    -A cali-INPUT -m comment --comment "cali:YGPbrUms7NId8xVa" -j MARK --set-xmark 0x0/0xf0000</span>
<span class="c">#    -A cali-INPUT -m comment --comment "cali:2gmY7Bg2i0i84Wk_" -j cali-from-host-endpoint</span>
<span class="c">#    -A cali-INPUT -m comment --comment "cali:q-Vz2ZT9iGE331LL" -m comment --comment "Host endpoint policy accepted packet." -j ACCEPT</span>
<span class="c">#    -A cali-OUTPUT -m comment --comment "cali:Mq1_rAdXXH3YkrzW" -j ACCEPT</span>
<span class="c">#    -A cali-OUTPUT -o cali+ -m comment --comment "cali:69FkRTJDvD5Vu6Vl" -j RETURN</span>
<span class="c">#    -A cali-OUTPUT -p ipencap -m comment --comment "cali:AnEsmO6bDZbQntWW" -m comment --comment "Allow IPIP packets to other Calico hosts" -m set --match-set cali40all-hosts-net dst -m addrtype --src-type LOCAL -j ACCEPT</span>
<span class="c">#    -A cali-OUTPUT -m comment --comment "cali:9e9Uf3GU5tX--Lxy" -j MARK --set-xmark 0x0/0xf0000</span>
<span class="c">#    -A cali-OUTPUT -m comment --comment "cali:0f3LDz_VKuHFaA2K" -m conntrack ! --ctstate DNAT -j cali-to-host-endpoint</span>
<span class="c">#    -A cali-OUTPUT -m comment --comment "cali:OgU2f8BVEAZ_fwkq" -m comment --comment "Host endpoint policy accepted packet." -j ACCEPT</span>
<span class="c">#    -A cali-from-wl-dispatch -m comment --comment "cali:zTj6P0TIgYvgz-md" -m comment --comment "Unknown interface" -j DROP</span>
<span class="c">#    -A cali-to-wl-dispatch -m comment --comment "cali:7KNphB1nNHw80nIO" -m comment --comment "Unknown interface" -j DROP</span>
<span class="c">#    -A cali-wl-to-host -m comment --comment "cali:Ee9Sbo10IpVujdIY" -j cali-from-wl-dispatch</span>
<span class="c">#    -A cali-wl-to-host -m comment --comment "cali:nSZbcOoG1xPONxb8" -m comment --comment "Configured DefaultEndpointToHostAction" -j ACCEPT</span>
  
<span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span> | <span class="nb">grep </span>cali
<span class="c"># =&gt; -N cali-OUTPUT</span>
<span class="c">#    -N cali-POSTROUTING</span>
<span class="c">#    -N cali-PREROUTING</span>
<span class="c">#    -N cali-fip-dnat</span>
<span class="c">#    -N cali-fip-snat</span>
<span class="c">#    -N cali-nat-outgoing</span>
<span class="c">#    -A PREROUTING -m comment --comment "cali:6gwbT8clXdHdC1b1" -j cali-PREROUTING</span>
<span class="c">#    -A OUTPUT -m comment --comment "cali:tVnHkvAo15HuiPy0" -j cali-OUTPUT</span>
<span class="c">#    -A POSTROUTING -m comment --comment "cali:O3lYWMrLQYEMJtB5" -j cali-POSTROUTING</span>
<span class="c">#    -A cali-OUTPUT -m comment --comment "cali:GBTAv2p5CwevEyJm" -j cali-fip-dnat</span>
<span class="c">#    -A cali-POSTROUTING -m comment --comment "cali:Z-c7XtVd2Bq7s_hA" -j cali-fip-snat</span>
<span class="c">#    -A cali-POSTROUTING -m comment --comment "cali:nYKhEzDlr11Jccal" -j cali-nat-outgoing</span>
<span class="c">#    -A cali-POSTROUTING -o tunl0 -m comment --comment "cali:SXWvdsbh4Mw7wOln" -m addrtype ! --src-type LOCAL --limit-iface-out -m addrtype --src-type LOCAL -j MASQUERADE --random-fully</span>
<span class="c">#    -A cali-PREROUTING -m comment --comment "cali:r6XmIziWUJsdOK6Z" -j cali-fip-dnat</span>
<span class="c">#    -A cali-nat-outgoing -m comment --comment "cali:flqWnvo8yq4ULQLa" -m set --match-set cali40masq-ipam-pools src -m set ! --match-set cali40all-ipam-pools dst -j MASQUERADE --random-fully</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p><strong>bird</strong> : BGP 라우팅 데몬으로, Calico CNI에서 라우팅을 수행합니다.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl get pod <span class="nt">-n</span> kube-system <span class="nt">-l</span> k8s-app<span class="o">=</span>calico-node <span class="nt">-o</span> wide
<span class="c"># =&gt; NAME                READY   STATUS    RESTARTS      AGE   IP               NODE     NOMINATED NODE   READINESS GATES</span>
<span class="c">#    calico-node-p5xpt   1/1     Running   1 (42m ago)   24h   192.168.10.10    k8s-m    &lt;none&gt;           &lt;none&gt;</span>
<span class="c">#    calico-node-545hj   1/1     Running   1 (42m ago)   24h   192.168.20.100   k8s-w0   &lt;none&gt;           &lt;none&gt;</span>
<span class="c">#    calico-node-rmzvb   1/1     Running   1 (42m ago)   24h   192.168.10.101   k8s-w1   &lt;none&gt;           &lt;none&gt;</span>
<span class="c">#    calico-node-sd9x8   1/1     Running   1 (42m ago)   24h   192.168.10.102   k8s-w2   &lt;none&gt;           &lt;none&gt;</span>
  
<span class="c"># Bird 라우팅 테이블 확인</span>
<span class="nv">$ </span>kubectl <span class="nt">-n</span> kube-system <span class="nb">exec</span> <span class="nt">-it</span> calico-node-545hj <span class="nt">--</span> birdcl show route
<span class="c"># =&gt; BIRD v0.3.3+birdv1.6.8 ready.</span>
<span class="c">#    0.0.0.0/0          via 10.0.2.2 on enp0s3 [kernel1 14:13:00] * (10)</span>
<span class="c">#    10.0.2.2/32        dev enp0s3 [kernel1 14:13:00] * (10)</span>
<span class="c">#    10.0.2.3/32        dev enp0s3 [kernel1 14:13:00] * (10)</span>
<span class="c">#    10.0.2.0/24        dev enp0s3 [direct1 14:12:59] * (240)</span>
<span class="c">#    172.16.184.0/24    via 192.168.20.254 on enp0s8 [Mesh_192_168_10_102 14:13:01 from 192.168.10.102] * (100/?) [i]</span>
<span class="c">#    192.168.10.0/24    via 192.168.20.254 on enp0s8 [kernel1 14:13:00] * (10)</span>
<span class="c">#    172.16.158.0/24    via 192.168.20.254 on enp0s8 [Mesh_192_168_10_101 14:13:30 from 192.168.10.101] * (100/?) [i]</span>
<span class="c">#    192.168.20.0/24    dev enp0s8 [direct1 14:12:59] * (240)</span>
<span class="c">#    172.16.116.0/24    via 192.168.20.254 on enp0s8 [Mesh_192_168_10_10 14:13:01 from 192.168.10.10] * (100/?) [i]</span>
<span class="c">#    172.16.34.0/24     blackhole [static1 14:12:59] * (200)</span>
<span class="c">#    172.16.34.0/32     dev tunl0 [direct1 14:12:59] * (240)</span>
<span class="c">#    172.16.34.6/32     dev cali6be4c908feb [kernel1 14:13:09] * (10)</span>
<span class="c">#    172.16.34.5/32     dev cali544ab3155a5 [kernel1 14:13:08] * (10)</span>
<span class="c">#    172.16.34.4/32     dev cali23a9e6edc85 [kernel1 14:13:08] * (10)</span>
  
<span class="nv">$ </span>kubectl <span class="nt">-n</span> kube-system <span class="nb">exec</span> <span class="nt">-it</span> calico-node-545hj <span class="nt">--</span> birdcl show protocol
<span class="c"># =&gt; BIRD v0.3.3+birdv1.6.8 ready.</span>
<span class="c">#    name     proto    table    state  since       info</span>
<span class="c">#    static1  Static   master   up     14:13:00</span>
<span class="c">#    kernel1  Kernel   master   up     14:13:00</span>
<span class="c">#    device1  Device   master   up     14:13:00</span>
<span class="c">#    direct1  Direct   master   up     14:13:00</span>
<span class="c">#    Mesh_192_168_10_10 BGP      master   up     14:13:02    Established</span>
<span class="c">#    Mesh_192_168_10_101 BGP      master   up     14:13:31    Established</span>
<span class="c">#    Mesh_192_168_10_102 BGP      master   up     14:13:02    Established</span>
  
<span class="nv">$ </span>kubectl <span class="nt">-n</span> kube-system <span class="nb">exec</span> <span class="nt">-it</span> calico-node-545hj <span class="nt">--</span> birdcl show status
<span class="c"># =&gt; BIRD v0.3.3+birdv1.6.8 ready.</span>
<span class="c">#    BIRD v0.3.3+birdv1.6.8</span>
<span class="c">#    Router ID is 192.168.20.100</span>
<span class="c">#    Current server time is 2024-09-19 14:57:39</span>
<span class="c">#    Last reboot on 2024-09-19 14:13:00</span>
<span class="c">#    Last reconfiguration on 2024-09-19 14:13:00</span>
<span class="c">#    Daemon is up and running</span>
</code></pre></div>    </div>
  </li>
</ul>

<h4 id="노드간의-bgp-전달과정-확인">노드간의 BGP 전달과정 확인</h4>

<ul>
  <li>Calico CNI에서는 BGP 프로토콜을 사용하여 노드간의 라우팅 정보를 교환합니다.
해당 역할을 BIRD가 수행하는데 그림으로 살펴보면 아래와 같습니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w3/20240921_kans_w3_3.png" alt="img.png" class="image-center" />
<em class="image-caption">Bird와 Felix의 역할 모식도 (출처: 추가예정)</em></p>

<ul>
  <li>노드간의 BGP 전달을 패킷을 캡쳐해서 확인해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 워커 노드를 종료한 상태에서 아래의 작업을 시작합니다.</span>

<span class="c"># 노드에서 패킷 캡쳐</span>
<span class="nv">$ </span>vagrant ssh k8s-m

<span class="c"># BGP는 TCP 179 포트를 사용하므로 해당 포트로 패킷을 캡쳐합니다.</span>
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> enp0s8 tcp port 179 <span class="nt">-w</span> bgp.cap

<span class="c"># 종료된 워커 노드를 시작합니다.</span>
<span class="c"># 워커 노드가 시작되면 BGP 연결이 재설정되고 패킷이 전송됩니다.</span>
</code></pre></div></div>

<ul>
  <li>
    <p>캡쳐된 패킷이 bgp.cap 파일로 저장되었으면, Wireshark를 사용하여 패킷을 확인합니다.</p>

    <p><img src="/assets/2024/kans-3th/w3/20240921_kans_w3_4.png" alt="20240921_kans_w3_4.png" /></p>
  </li>
  <li>
    <p>워커 노드에 접속해서 노드 ip와 ipip tunneling ip를 확인합니다.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 워커노드 k8s-w2 접속</span>
<span class="nv">$ </span>vagrant ssh k8s-w2
  
<span class="nv">$ </span>ip addr
<span class="c"># =&gt; ...</span>
<span class="c">#    3: enp0s8: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000</span>
<span class="c">#        link/ether 08:00:27:af:2e:7a brd ff:ff:ff:ff:ff:ff</span>
<span class="c">#        inet &lt;span style="color: red;"&gt;192.168.10.102&lt;/span&gt;/24 brd 192.168.10.255 scope global enp0s8  &lt;span style="color: red;"&gt;# node ip&lt;/span&gt;   </span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    4: tunl0@NONE: &lt;NOARP,UP,LOWER_UP&gt; mtu 1480 qdisc noqueue state UNKNOWN group default qlen 1000</span>
<span class="c">#        link/ipip 0.0.0.0 brd 0.0.0.0                                  </span>
<span class="c">#        inet &lt;span style="color: red;"&gt;172.16.184.0&lt;/span&gt;/32 scope global tunl0                        &lt;span style="color: red;"&gt;# ipip tunneling ip&lt;/span&gt;</span>
<span class="c">#           valid_lft forever preferred_lft forever </span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>컨트롤 플레인 노드에 접속해서 라우팅 테이블을 확인해보겠습니다.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 컨트롤 플레인 노드 k8s-m 접속</span>
<span class="nv">$ </span>vagrant ssh k8s-m
  
<span class="c"># 라우팅 테이블 확인</span>
<span class="nv">$ </span>ip route show 172.16.184.0/24
<span class="c"># =&gt; &lt;span style="color: red;"&gt;172.16.184.0&lt;/span&gt;/24 via &lt;span style="color: red;"&gt;192.168.10.102&lt;/span&gt; dev tunl0 proto bird onlink</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>위와 같이 BGP를 통해 노드간 라우팅 정보를 교환하고, 라우팅 테이블을 업데이트하는 것을 확인할 수 있었습니다.</p>
  </li>
</ul>

<h3 id="calico-통신-흐름-확인">Calico 통신 흐름 확인</h3>

<ul>
  <li>Calico CNI를 사용한 통신을 확인해보겠습니다.</li>
</ul>

<h4 id="동일-노드에서-파드pod-간-통신">동일 노드에서 파드(Pod) 간 통신</h4>

<ul>
  <li>동일 노드에서 파드간 통신은 기본적으로 직접 통신합니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w3/20240921_kans_w3_5.png" alt="img.png" class="w-80 image-center" /></p>

<ul>
  <li>iptables FORWARD Rule 정책에서 파드간 포워딩 정책을 허용합니다.</li>
  <li>calice# 인터페이스에 proxy arp 설정을 통해 게이트웨이의 MAC 주소를 파드가 전달 받아 사용합니다.</li>
  <li>동일 노드 내의 파드 간 통신에서는 tunnel interface를 사용하지 않습니다.</li>
</ul>

<h5 id="파드-배포-전-기본-상태-확인">파드 배포 전 기본 상태 확인</h5>

<ul>
  <li>파드 배포 전에는 아래와 같이 파드가 없는 상태입니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w3/20240921_kans_w3_6.png" alt="img.png" class="w-70 image-center" />
<em class="image-caption">파드 배포 전 상태 (출처: 추가예정)</em></p>

<ul>
  <li>
    <p>파드 생성 전 노드(k8s-w1) Shell 에서 기본 정보 확인</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 네트워크 인터페이스 정보 확인 : 터널(ipip) 인터페이스가 존재</span>
<span class="nv">$ </span>ip <span class="nt">-c</span> <span class="nt">-d</span> addr show tunl0
<span class="c"># =&gt; 4: tunl0@NONE: &lt;NOARP,UP,LOWER_UP&gt; mtu 1480 qdisc noqueue state UNKNOWN group default qlen 1000</span>
<span class="c">#        link/ipip 0.0.0.0 brd 0.0.0.0 promiscuity 0 minmtu 0 maxmtu 0</span>
<span class="c">#        &lt;span style="color: red;"&gt;ipip&lt;/span&gt; any remote any local any ttl inherit nopmtudisc numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535</span>
<span class="c">#        inet &lt;span style="color: red;"&gt;172.16.158.0/32&lt;/span&gt; scope global tunl0</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
  
<span class="c"># 네트워크 네임스페이스 확인</span>
<span class="nv">$ </span>lsns <span class="nt">-t</span> net
<span class="c"># =&gt;         NS TYPE NPROCS   PID USER     NETNSID NSFS                                                COMMAND</span>
<span class="c">#    4026531840 net     145     1 root  unassigned                                                     /sbin/i</span>
<span class="c">#    4026532204 net       2  2009 65535          0 /run/netns/cni-6923c332-7c35-669a-8787-232597fa3fa8 /pause</span>
  
<span class="c"># 네트워크 라우팅 경로 정보 확인</span>
<span class="c"># 이중 bird 는 bird 데몬이 BGP 라우팅 프로토콜에 의해 파드 네트워크 대역을 전달받거나 전달하는 경로 → 각각 노드의 파드 대역입니다</span>
<span class="nv">$ </span>ip <span class="nt">-c</span> route | <span class="nb">grep </span>bird
<span class="c"># =&gt; blackhole 172.16.158.0/24 proto bird</span>
<span class="c"># =&gt; 172.16.34.0/24 via 192.168.20.100 dev tunl0 proto bird onlink</span>
<span class="c"># =&gt; 172.16.116.0/24 via 192.168.10.10 dev tunl0 proto bird onlink</span>
<span class="c"># =&gt; 172.16.184.0/24 via 192.168.10.102 dev tunl0 proto bird onlink</span>
  
<span class="c"># 아래 tunl0 Iface 에 목적지 네트워크 대역은 ipip 인캡슐레이션에 의해서 각 노드에 전달됩니다 → 각각 노드의 파드 대역입니다</span>
<span class="nv">$ </span>route <span class="nt">-n</span>
<span class="c"># =&gt; Kernel IP routing table</span>
<span class="c">#    Destination     Gateway         Genmask         Flags Metric Ref    Use Iface</span>
<span class="c">#    172.16.34.0     192.168.20.100  255.255.255.0   UG    0      0        0 tunl0</span>
<span class="c">#    172.16.116.0    192.168.10.10   255.255.255.0   UG    0      0        0 tunl0</span>
<span class="c">#    172.16.158.0    0.0.0.0         255.255.255.0   U     0      0        0 *</span>
<span class="c">#    172.16.184.0    192.168.10.102  255.255.255.0   UG    0      0        0 tunl0</span>
<span class="c">#    ...</span>
  
<span class="c"># (옵션) iptables rule 갯수 확인</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> filter <span class="nt">-S</span> | <span class="nb">grep </span>cali | <span class="nb">wc</span> <span class="nt">-l</span>
<span class="c"># =&gt; 69</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span> | <span class="nb">grep </span>cali | <span class="nb">wc</span> <span class="nt">-l</span>
<span class="c"># =&gt; 15</span>
</code></pre></div>    </div>
  </li>
</ul>

<h5 id="파드-배포-후-상태-확인">파드 배포 후 상태 확인</h5>

<ul>
  <li>노드(k8s-w1)에 파드가 배포되면 아래와 같이 파드가 생성되고, 통신이 가능한 상태입니다.</li>
  <li>
    <p>node1-pod2.yaml 파일 작성</p>

    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># node1-pod2.yaml </span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">pod1</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">nodeName</span><span class="pi">:</span> <span class="s">k8s-w1</span>  
  <span class="na">containers</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">pod1</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">nicolaka/netshoot</span>  
    <span class="na">command</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">tail"</span><span class="pi">]</span>
    <span class="na">args</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">-f"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">/dev/null"</span><span class="pi">]</span>
  <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">pod2</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">nodeName</span><span class="pi">:</span> <span class="s">k8s-w1</span>
  <span class="na">containers</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">pod2</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">nicolaka/netshoot</span>
    <span class="na">command</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">tail"</span><span class="pi">]</span>
    <span class="na">args</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">-f"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">/dev/null"</span><span class="pi">]</span>
  <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
</code></pre></div>    </div>
  </li>
  <li>파드 생성
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl apply <span class="nt">-f</span> node1-pod2.yaml
<span class="c"># =&gt; pod/pod1 created</span>
<span class="c">#    pod/pod2 created</span>
</code></pre></div>    </div>
  </li>
  <li>컨트롤 플레인에서 파드 생성 후 확인</li>
</ul>

<p><img src="/assets/2024/kans-3th/w3/20240921_kans_w3_7.png" alt="img.png" class="w-80 image-center" /></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>vagrant ssh k8s-m 

<span class="c"># [터미널1] k8s-m 모니터링</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> calicoctl get workloadEndpoint

<span class="c"># 생성된 파드 정보 확인</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-o</span> wide
<span class="c"># =&gt; NAME   READY   STATUS    RESTARTS        AGE   IP             NODE     NOMINATED NODE   READINESS GATES</span>
<span class="c">#    pod1   1/1     Running   1 (6m56s ago)   16h   172.16.158.9   k8s-w1   &lt;none&gt;           &lt;none&gt;</span>
<span class="c">#    pod2   1/1     Running   1 (6m56s ago)   16h   172.16.158.8   k8s-w1   &lt;none&gt;           &lt;none&gt;</span>

<span class="c"># calicoctl 이용한 endpoint 확인</span>
<span class="nv">$ </span>calicoctl get workloadendpoints
<span class="c"># =&gt; WORKLOAD   NODE     NETWORKS          INTERFACE</span>
<span class="c">#    pod1       k8s-w1   172.16.158.9/32   calice0906292e2</span>
<span class="c">#    pod2       k8s-w1   172.16.158.8/32   calibd2348b4f67</span>
</code></pre></div></div>

<ul>
  <li>
    <p>워커노드에서 파드 생성 후 확인</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>vagrant ssh k8s-w1 
  
<span class="c"># 네트워크 인터페이스 정보 확인 : calice#~ 2개 추가됨!, 각각 net ns(네임스페이스) 0, 1로 호스트와 구별됨</span>
<span class="nv">$ </span>ip <span class="nt">-c</span> <span class="nb">link</span>
<span class="c"># =&gt; ...</span>
<span class="c">#    4: tunl0@NONE: &lt;NOARP,UP,LOWER_UP&gt; mtu 1480 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000</span>
<span class="c">#        link/ipip 0.0.0.0 brd 0.0.0.0</span>
<span class="c">#    &lt;span style="color: #393;"&gt;7: cali2789c1b51d6@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1480 qdisc noqueue state UP mode DEFAULT group default qlen 1000&lt;/span&gt;</span>
<span class="c">#        &lt;span style="color: #393;"&gt;link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netns cni-a67773eb-f809-7ce1-72f5-e16e93cbe4c4&lt;/span&gt;</span>
<span class="c">#    &lt;span style="color: #393;"&gt;8: calibd2348b4f67@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1480 qdisc noqueue state UP mode DEFAULT group default qlen 1000&lt;/span&gt;</span>
<span class="c">#        &lt;span style="color: #393;"&gt;link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netns cni-511c24a7-0a0a-0c9a-29c2-345cfcc0dd21&lt;/span&gt;</span>
<span class="c">#    &lt;span style="color: #393;"&gt;9: calice0906292e2@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1480 qdisc noqueue state UP mode DEFAULT group default qlen 1000&lt;/span&gt;</span>
<span class="c">#        &lt;span style="color: #393;"&gt;link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netns cni-47754704-48d2-04a9-28fe-e8bff1be17ba&lt;/span&gt;</span>
  
<span class="c"># 네트워크 네임스페이스 확인 : 아래 2개 pause(infra 컨테이너)가 각각 파드별로 생성됨 - 위 link-netnsid 0, link-netnsid 1 매칭됨</span>
<span class="nv">$ </span>lsns <span class="nt">-t</span> net
<span class="c"># =&gt;         NS TYPE NPROCS   PID USER     NETNSID NSFS                                                COMMAND</span>
<span class="c">#    4026531840 net     146     1 root  unassigned                                                     /sbin/init</span>
<span class="c">#    4026532205 net       2  2092 65535          0 /run/netns/cni-a67773eb-f809-7ce1-72f5-e16e93cbe4c4 /pause</span>
<span class="c">#    4026532282 net       2  2317 65535          1 /run/netns/cni-511c24a7-0a0a-0c9a-29c2-345cfcc0dd21 /pause</span>
<span class="c">#    4026532343 net       2  2431 65535          2 /run/netns/cni-47754704-48d2-04a9-28fe-e8bff1be17ba /pause</span>
  
<span class="c"># 파드의 IP/32bit 호스트 라우팅 대역이 라우팅 테이블에 추가됨</span>
<span class="nv">$ </span>ip <span class="nt">-c</span> route
<span class="c"># =&gt; ...</span>
<span class="c">#    blackhole 172.16.158.0/24 proto bird</span>
<span class="c">#    &lt;span style="color: #393;"&gt;172.16.158.7 dev cali2789c1b51d6 scope link&lt;/span&gt;</span>
<span class="c">#    &lt;span style="color: #393;"&gt;172.16.158.8 dev calibd2348b4f67 scope link&lt;/span&gt;</span>
<span class="c">#    &lt;span style="color: #393;"&gt;172.16.158.9 dev calice0906292e2 scope link&lt;/span&gt;</span>
<span class="c">#    ...</span>
  
<span class="c"># (옵션) iptables rule 갯수 확인</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> filter <span class="nt">-S</span> | <span class="nb">grep </span>cali | <span class="nb">wc</span> <span class="nt">-l</span>
<span class="c"># =&gt; 121</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span> | <span class="nb">grep </span>cali | <span class="nb">wc</span> <span class="nt">-l</span>
<span class="c"># =&gt; 15</span>
</code></pre></div>    </div>
  </li>
</ul>

<h5 id="파드간-통신-실행-및-확인">파드간 통신 실행 및 확인</h5>

<ul>
  <li>
    <p>파드간 통신 실행 이해를 위한 준비
<img src="/assets/2024/kans-3th/w3/20240921_kans_w3_8.png" alt="img.png" class="w-80 image-center" /></p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># iptables 필터 테이블에 FORWARD 리스트 중 cali-FORWARD 룰 정보를 필터링해서 watch 로 확인</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> <span class="nt">-n</span> 1 <span class="s2">"iptables -v --numeric --table filter --list FORWARD | egrep '(cali-FORWARD|pkts)'"</span> 
  
<span class="c"># (컨트롤플레인) 파드 연결된 veth 를 변수를 확인</span>
<span class="nv">$ VETH1</span><span class="o">=</span><span class="si">$(</span>calicoctl get workloadEndpoint | <span class="nb">grep </span>pod1 | <span class="nb">awk</span> <span class="s1">'{print $4}'</span><span class="si">)</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$VETH1</span>
<span class="c"># =&gt; calice0906292e2</span>
<span class="nv">$ VETH2</span><span class="o">=</span><span class="si">$(</span>calicoctl get workloadEndpoint | <span class="nb">grep </span>pod2 | <span class="nb">awk</span> <span class="s1">'{print $4}'</span><span class="si">)</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$VETH2</span>
<span class="c"># =&gt; calibd2348b4f67</span>
  
<span class="c"># (워커노드1) 위에서 확인한 파드 연결된 veth 를 변수에 지정</span>
<span class="nv">$ VETH1</span><span class="o">=</span>calice0906292e2
<span class="nv">$ VETH2</span><span class="o">=</span>calibd2348b4f67
  
<span class="c"># 노드1 calice# 인터페이스의 proxy arp 설정 확인</span>
<span class="c"># cat /proc/sys/net/ipv4/conf/&lt;자신의 pod1에 연결된 calice# 이름&gt;/proxy_arp</span>
<span class="nv">$ </span><span class="nb">cat</span> /proc/sys/net/ipv4/conf/<span class="nv">$VETH1</span>/proxy_arp
<span class="c"># =&gt; 1</span>
<span class="nv">$ </span><span class="nb">cat</span> /proc/sys/net/ipv4/conf/<span class="nv">$VETH2</span>/proxy_arp
<span class="c"># =&gt; 1</span>
  
<span class="c"># 파드1 혹은 파드2에 veth 로 연결된 호스트 네트워크 인터페이스 calice# 중 1개 선택해서 tcpdump</span>
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> <span class="nv">$VETH1</span> <span class="nt">-nn</span>
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> <span class="nv">$VETH2</span> <span class="nt">-nn</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>파드1 -&gt; 파드2 ping 통신을 확인해 보겠습니다.</p>
  </li>
</ul>

<p><img src="/assets/2024/kans-3th/w3/20240921_kans_w3_9.png" alt="img.png" class="w-80 image-center" /></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 파드1 Shell 에서 실행 : 정상 통신!</span>
<span class="nv">$ </span>kubectl <span class="nb">exec </span>pod1 <span class="nt">-it</span> <span class="nt">--</span> zsh
<span class="c"># --------------------</span>
<span class="c"># ping -c 10 &lt;파드2 IP&gt;</span>
<span class="nv">$ </span>ping <span class="nt">-c</span> 10 172.16.158.8

<span class="c"># 게이트웨이 169.254.1.1 의 MAC 주소를 ARP 에 의해서 학습되었습니다.</span>
<span class="nv">$ </span>ip <span class="nt">-c</span> <span class="nt">-s</span> neigh
<span class="c"># =&gt; 10.0.2.15 dev eth0 lladdr ee:ee:ee:ee:ee:ee  used 675/735/675probes 0 STALE</span>
<span class="c">#    &lt;span style="color: #900;"&gt;169.254.1.1&lt;/span&gt; dev eth0 lladdr &lt;span style="color: #900;"&gt;ee:ee:ee:ee:ee:ee&lt;/span&gt;  ref 1 used 3/3/3probes 1 REACHABLE</span>

<span class="c"># 노드에서 확인</span>
<span class="c"># iptables 에 기본 FORWARD 는 DROP 이지만, 아래 cali-FORWARD Rule에 의해 허용되며, pkts 카운트가 증가합니다.</span>
<span class="nv">$ </span>iptables <span class="nt">-v</span> <span class="nt">--numeric</span> <span class="nt">--table</span> filter <span class="nt">--list</span> FORWARD | egrep <span class="s1">'(cali-FORWARD|pkts)'</span>
<span class="c"># =&gt; pkts bytes target     prot opt in     out     source               destination</span>
<span class="c">#    29375 6505K cali-FORWARD  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* cali:wUHhoiAYhphO9Mso */</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s2">"iptables -v --numeric --table filter --list FORWARD | egrep '(cali-FORWARD|pkts)'"</span>

<span class="c"># 파드1에서 게이트웨이의 IP인 169.254.1.1 의 MAC 주소를 알기 위해서 ARP Request 를 보낸다</span>
<span class="c"># 이때 veth 연결된 calice#~ 에 proxy arp 설정이 되어 있고, 자신의 mac 주소(ee:ee:ee:ee:ee:ee)를 알려주고, 이후 정상 통신됨</span>
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> <span class="nv">$VETH1</span> <span class="nt">-nn</span>
<span class="c"># =&gt; tcpdump: verbose output suppressed, use -v[v]... for full protocol decode</span>
<span class="c">#    listening on calice0906292e2, link-type EN10MB (Ethernet), snapshot length 262144 bytes</span>
<span class="c">#    11:27:01.638238 IP 172.16.158.9 &gt; 172.16.158.8: ICMP echo request, id 134, seq 1, length 64</span>
<span class="c">#    11:27:01.638430 IP 172.16.158.8 &gt; 172.16.158.9: ICMP echo reply, id 134, seq 1, length 64</span>
<span class="c">#    ...</span>
<span class="c">#    11:27:06.675426 &lt;span style="color: #900;"&gt;ARP, Request who-has 169.254.1.1&lt;/span&gt; tell 172.16.158.9, length 28</span>
<span class="c">#    11:27:06.675498 &lt;span style="color: #900;"&gt;ARP, Reply 169.254.1.1 is-at ee:ee:ee:ee:ee:ee&lt;/span&gt;, length 28</span>

<span class="nv">$ </span>tcpdump <span class="nt">-i</span> <span class="nv">$VETH2</span> <span class="nt">-nn</span>
<span class="c"># =&gt; tcpdump: verbose output suppressed, use -v[v]... for full protocol decode</span>
<span class="c">#    listening on calibd2348b4f67, link-type EN10MB (Ethernet), snapshot length 262144 bytes</span>
<span class="c">#    11:27:01.638389 IP 172.16.158.9 &gt; 172.16.158.8: ICMP echo request, id 134, seq 1, length 64</span>
<span class="c">#    11:27:01.638409 IP 172.16.158.8 &gt; 172.16.158.9: ICMP echo reply, id 134, seq 1, length 64</span>
<span class="c">#    ...</span>
<span class="c">#    11:27:16.151004 &lt;span style="color: #900;"&gt;ARP, Request who-has 169.254.1.1&lt;/span&gt; tell 172.16.158.8, length 28</span>
<span class="c">#    11:27:16.151780 &lt;span style="color: #900;"&gt;ARP, Reply 169.254.1.1 is-at ee:ee:ee:ee:ee:ee&lt;/span&gt;, length 28</span>

<span class="c"># 호스트에서 calice0906292e2 의 MAC 주소 다시 확인</span>
<span class="nv">$ </span>ip <span class="nt">-c</span> <span class="nt">-d</span> <span class="nb">link</span>
<span class="c"># =&gt; ... </span>
<span class="c">#    9: &lt;span style="color: #900;"&gt;calice0906292e2&lt;/span&gt;@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1480 qdisc noqueue state UP mode DEFAULT group default qlen 1000</span>
<span class="c">#        link/ether &lt;span style="color: #900;"&gt;ee:ee:ee:ee:ee:ee&lt;/span&gt; brd ff:ff:ff:ff:ff:ff link-netns cni-47754704-48d2-04a9-28fe-e8bff1be17ba promiscuity 1 minmtu 68 maxmtu 65535</span>
<span class="c">#        veth addrgenmode eui64 numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535</span>
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w3/20240921_kans_w3_10.png" alt="img.png" class="image-center mb-0" />
<em class="image-caption">파드간 통신 확인 실습</em></p>

<ul>
  <li>실습결과 calice# 인터페이스에 proxy arp 설정을 통해 게이트웨이의 MAC 주소를 파드가 전달 받아 사용하는 것을 확인할 수 있었습니다.</li>
  <li>169.254.1.1은 Calico CNI에서 사용하는 게이트웨이 주소로, 같은 노드의 파드끼리 통신시 사용되는 것을 확인하였습니다.</li>
</ul>

<h4 id="파드---외부인터넷-통신">파드 -&gt; 외부(인터넷) 통신</h4>

<ul>
  <li>이번에는 파드에서 외부(인터넷)로 통신하는 과정을 확인해보겠습니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w3/20240921_kans_w3_11.png" alt="img.png" class="w-80 image-center" /></p>

<ul>
  <li>calico는 기본 설정에 <code class="language-plaintext highlighter-rouge">natOutgoing: true</code>여서 파드에서 외부로 통신할 때는 노드의 IP 주소로 MASQUERADE(Source NAT)을 수행하여 외부로 통신합니다.</li>
  <li>calice# 인터페이스에 proxy arp 설정을 통해 게이트웨이의 MAC 주소를 파드가 전달 받아 사용합니다.</li>
  <li>외부로 통신할 때는 tunnel interface를 사용하지 않습니다.</li>
</ul>

<h5 id="파드-배포-전-기본-상태-확인-1">파드 배포 전 기본 상태 확인</h5>

<ul>
  <li>calico 설정 및 노드의 iptables 설정을 확인하겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 마스터 노드에서 확인 : natOutgoing 의 기본값은 true 이다</span>
<span class="nv">$ </span>calicoctl get ippool <span class="nt">-o</span> wide
<span class="c"># =&gt; NAME                  CIDR            NAT    IPIPMODE   VXLANMODE   DISABLED   DISABLEBGPEXPORT   SELECTOR</span>
<span class="c">#    default-ipv4-ippool   172.16.0.0/16   true   Always     Never       false      false              all()</span>

<span class="c"># 노드에서 확인 : 노드에서 외부로 통신 시 MASQUERADE 동작 Rule 확인</span>
<span class="nv">$ </span>iptables <span class="nt">-n</span> <span class="nt">-t</span> nat <span class="nt">--list</span> cali-nat-outgoing
<span class="c"># =&gt; Chain cali-nat-outgoing (1 references)</span>
<span class="c">#    target     prot opt source               destination</span>
<span class="c">#    MASQUERADE  all  --  0.0.0.0/0            0.0.0.0/0            /* cali:flqWnvo8yq4ULQLa */ match-set cali40masq-ipam-pools src ! match-set cali40all-ipam-pools dst random-fully</span>

<span class="nv">$ </span>ipset list
<span class="nv">$ </span>ipset list cali40masq-ipam-pools
<span class="c"># =&gt; Name: cali40masq-ipam-pools</span>
<span class="c">#    Type: hash:net</span>
<span class="c">#    Revision: 7</span>
<span class="c">#    Header: family inet hashsize 1024 maxelem 1048576 bucketsize 12 initval 0xe6c37fa0</span>
<span class="c">#    Size in memory: 504</span>
<span class="c">#    References: 1</span>
<span class="c">#    Number of entries: 1</span>
<span class="c">#    Members:</span>
<span class="c">#    172.16.0.0/16</span>
</code></pre></div></div>

<h5 id="파드-배포-및-외부-통신-확인">파드 배포 및 외부 통신 확인</h5>

<ul>
  <li>
    <p>node1-pod1.yaml 파일 작성</p>

    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">pod1</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">nodeName</span><span class="pi">:</span> <span class="s">k8s-w1</span>
  <span class="na">containers</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">pod1</span>
      <span class="na">image</span><span class="pi">:</span> <span class="s">nicolaka/netshoot</span>
      <span class="na">command</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">tail"</span><span class="pi">]</span>
      <span class="na">args</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">-f"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">/dev/null"</span><span class="pi">]</span>
  <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>파드 생성 후 확인</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 파드 생성</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> node1-pod1.yaml
  
<span class="c"># 생성된 파드 확인</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-o</span> wide
<span class="c"># =&gt; NAME   READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES</span>
<span class="c">#    pod1   1/1     Running   0          25s   172.16.158.10   k8s-w1   &lt;none&gt;           &lt;none&gt;</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>파드간 통신 실행 이해를 위한 준비</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 노드에서 실행</span>
<span class="c"># iptables NAT MASQUERADE 모니터링 : pkts 증가 확인</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s1">'iptables -n -v -t nat --list cali-nat-outgoing'</span>
  
<span class="c"># (컨트롤플레인 노드) 파드 연결된 veth 를 변수를 확인</span>
<span class="nv">$ VETH1</span><span class="o">=</span><span class="si">$(</span>calicoctl get workloadEndpoint | <span class="nb">grep </span>pod1 | <span class="nb">awk</span> <span class="s1">'{print $4}'</span><span class="si">)</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$VETH1</span>
<span class="c"># =&gt; calice0906292e2</span>
  
<span class="c"># (워커노드1) 위에서 확인한 파드 연결된 veth 를 변수에 지정</span>
<span class="nv">$ VETH1</span><span class="o">=</span>calice0906292e2
  
<span class="c"># 패킷 덤프 실행</span>
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> any <span class="nt">-nn</span> icmp
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> <span class="nv">$VETH1</span> <span class="nt">-nn</span> icmp
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> tunl0 <span class="nt">-nn</span> icmp
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> ens5 <span class="nt">-nn</span> icmp    <span class="c"># [실습환경 A Type]</span>
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> enp0s8 <span class="nt">-nn</span> icmp  <span class="c"># [실습환경 B Type]</span>
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> enp0s3 <span class="nt">-nn</span> icmp  <span class="c"># [실습환경 B Type]</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>외부 통신 실행 및 확인</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 파드에서 외부 정상 통신 확인</span>
<span class="nv">$ </span>kubectl <span class="nb">exec </span>pod1 <span class="nt">-it</span> <span class="nt">--</span> zsh
<span class="nt">----------------------------</span>
  
<span class="c"># 혹은 통신 확인 </span>
<span class="nv">$ </span>ping <span class="nt">-c</span> 10 8.8.8.8
  
<span class="c"># The right way to check the weather - 링크</span>
<span class="nv">$ </span>curl wttr.in/seoul
<span class="nv">$ </span>curl <span class="s1">'wttr.in/seoul?format=3'</span>
<span class="nv">$ </span>curl <span class="s1">'wttr.in/busan?format=3'</span>
<span class="nv">$ </span>curl <span class="nt">-s</span> <span class="s1">'wttr.in/{London,Busan}'</span>
<span class="nv">$ </span>curl v3.wttr.in/Seoul.sxl
<span class="nv">$ </span>curl wttr.in/Moon
<span class="nv">$ </span>curl wttr.in/:help
  
<span class="c"># 패킷 덤프 내용 확인</span>
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> calice0906292e2 <span class="nt">-nn</span> icmp
<span class="c"># =&gt; tcpdump: verbose output suppressed, use -v[v]... for full protocol decode</span>
<span class="c">#    listening on calice0906292e2, link-type EN10MB (Ethernet), snapshot length 262144 bytes</span>
<span class="c">#    12:23:43.792218 IP 172.16.158.10 &gt; 8.8.8.8: ICMP echo request, id 124, seq 1, length 64</span>
<span class="c">#    12:23:43.831040 IP 8.8.8.8 &gt; 172.16.158.10: ICMP echo reply, id 124, seq 1, length 64</span>
<span class="c">#    12:23:44.797713 IP 172.16.158.10 &gt; 8.8.8.8: ICMP echo request, id 124, seq 2, length 64</span>
<span class="c">#    12:23:44.836854 IP 8.8.8.8 &gt; 172.16.158.10: ICMP echo reply, id 124, seq 2, length 64</span>
<span class="c">#    ...</span>
  
<span class="c"># 아래 10.0.2.15는 VM의 1번 네트워크 인터페이스의 IP이며, 출발지 IP가 변경되어서 외부로 나감</span>
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> enp0s3 <span class="nt">-nn</span> icmp
<span class="c"># =&gt; tcpdump: verbose output suppressed, use -v[v]... for full protocol decode</span>
<span class="c">#    listening on enp0s3, link-type EN10MB (Ethernet), snapshot length 262144 bytes</span>
<span class="c">#    12:23:43.792299 IP 10.0.2.15 &gt; 8.8.8.8: ICMP echo request, id 25687, seq 1, length 64</span>
<span class="c">#    12:23:43.830978 IP 8.8.8.8 &gt; 10.0.2.15: ICMP echo reply, id 25687, seq 1, length 64</span>
<span class="c">#    12:23:44.797877 IP 10.0.2.15 &gt; 8.8.8.8: ICMP echo request, id 25687, seq 2, length 64</span>
<span class="c">#    12:23:44.836812 IP 8.8.8.8 &gt; 10.0.2.15: ICMP echo reply, id 25687, seq 2, length 64</span>
  
<span class="c"># nat MASQUERADE rule 카운트(pkts)가 증가!</span>
<span class="c">## 출발지 매칭은 cali40masq-ipam-pools 을 사용</span>
<span class="c"># watch -d 'iptables -n -v -t nat --list cali-nat-outgoing'</span>
<span class="nv">$ </span>iptables <span class="nt">-n</span> <span class="nt">-v</span> <span class="nt">-t</span> nat <span class="nt">--list</span> cali-nat-outgoing
<span class="c"># =&gt; Chain cali-nat-outgoing (1 references)</span>
<span class="c">#     pkts bytes target     prot opt in     out     source               destination</span>
<span class="c">#        9   636 MASQUERADE  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* cali:flqWnvo8yq4ULQLa */ match-set cali40masq-ipam-pools src ! match-set cali40all-ipa</span>
<span class="c">#    m-pools dst random-fully</span>
  
<span class="c"># IPSET 으로 의 cali40masq-ipam-pools IP 대역 정보 확인 : 172.16.0.0/16 대역임을 확인</span>
<span class="nv">$ </span>ipset list cali40masq-ipam-pools
<span class="c"># =&gt; Name: cali40masq-ipam-pools</span>
<span class="c">#    Type: hash:net</span>
<span class="c">#    Revision: 7</span>
<span class="c">#    Header: family inet hashsize 1024 maxelem 1048576 bucketsize 12 initval 0x59a55159</span>
<span class="c">#    Size in memory: 504</span>
<span class="c">#    References: 1</span>
<span class="c">#    Number of entries: 1</span>
<span class="c">#    Members:</span>
<span class="c">#    172.16.0.0/16</span>
</code></pre></div>    </div>

    <p><img src="/assets/2024/kans-3th/w3/20240921_kans_w3_12.png" alt="20240921_kans_w3_12.png" /></p>
  </li>
  <li>이렇게 외부와의 통신시 터널링(tunl0)은 통하지 않고 calice# 인터페이스와 enp0s3 인터페이스를 통해 외부로 통신하는 것을 확인할 수 있었습니다.</li>
  <li>enp0s8 인터페이스는 virtualbox상의 host only 인터페이스여서 패킷이 전달되지 않습니다.</li>
</ul>

<h4 id="다른-노드의-파드--파드간-통신">다른 노드의 파드 &lt;=&gt; 파드간 통신</h4>

<ul>
  <li>이번에는 서로 다른 노드의 파드간의 통신을 알아보겠습니다.</li>
  <li>다른 노드의 파드와의 통신은 <strong>IPIP</strong>(IP in IP) 터널링을 통해 통신합니다.</li>
  <li>각 노드에서 파드 네트워크 대역은 Bird에 의해서 BGP로 전파되며, Felix에 의해 노드의 라우팅 테이블에 자동으로 추가/삭제 됩니다.</li>
  <li>다른 노드간의 통신은 tunl0 인터페이스를 통해 IP 헤더에 IPIP 헤더를 추가하여, 상대방 노드로 도착 후 tunl0 인터페이스에서 IPIP 헤더를 제거하고, 최종적으로 상대방 파드로 전달됩니다.
<img src="/assets/2024/kans-3th/w3/20240921_kans_w3_14.png" alt="img.png" class="w-80 image-center" />
<em class="image-caption">IPIP 동작방식 <a href="https://en.wikipedia.org/wiki/IP_in_IP">출처</a></em></li>
  <li>그림으로 나타내면 아래와 같습니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w3/20240921_kans_w3_13.png" alt="img.png" /></p>

<h5 id="파드-배포-전-기본-상태-확인-2">파드 배포 전 기본 상태 확인</h5>

<ul>
  <li>노드에서 Bird 라우팅 정보와 Felix 라우팅 테이블 정보를 확인합니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 컨트롤플레인</span>
<span class="nv">$ </span>route | <span class="nb">head</span> <span class="nt">-2</span> <span class="p">;</span> route <span class="nt">-n</span> | <span class="nb">grep </span>tunl0
<span class="c"># =&gt; Kernel IP routing table</span>
<span class="c">#    Destination     Gateway         Genmask         Flags Metric Ref    Use Iface</span>
<span class="c">#    172.16.34.0     192.168.20.100  255.255.255.0   UG    0      0        0 tunl0</span>
<span class="c">#    172.16.158.0    192.168.10.101  255.255.255.0   UG    0      0        0 tunl0</span>
<span class="c">#    172.16.184.0    192.168.10.102  255.255.255.0   UG    0      0        0 tunl0</span>

<span class="c"># 노드1</span>
<span class="nv">$ </span>route | <span class="nb">head</span> <span class="nt">-2</span> <span class="p">;</span> route <span class="nt">-n</span> | <span class="nb">grep </span>tunl0
<span class="c"># =&gt; Kernel IP routing table</span>
<span class="c">#    Destination     Gateway         Genmask         Flags Metric Ref    Use Iface</span>
<span class="c">#    172.16.34.0     192.168.20.100  255.255.255.0   UG    0      0        0 tunl0</span>
<span class="c">#    172.16.116.0    192.168.10.10   255.255.255.0   UG    0      0        0 tunl0</span>
<span class="c">#    172.16.184.0    192.168.10.102  255.255.255.0   UG    0      0        0 tunl0</span>
</code></pre></div></div>

<ul>
  <li>노드의 tunl0 정보 확인해 보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 노드1</span>
<span class="nv">$ </span>ifconfig enp0s3
<span class="c"># =&gt; enp0s3: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  &lt;span style="color: red;"&gt;mtu 1500&lt;/span&gt;</span>
<span class="c">#             inet 10.0.2.15  netmask 255.255.255.0  broadcast 10.0.2.255</span>

<span class="nv">$ </span>ifconfig tunl0
<span class="c"># =&gt; tunl0: flags=193&lt;UP,RUNNING,NOARP&gt;  &lt;span style="color: red;"&gt;mtu 1480&lt;/span&gt;</span>
<span class="c">#            inet 172.16.158.0  netmask 255.255.255.255</span>
<span class="c">#            tunnel   txqueuelen 1000  (IPIP Tunnel)</span>
<span class="c">#            RX packets 11581  bytes 917302 (917.3 KB)</span>
<span class="c">#            RX errors 0  dropped 0  overruns 0  frame 0</span>
<span class="c">#            TX packets 12036  bytes 2774296 (2.7 MB)</span>
<span class="c">#            TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span>

<span class="c"># 노드2</span>
<span class="nv">$ </span>ifconfig tunl0
<span class="c"># =&gt; tunl0: flags=193&lt;UP,RUNNING,NOARP&gt;  &lt;span style="color: red;"&gt;mtu 1480&lt;/span&gt;</span>
<span class="c">#            inet 172.16.184.0  netmask 255.255.255.255</span>
<span class="c">#            tunnel   txqueuelen 1000  (IPIP Tunnel)</span>
<span class="c">#            RX packets 0  bytes 0 (0.0 B)</span>
<span class="c">#            RX errors 0  dropped 0  overruns 0  frame 0</span>
<span class="c">#            TX packets 0  bytes 0 (0.0 B)</span>
<span class="c">#            TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span>
</code></pre></div></div>

<ul>
  <li>터널 인터페이스인 tunl0에 IP가 할당되어있고, MTU는 1480으로 설정되어 있습니다. enp0s3 인터페이스는 mtu가 1500인데 tunl0 인터페이스는 mtu가 1480으로 설정된 이유는, IPIP의 20바이트 헤더를 추가하기 위해서입니다.</li>
</ul>

<h5 id="파드-배포">파드 배포</h5>

<ul>
  <li>테스트를 위해 노드1과 노드2에 각각 파드 1개씩 생성하겠습니다.</li>
  <li>node2-pod2.yaml 생성
    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># node2-pod2.yaml</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">pod1</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">nodeName</span><span class="pi">:</span> <span class="s">k8s-w1</span>
  <span class="na">containers</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">pod1</span>
      <span class="na">image</span><span class="pi">:</span> <span class="s">nicolaka/netshoot</span>
      <span class="na">command</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">tail"</span><span class="pi">]</span>
      <span class="na">args</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">-f"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">/dev/null"</span><span class="pi">]</span>
  <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">pod2</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">nodeName</span><span class="pi">:</span> <span class="s">k8s-w2</span>
  <span class="na">containers</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">pod2</span>
      <span class="na">image</span><span class="pi">:</span> <span class="s">nicolaka/netshoot</span>
      <span class="na">command</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">tail"</span><span class="pi">]</span>
      <span class="na">args</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">-f"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">/dev/null"</span><span class="pi">]</span>
  <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>파드 생성 후 확인</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl apply <span class="nt">-f</span> node2-pod2.yaml
<span class="c"># =&gt; pod/pod1 created</span>
<span class="c">#    pod/pod2 created</span>
  
<span class="c"># calicoctl 이용한 endpoint 확인</span>
<span class="nv">$ </span>calicoctl get workloadendpoints
<span class="c"># =&gt; WORKLOAD   NODE     NETWORKS           INTERFACE</span>
<span class="c">#    pod1       k8s-w1   172.16.158.11/32   calice0906292e2</span>
<span class="c">#    pod2       k8s-w2   172.16.184.16/32   calibd2348b4f67</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>pod1이 node1에, pod2가 node2에 생성되었음을 확인할 수 있습니다. 파드 생성 후 상태는 아래와 같습니다.</p>

    <p><img src="/assets/2024/kans-3th/w3/20240921_kans_w3_15.png" alt="img.png" /></p>
  </li>
</ul>

<h5 id="파드간-통신-실행-및-확인-1">파드간 통신 실행 및 확인</h5>

<ul>
  <li>Calico CNI는 다른 노드의 파드간 통신을 위해 IPIP 터널링을 사용합니다.</li>
  <li>IPIP를 사용하려면 <strong>클라우드 서비스에서 IPIP 터널링을 허용해야</strong> 합니다. 현재 <strong>Azure는 IPIP 패킷을 허용하지 않고</strong> 있으며, 이러한 경우 <strong>Flannel을 사용하거나</strong>, <strong>Calico의 VXLAN을 사용</strong>할 수 있습니다.</li>
  <li>다음 그림과 같이 tunl0와 eth0 인터페이스에 tcpdump를 사용하여 패킷을 캡쳐해보겠습니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w3/20240921_kans_w3_16.png" alt="img.png" /></p>

<ul>
  <li>노드1과 노드2에서 각각 아래와 같이 실행합니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># tunl0 인터페이스 TX/RX 패킷 카운트 모니터링 실행</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s1">'ifconfig tunl0 | head -2 ; ifconfig tunl0 | grep bytes'</span>

<span class="c"># 패킷 덤프 tunl0</span>
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> tunl0 <span class="nt">-nn</span>

<span class="c"># 패킷 덤프 : IP 헤더의 상위 프로토콜을 IPIP(4)인 패킷만 필터링</span>
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> enp0s8 <span class="nt">-nn</span> ip proto 4
</code></pre></div></div>

<ul>
  <li>파드1에서 파드2로 ping 통신을 실행해보겠습니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w3/20240921_kans_w3_17.png" alt="img.png" /></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 마스터 노드에서 pod1 Shell 접속</span>
<span class="nv">$ </span>kubectl <span class="nb">exec </span>pod1 <span class="nt">-it</span> <span class="nt">--</span> zsh

<span class="c"># pod1 에서 pod2 로 핑 통신 : 정상 통신!</span>
<span class="c"># ping -c 10 &lt;pod2 IP&gt;</span>
<span class="nv">$ </span>ping <span class="nt">-c</span> 10 172.16.184.16
<span class="c"># =&gt; PING 172.16.184.16 (172.16.184.16) 56(84) bytes of data.</span>
<span class="c">#    64 bytes from 172.16.184.16: icmp_seq=1 ttl=62 time=1.42 ms</span>
<span class="c">#    64 bytes from 172.16.184.16: icmp_seq=2 ttl=62 time=2.32 ms</span>
<span class="c">#    ...</span>

<span class="c"># tunl0 인터페이스 TX/RX 패킷 카운트 모니터링 확인 : TX/RX 패킷 카운트가 각각 10개로 증가했다</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s1">'ifconfig tunl0 | head -2 ; ifconfig tunl0 | grep bytes'</span>
<span class="c"># =&gt; tunl0: flags=193&lt;UP,RUNNING,NOARP&gt;  mtu 1480</span>
<span class="c">#            inet 172.16.184.0  netmask 255.255.255.255</span>
<span class="c">#            RX packets 10  bytes 840 (840.0 B)</span>
<span class="c">#            TX packets 10  bytes 840 (840.0 B)</span>

<span class="c"># 패킷 덤프 : tunl0 - 터널 인터페이스에 파드간 IP 패킷 정보 확인!</span>
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> tunl0 <span class="nt">-nn</span>
<span class="c"># =&gt; tcpdump: verbose output suppressed, use -v or -vv for full protocol decode</span>
<span class="c">#    listening on tunl0, link-type RAW (Raw IP), capture size 262144 bytes</span>
<span class="c">#    15:42:06.220413 IP 172.16.158.11 &gt; 172.16.184.16: ICMP echo request, id 259, seq 1, length 64</span>
<span class="c">#    15:42:06.220720 IP 172.16.184.16 &gt; 172.16.158.11: ICMP echo reply, id 259, seq 1, length 64</span>
<span class="c">#    15:42:07.250941 IP 172.16.158.11 &gt; 172.16.184.16: ICMP echo request, id 259, seq 2, length 64</span>
<span class="c">#    15:42:07.252035 IP 172.16.184.16 &gt; 172.16.158.11: ICMP echo reply, id 259, seq 2, length 64</span>
<span class="c">#    ...</span>
...

<span class="c"># 패킷 덤프 : eth0(enp#~) - IP Outer 헤더 안쪽에 IP 헤더 1개가 더 있음을 알 수 있습니다.</span>
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> enp0s8 <span class="nt">-nn</span> proto 4 
<span class="c"># =&gt; 15:42:06.220427 &lt;span style="color: red;"&gt;IP&lt;/span&gt; 192.168.10.101 &gt; 192.168.10.102: &lt;span style="color: red;"&gt;IP&lt;/span&gt; 172.16.158.11 &gt; 172.16.184.16: &lt;span style="color: red;"&gt;ICMP echo&lt;/span&gt; request, id 259, seq 1, length 64</span>
<span class="c">#    15:42:06.220720 &lt;span style="color: red;"&gt;IP&lt;/span&gt; 192.168.10.102 &gt; 192.168.10.101: &lt;span style="color: red;"&gt;IP&lt;/span&gt; 172.16.184.16 &gt; 172.16.158.11: &lt;span style="color: red;"&gt;ICMP echo&lt;/span&gt; reply, id 259, seq 1, length 64</span>
<span class="c">#    15:42:07.250959 &lt;span style="color: red;"&gt;IP&lt;/span&gt; 192.168.10.101 &gt; 192.168.10.102: &lt;span style="color: red;"&gt;IP&lt;/span&gt; 172.16.158.11 &gt; 172.16.184.16: &lt;span style="color: red;"&gt;ICMP echo&lt;/span&gt; request, id 259, seq 2, length 64</span>
<span class="c">#    15:42:07.252035 &lt;span style="color: red;"&gt;IP&lt;/span&gt; 192.168.10.102 &gt; 192.168.10.101: &lt;span style="color: red;"&gt;IP&lt;/span&gt; 172.16.184.16 &gt; 172.16.158.11: &lt;span style="color: red;"&gt;ICMP echo&lt;/span&gt; reply, id 259, seq 2, length 64</span>
</code></pre></div></div>

<ul>
  <li>실습 결과 tunl0 인터페이스를 통한 패킷이 IP 헤더에 IPIP 헤더가 추가되어 노드의 enp0s8 인터페이스로 전달되는 것을 확인할 수 있었습니다.</li>
  <li>enp0s8 인터페이스에는 파드의 IP인 172.16.x.x 대역의 IP 패킷이 노드의 IP 대역인 192.168.x.x 대역으로 감싸져서 전달되는 것을 확인할 수 있었습니다.</li>
  <li>캡쳐된 패킷을 wireshark로 확인해보겠습니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w3/20240921_kans_w3_18.png" alt="img.png" /></p>

<ul>
  <li>실제 ICMP 프로토콜의 상위에 파드의 IP 프로토콜이 있고, 그 위에 노드의 IP 프로토콜이 있는 것을 확인할 수 있습니다.</li>
</ul>

<hr />

<h3 id="calico-네트워크-모드">Calico 네트워크 모드</h3>

<ul>
  <li>Calico는 다양한 네트워크 모드를 지원합니다. 네트워크 모드는 Calico의 CNI 설정을 통해 설정할 수 있습니다.</li>
  <li>
    <p>Calico의 네트워크 모드는 다음과 같습니다.</p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">네트워크 모드</th>
          <th style="text-align: left">설명</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center">IPIP</td>
          <td style="text-align: left">IPIP 터널링을 사용하여 노드 간 통신을 위한 네트워크 모드입니다.</td>
        </tr>
        <tr>
          <td style="text-align: center">Direct</td>
          <td style="text-align: left">호스트의 물리 네트워크 인터페이스를 사용하여 노드 간 통신을 위한 네트워크 모드입니다.</td>
        </tr>
        <tr>
          <td style="text-align: center">VXLAN</td>
          <td style="text-align: left">Flannel에서 사용했던 VXLAN을 사용하여 노드 간 통신을 위한 네트워크 모드입니다.</td>
        </tr>
        <tr>
          <td style="text-align: center">Pod 패킷 암호화</td>
          <td style="text-align: left">WireGuard를 사용하여 노드 간 통신을 암호화 하기 위한 네트워크 모드입니다.</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>성능은 Direct &gt; IPIP &gt; VXLAN 순으로 높지만, IPIP는 클라우드 서비스에서 IPIP 터널링을 허용해야 하고,
Direct는 물리 네트워크를 통해 직접 통신하므로 파드의 라우팅이나 방화벽 정책을 적용하는것이 까다로운 단점이 있습니다.</li>
  <li>Pod 패킷 암호화는 WireGuard를 사용하여 노드 간 통신을 암호화하는 방법으로, 가장 안전하지만 암호화에 따른 
부하와 지연이 발생하게 됩니다.</li>
  <li>이러한 특성들을 잘 이해하여 적절한 네트워크 모드를 선택하여 사용해야 합니다.</li>
  <li>각각의 mode에 대해 알아보겠습니다.</li>
</ul>

<h4 id="ipip-모드">IPIP 모드</h4>

<ul>
  <li>IPIP 모드는 노드 간 통신을 위한 네트워크 모드로, IPIP 터널링을 사용하여 노드 간 통신을 합니다.</li>
  <li>앞선 실습들이 모두 IPIP 모드로 진행되었습니다. Calico의 기본 네트워크 모드이며, 적절한 속도와 좋은 사용성을 제공합니다.</li>
  <li>단점으로는 클라우드 서비스에서 IPIP 터널링을 허용해야 하며, IPIP 터널링으로 인한 오버헤드가 발생할 수 있습니다.</li>
</ul>

<h5 id="ipip-모드-설정">IPIP 모드 설정</h5>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">projectcalico.org/v3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">IPPool</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">default-ipv4-ippool</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">allowedUses</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">Workload</span>
  <span class="pi">-</span> <span class="s">Tunnel</span>
  <span class="na">blockSize</span><span class="pi">:</span> <span class="m">24</span>
  <span class="na">cidr</span><span class="pi">:</span> <span class="s">172.16.0.0/16</span>
  <span class="na">natOutgoing</span><span class="pi">:</span> <span class="kc">true</span>
  <span class="na">ipipMode</span><span class="pi">:</span> <span class="s">Always</span>  <span class="c1"># IPIP 모드 설정</span>
  <span class="na">vxlanMode</span><span class="pi">:</span> <span class="s">Never</span>
</code></pre></div></div>

<h5 id="통신-흐름">통신 흐름</h5>

<p><img src="/assets/2024/kans-3th/w3/20240921_kans_w3_19.png" alt="img.png" class="w-80 image-center" /></p>

<ul>
  <li>다른 노드와의 파드간의 통신은 tunl0 인터페이스를 통해 IP 헤더에 IP 헤더를 감싸서(IP-in-IP) 상대 노드에 도달후 IPIP 헤더를 제거하고 최종적으로 상대방 파드로 전달됩니다.</li>
  <li>다른 노드의 파드 대역은 BGP로 전파되며, Felix에 의해 노드의 라우팅 테이블에 자동으로 추가/삭제 됩니다.</li>
</ul>

<h4 id="direct-모드">Direct 모드</h4>

<ul>
  <li>Direct 모드는 노드 간 통신을 위한 네트워크 모드로, 호스트의 물리 네트워크 인터페이스를 사용하여 노드 간 통신을 합니다.</li>
  <li>클라우드 사업자 네트워크에서는 NIC에 매칭되지 않은 IP 패킷이 차단되기 때문에, NIC에서 Source/Destination Check 기능을 해제해야 합니다. <a href="https://docs.aws.amazon.com/ko_kr/vpc/latest/userguide/VPC_NAT_Instance.html#EIP_Disable_SrcDestCheck">링크</a>
    <ul>
      <li>AWS에서는 다음의 AWS CLI 명령으로 NIC의 Source/Destination Check 기능을 해제할 수 있습니다.
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>aws ec2 modify-instance-attribute <span class="nt">--instance-id</span> &lt;INSTANCE_ID&gt; <span class="nt">--source-dest-check</span> <span class="s2">"{</span><span class="se">\"</span><span class="s2">Value</span><span class="se">\"</span><span class="s2">: false}"</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>Virtual box에서는 NIC의 promiscuous mode를 사용해야 합니다.</li>
</ul>

<h5 id="direct-모드-설정">Direct 모드 설정</h5>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">projectcalico.org/v3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">IPPool</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">default-ipv4-ippool</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">allowedUses</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">Workload</span>
  <span class="pi">-</span> <span class="s">Tunnel</span>
  <span class="na">blockSize</span><span class="pi">:</span> <span class="m">24</span>
  <span class="na">cidr</span><span class="pi">:</span> <span class="s">172.16.0.0/16</span>
  <span class="na">natOutgoing</span><span class="pi">:</span> <span class="kc">true</span>
  <span class="na">ipipMode</span><span class="pi">:</span> <span class="s">Never</span>   <span class="c1"># IPIP 모드 사용하지 않음</span>
  <span class="na">vxlanMode</span><span class="pi">:</span> <span class="s">Never</span>
</code></pre></div></div>

<h5 id="통신-흐름-1">통신 흐름</h5>

<p><img src="/assets/2024/kans-3th/w3/20240921_kans_w3_20.png" alt="img.png" class="w-80 image-center" /></p>

<h5 id="cross-subnet-모드">Cross Subnet 모드</h5>

<ul>
  <li>Direct 모드는 Cross Subnet 모드로 사용할 수 있습니다.</li>
  <li>
    <p>Cross Subnet 모드는 노드간의 네트워크 대역이 다를때 사용할 수 있으며, <strong>노드간 같은 네트워크 대역이면 Direct 모드</strong>로 동작하고,
<strong>다른 네트워크 대역이면 IPIP/VXLAN 모드</strong>로 동작합니다.</p>
  </li>
  <li>
    <p>IPIP를 이용한 Cross Subnet 모드 설정</p>

    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">projectcalico.org/v3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">IPPool</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">default-ipv4-ippool</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">allowedUses</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">Workload</span>
  <span class="pi">-</span> <span class="s">Tunnel</span>
  <span class="na">blockSize</span><span class="pi">:</span> <span class="m">24</span>
  <span class="na">cidr</span><span class="pi">:</span> <span class="s">172.16.0.0/16</span>
  <span class="na">natOutgoing</span><span class="pi">:</span> <span class="kc">true</span>
  <span class="na">ipipMode</span><span class="pi">:</span> <span class="s">CrossSubnet</span>   <span class="c1"># IPIP 모드를 Subnet이 다를때만 사용</span>
  <span class="na">vxlanMode</span><span class="pi">:</span> <span class="s">Never</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>VXLAN를 이용한 Cross Subnet 모드 설정</p>

    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">projectcalico.org/v3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">IPPool</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">default-ipv4-ippool</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">allowedUses</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">Workload</span>
  <span class="pi">-</span> <span class="s">Tunnel</span>
  <span class="na">blockSize</span><span class="pi">:</span> <span class="m">24</span>
  <span class="na">cidr</span><span class="pi">:</span> <span class="s">172.16.0.0/16</span>
  <span class="na">natOutgoing</span><span class="pi">:</span> <span class="kc">true</span>
  <span class="na">ipipMode</span><span class="pi">:</span> <span class="s">Never</span>   
  <span class="na">vxlanMode</span><span class="pi">:</span> <span class="s">CrossSubnet</span>      <span class="c1"># VXLAN 모드를 Subnet이 다를때만 사용</span>
</code></pre></div>    </div>
  </li>
</ul>

<h4 id="vxlan-모드">VXLAN 모드</h4>

<ul>
  <li>VXLAN 모드는 Flannel에서 사용했던 VXLAN을 사용하여 노드 간 통신을 위한 네트워크 모드입니다.</li>
  <li>IPIP 모드와 비슷하지만, VXLAN은 IPIP보다 속도가 느리지만, 클라우드 서비스에서 IPIP 터널링을 허용하지 않을 때 사용할 수 있습니다.</li>
</ul>

<h5 id="vxlan-모드-설정">VXLAN 모드 설정</h5>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">projectcalico.org/v3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">IPPool</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">default-ipv4-ippool</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">allowedUses</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">Workload</span>
  <span class="pi">-</span> <span class="s">Tunnel</span>
  <span class="na">blockSize</span><span class="pi">:</span> <span class="m">24</span>
  <span class="na">cidr</span><span class="pi">:</span> <span class="s">172.16.0.0/16</span>
  <span class="na">natOutgoing</span><span class="pi">:</span> <span class="kc">true</span>
  <span class="na">ipipMode</span><span class="pi">:</span> <span class="s">Never</span>  
  <span class="na">vxlanMode</span><span class="pi">:</span> <span class="s">Always</span> <span class="c1"># VXLAN 모드 설정</span>
</code></pre></div></div>

<h5 id="통신-흐름-2">통신 흐름</h5>

<p><img src="/assets/2024/kans-3th/w3/20240921_kans_w3_21.png" alt="img.png" class="w-80 image-center" /></p>

<ul>
  <li>VXLAN은 vxlan 인터페이스를 통해 L2 패킷을 UDP - VXLAN으로 감싸서 상대측 노드로 전달 후 vxlan 인터페이스에서 VXLAN에서 실제 L2 프레임을 추출하여 최종적으로 상대방 파드로 전달됩니다.</li>
  <li>이때 BGP 는 미사용되며, VXLAN L3 라우팅을 통해서 동작합니다.</li>
</ul>

<h4 id="파드-패킷-암호화-네트워크-레벨">파드 패킷 암호화 (네트워크 레벨)</h4>

<ul>
  <li>WireGuard를 사용하여 노드 간 통신을 암호화하는 방법으로, 가장 안전하지만 암호화에 따른 부하와 지연이 발생하게 됩니다.</li>
  <li>최근 대두되고 있는 제로 트러스트 네트워크 환경에서 사용할 수 있습니다.</li>
  <li>사용되는 WireGuard는 IPSec이나 OpenVPN과 같은 VPN 프로토콜이며, 기존의 VPN들 보다 빠르고 간단하며, 적은 리소스를 사용합니다.</li>
</ul>

<h5 id="wireguard-모드-설정">WireGuard 모드 설정</h5>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">projectcalico.org/v3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">FelixConfiguration</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">default</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">bpfConnectTimeLoadBalancing</span><span class="pi">:</span> <span class="s">TCP</span>
  <span class="na">bpfHostNetworkedNATWithoutCTLB</span><span class="pi">:</span> <span class="s">Enabled</span>
  <span class="na">bpfLogLevel</span><span class="pi">:</span> <span class="s2">"</span><span class="s">"</span>
  <span class="na">floatingIPs</span><span class="pi">:</span> <span class="s">Disabled</span>
  <span class="na">logSeverityScreen</span><span class="pi">:</span> <span class="s">Info</span>
  <span class="na">reportingInterval</span><span class="pi">:</span> <span class="s">0s</span>
  <span class="na">wireguardEnabled</span><span class="pi">:</span> <span class="kc">true</span>    <span class="c1"># wireguard 사용 설정</span>
</code></pre></div></div>

<h5 id="통신-흐름-3">통신 흐름</h5>

<p><img src="/assets/2024/kans-3th/w3/20240921_kans_w3_22.png" alt="img_1.png" class="w-80 image-center" /></p>

<ul>
  <li>원본 패킷이 wireg 인터페이스를 통해 WireGuard로 암호화되어 상대방 노드로 전달되며, 상대방 노드에서는 wireg 인터페이스를 통해 복호화하여 최종적으로 상대방 파드로 전달됩니다.</li>
</ul>

<hr />

<h2 id="마치며">마치며</h2>

<p>지금까지 Calico의 기본적인 구성과 동작 방식에 대해 알아보았습니다.
Flannel에 비해 많은 기능을 제공하고, 다양한 네트워크를 통해 좀 더 안전하고 빠른 네트워크 환경을 제공하는것 같습니다.
IP-in-IP 개념은 처음 보는 내용이라 신선하였습니다. 하지만 IPIP의 경우 클라우드 환경에 따라 사용하기 어려울 수도 있고,
Direct 모드는 promiscuous mode로 인해 불필요한 오버헤드와 보안 이슈가 발생할 수 있을 것 같습니다.
VXLAN을 사용하자니 Flannel에 비해 우위가 크게 없어 보이고, 제로트러스트가 필요한 환경에서 파드 패킷 암호화를 
사용하는 것은 좋아보입니다.</p>

<p>기존에 많이들 쓴다고 해서 Calico를 사용했었는데 이렇게 심오한 세계가 있다는 것을 알게 되어서 기쁩니다.
앞으로 배우게될 Cilium CNI가 더 기대가 됩니다.</p>]]></content><author><name></name></author><category term="kans" /><category term="kubernetes," /><category term="network," /><category term="linux" /><summary type="html"><![CDATA[지난주에 이어 이번주에는 Calico CNI와 Calico Network Mode에 대해 알아보겠습니다.]]></summary></entry><entry><title type="html">[KANS 3기] K8S Flannel CNI &amp;amp; PAUSE</title><link href="https://sweetlittlebird.github.io/posts/2024-09-07-KANS-Study-Week2/" rel="alternate" type="text/html" title="[KANS 3기] K8S Flannel CNI &amp;amp; PAUSE" /><published>2024-09-07T17:40:18+09:00</published><updated>2024-09-07T17:40:18+09:00</updated><id>https://sweetlittlebird.github.io/posts/KANS%20Study%20-%20Week2</id><content type="html" xml:base="https://sweetlittlebird.github.io/posts/2024-09-07-KANS-Study-Week2/"><![CDATA[<h2 id="들어가며">들어가며</h2>

<p>지난주에 이어 이번주에는 쿠버네티스에대해 간략하게 알아보고 KIND, PAUSE 컨테이너와 Flannel CNI에 대해 알아보겠습니다.
KANS 3기 2주차 스터디를 시작하겠습니다.</p>

<h2 id="쿠버네티스-소개">쿠버네티스 소개</h2>

<ul>
  <li>쿠버네티스는 구글에서 오픈소스로 공개한 컨테이너화된 애플리케이션을 자동으로 배포, 스케일링 및 관리하는 오픈소스 플랫폼입니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w2/20240907_kans_w2_1.gif" alt="20240907_kans_w2_1.gif" class="image-center" />
<em class="image-caption">출처: <a href="https://blog.naver.com/love_tolty/222167051615">https://blog.naver.com/love_tolty/222167051615</a></em></p>

<ul>
  <li>
    <p>쿠버네티스는 위의 그림과 같이 다양한 컴포넌트로 이루어져 있습니다. 각 요소를 살펴보면 아래와 같습니다.</p>
  </li>
  <li><strong>Control Plane(마스터 노드)</strong> : 마스터는 단일 서버 혹은 고가용성을 위한 클러스터 마스터로 구축
    <ul>
      <li>kube-<strong>apiserver</strong> : <strong>모든 요청</strong>을 받아 드리는 <strong>API 서버</strong></li>
      <li>etcd : 클러스터내 모든 메타 정보를 저장하는 key/value DB 서비스</li>
      <li>kube-scheduler : 컨테이너를 워커 노드에 배치하는 스케줄러</li>
      <li>kube-controller-manager : 현재 상태와 바라는 상태를 지속적으로 확인하며 특정 이벤트에 따라 특정 동작을 수행하는 컨트롤러 - <a href="https://kubernetes.io/docs/concepts/architecture/controller/">링크</a></li>
      <li>cloud-controller-manager : (AWS, GCP, Azure 등 클라우드 플랫폼에 특화된 리소스를 제어하는 클라우드 컨트롤러 - <a href="https://kubernetes.io/docs/concepts/architecture/cloud-controller/">링크</a></li>
    </ul>
  </li>
  <li><strong>Worker Node(워커 노드)</strong> - <a href="https://kubernetes.io/docs/concepts/architecture/nodes/">링크</a>
    <ul>
      <li>kubelet : 마스터의 명령에 따라 컨테이너의 라이프 사이클을 관리하는 노드 관리자</li>
      <li>kube-proxy : 컨테이너의 네트워킹을 책임지는 프록시, 네트워크 규칙을 유지 관리</li>
      <li>Container Runtime : 실제 컨테이너를 실행하는 컨테이너 실행 환경, (ContainerD, CRI-O, …) - <a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/">링크</a></li>
    </ul>
  </li>
  <li><strong>Addon(애드온)</strong>
    <ul>
      <li>CNI : Container Network Interface 는 k8s 네트워크 환경을 구성해줍니다.
        <ul>
          <li>예) Flannel, Calico, Weave Net, Cilium, …</li>
        </ul>
      </li>
      <li>DNS : 클러스터 내부 DNS 서비스를 제공합니다.
        <ul>
          <li>예) CoreDNS, Kube-DNS, …</li>
        </ul>
      </li>
      <li>기타 : 모니터링, 대시보드, 로깅 등등</li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="kind-소개-및-설치">kind 소개 및 설치</h2>

<p><img src="/assets/2024/kans-3th/w2/20240907_kans_w2_2.png" alt="img.png" class="w-30 image-center" /></p>

<p>kind는 <strong>K</strong>ubernetes <strong>IN</strong> <strong>D</strong>ocker의 약자로, 로컬 환경에서 쿠버네티스 클러스터를 쉽게 구성할 수 있도록 도와주는 도구입니다. 이름에서 알 수 있듯이 Kubernetes를 Docker 안에서 DIND(Docker in Docker) 방식으로 구동시켜주는 도구입니다.
minikube나 k3s 등과 달리 <strong>Docker만 설치되어 있으면 손쉽게 쿠버네티스 클러스터를 구성</strong>할 수 있습니다.</p>

<ul>
  <li>kind의 구조를 그림으로 표현하면 아래와 같습니다.
<img src="/assets/2024/kans-3th/w2/20240907_kans_w2_3.png" alt="img.png" class="image-center" />
<em class="image-caption">출처 : <a href="[https://kind.sigs.k8s.io/docs/design/initial/">https://kind.sigs.k8s.io/docs/design/initial/</a></em></li>
</ul>

<h3 id="설치">설치</h3>

<p>제가 사용중인 macOS를 기준으로 작성하였습니다. macOS에서 테라폼을 설치하려면 Homebrew를 이용하여 설치할 수 있습니다.
(홈브루 설치 방법은 <a href="https://whalec.io/homebrew-설치-및-사용-방법">https://whalec.io/homebrew-설치-및-사용-방법</a> 를 참고하세요.)</p>

<ul>
  <li>kind 설치 및 필수 툴 설치</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Install Kind</span>
<span class="nv">$ </span>brew <span class="nb">install </span>kind
<span class="nv">$ </span>kind <span class="nt">--version</span>
<span class="c"># =&gt; kind version 0.24.0</span>

<span class="c"># Install kubectl</span>
<span class="nv">$ </span>brew <span class="nb">install </span>kubernetes-cli
<span class="nv">$ </span>kubectl version <span class="nt">--client</span><span class="o">=</span><span class="nb">true</span>
<span class="c"># =&gt; Client Version: v1.31.0</span>
<span class="c">#    Kustomize Version: v5.4.2</span>

<span class="c"># Install Helm</span>
<span class="nv">$ </span>brew <span class="nb">install </span>helm
<span class="nv">$ </span>helm version
<span class="c"># =&gt; version.BuildInfo{Version:&amp;quot;v3.15.4&amp;quot;, GitCommit:&amp;quot;fa9efb07d9d8debbb4306d72af76a383895aa8c4&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;, GoVersion:&amp;quot;go1.22.6&amp;quot;}</span>

<span class="c"># Install Wireshark : 캡처된 패킷 확인</span>
<span class="nv">$ </span>brew <span class="nb">install</span> <span class="nt">--cask</span> wireshark

<span class="c"># (선택) kubectl 출력 시 하이라이트 처리</span>
<span class="nv">$ </span>brew <span class="nb">install </span>kubecolor
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"alias kubectl=kubecolor"</span> <span class="o">&gt;&gt;</span> ~/.zshrc
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"compdef kubecolor=kubectl"</span> <span class="o">&gt;&gt;</span> ~/.zshrc
</code></pre></div></div>

<h3 id="1-node-클러스터-구성-테스트">1-Node 클러스터 구성 테스트</h3>

<ul>
  <li>간단한 클러스터를 만들고 테스트 해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 클러스터 배포 전 확인</span>
<span class="nv">$ </span>docker ps

<span class="c"># Create a cluster with kind</span>
<span class="nv">$ </span>kind create cluster
<span class="c"># =&gt; Creating cluster &amp;quot;kind&amp;quot; ...</span>
<span class="c">#    &lt;span style="color:green;"&gt;✓&lt;/span&gt; Ensuring node image (kindest/node:v1.31.0) 🖼</span>
<span class="c">#    &lt;span style="color:green;"&gt;✓&lt;/span&gt; Preparing nodes 📦 </span>
<span class="c">#    &lt;span style="color:green;"&gt;✓&lt;/span&gt; Writing configuration 📜</span>
<span class="c">#    &lt;span style="color:green;"&gt;✓&lt;/span&gt; Starting control-plane 🕹️</span>
<span class="c">#    &lt;span style="color:green;"&gt;✓&lt;/span&gt; Installing CNI 🔌</span>
<span class="c">#    &lt;span style="color:green;"&gt;✓&lt;/span&gt; Installing StorageClass 💾</span>
<span class="c">#    Set kubectl context to &amp;quot;kind-kind&amp;quot;</span>
<span class="c">#    You can now use your cluster with:</span>
<span class="c">#    </span>
<span class="c">#    kubectl cluster-info --context kind-kind </span>
<span class="c">#    </span>
<span class="c">#    Not sure what to do next? 😅  Check out https://kind.sigs.k8s.io/docs/user/quick-start/</span>

<span class="c"># 클러스터 배포 확인</span>
<span class="nv">$ </span>kind get clusters
<span class="nv">$ </span>kind get nodes
<span class="nv">$ </span>kubectl cluster-info

<span class="c"># 노드 정보 확인</span>
<span class="nv">$ </span>kubectl get node <span class="nt">-o</span> wide

<span class="c"># 파드 정보 확인</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-A</span>
<span class="nv">$ </span>kubectl get componentstatuses

<span class="c"># 컨트롤플레인 (컨테이너) 노드 1대가 실행</span>
<span class="nv">$ </span>docker ps
<span class="nv">$ </span>docker images

<span class="c"># kube config 파일 확인</span>
<span class="nv">$ </span><span class="nb">cat</span> ~/.kube/config

<span class="c"># nginx 파드 배포 및 확인 : 컨트롤플레인 노드인데 파드가 배포 될까요?</span>
<span class="nv">$ </span>kubectl run nginx <span class="nt">--image</span><span class="o">=</span>nginx:alpine
<span class="nv">$ </span>kubectl get pod <span class="nt">-owide</span>

<span class="c"># 노드에 Taints 정보 확인</span>
<span class="nv">$ </span>kubectl describe node | <span class="nb">grep </span>Taints
<span class="c"># =&gt; Taints:             &lt;none&gt;</span>

<span class="c"># 클러스터 삭제</span>
<span class="nv">$ </span>kind delete cluster

<span class="c"># kube config 삭제 확인</span>
<span class="nv">$ </span><span class="nb">cat</span> ~/.kube/config
</code></pre></div></div>

<ul>
  <li>테스트 중에 노드가 커트롤 플레인 하나 뿐인데도 파드가 배포되는것을 확인할 수 있습니다.</li>
  <li><code class="language-plaintext highlighter-rouge">kubectl describe node | grep Taints</code>를 통해 확인해본 결과 <code class="language-plaintext highlighter-rouge">=&gt; Taints: &lt;none&gt;</code>로 컨트롤 플레인에 보통 걸려있는 taint가 없어서 파드가 배포되는 것을 확인할 수 있습니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w2/20240907_kans_w2_4.png" alt="img.png" /></p>

<h3 id="2-node-클러스터-구성-테스트">2-Node 클러스터 구성 테스트</h3>

<ul>
  <li>이번에는 좀 더 나아가서 control-plane과 worker의 2개의 노드로 구성된 KIND 클러스터를 만들고, 클러스터 구성을 확인해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 클러스터 배포 전 확인</span>
<span class="nv">$ </span>docker ps
<span class="c"># =&gt; CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES</span>

<span class="c"># kind 는 별도 도커 네트워크 생성 후 사용 : 기본값 172.18.0.0/16</span>
<span class="nv">$ </span>docker network <span class="nb">ls</span>
<span class="c"># =&gt; NETWORK ID     NAME                     DRIVER    SCOPE</span>
<span class="c">#    ...</span>
<span class="c">#    3bbcc6aa8f38   kind                     bridge    local</span>
<span class="c">#    ...</span>

<span class="nv">$ </span>docker inspect kind | jq
<span class="c"># =&gt; [</span>
<span class="c">#      {</span>
<span class="c">#        &amp;quot;Name&amp;quot;: &amp;quot;kind&amp;quot;,</span>
<span class="c">#        &amp;quot;Driver&amp;quot;: &amp;quot;bridge&amp;quot;,</span>
<span class="c">#        ...</span>
<span class="c">#        &amp;quot;IPAM&amp;quot;: {</span>
<span class="c">#          &amp;quot;Driver&amp;quot;: &amp;quot;default&amp;quot;,</span>
<span class="c">#          &amp;quot;Options&amp;quot;: {},</span>
<span class="c">#          &amp;quot;Config&amp;quot;: [</span>
<span class="c">#            {</span>
<span class="c">#              &amp;quot;Subnet&amp;quot;: &amp;quot;172.20.0.0/16&amp;quot;,</span>
<span class="c">#              &amp;quot;Gateway&amp;quot;: &amp;quot;172.20.0.1&amp;quot;</span>
<span class="c">#            }</span>
<span class="c">#          ]</span>
<span class="c">#        },</span>
<span class="c">#       ...</span>
<span class="c">#      }</span>
<span class="c">#    ]</span>

<span class="c"># KIND로 control-plane, worker라는 2개의 노드를 가진 클러스터 만들기</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span> <span class="no">EOT</span><span class="sh"> &gt; kind-2node.yaml 
# two node (one workers) cluster config
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
- role: worker
</span><span class="no">EOT
</span><span class="nv">$ </span>kind create cluster <span class="nt">--config</span> kind-2node.yaml <span class="nt">--name</span> myk8s

<span class="c"># 확인</span>
<span class="nv">$ </span>kind get nodes <span class="nt">--name</span> myk8s
<span class="c"># =&gt; myk8s-worker</span>
<span class="c">#    myk8s-control-plane</span>

<span class="c"># k8s api 주소 확인</span>
<span class="nv">$ </span>kubectl cluster-info
<span class="c"># =&gt; &lt;span style="color:green;"&gt;Kubernetes control plane&lt;/span&gt; is running at &lt;span style="color:olive;"&gt;https://127.0.0.1:58638&lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:green;"&gt;CoreDNS&lt;/span&gt; is running at &lt;span style="color:olive;"&gt;https://127.0.0.1:58638/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy&lt;/span&gt;</span>
<span class="c">#    </span>
<span class="c">#    To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.</span>

<span class="c"># 호스트에서 접속 테스트 </span>
<span class="nv">$ </span>curl <span class="nt">-k</span> https://localhost:58638
<span class="c"># =&gt; {</span>
<span class="c">#      &amp;quot;kind&amp;quot;: &amp;quot;Status&amp;quot;,</span>
<span class="c">#      &amp;quot;apiVersion&amp;quot;: &amp;quot;v1&amp;quot;,</span>
<span class="c">#      &amp;quot;metadata&amp;quot;: {},</span>
<span class="c">#      &amp;quot;status&amp;quot;: &amp;quot;Failure&amp;quot;,</span>
<span class="c">#      &amp;quot;message&amp;quot;: &amp;quot;forbidden: User \&amp;quot;system:anonymous\&amp;quot; cannot get path \&amp;quot;/\&amp;quot;&amp;quot;,</span>
<span class="c">#      &amp;quot;reason&amp;quot;: &amp;quot;Forbidden&amp;quot;,</span>
<span class="c">#      &amp;quot;details&amp;quot;: {},</span>
<span class="c">#      &amp;quot;code&amp;quot;: 403</span>
<span class="c">#    } # 호스트에서 접속이 됩니다! 왜 그럴까요?</span>

<span class="nv">$ </span>docker ps <span class="c"># 포트 포워딩 정보 확인</span>
<span class="c"># =&gt; CONTAINER ID   IMAGE                  COMMAND                  CREATED         STATUS         PORTS                                  NAMES</span>
<span class="c">#    6228f280b992   kindest/node:v1.31.0   &amp;quot;/usr/local/bin/entr…&amp;quot;   3 minutes ago   Up 3 minutes   0.0.0.0:30000-30001-&amp;gt;30000-30001/tcp   myk8s-worker</span>
<span class="c">#    219113c36204   kindest/node:v1.31.0   &amp;quot;/usr/local/bin/entr…&amp;quot;   3 minutes ago   Up 3 minutes   127.0.0.1:58638-&amp;gt;6443/tcp              myk8s-control-plane</span>

<span class="c"># 도커에서 127.0.0.1:58638-&amp;gt;6443/tcp 로 포트포워딩을 하기 때문인것을 확인할 수 있습니다. </span>

<span class="c"># apiserver 프로세스 확인</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane ss <span class="nt">-tnlp</span> | <span class="nb">grep </span>6443
<span class="c"># =&gt; LISTEN 0      4096               *:6443             *:*    users:((&amp;quot;kube-apiserver&amp;quot;,pid=584,fd=3)) </span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-n</span> kube-system <span class="nt">-l</span> <span class="nv">component</span><span class="o">=</span>kube-apiserver <span class="nt">-owide</span> <span class="c"># 파드 IP 확인</span>
<span class="c"># =&gt; NAME                                 READY   STATUS    RESTARTS   AGE     IP           NODE                  NOMINATED NODE   READINESS GATES</span>
<span class="c">#    kube-apiserver-myk8s-control-plane   1/1     Running   0          5m39s   172.20.0.2   myk8s-control-plane   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c"># kube-apiserver 파드 상세 정보 확인</span>
<span class="nv">$ </span>kubectl describe  pod <span class="nt">-n</span> kube-system <span class="nt">-l</span> <span class="nv">component</span><span class="o">=</span>kube-apiserver

<span class="c"># health check 주소 접속 확인</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane curl <span class="nt">-k</span> https://localhost:6443/livez <span class="p">;</span><span class="nb">echo</span>
<span class="c"># =&gt; ok</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane curl <span class="nt">-k</span> https://localhost:6443/readyz <span class="p">;</span><span class="nb">echo</span>
<span class="c"># =&gt; ok</span>

<span class="c"># 노드 정보 확인 : CRI 는 containerd 사용</span>
<span class="nv">$ </span>kubectl get node <span class="nt">-o</span> wide
<span class="c"># =&gt; NAME                  STATUS   ROLES           AGE     VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                         KERNEL-VERSION     CONTAINER-RUNTIME</span>
<span class="c">#    myk8s-control-plane   Ready    control-plane   7m15s   v1.31.0   172.20.0.2    &amp;lt;none&amp;gt;        Debian GNU/Linux 12 (bookworm)   5.10.76-linuxkit   containerd://1.7.18</span>
<span class="c">#    myk8s-worker          Ready    &amp;lt;none&amp;gt;          7m1s    v1.31.0   172.20.0.3    &amp;lt;none&amp;gt;        Debian GNU/Linux 12 (bookworm)   5.10.76-linuxkit   containerd://1.7.18</span>

<span class="c"># 파드 정보 확인 : CNI 는 kindnet 사용</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-A</span> <span class="nt">-owide</span>
<span class="c"># =&gt; NAMESPACE            NAME                                          READY   STATUS    RESTARTS   AGE   IP           NODE                  NOMINATED NODE   READINESS GATES</span>
<span class="c">#    kube-system          coredns-6f6b679f8f-9gxnw                      1/1     Running   0          10m   10.244.0.4   myk8s-control-plane   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    kube-system          etcd-myk8s-control-plane                      1/1     Running   0          10m   172.20.0.2   myk8s-control-plane   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    kube-system          kindnet-b5brb                                 1/1     Running   0          10m   172.20.0.3   myk8s-worker          &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    kube-system          kube-apiserver-myk8s-control-plane            1/1     Running   0          10m   172.20.0.2   myk8s-control-plane   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    kube-system          kube-controller-manager-myk8s-control-plane   1/1     Running   0          10m   172.20.0.2   myk8s-control-plane   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    kube-system          kube-proxy-tbh7b                              1/1     Running   0          10m   172.20.0.3   myk8s-worker          &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    kube-system          kube-scheduler-myk8s-control-plane            1/1     Running   0          10m   172.20.0.2   myk8s-control-plane   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    local-path-storage   local-path-provisioner-57c5987fd4-nfcv8       1/1     Running   0          10m   10.244.0.2   myk8s-control-plane   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    ...</span>

<span class="c"># 네임스페이스 확인 </span>
<span class="nv">$ </span>kubectl get namespaces
<span class="c"># =&gt; NAME                 STATUS   AGE</span>
<span class="c">#    default              Active   11m</span>
<span class="c">#    kube-node-lease      Active   11m</span>
<span class="c">#    kube-public          Active   11m</span>
<span class="c">#    kube-system          Active   11m</span>
<span class="c">#    local-path-storage   Active   10m</span>

<span class="c"># 디버그용 내용 출력에 ~/.kube/config 권한 인증 로드</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-v6</span>
<span class="c"># =&gt; I0907 21:08:08.698472   40997 loader.go:395] Config loaded from file:  /Users/psyche/.kube/config</span>
<span class="c">#    I0907 21:08:08.709347   40997 round_trippers.go:553] GET https://127.0.0.1:58638/api/v1/namespaces/default/pods?limit=500 200 OK in 8 milliseconds</span>
<span class="c">#    No resources found in default namespace.</span>

<span class="c"># kube config 파일 확인</span>
<span class="nv">$ </span><span class="nb">cat</span> ~/.kube/config

<span class="c"># local-path 라는 StorageClass 가 설치, local-path 는 노드의 로컬 저장소를 활용함</span>
<span class="c"># 로컬 호스트의 path 를 지정할 필요 없이 local-path provisioner 이 볼륨을 관리</span>
<span class="nv">$ </span>kubectl get sc
<span class="c"># =&gt; NAME                 PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE</span>
<span class="c">#    standard (default)   rancher.io/local-path   Delete          WaitForFirstConsumer   false                  12m</span>
<span class="nv">$ </span>kubectl get deploy <span class="nt">-n</span> local-path-storage
</code></pre></div></div>

<h3 id="쿠버네티스-관련-정보-조사">쿠버네티스 관련 정보 조사</h3>

<p>이번에는 KIND 내부의 쿠버네티스 관련 정보를 살펴보겠습니다.
원활한 테스트를 위해 필요한 툴을 설치하고, KIND 내부의 쿠버네티스 관련 정보를 살펴보겠습니다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 툴 설치</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane sh <span class="nt">-c</span> <span class="s1">'apt update &amp;&amp; apt install tree jq psmisc lsof wget bridge-utils tcpdump htop git nano -y'</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker sh <span class="nt">-c</span> <span class="s1">'apt update &amp;&amp; apt install tree jq psmisc lsof wget bridge-utils tcpdump htop git nano -y'</span>

<span class="c"># static pod manifest 위치 찾기</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane <span class="nb">grep </span>staticPodPath /var/lib/kubelet/config.yaml
<span class="c"># =&gt; staticPodPath: /etc/kubernetes/manifests</span>

<span class="c"># static pod 정보 확인 : kubectl 및 control plane 에서 관리되지 않고 kubelet 을 통해 지정한 컨테이너를 배포</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane tree /etc/kubernetes/manifests/
<span class="c"># =&gt; /etc/kubernetes/manifests/</span>
<span class="c">#    |-- etcd.yaml</span>
<span class="c">#    |-- kube-apiserver.yaml</span>
<span class="c">#    |-- kube-controller-manager.yaml</span>
<span class="c">#    `-- kube-scheduler.yaml</span>

<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker tree /etc/kubernetes/manifests/
<span class="c"># =&gt; /etc/kubernetes/manifests/</span>

<span class="c"># 워커 노드(컨테이너) bash 진입</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker bash
<span class="c"># ---------------------------------</span>
<span class="nv">$ </span><span class="nb">whoami</span>
<span class="c"># =&gt; root</span>

<span class="c"># kubelet 상태 확인</span>
<span class="nv">$ </span>systemctl status kubelet
<span class="c"># =&gt; ● kubelet.service - kubelet: The Kubernetes Node Agent</span>
<span class="c">#         Loaded: loaded (/etc/systemd/system/kubelet.service; enabled; preset: enabled)</span>
<span class="c">#        Drop-In: /etc/systemd/system/kubelet.service.d</span>
<span class="c">#                 └─10-kubeadm.conf, 11-kind.conf</span>
<span class="c">#         Active: active (running) since Sat 2024-09-07 11:56:44 UTC; 48min ago</span>
<span class="c">#           Docs: http://kubernetes.io/docs/</span>
<span class="c">#       Main PID: 236 (kubelet)</span>
<span class="c">#          Tasks: 15 (limit: 2254)</span>
<span class="c">#         Memory: 32.9M</span>
<span class="c">#            CPU: 1min 7.074s</span>
<span class="c">#         CGroup: /kubelet.slice/kubelet.service</span>
<span class="c">#                 └─236 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/conf&gt;</span>
<span class="c">#    ...</span>

<span class="c"># 컨테이너 확인</span>
<span class="nv">$ </span>docker ps
<span class="c"># =&gt; bash: docker: command not found</span>
<span class="nv">$ </span>crictl ps
<span class="c"># =&gt; CONTAINER           IMAGE               CREATED             STATE               NAME                ATTEMPT             POD ID              POD</span>
<span class="c">#    dd7ff0509e335       6a23fa8fd2b78       49 minutes ago      Running             kindnet-cni         0                   99f981aced025       kindnet-b5brb</span>
<span class="c">#    41b224e66bc3c       c573e1357a14e       49 minutes ago      Running             kube-proxy          0                   8880634c13ba5       kube-proxy-tbh7b</span>
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">docker ps</code>했을때는 <code class="language-plaintext highlighter-rouge">command not found</code>가 나오고 <code class="language-plaintext highlighter-rouge">crictl ps</code>로 했을때 컨테이너 정보가 나오는 것을 보면, KIND는 이름과는 다르게 Docker 대신 CRI(Container Runtime Interface)를 사용하고 있기 때문에 <code class="language-plaintext highlighter-rouge">docker</code> 명령어 대신 <code class="language-plaintext highlighter-rouge">crictl</code> 명령어를 사용해야 합니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># kube-proxy 확인</span>
<span class="nv">$ </span>pstree
<span class="c"># =&gt; systemd-+-containerd---15*[{containerd}]</span>
<span class="c">#            |-containerd-shim-+-kube-proxy---8*[{kube-proxy}]</span>
<span class="c">#            |                 |-pause</span>
<span class="c">#            |                 `-12*[{containerd-shim}]</span>
<span class="c">#            |-containerd-shim-+-kindnetd---11*[{kindnetd}]</span>
<span class="c">#            |                 |-pause</span>
<span class="c">#            |                 `-12*[{containerd-shim}]</span>
<span class="c">#            |-kubelet---14*[{kubelet}]</span>
<span class="c">#            `-systemd-journal</span>
<span class="nv">$ </span>pstree <span class="nt">-p</span>
<span class="c"># kube-proxy 프로세스 정보</span>
<span class="nv">$ </span>ps afxuwww |grep proxy 
<span class="c"># =&gt; root         387  0.0  1.1 1290144 24112 ?       Ssl  11:56   0:02  \_ /usr/local/bin/kube-proxy --config=/var/lib/kube-proxy/config.conf --hostname-override=myk8s-worker</span>
<span class="c"># 방화벽 설정 확인</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> filter <span class="nt">-S</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> mangle <span class="nt">-S</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> raw <span class="nt">-S</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> security <span class="nt">-S</span>

<span class="c"># tcp listen 포트 정보 확인</span>
<span class="nv">$ </span>ss <span class="nt">-tnlp</span>

<span class="c"># 빠져나오기</span>
<span class="nv">$ </span><span class="nb">exit</span>
<span class="c"># ---------------------------------</span>
</code></pre></div></div>

<h3 id="파드-생성-및-확인">파드 생성 및 확인</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 파드 생성</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl create -f -
apiVersion: v1
kind: Pod
metadata:
  name: netpod
spec:
  containers:
  - name: netshoot-pod
    image: nicolaka/netshoot
    command: ["tail"]
    args: ["-f", "/dev/null"]
  terminationGracePeriodSeconds: 0
---
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx-pod
    image: nginx:alpine
  terminationGracePeriodSeconds: 0
</span><span class="no">EOF
</span><span class="c"># =&gt; pod/netpod created</span>
<span class="c">#    pod/nginx created</span>

<span class="c"># 파드 확인</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-owide</span>
<span class="c"># =&gt; NAME     READY   STATUS    RESTARTS   AGE   IP           NODE           NOMINATED NODE   READINESS GATES</span>
<span class="c">#    netpod   1/1     Running   0          52s   10.244.1.3   myk8s-worker   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    nginx    1/1     Running   0          52s   10.244.1.2   myk8s-worker   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>

<span class="c"># netpod 파드에서 nginx 웹 접속</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> netpod <span class="nt">--</span> curl <span class="nt">-s</span> <span class="si">$(</span>kubectl get pod nginx <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.status.podIP<span class="o">}</span><span class="si">)</span> | <span class="nb">grep</span> <span class="nt">-o</span> <span class="s2">"&lt;title&gt;.*&lt;/title&gt;"</span>
<span class="c"># =&gt; &lt;title&gt;Welcome to nginx!&lt;/title&gt;</span>
</code></pre></div></div>

<h3 id="컨트롤-플레인-컨테이너-정보-확인">컨트롤 플레인 컨테이너 정보 확인</h3>

<p>이번에는 컨트롤 플레인 컨테이너의 정보를 확인해보겠습니다. kind는 Docker IN Docker 방식으로 컨트롤 플레인을 구성하기 때문에
컨트롤 플레인에서 정보를 확인할 때와 호스트에서 docker 정보를 확인할때와 차이가 있습니다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 도커 컨테이너 확인</span>
<span class="nv">$ </span>docker ps
<span class="c"># =&gt; CONTAINER ID   IMAGE                  COMMAND                  CREATED       STATUS       PORTS                                  NAMES</span>
<span class="c">#    6228f280b992   kindest/node:v1.31.0   &amp;quot;/usr/local/bin/entr…&amp;quot;   2 hours ago   Up 2 hours   0.0.0.0:30000-30001-&amp;gt;30000-30001/tcp   myk8s-worker</span>
<span class="c">#    219113c36204   kindest/node:v1.31.0   &amp;quot;/usr/local/bin/entr…&amp;quot;   2 hours ago   Up 2 hours   127.0.0.1:58638-&amp;gt;6443/tcp              myk8s-control-plane</span>

<span class="nv">$ </span>docker inspect myk8s-control-plane | jq
...
      <span class="s2">"Entrypoint"</span>: <span class="o">[</span>
        <span class="s2">"/usr/local/bin/entrypoint"</span>,
        <span class="s2">"/sbin/init"</span>
      <span class="o">]</span>,
...

<span class="c"># 컨트롤플레인 컨테이너 bash 접속 후 확인</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane bash
<span class="nt">-------------------------------------------</span>
<span class="c"># CPU 정보 확인</span>
<span class="nv">$ </span><span class="nb">arch</span>
<span class="c"># =&gt; aarch64      # intel 호환 CPU인 경우 x86_64가 표시됩니다.</span>

<span class="c"># 기본 사용자 확인</span>
<span class="nv">$ </span><span class="nb">whoami</span>
<span class="c"># =&gt; root</span>

<span class="c"># 네트워크 정보 확인</span>
<span class="nv">$ </span>ip <span class="nt">-br</span> <span class="nt">-c</span> <span class="nt">-4</span> addr
<span class="c"># =&gt; lo               UNKNOWN        127.0.0.1/8</span>
<span class="c">#    vethfbd4a037@if4 UP             10.244.0.1/32</span>
<span class="c">#    veth50a51781@if4 UP             10.244.0.1/32</span>
<span class="c">#    veth1822edcc@if4 UP             10.244.0.1/32</span>
<span class="c">#    eth0@if17        UP             172.20.0.2/16</span>
<span class="nv">$ </span>ip <span class="nt">-c</span> route
<span class="c"># =&gt; 10.244.0.2 dev vethfbd4a037 scope host</span>
<span class="c">#    10.244.0.3 dev veth50a51781 scope host</span>
<span class="c">#    10.244.0.4 dev veth1822edcc scope host</span>
<span class="c">#    10.244.1.0/24 via 172.20.0.3 dev eth0</span>
<span class="c">#    172.20.0.0/16 dev eth0 proto kernel scope link src 172.20.0.2</span>
<span class="nv">$ </span><span class="nb">cat</span> /etc/resolv.conf
<span class="c"># =&gt; nameserver 192.168.65.2</span>
<span class="c">#    options ndots:0</span>

<span class="c"># Entrypoint 정보 확인</span>
<span class="nv">$ </span><span class="nb">cat</span> /usr/local/bin/entrypoint

<span class="c"># 프로세스 확인 : PID 1 은 /sbin/init</span>
<span class="nv">$ </span>ps <span class="nt">-ef</span>
<span class="c"># =&gt; UID          PID    PPID  C STIME TTY          TIME CMD</span>
<span class="c">#    root           1       0  0 11:56 ?        00:00:01 /sbin/init</span>
<span class="c">#    ...</span>

<span class="c"># kind는 docker 안에서 docker를 운영하기 위해 OS를 흉내내기 위해 자체적으로 systemd를 사용하기 때문에</span>
<span class="c"># 위와 같이 PID 1이 /sbin/init 가 되고, systemctl 명령도 사용할 수 있습니다.</span>
 
<span class="c"># 컨테이터 런타임 정보 확인</span>
<span class="nv">$ </span>systemctl status containerd

<span class="c"># DinD 컨테이너 확인 : crictl 사용</span>
<span class="nv">$ </span>crictl version
<span class="c"># =&gt; Version:  0.1.0</span>
<span class="c">#    RuntimeName:  containerd</span>
<span class="c">#    RuntimeVersion:  v1.7.18</span>
<span class="c">#    RuntimeApiVersion:  v1</span>
<span class="nv">$ </span>crictl info
<span class="nv">$ </span>crictl ps <span class="nt">-o</span> json | jq <span class="nt">-r</span> <span class="s1">'.containers[] | {NAME: .metadata.name, POD: .labels["io.kubernetes.pod.name"]}'</span>
<span class="nv">$ </span>crictl ps
<span class="c"># =&gt; CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD</span>
<span class="c">#    075e7a9f5f7a3       2437cf7621777       2 hours ago         Running             coredns                   0                   5e7c18501fc65       coredns-6f6b679f8f-9gxnw</span>
<span class="c">#    6f61738127a55       2437cf7621777       2 hours ago         Running             coredns                   0                   3b69c8d195b5d       coredns-6f6b679f8f-fk27q</span>
<span class="c">#    ...</span>

<span class="c"># 파드 이미지 확인</span>
<span class="nv">$ </span>crictl images
<span class="c"># =&gt; IMAGE                                           TAG                  IMAGE ID            SIZE</span>
<span class="c">#    docker.io/library/nginx                         alpine               9d6767b714bf1       20.2MB</span>
<span class="c">#    docker.io/nicolaka/netshoot                     latest               eead9e442471d       178MB</span>
<span class="c">#    ...</span>

<span class="c"># kubectl 확인</span>
<span class="nv">$ </span>kubectl get node <span class="nt">-v6</span>
<span class="nv">$ </span><span class="nb">cat</span> /etc/kubernetes/admin.conf

<span class="nv">$ </span><span class="nb">exit</span>
<span class="nt">-------------------------------------------</span>

<span class="c"># 도커 컨테이너 확인 : 다시 한번 자신의 호스트PC에서 도커 컨테이너 확인, DinD 컨테이너가 호스트에서 보이는지 확인</span>
<span class="nv">$ </span>docker ps
<span class="c"># =&gt; CONTAINER ID   IMAGE                  COMMAND                  CREATED       STATUS       PORTS                                  NAMES</span>
<span class="c">#    6228f280b992   kindest/node:v1.31.0   "/usr/local/bin/entr…"   2 hours ago   Up 2 hours   0.0.0.0:30000-30001-&gt;30000-30001/tcp   myk8s-worker</span>
<span class="c">#    219113c36204   kindest/node:v1.31.0   "/usr/local/bin/entr…"   2 hours ago   Up 2 hours   127.0.0.1:58638-&gt;6443/tcp              myk8s-control-plane</span>
<span class="nv">$ </span>docker port myk8s-control-plane

<span class="c"># kubectl 확인 : k8s api 호출 주소 확인</span>
<span class="nv">$ </span>kubectl get node <span class="nt">-v6</span> 
</code></pre></div></div>

<ul>
  <li>KIND의 컨트롤 플레인에서 <code class="language-plaintext highlighter-rouge">crictl ps</code>로 컨테이너를 확인할때와 호스트에서 <code class="language-plaintext highlighter-rouge">docker ps</code>로 확인할때의 차이가 나는것을 확인할 수 있습니다.</li>
  <li>
    <p>이것은 앞에서도 docker 컨테이너 안에서 docker (정확히는 containerd)를 별도로 사용하기 때문에 발생하는 현상입니다. 이것이 DIND(Docker IN Docker)입니다.
—만약 같은 컨테이너가 나온다면 Docker OUT Docker로 동작하기 때문일것입니다—</p>
  </li>
  <li>클러스터를 삭제하겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 클러스터 삭제</span>
<span class="nv">$ </span>kind delete cluster <span class="nt">--name</span> myk8s
<span class="c"># =&gt; Deleting cluster "myk8s" ...</span>
<span class="c">#    Deleted nodes: ["myk8s-worker" "myk8s-control-plane"]</span>
<span class="nv">$ </span>docker ps
<span class="c"># =&gt; CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES</span>
</code></pre></div></div>

<h3 id="multi-node-클러스터-with-kube-ops-view--mapping-ports">Multi-Node 클러스터 with kube-ops-view &amp; mapping ports</h3>

<p>이번에는 KIND로 Multi-Node 클러스터를 구성하고, kube-ops-view를 설치하여 클러스터 정보를 시각화하고, 포트 매핑을 통해 호스트에서 접속할 수 있도록 설정해보겠습니다.</p>

<ul>
  <li>클러스터 구성 및 노드 정보 확인</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># '컨트롤플레인, 워커 노드 1대' 클러스터 배포 : 파드에 접속하기 위한 포트 맵핑 설정</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOT</span><span class="sh">&gt; kind-2node.yaml
# two node (one workers) cluster config
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
- role: worker
  extraPortMappings:
  - containerPort: 31000
    hostPort: 31000
    listenAddress: "0.0.0.0" # Optional, defaults to "0.0.0.0"
    protocol: tcp # Optional, defaults to tcp
  - containerPort: 31001
    hostPort: 31001
</span><span class="no">EOT

</span><span class="nv">$ CLUSTERNAME</span><span class="o">=</span>myk8s
<span class="nv">$ </span>kind create cluster <span class="nt">--config</span> kind-2node.yaml <span class="nt">--name</span> <span class="nv">$CLUSTERNAME</span>
<span class="c"># =&gt; Creating cluster "myk8s" ...</span>
<span class="c">#     ✓ Ensuring node image (kindest/node:v1.31.0) 🖼</span>
<span class="c">#     ✓ Preparing nodes 📦 📦</span>
<span class="c">#     ✓ Writing configuration 📜</span>
<span class="c">#     ✓ Starting control-plane 🕹️</span>
<span class="c">#     ✓ Installing CNI 🔌</span>
<span class="c">#     ✓ Installing StorageClass 💾</span>
<span class="c">#     ✓ Joining worker nodes 🚜</span>
<span class="c">#    Set kubectl context to "kind-myk8s"</span>
<span class="c">#    You can now use your cluster with:</span>
<span class="c">#    </span>
<span class="c">#    kubectl cluster-info --context kind-myk8s</span>
<span class="c">#    </span>
<span class="c">#    Have a nice day! 👋</span>

<span class="c"># 배포 확인</span>
<span class="nv">$ </span>kind get clusters
<span class="c"># =&gt; myk8s</span>
<span class="nv">$ </span>kind get nodes <span class="nt">--name</span> <span class="nv">$CLUSTERNAME</span>
<span class="c"># =&gt; myk8s-control-plane</span>
<span class="c">#    myk8s-worker</span>

<span class="c"># 노드 확인</span>
<span class="nv">$ </span>kubectl get nodes <span class="nt">-o</span> wide
<span class="c"># =&gt; NAME                  STATUS   ROLES           AGE     VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                         KERNEL-VERSION     CONTAINER-RUNTIME</span>
<span class="c">#    myk8s-control-plane   Ready    control-plane   2m12s   v1.31.0   172.20.0.2    &amp;lt;none&amp;gt;        Debian GNU/Linux 12 (bookworm)   5.10.76-linuxkit   containerd://1.7.18</span>
<span class="c">#    myk8s-worker          Ready    &amp;lt;none&amp;gt;          119s    v1.31.0   172.20.0.3    &amp;lt;none&amp;gt;        Debian GNU/Linux 12 (bookworm)   5.10.76-linuxkit   containerd://1.7.18</span>

<span class="c"># 노드에 Taints 정보 확인</span>
<span class="nv">$ </span>kubectl describe node <span class="nv">$CLUSTERNAME</span><span class="nt">-control-plane</span> | <span class="nb">grep </span>Taints
<span class="c"># =&gt; Taints:             node-role.kubernetes.io/control-plane:NoSchedule</span>

<span class="nv">$ </span>kubectl describe node <span class="nv">$CLUSTERNAME</span><span class="nt">-worker</span> | <span class="nb">grep </span>Taints
<span class="c"># =&gt; Taints:             &amp;lt;none&amp;gt;</span>

<span class="c"># control-plane 노드에는 taints가 걸려있어서 스케쥴링이 되지 않고 </span>
<span class="c"># worker 노드에는 taints가 없어서 스케쥴링이 될 것을 예상할 수 있습니다.</span>

<span class="c"># 컨테이너 확인 : 컨테이너 갯수, 컨테이너 이름 확인</span>
<span class="c"># kind yaml 에 포트 맵핑 정보 처럼, 자신의 PC 호스트에 31000 포트 접속 시, 워커노드(실제로는 컨테이너)에 TCP 31000 포트로 연결</span>
<span class="c"># 즉, 워커노드에 NodePort TCP 31000 설정 시 자신의 PC 호스트에서 접속 가능!</span>
<span class="nv">$ </span>docker ps
<span class="c"># =&gt; CONTAINER ID   IMAGE                  COMMAND                  CREATED         STATUS         PORTS                                  NAMES</span>
<span class="c">#    7724a7ff92bb   kindest/node:v1.31.0   &amp;quot;/usr/local/bin/entr…&amp;quot;   6 minutes ago   Up 6 minutes   127.0.0.1:58498-&amp;gt;6443/tcp              myk8s-control-plane</span>
<span class="c">#    5b7fa2f98703   kindest/node:v1.31.0   &amp;quot;/usr/local/bin/entr…&amp;quot;   6 minutes ago   Up 6 minutes   0.0.0.0:31000-31001-&amp;gt;31000-31001/tcp   myk8s-worker</span>
<span class="nv">$ </span>docker port <span class="nv">$CLUSTERNAME</span><span class="nt">-worker</span>
<span class="c"># =&gt; 31000/tcp -&amp;gt; 0.0.0.0:31000</span>
<span class="c">#    31001/tcp -&amp;gt; 0.0.0.0:31001</span>

<span class="c"># 각 노드들의 정보 확인을 docker를 통해서 확인해 볼 수도 있습니다.</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> <span class="nv">$CLUSTERNAME</span><span class="nt">-control-plane</span> ip <span class="nt">-br</span> <span class="nt">-c</span> <span class="nt">-4</span> addr
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> <span class="nv">$CLUSTERNAME</span><span class="nt">-worker</span>  ip <span class="nt">-br</span> <span class="nt">-c</span> <span class="nt">-4</span> addr
</code></pre></div></div>

<ul>
  <li>
    <p>이번에 KIND를 통해 만든 클러스터는 호스트에서 31000, 31001 포트로 접속시 워커노드(컨테이너)의 31000, 31001 포트로 
연결되도록 설정되는데, 그 이유는 KIND 클러스터를 생성할때 아래와 같이 포트를 열것을 지정했기 때문입니다.</p>

    <div class="language-yml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">...</span>
<span class="pi">-</span> <span class="na">role</span><span class="pi">:</span> <span class="s">worker</span>
  <span class="na">extraPortMappings</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">31000</span>
    <span class="na">hostPort</span><span class="pi">:</span> <span class="m">31000</span>
    <span class="na">listenAddress</span><span class="pi">:</span> <span class="s2">"</span><span class="s">0.0.0.0"</span> <span class="c1"># Optional, defaults to "0.0.0.0"</span>
    <span class="na">protocol</span><span class="pi">:</span> <span class="s">tcp</span> <span class="c1"># Optional, defaults to tcp</span>
  <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">31001</span>
    <span class="na">hostPort</span><span class="pi">:</span> <span class="m">31001</span>
<span class="nn">...</span>
</code></pre></div>    </div>
    <p>추가적인 포트 매핑이 필요한 경우 위와 같이 <code class="language-plaintext highlighter-rouge">extraPortMappings</code>에 추가하여 포트를 매핑할 수 있습니다.</p>
  </li>
  <li>
    <p>Kube-ops-view 설치 : Node port 31000</p>
  </li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># kube-ops-view</span>
<span class="c"># helm show values geek-cookbook/kube-ops-view</span>
<span class="nv">$ </span>helm repo add geek-cookbook https://geek-cookbook.github.io/charts/
<span class="nv">$ </span>helm <span class="nb">install </span>kube-ops-view geek-cookbook/kube-ops-view <span class="nt">--version</span> 1.2.2 <span class="nt">--set</span> service.main.type<span class="o">=</span>NodePort,service.main.ports.http.nodePort<span class="o">=</span>31000 <span class="nt">--set</span> env.TZ<span class="o">=</span><span class="s2">"Asia/Seoul"</span> <span class="nt">--namespace</span> kube-system

<span class="c"># 설치 확인</span>
<span class="nv">$ </span>kubectl get deploy,pod,svc,ep <span class="nt">-n</span> kube-system <span class="nt">-l</span> app.kubernetes.io/instance<span class="o">=</span>kube-ops-view
<span class="c"># =&gt; NAME                            READY   UP-TO-DATE   AVAILABLE   AGE</span>
<span class="c">#    deployment.apps/kube-ops-view   0/1     1            0           18s</span>
<span class="c">#    </span>
<span class="c">#    NAME                                 READY   STATUS              RESTARTS   AGE</span>
<span class="c">#    pod/kube-ops-view-657dbc6cd8-tmkl5   0/1     ContainerCreating   0          18s</span>
<span class="c">#    </span>
<span class="c">#    NAME                    TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE</span>
<span class="c">#    service/kube-ops-view   NodePort   10.96.212.51   &amp;lt;none&amp;gt;        8080:31000/TCP   18s</span>
<span class="c">#    </span>
<span class="c">#    NAME                      ENDPOINTS   AGE</span>
<span class="c">#    endpoints/kube-ops-view   &amp;lt;none&amp;gt;      18s</span>

<span class="c"># kube-ops-view 접속 URL 확인 (1.5 , 2 배율)</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"KUBE-OPS-VIEW URL = http://localhost:31000/#scale=1.5"</span>
<span class="c"># =&gt; KUBE-OPS-VIEW URL = http://localhost:31000/#scale=1.5</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"KUBE-OPS-VIEW URL = http://localhost:31000/#scale=2"</span>
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w2/20240907_kans_w2_5.png" alt="img.png" /></p>

<p>클러스터 구성시 노드의 31000포트를 호스트의 31000에 매핑시켜서 호스트에서 위와 같이 열 수 있습니다.</p>

<ul>
  <li>nginx 설치 : NodePort 31001</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 디플로이먼트와 서비스 배포</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl create -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deploy-websrv
spec:
  replicas: 2
  selector:
    matchLabels:
      app: deploy-websrv
  template:
    metadata:
      labels:
        app: deploy-websrv
    spec:
      terminationGracePeriodSeconds: 0
      containers:
      - name: deploy-websrv
        image: nginx:alpine
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: deploy-websrv
spec:
  ports:
    - name: svc-webport
      port: 80
      targetPort: 80
      nodePort: 31001
  selector:
    app: deploy-websrv
  type: NodePort
</span><span class="no">EOF

</span><span class="c"># 확인</span>
<span class="nv">$ </span>docker ps
<span class="c"># =&gt; CONTAINER ID   IMAGE                  COMMAND                  CREATED          STATUS          PORTS                                  NAMES</span>
<span class="c">#    5b7fa2f98703   kindest/node:v1.31.0   &amp;quot;/usr/local/bin/entr…&amp;quot;   15 minutes ago   Up 15 minutes   0.0.0.0:31000-31001-&amp;gt;31000-31001/tcp   myk8s-worker</span>
<span class="c">#    ...</span>

<span class="nv">$ </span>kubectl get deploy,svc,ep deploy-websrv
<span class="c"># =&gt; ...</span>
<span class="c">#    NAME                    TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE</span>
<span class="c">#    service/deploy-websrv   NodePort   10.96.115.233   &amp;lt;none&amp;gt;        80:31001/TCP   47s</span>
<span class="c">#    ...</span>

<span class="c"># 자신의 PC에 호스트 포트 31001 접속 시 쿠버네티스 서비스에 접속 확인</span>
<span class="nv">$ </span>open http://localhost:31001
<span class="nv">$ </span>curl <span class="nt">-s</span> localhost:31001 | <span class="nb">grep</span> <span class="nt">-o</span> <span class="s2">"&lt;title&gt;.*&lt;/title&gt;"</span>
<span class="c"># =&gt; &lt;title&gt;Welcome to nginx!&lt;/title&gt;</span>

<span class="c"># 디플로이먼트와 서비스 삭제</span>
<span class="nv">$ </span>kubectl delete deploy,svc deploy-websrv
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w2/20240907_kans_w2_6.png" alt="img.png" class="image-center" />
<em class="image-caption">31001 포트로 접속시 nginx 페이지가 나오는 것을 확인할 수 있습니다.</em></p>

<p><img src="/assets/2024/kans-3th/w2/20240907_kans_w2_7.png" alt="img_1.png" />
<em class="image-caption">kube-ops-view에서 deploy된 정보를 확인할 수 있습니다.</em></p>

<hr />

<h2 id="파드--pause-컨테이너">파드 &amp; PAUSE 컨테이너</h2>

<p><strong>파드(pod)</strong>는 쿠버네티스에서 <strong>배포하는 최소 단위</strong>이며, 파드 내부에는 <strong>여러 컨테이너가 포함될 수</strong> 있습니다. 
파드 내부에는 PAUSE 컨테이너가 존재하며, <strong>PAUSE 컨테이너</strong>는 <strong>Network/IPC/UTS 네임스페이스</strong>를 <strong>생성</strong>하고 <strong>유지</strong>/<strong>공유</strong>하는 역할울 합니다. 
네임스페이스와 네트워크 등에 대해서는 1주차에서 학습한 내용이 많이 도움되었습니다. <a href="https://sweetlittlebird.github.io/posts/2024-08-27-KANS-Study-Week1/">1주차 링크</a></p>

<h3 id="k8s-cri-container-runtime-interface">K8S CRI (Container Runtime Interface)</h3>

<p>먼저 파드와 PAUSE 컨테이너에 대해 알아보기전에 앞선 실습에서 보았던 CRI(Container Runtime Interface)에 대해 알아보겠습니다.
쿠버네티스는 컨테이너를 관리하기 위해 <strong>CRI(Container Runtime Interface)</strong>를 사용합니다.</p>

<p>CRI의 탄생 배경은 먼저 Docker에서 부터 찾아볼 수 있습니다.
Docker가 대성공하고 컨테이너 기술이 확산되면서, 쿠버네티스도 Docker를 기본 컨테이너 런타임으로 사용했습니다.
하지만 Docker Inc라는 회사에 종속되는것을 우려하여, 표준화된 인터페이스를 만들어서 다양한 컨테이너 런타임을 지원하고자 했습니다.
그래서 CRI가 탄생하게 되었습니다. CRI라는 표준 인터페이스만 지키면 어떤 컨테이너 런타임이라도 쿠버네티스에서 사용할 수 있게 되었습니다.
이 과정에서 아쉬운건 Docker는 CRI를 지원하지 않았기 때문에, Docker를 사용하는 경우에는 Docker shim이라는 프록시를 사용해야 했었습니다.</p>

<h3 id="파드-pod">파드 (Pod)</h3>

<p>컨테이너 애플리케이션의 기본 단위를 파드(Pod)라고 부르며, 파드는 1개 이상의 컨테이너로 구성된 컨테이너의 집합입니다.</p>

<p><img src="/assets/2024/kans-3th/w2/20240907_kans_w2_8.png" alt="img.png" /></p>

<ul>
  <li>Pod는 1개 이상의 컨테이너를 가질 수 있습니다.</li>
  <li>Pod내에 실행되는 컨테이너들은 동일한 노드에 할당되며 동일한 생명 주기(Life-cycle)를 갖습니다.</li>
  <li>Pod는 노드 IP 와 별개로 클러스터 내에서 접근 가능한 IP를 할당 받으며, 다른 노드에 위치한 Pod 도 <strong>CNI를 통해</strong> NAT 없이 Pod IP로 접근 가능합니다.</li>
  <li>Pod내에 있는 컨테이너들은 서로 IP를 공유합니다. 같은 Pod내의 컨테이너끼리는 localhost 통해 서로 접근가능 합니다.
    <ul>
      <li><strong>pause</strong> 컨테이너가 network ns 를 만들어 주고, 내부의 컨테이너들은 해당 net ns 를 공유하기 때문에 IP를 공유하게 됩니다.</li>
    </ul>
  </li>
  <li>Pod 안의 컨테이너들은 동일한 볼륨과 연결이 가능하여 파일 시스템을 기반으로 서로 파일을 주고받을 수 있습니다.</li>
  <li>Pod는 리소스 제약이 있는 격리된 환경의 애플리케이션 컨테이너 그룹으로 구성됩니다.</li>
  <li>포드를 시작하기 전에 kubelet은 RuntimeService.RunPodSandbox를 호출하여 환경을 만듭니다.</li>
  <li>Kubelet은 RPC를 통해 컨테이너의 수명 주기를 관리하고, 컨테이너 수명 주기 후크와 활성/준비 확인을 실행하며, Pod의 재시작 정책을 준수합니다</li>
</ul>

<h3 id="pause-컨테이너">PAUSE 컨테이너</h3>

<ul>
  <li>쿠버네티스에서 <strong>pause</strong> 컨테이너는 포드의 모든 컨테이너에 대한 “<strong>부모 컨테이너</strong>” 역할을 합니다. - <a href="https://sklar.rocks/what-is-a-pod-sandbox/">Link</a></li>
  <li><strong>pause</strong> 컨테이너에는 두 가지 핵심 책임이 있습니다.
    <ol>
      <li>파드에서 Linux <strong>네임스페이스 공유의 기반</strong> 역할을 합니다. (Network, IPC, UTS 네임스페이스)</li>
      <li>PID(프로세스 ID) 네임스페이스 공유가 활성화되면 각 포드에 대한 <strong>PID 1 역할</strong>을 하며 <strong>좀비 프로세스를 거둡</strong>니다.</li>
    </ol>
  </li>
  <li>pause의 핵심 소스코드는 <a href="https://github.com/kubernetes/kubernetes/blob/master/build/pause/linux/pause.c">여기</a>에서 확인할 수 있습니다. 
매우 짧지만 중요한 코드입니다. 상세하게 코드분석한 분이 있어서 자세히 알고 싶으신 분은 다음 링크를 참고하세요.
<a href="https://mateon.tistory.com/127">한글 링크</a>
<a href="https://www.ianlewis.org/en/almighty-pause-container">영문 링크</a></li>
</ul>

<h4 id="pause-컨테이너-실습">Pause 컨테이너 실습</h4>

<ul>
  <li>Pause 컨테이너의 동작에 대해 실습하기 위한 환경을 구성해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># '컨트롤플레인, 워커 노드 1대' 클러스터 배포 : 파드에 접속하기 위한 포트 맵핑 설정</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOT</span><span class="sh">&gt; kind-2node.yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
- role: worker
  extraPortMappings:
  - containerPort: 30000
    hostPort: 30000
  - containerPort: 30001
    hostPort: 30001
</span><span class="no">EOT
</span><span class="nv">$ </span>kind create cluster <span class="nt">--config</span> kind-2node.yaml <span class="nt">--name</span> myk8s

<span class="c"># 툴 설치</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane sh <span class="nt">-c</span> <span class="s1">'apt update &amp;&amp; apt install tree jq psmisc lsof wget bridge-utils tcpdump htop git nano -y'</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker        sh <span class="nt">-c</span> <span class="s1">'apt update &amp;&amp; apt install tree jq psmisc lsof wget bridge-utils tcpdump htop -y'</span>

<span class="c"># 확인</span>
<span class="nv">$ </span>kubectl get nodes <span class="nt">-o</span> wide
<span class="nv">$ </span>docker ps
<span class="nv">$ </span>docker port myk8s-worker
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane ip <span class="nt">-br</span> <span class="nt">-c</span> <span class="nt">-4</span> addr
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker  ip <span class="nt">-br</span> <span class="nt">-c</span> <span class="nt">-4</span> addr

<span class="c"># kube-ops-view</span>
<span class="nv">$ </span>helm repo add geek-cookbook https://geek-cookbook.github.io/charts/
<span class="nv">$ </span>helm <span class="nb">install </span>kube-ops-view geek-cookbook/kube-ops-view <span class="nt">--version</span> 1.2.2 <span class="nt">--set</span> service.main.type<span class="o">=</span>NodePort,service.main.ports.http.nodePort<span class="o">=</span>30000 <span class="nt">--set</span> env.TZ<span class="o">=</span><span class="s2">"Asia/Seoul"</span> <span class="nt">--namespace</span> kube-system

<span class="c"># 설치 확인</span>
<span class="nv">$ </span>kubectl get deploy,pod,svc,ep <span class="nt">-n</span> kube-system <span class="nt">-l</span> app.kubernetes.io/instance<span class="o">=</span>kube-ops-view

<span class="c"># kube-ops-view 접속 URL 확인 (1.5 , 2 배율)</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"KUBE-OPS-VIEW URL = http://localhost:30000/#scale=1.5"</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"KUBE-OPS-VIEW URL = http://localhost:30000/#scale=2"</span>
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w2/20240907_kans_w2_9.png" alt="img.png" /></p>

<ul>
  <li>worker 노드에 진입 후 네임스페이스 격리를 확인해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># [터미널1] myk8s-worker bash 진입 후 실행 및 확인</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker bash
<span class="nt">----------------------------------</span>
<span class="nv">$ </span>systemctl list-unit-files | <span class="nb">grep</span> <span class="s1">'enabled         enabled'</span>
<span class="c"># =&gt; containerd.service                                                                    enabled         enabled</span>
<span class="c">#    kubelet.service                                                                       enabled         enabled</span>
<span class="c">#    ...</span>

<span class="c"># 확인 : kubelet에 --container-runtime-endpoint=unix:///run/containerd/containerd.sock</span>
<span class="nv">$ </span>pstree <span class="nt">-aln</span>
<span class="c"># =&gt; systemd</span>
<span class="c">#      |-systemd-journal</span>
<span class="c">#      |-containerd</span>
<span class="c">#      |   `-15*[{containerd}]</span>
<span class="c">#      |-kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///run/containerd/containerd.sock --node-ip=172.20.0.3 --node-labels= --pod-infra-container-image=registry.k8s.io/pause:3.10 --provider-id=kind://docker/myk8s/myk8s-worker --runtime-cgroups=/system.slice/containerd.service</span>
<span class="c">#      |   `-13*[{kubelet}]</span>
<span class="c">#      |-containerd-shim -namespace k8s.io -id 3368a087d8af3e241201257993e178d6a7d8ea23d3148cdf2ec5392f9db49832 -address /run/containerd/containerd.sock</span>
<span class="c">#      |   |-11*[{containerd-shim}]</span>
<span class="c">#      |   |-pause</span>
<span class="c">#      |   `-kindnetd</span>
<span class="c">#      |       `-11*[{kindnetd}]</span>
<span class="c">#      |-containerd-shim -namespace k8s.io -id e983e9fcff0162dec6128e014ae9092454fe0fd748ba237320aec17fa85fb17b -address /run/containerd/containerd.sock</span>
<span class="c">#      |   |-11*[{containerd-shim}]</span>
<span class="c">#      |   |-pause</span>
<span class="c">#      |   `-kube-proxy --config=/var/lib/kube-proxy/config.conf --hostname-override=myk8s-worker</span>
<span class="c">#      |       `-8*[{kube-proxy}]</span>
<span class="c">#      `-containerd-shim -namespace k8s.io -id 9cdabafac520e373ff0efdbbbe8b87bdfd7a0d02bd897863b609eefc4ae21b36 -address /run/containerd/containerd.sock</span>
<span class="c">#          |-12*[{containerd-shim}]</span>
<span class="c">#          |-pause</span>
<span class="c">#          `-python3 /usr/local/bin/python3 -m kube_ops_view</span>
<span class="c">#              `-2*[{python3}]</span>
          
<span class="c"># 확인 : 파드내에 pause 컨테이너와 kube_ops_view 컨테이너, 네임스페이스 정보</span>
<span class="nv">$ </span>pstree <span class="nt">-aclnpsS</span>
<span class="c"># =&gt; ...</span>
<span class="c">#      `-containerd-shim,1089 -namespace k8s.io -id 9cdabafac520e373ff0efdbbbe8b87bdfd7a0d02bd897863b609eefc4ae21b36 -address /run/containerd/contai</span>
<span class="c">#    nerd.sock</span>
<span class="c">#          |-{containerd-shim},1090</span>
<span class="c">#          ...</span>
<span class="c">#          |-pause,1110,ipc,mnt,net,pid,uts</span>
<span class="c">#          |-python3,1173,cgroup,ipc,mnt,net,pid,uts /usr/local/bin/python3 -m kube_ops_view</span>
<span class="c">#          ...</span>
<span class="c">#    ...</span>
      
<span class="c"># 네임스페이스 확인 : lsns - List system namespaces</span>
<span class="nv">$ </span>lsns <span class="nt">-p</span> 1
<span class="nv">$ </span>lsns <span class="nt">-p</span> <span class="nv">$$</span>
<span class="c"># =&gt;         NS TYPE   NPROCS PID USER COMMAND</span>
<span class="c">#    4026531834 time       15   1 root /sbin/init</span>
<span class="c">#    4026531837 user       15   1 root /sbin/init</span>
<span class="c">#    4026532329 mnt         9   1 root /sbin/init</span>
<span class="c">#    4026532330 uts        13   1 root /sbin/init</span>
<span class="c">#    4026532338 ipc         9   1 root /sbin/init</span>
<span class="c">#    4026532339 pid         9   1 root /sbin/init</span>
<span class="c">#    4026532341 net        13   1 root /sbin/init</span>
<span class="c">#    4026532417 cgroup     13   1 root /sbin/init</span>

<span class="c"># 파드의 pause 컨테이너는 노드의 NS와 다른 5개의 NS를 가짐 : mnt/pid 는 pasue 자신만 사용, net/uts/ipc는 app 컨테이너를 위해서 먼저 생성해둠</span>
<span class="nv">$ </span>lsns <span class="nt">-p</span> 1797  <span class="c"># kube_ops_view 파드의 pause 컨테이너</span>
<span class="c"># =&gt;         NS TYPE   NPROCS   PID USER  COMMAND</span>
<span class="c">#    4026531834 time       15     1 root  /sbin/init</span>
<span class="c">#    4026531837 user       15     1 root  /sbin/init</span>
<span class="c">#    4026532417 cgroup     13     1 root  /sbin/init </span>
<span class="c">#    4026532805 net         2  1110 65535 /pause      # Node NS와 다름  </span>
<span class="c">#    4026532883 mnt         1  1110 65535 /pause      # Node NS와 다름</span>
<span class="c">#    4026532884 uts         2  1110 65535 /pause      # Node NS와 다름</span>
<span class="c">#    4026532885 ipc         2  1110 65535 /pause      # Node NS와 다름</span>
<span class="c">#    4026532886 pid         1  1110 65535 /pause      # Node NS와 다름</span>

<span class="c"># app 컨테이너(kube_ops_view)는 호스트NS와 다른 6개의 NS를 가짐 : mnt/pid/cgroup 는 자신만 사용, net/uts/ipc는 pause 컨테이너가 생성한 것을 공유 사용함</span>
<span class="nv">$ </span>pgrep <span class="nt">-f</span> kube_ops_view
<span class="c"># =&gt; 1173</span>
<span class="nv">$ </span>lsns <span class="nt">-p</span> <span class="si">$(</span>pgrep <span class="nt">-f</span> kube_ops_view<span class="si">)</span>
<span class="c"># =&gt;         NS TYPE   NPROCS   PID USER  COMMAND</span>
<span class="c">#    4026531834 time       15     1 root  /sbin/init</span>
<span class="c">#    4026531837 user       15     1 root  /sbin/init</span>
<span class="c">#    4026532805 net         2  1110 65535 /pause      # pause 컨테이너와 공유</span>
<span class="c">#    4026532884 uts         2  1110 65535 /pause      # pause 컨테이너와 공유</span>
<span class="c">#    4026532885 ipc         2  1110 65535 /pause      # pause 컨테이너와 공유</span>
<span class="c">#    4026532887 mnt         1  1173 1000  /usr/bin/qemu-x86_64 /usr/local/bin/python3 -m kube_ops_view  # 자신만 사용</span>
<span class="c">#    4026532888 pid         1  1173 1000  /usr/bin/qemu-x86_64 /usr/local/bin/python3 -m kube_ops_view  # 자신만 사용</span>
<span class="c">#    4026532889 cgroup      1  1173 1000  /usr/bin/qemu-x86_64 /usr/local/bin/python3 -m kube_ops_view  # 자신만 사용</span>
</code></pre></div></div>

<ul>
  <li>위와 같이 파드 내부의 pause 컨테이너와 kube_ops_view 컨테이너는 net, uts, ipc 네임스페이스를 공유하고, mnt, pid, cgroup 네임스페이스는 각각의 컨테이너가 사용하는 것을 확인할 수 있습니다.</li>
  <li>이렇게 pause 컨테이너가 파드 내부의 컨테이너들이 공유하는 네임스페이스를 생성하고 유지하는 역할을 하는것을 확인 할 수 있었습니다.</li>
  <li>마지막으로 DIND를 위한 containerd.sock 과 cgroup2fs, sys, proc 등의 정보를 확인해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># containerd.sock 정보 확인 (docker.sock과 비슷한 역할)</span>
<span class="nv">$ </span><span class="nb">ls</span> <span class="nt">-l</span> /run/containerd/containerd.sock
<span class="c"># =&gt; srw-rw---- 1 root root 0 Sep  7 15:20 /run/containerd/containerd.sock</span>

<span class="c"># 특정 소켓 파일을 사용하는 프로세스 확인</span>
<span class="nv">$ </span>lsof /run/containerd/containerd.sock
<span class="c"># =&gt; COMMAND   PID USER   FD   TYPE             DEVICE SIZE/OFF   NODE NAME</span>
<span class="c">#    container 106 root    9u  unix 0x0000000000000000      0t0 726698 /run/containerd/containerd.sock type=STREAM (LISTEN)</span>
<span class="c">#    container 106 root   11u  unix 0x0000000000000000      0t0 734691 /run/containerd/containerd.sock type=STREAM (CONNECTED)</span>
<span class="c">#    container 106 root   12u  unix 0x0000000000000000      0t0 738482 /run/containerd/containerd.sock type=STREAM (CONNECTED)</span>
<span class="c">#    container 106 root   14u  unix 0x0000000000000000      0t0 736712 /run/containerd/containerd.sock type=STREAM (CONNECTED)</span>

<span class="c"># /sys 디렉터리 확인</span>
<span class="nv">$ </span>findmnt <span class="nt">-A</span>
<span class="c"># =&gt; TARGET                                                  SOURCE                 FSTYPE    OPTIONS</span>
<span class="c">#    /                                                       overlay                overlay   rw,relatime,lowerdir=/var/lib/docker/overlay2/l/5UMBJJ</span>
<span class="c">#    ...</span>
<span class="c">#    |-/sys                                                  sysfs                  sysfs     ro,nosuid,nodev,noexec,relatime</span>
<span class="c">#    | |-/sys/kernel/tracing                                 tracefs                tracefs   rw,nosuid,nodev,noexec,relatime</span>
<span class="c">#    | |-/sys/kernel/debug                                   debugfs                debugfs   rw,nosuid,nodev,noexec,relatime</span>
<span class="c">#    | |-/sys/fs/fuse/connections                            fusectl                fusectl   rw,nosuid,nodev,noexec,relatime</span>
<span class="c">#    | |-/sys/kernel/config                                  configfs               configfs  rw,nosuid,nodev,noexec,relatime</span>
<span class="c">#    | `-/sys/fs/cgroup                                      cgroup                 cgroup2   rw,nosuid,nodev,noexec,relatime</span>
<span class="c">#    ...</span>

<span class="c"># cgroup 정보 확인</span>
<span class="nv">$ </span>findmnt <span class="nt">-t</span> cgroup2
<span class="c"># =&gt; TARGET         SOURCE FSTYPE  OPTIONS</span>
<span class="c">#    /sys/fs/cgroup cgroup cgroup2 rw,nosuid,nodev,noexec,relatime</span>
<span class="nv">$ </span><span class="nb">grep </span>cgroup /proc/filesystems
<span class="c"># =&gt; nodev	cgroup</span>
<span class="c">#    nodev	cgroup2</span>
<span class="nv">$ </span><span class="nb">stat</span> <span class="nt">-fc</span> %T /sys/fs/cgroup/
<span class="c"># =&gt; cgroup2fs</span>

<span class="nv">$ </span><span class="nb">exit</span>
<span class="nt">----------------------------------</span>
</code></pre></div></div>

<ul>
  <li>신규파드를 배포하고 확인해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># [터미널2] kubectl 명령 실행 및 확인</span>

<span class="c"># Pod 생성 : YAML 파일에 컨테이너가 사용할 포트(TCP 80)을 설정</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: myweb
spec:
  containers:
  - image: nginx:alpine
    name: myweb-container
    ports:
    - containerPort: 80
      protocol: TCP
  terminationGracePeriodSeconds: 0
</span><span class="no">EOF

</span><span class="c"># Pod 정보 확인 : pause 컨테이너 정보가 보이는지 확인</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-o</span> wide
<span class="c"># =&gt; NAME    READY   STATUS    RESTARTS   AGE   IP           NODE           NOMINATED NODE   READINESS GATES</span>
<span class="c">#    myweb   1/1     Running   0          15s   10.244.1.3   myk8s-worker   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="nv">$ </span>kubectl describe pod myweb | <span class="nb">grep</span> <span class="nt">-i</span> pause
<span class="nv">$ </span>kubectl get pod myweb <span class="nt">-o</span> json | <span class="nb">grep</span> <span class="nt">-i</span> pause

<span class="nt">---</span>

<span class="c"># [터미널1] myk8s-worker bash 진입 후 실행 및 확인</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker bash
<span class="nt">----------------------------------</span>
<span class="nv">$ </span>crictl ps
<span class="nv">$ </span>pstree <span class="nt">-aln</span>
<span class="nv">$ </span>pstree <span class="nt">-aclnpsS</span> <span class="c"># 파드내에 pause 컨테이너와 app 컨테이너, 네임스페이스 정보</span>
<span class="c"># =&gt;   `-containerd-shim,1673 -namespace k8s.io -id 482ba93ecf76d46938d60e58f363e63e681d8a779439f270ff9c8417ad25a641 -address /run/containerd/containerd.sock</span>
<span class="c">#          |-{containerd-shim},1674</span>
<span class="c">#          ...</span>
<span class="c">#          |-pause,1693,ipc,mnt,net,pid,uts</span>
<span class="c">#          |-nginx,1754,cgroup,ipc,mnt,net,pid,uts</span>

<span class="c"># 네임스페이스 확인 : lsns - List system namespaces</span>
<span class="nv">$ </span>lsns <span class="nt">-p</span> 1
<span class="nv">$ </span>lsns <span class="nt">-p</span> <span class="nv">$$</span>
<span class="c"># =&gt;         NS TYPE   NPROCS PID USER COMMAND</span>
<span class="c">#    4026531834 time       25   1 root /sbin/init</span>
<span class="c">#    4026531837 user       25   1 root /sbin/init</span>
<span class="c">#    4026532329 mnt        10   1 root /sbin/init</span>
<span class="c">#    4026532330 uts        14   1 root /sbin/init</span>
<span class="c">#    4026532338 ipc        10   1 root /sbin/init</span>
<span class="c">#    4026532339 pid        10   1 root /sbin/init</span>
<span class="c">#    4026532341 net        14   1 root /sbin/init</span>
<span class="c">#    4026532417 cgroup     15   1 root /sbin/init</span>
<span class="nv">$ </span>lsns <span class="nt">-p</span> 1693 
<span class="c"># =&gt;         NS TYPE   NPROCS   PID USER  COMMAND</span>
<span class="c">#    4026531834 time       25     1 root  /sbin/init</span>
<span class="c">#    4026531837 user       25     1 root  /sbin/init</span>
<span class="c">#    4026532417 cgroup     15     1 root  /sbin/init</span>
<span class="c">#    4026532891 net         9  1693 65535 /pause</span>
<span class="c">#    4026532969 mnt         1  1693 65535 /pause</span>
<span class="c">#    4026532970 uts         9  1693 65535 /pause</span>
<span class="c">#    4026532971 ipc         9  1693 65535 /pause</span>
<span class="c">#    4026532972 pid         1  1693 65535 /pause</span>
<span class="nv">$ </span>lsns <span class="nt">-p</span> <span class="si">$(</span>pgrep <span class="nt">-n</span> nginx<span class="si">)</span> <span class="c"># app 컨테이너(nginx)</span>
<span class="c"># =&gt;         NS TYPE   NPROCS   PID USER  COMMAND</span>
<span class="c">#    4026531834 time       25     1 root  /sbin/init</span>
<span class="c">#    4026531837 user       25     1 root  /sbin/init</span>
<span class="c">#    4026532891 net         9  1693 65535 /pause</span>
<span class="c">#    4026532970 uts         9  1693 65535 /pause</span>
<span class="c">#    4026532971 ipc         9  1693 65535 /pause</span>
<span class="c">#    4026532973 mnt         8  1754 root  nginx: master process nginx -g daemon off;</span>
<span class="c">#    4026532974 pid         8  1754 root  nginx: master process nginx -g daemon off;</span>
<span class="c">#    4026532975 cgroup      8  1754 root  nginx: master process nginx -g daemon off;</span>
<span class="nt">----------------------------------</span>
<span class="c"># [터미널2] kubectl 명령 실행 및 확인</span>
<span class="nv">$ </span>kubectl delete pod myweb
</code></pre></div></div>

<ul>
  <li>위와 같이 <code class="language-plaintext highlighter-rouge">kubectl</code>과 같은 high-level에서는 <code class="language-plaintext highlighter-rouge">pause</code> 컨테이너가 보이지 않지만, <code class="language-plaintext highlighter-rouge">ps</code>나 <code class="language-plaintext highlighter-rouge">lsns</code>와 같은 OS에 접근 가능한 명령으로는 
<code class="language-plaintext highlighter-rouge">pause</code> 컨테이너가 확인이 되었습니다.</li>
  <li>이는 pause 컨테이너는 공기와 같이 항상 파드에 존재하기 때문에 굳이 보여줘서 화면만 복잡하게 할 뿐 보여줄 필요가 없다 판단한것 같습니다.</li>
  <li>PAUSE Container가 ns를 공유하는 특성을 이용하여,
애플리케이션과 런타임 의존성만 포함하는 최소화된 이미지기반 컨테이너인 Distroless Container를 대상으로
Ephemeral Containers와 같은 형태로 디버깅에 활용할 수 있습니다. <a href="https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/">Link</a></li>
</ul>

<h2 id="cni-container-network-interface">CNI (Container Network Interface)</h2>

<ul>
  <li>쿠버네티스는 CNI(Container Network Interface)를 사용하여 네트워크를 관리합니다. 
CNI는 컨테이너 런타임과 네트워크 플러그인을 연결하는 인터페이스로 네트워크 인터페이스를 생성하고, IP 주소를 할당하며,
네트워크 정책을 적용하는 역할을 수행합니다.</li>
  <li>CNI의 주요 기능
    <ul>
      <li><strong>네트워크 인터페이스 생성</strong>: CNI 플러그인은 컨테이너에 네트워크 인터페이스(예: 가상 이더넷 장치)를 생성하고 컨테이너 네임스페이스에 이를 연결합니다.</li>
      <li><strong>IP 주소 할당</strong>: 플러그인은 네트워크 인터페이스에 IP 주소를 할당하고, 필요에 따라 IP 주소 관리를 처리합니다.</li>
      <li><strong>네트워크 정책 적용</strong>: 네트워크 정책을 통해 트래픽을 제어할 수 있으며, CNI 플러그인은 이를 구현하는 데 사용됩니다.</li>
      <li><strong>다양한 네트워크 모드 지원</strong>: 다양한 네트워크 토폴로지와 요구 사항을 지원하기 위해 여러 네트워크 모드(예: 브리지, VLAN, 오버레이 네트워크 등)를 지원합니다.</li>
    </ul>
  </li>
  <li>CNI의 작동 방식
    <ul>
      <li>CNI는 기본적으로 플러그인 기반 구조를 따릅니다. 오케스트레이션 도구는 특정 이벤트가 발생할 때 CNI 플러그인을 호출하여 필요한 네트워크 설정을 수행합니다. 각 CNI 플러그인은 JSON 형식의 구성 파일로 정의되며, 이를 통해 플러그인의 동작을 제어할 수 있습니다.</li>
    </ul>
  </li>
  <li>CNI 플러그인의 종류 : CNI 플러그인은 다양한 종류가 있으며, 각기 다른 네트워킹 요구 사항을 충족시킵니다. 대표적인 CNI 플러그인은 다음과 같습니다.
    <ul>
      <li><strong>Flannel</strong>: 간단하고 사용하기 쉬운 오버레이 네트워크 플러그인입니다.</li>
      <li><strong>Calico</strong>: 네트워크 정책과 보안을 강조하는 플러그인으로, 네트워크 격리 및 정책 적용에 강점이 있습니다.</li>
      <li><strong>Weave</strong>: 자동 메쉬 네트워크와 서비스 디스커버리를 제공하는 플러그인입니다.</li>
      <li><strong>Cilium</strong>: 고성능 BPF 기반 네트워킹 및 보안을 제공하는 플러그인입니다.</li>
      <li><strong>Multus</strong>: 여러 CNI 플러그인을 사용하여 컨테이너에 여러 네트워크 인터페이스를 지원하는 플러그인입니다.</li>
    </ul>
  </li>
</ul>

<h3 id="4가지-요구사항과-4가지-문제">4가지 요구사항과 4가지 문제</h3>

<p>쿠버네티스의 네트워크 모델은 4가지 요구사항을 만족해아하며 4가지 문제를 해결해야 합니다.</p>

<ul>
  <li>4가지 요구사항
    <ol>
      <li>파드와 파드 간 통신 시 NAT(Network Address Translation) 없이 통신이 가능해야 합니다.</li>
      <li>노드의 에이전트(예) kubelet, 시스템 데몬)는 Pod와 통신이 가능해야 합니다.</li>
      <li>호스트 네트워크를 사용하는 파드는 NAT 없이 파드와 통신이 가능해야 합니다.</li>
      <li>서비스 클러스터 IP 대역과 파드가 사용하는 IP 대역은 중복되지 않아야 합니다.</li>
    </ol>
  </li>
  <li>해결해야 하는 문제
    <ol>
      <li>파드 내 컨테이너는 Loopback을 통한 통신을 할 수 있도록 해야 합니다.</li>
      <li>파드 간 통신을 할 수 있어야 합니다.</li>
      <li>클러스터 내부에서 Service를 통한 통신을 할 수 있어야 합니다.</li>
      <li>클러스터 외부에서 Service를 통한 통신을 할 수 있어야 합니다.</li>
    </ol>
  </li>
</ul>

<p>위와 같은 요구사항과 문제를 해결하고 원활한 네트워크 통신을 위해 CNI(Container Network Interface)를 정의했습니다.
CNI 플러그인들은 이러한 요구사항들을 기반으로 만들어졌습니다.</p>

<p><img src="/assets/2024/kans-3th/w2/20240907_kans_w2_10.png" alt="img.png" class="image-center" />
<em class="image-caption">CNI 플러그인 동작 (출처: 추가예정)</em></p>

<p>Kubelet을 통해 파드가 신규 생성될 때 네트워크 관련 설정 추가 필요합니다. 
CNI 플러그인은 전달되는 설정 정의서를 보고 실제 파드가 통신하기 위한 네트워크 설정들을 실행하게 됩니다.
또한 CNI 플러그인은 IPAM(IP Address Management), 
즉 IP 할당 관리를 수행해야 하며, 파드 간 통신을 위한 라우팅 설정을 처리해야 합니다.</p>

<h2 id="flannel">Flannel</h2>

<ul>
  <li>Flannel은 쿠버네티스의 네트워크 요구사항을 충족하는 가장 간단하고 사용하기 쉬운 오버레이 네트워크 플러그인입니다.</li>
  <li>Flannel은 가상 네트워크를 생성하여 파드 간 통신을 가능하게 하며, VXLAN, UDP, Host-GW 등의 백엔드를 지원합니다.
하지만 VXLAN 사용이 권장됩니다.</li>
  <li>VXLAN은 Virtual eXtensible Local Area Network의 약자로, 물리적인 네트워크 환경에서  논리적인 가상의 네트워크 환경을 만들어 주는 것으로, UDP 8472  포트를 통해 노드 간 터널링 기법으로 통신하는 기술입니다. </li>
</ul>

<p><img src="/assets/2024/kans-3th/w2/20240907_kans_w2_11.png" alt="img.png" class="image-center" />
<em class="image-caption">Flannel 구조 (출처: 추가예정)</em></p>

<ul>
  <li>위의 그림과 같이 파드의 eth0 네트워크 인터페이스는 호스트 네임스페이스의 veth 인터페이스와 연결되고,
veth는 cni0와 연결됩니다.</li>
  <li>이를 통해 같은 노드에서 통신시 cni0 브릿지를 통해서 통신하고, 다른 노드와 통신시 VXLAN을 통해 통신합니다.</li>
  <li>VXLAN으로 가는 과정은 cni0 브릿지를 통해 flannel.1 인터페이스로 가고, flannel.1은 호스트의 eth0을 통해 다른 노드에 전송을 합니다.
이때 <strong>flannel.1은 VTEP(Vxlan Tunnel End Point)</strong>라고 하며 패킷을 감싸서 목표 node의 IP로 전송하면, 목표 node에서 감싼 패킷을 풀어서 
해당 파드의 IP로 다시 보내는 역할을 수행합니다.</li>
  <li>각 노드마다 파드에 할당할 수 있는 IP 네트워크 대역이 있고, flannel을 통하여 ETCD나 Kubernetes API에 전달되어, 모든 노드는 해당 정보를 자신의 라우팅 테이블에 업데이트합니다.
이를 통해 각각 다른 노드의 파드끼리도 내부 IP 주소를 통해 통신이 가능하게 됩니다.</li>
</ul>

<h3 id="kind-와-flannel-설치">Kind 와 Flannel 설치</h3>

<ul>
  <li>Kind 클러스터에 Flannel을 설치해보겠습니다. kind는 기본 CNI로 kindnet을 사용하는데 실습을 위해 kindnet을 끄고 클러스터를 구축하겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">&gt; kind-cni.yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  labels:
    mynode: control-plane
  extraPortMappings:
  - containerPort: 30000
    hostPort: 30000
  - containerPort: 30001
    hostPort: 30001
  - containerPort: 30002
    hostPort: 30002
  kubeadmConfigPatches:
  - |
    kind: ClusterConfiguration
    controllerManager:
      extraArgs:
        bind-address: 0.0.0.0
    etcd:
      local:
        extraArgs:
          listen-metrics-urls: http://0.0.0.0:2381
    scheduler:
      extraArgs:
        bind-address: 0.0.0.0
  - |
    kind: KubeProxyConfiguration
    metricsBindAddress: 0.0.0.0
- role: worker
  labels:
    mynode: worker
- role: worker
  labels:
    mynode: worker2
networking:
  disableDefaultCNI: true
</span><span class="no">EOF
</span><span class="nv">$ </span>kind create cluster <span class="nt">--config</span> kind-cni.yaml <span class="nt">--name</span> myk8s <span class="nt">--image</span> kindest/node:v1.30.4

<span class="c"># 배포 확인</span>
<span class="nv">$ </span>kind get clusters
<span class="nv">$ </span>kind get nodes <span class="nt">--name</span> myk8s
<span class="nv">$ </span>kubectl cluster-info

<span class="c"># 네트워크 확인</span>
<span class="nv">$ </span>kubectl cluster-info dump | <span class="nb">grep</span> <span class="nt">-m</span> 2 <span class="nt">-E</span> <span class="s2">"cluster-cidr|service-cluster-ip-range"</span>

<span class="c"># 노드 확인 : CRI</span>
<span class="nv">$ </span>kubectl get nodes <span class="nt">-o</span> wide

<span class="c"># 노드 라벨 확인</span>
<span class="nv">$ </span>kubectl get nodes myk8s-control-plane <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.metadata.labels<span class="o">}</span> | jq
...
<span class="s2">"mynode"</span>: <span class="s2">"control-plane"</span>,
...

<span class="nv">$ </span>kubectl get nodes myk8s-worker <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.metadata.labels<span class="o">}</span> | jq
<span class="nv">$ </span>kubectl get nodes myk8s-worker2 <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.metadata.labels<span class="o">}</span> | jq

<span class="c"># 컨테이너 확인 : 컨테이너 갯수, 컨테이너 이름 확인</span>
<span class="nv">$ </span>docker ps
<span class="nv">$ </span>docker port myk8s-control-plane
<span class="nv">$ </span>docker port myk8s-worker
<span class="nv">$ </span>docker port myk8s-worker2

<span class="c"># 컨테이너 내부 정보 확인</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane ip <span class="nt">-br</span> <span class="nt">-c</span> <span class="nt">-4</span> addr
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker  ip <span class="nt">-br</span> <span class="nt">-c</span> <span class="nt">-4</span> addr
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker2  ip <span class="nt">-br</span> <span class="nt">-c</span> <span class="nt">-4</span> addr

<span class="c">#</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane sh <span class="nt">-c</span> <span class="s1">'apt update &amp;&amp; apt install tree jq psmisc lsof wget bridge-utils tcpdump iputils-ping htop git nano -y'</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker  sh <span class="nt">-c</span> <span class="s1">'apt update &amp;&amp; apt install tree jq psmisc lsof wget bridge-utils tcpdump iputils-ping -y'</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker2 sh <span class="nt">-c</span> <span class="s1">'apt update &amp;&amp; apt install tree jq psmisc lsof wget bridge-utils tcpdump iputils-ping -y'</span>
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w2/20240907_kans_w2_12.png" alt="img.png" /></p>

<ul>
  <li>다음 명령을 통해 bridge 실행파일을 생성해서 각 인스턴스마다 배포해야합니다. 먼저 다음과 같이 bridge 파일을 생헝 후 로컬에 복사하겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane bash
<span class="nt">---------------------------------------</span>
<span class="c"># 빌드환경 구성</span>
<span class="nv">$ </span>apt update <span class="o">&amp;&amp;</span> apt <span class="nb">install </span>golang git <span class="nt">-y</span>
<span class="nv">$ </span>git clone https://github.com/containernetworking/plugins
<span class="nv">$ </span><span class="nb">cd </span>plugins
<span class="nv">$ </span><span class="nb">chmod</span> +x build_linux.sh

<span class="c"># 빌드</span>
<span class="nv">$ </span>./build_linux.sh

<span class="c"># 파일 권한 확인 755</span>
<span class="nv">$ </span><span class="nb">ls</span> <span class="nt">-l</span> bin
<span class="c"># =&gt; -rwxr-xr-x 1 root root  4471145 Sep  7 16:53 bridge</span>
<span class="c">#    ...</span>

<span class="nv">$ </span><span class="nb">exit</span>
<span class="nt">---------------------------------------</span>

<span class="c"># 자신의 PC에 복사 : -a 권한 보존하여 복사(755)</span>
<span class="nv">$ </span>docker <span class="nb">cp</span> <span class="nt">-a</span> myk8s-control-plane:/plugins/bin/bridge <span class="nb">.</span>
<span class="nv">$ </span><span class="nb">ls</span> <span class="nt">-l</span> bridge
<span class="c"># =&gt; .rwxr-xr-x root staff 4.3 MB Sun Sep  0 00:00:00 2024 bridge</span>
</code></pre></div></div>

<ul>
  <li>이제 Flannel을 설치하겠습니다. 현재 기본 CNI 인 kindnet 없이 설치했기 때문에 node가 not ready 상태여서 스케쥴링이 안 되고 있습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>watch <span class="nt">-d</span> kubectl get pod <span class="nt">-A</span> <span class="nt">-owide</span>

<span class="c">#</span>
<span class="nv">$ </span>kubectl describe pod <span class="nt">-n</span> kube-system <span class="nt">-l</span> k8s-app<span class="o">=</span>kube-dns | <span class="nb">grep </span>Events: <span class="nt">-A</span> 6
<span class="c"># =&gt; Events:</span>
<span class="c">#      Type     Reason            Age                  From               Message</span>
<span class="c">#      ----     ------            ----                 ----               -------</span>
<span class="c">#      Warning  FailedScheduling  19m                  default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.</span>

<span class="c"># 기본 CNI 인 kindnet 없이 설치했기 때문에 node가 not ready 상태여서 스케쥴링이 안 되고 있습니다.</span>

<span class="c"># Flannel cni 설치</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml
<span class="c"># =&gt; namespace/kube-flannel created</span>
<span class="c">#    clusterrole.rbac.authorization.k8s.io/flannel created</span>
<span class="c">#    clusterrolebinding.rbac.authorization.k8s.io/flannel created</span>
<span class="c">#    serviceaccount/flannel created</span>
<span class="c">#    configmap/kube-flannel-cfg created</span>
<span class="c">#    daemonset.apps/kube-flannel-ds created</span>

<span class="c"># namespace 에 pod-security.kubernetes.io/enforce=privileged Label 확인 </span>
<span class="nv">$ </span>kubectl get ns <span class="nt">--show-labels</span>
<span class="c"># =&gt; ...</span>
<span class="c">#    kube-flannel         Active   60s   k8s-app=flannel,kubernetes.io/metadata.name=kube-flannel,pod-security.kubernetes.io/enforce=privileged</span>
<span class="nv">$ </span>kubectl get ds,pod,cm <span class="nt">-n</span> kube-flannel <span class="nt">-owide</span>
<span class="c"># =&gt; NAME                             DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE    CONTAINERS     IMAGES                              SELECTOR</span>
<span class="c">#    daemonset.apps/kube-flannel-ds   3         3         3       3            3           &amp;lt;none&amp;gt;          100s   kube-flannel   docker.io/flannel/flannel:v0.25.6   app=flannel</span>
<span class="c">#    </span>
<span class="c">#    NAME                        READY   STATUS    RESTARTS   AGE    IP           NODE                  NOMINATED NODE   READINESS GATES</span>
<span class="c">#    pod/kube-flannel-ds-2l9p6   1/1     Running   0          100s   172.20.0.2   myk8s-worker2         &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    pod/kube-flannel-ds-67ftf   1/1     Running   0          100s   172.20.0.3   myk8s-worker          &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    pod/kube-flannel-ds-87wv8   1/1     Running   0          100s   172.20.0.4   myk8s-control-plane   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    </span>
<span class="c">#    NAME                         DATA   AGE</span>
<span class="c">#    configmap/kube-flannel-cfg   2      100s</span>
<span class="c">#    configmap/kube-root-ca.crt   1      100s</span>

<span class="c"># kube-flannel-ds가 daemonset으로 노드마다 실행중인것을 확인할 수 있습니다.</span>

<span class="nv">$ </span>kubectl describe cm <span class="nt">-n</span> kube-flannel kube-flannel-cfg

<span class="nv">$ </span>kubectl describe ds <span class="nt">-n</span> kube-flannel kube-flannel-ds

<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> ds/kube-flannel-ds <span class="nt">-n</span> kube-flannel <span class="nt">-c</span> kube-flannel <span class="nt">--</span> <span class="nb">ls</span> <span class="nt">-l</span> /etc/kube-flannel


<span class="c"># failed to find plugin "bridge" in path [/opt/cni/bin]</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-A</span> <span class="nt">-owide</span>
<span class="nv">$ </span>kubectl describe pod <span class="nt">-n</span> kube-system <span class="nt">-l</span> k8s-app<span class="o">=</span>kube-dns
<span class="c"># =&gt; Warning  FailedCreatePodSandBox  2m16s                   kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox "b0023dea7d730b58cfea0163318641ff1d000d368f8ad3b552c53040b371388c": plugin type="flannel" failed (add): failed to delegate add: failed to find plugin "bridge" in path [/opt/cni/bin]</span>
<span class="c">#    Warning  FailedCreatePodSandBox  2m15s                   kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox "0454cbf9ae3c434f163865f6cd261ef2fa298a2de107471600195ebffa0b44cc": plugin type="flannel" failed (add): failed to delegate add: failed to find plugin "bridge" in path [/opt/cni/bin]</span>
</code></pre></div></div>

<ul>
  <li>현재 /opt/cni/bin/bridge 파일이 없어서 오류가 발생하고 있습니다. 이를 해결하기 위해 bridge 파일을 복사하겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># bridge 파일 복사</span>
<span class="nv">$ </span>docker <span class="nb">cp </span>bridge myk8s-control-plane:/opt/cni/bin/bridge
<span class="nv">$ </span>docker <span class="nb">cp </span>bridge myk8s-worker:/opt/cni/bin/bridge
<span class="nv">$ </span>docker <span class="nb">cp </span>bridge myk8s-worker2:/opt/cni/bin/bridge

<span class="c"># 권한 부여</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane  <span class="nb">chmod </span>755 /opt/cni/bin/bridge
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker         <span class="nb">chmod </span>755 /opt/cni/bin/bridge
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker2        <span class="nb">chmod </span>755 /opt/cni/bin/bridge
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane  <span class="nb">chown </span>root:root /opt/cni/bin/bridge
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker         <span class="nb">chown </span>root:root /opt/cni/bin/bridge
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker2        <span class="nb">chown </span>root:root /opt/cni/bin/bridge

<span class="c"># bridge 파일이 잘 복사되었는지 확인합니다.</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane  <span class="nb">ls</span> <span class="nt">-l</span> /opt/cni/bin/
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker  <span class="nb">ls</span> <span class="nt">-l</span> /opt/cni/bin/
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker2 <span class="nb">ls</span> <span class="nt">-l</span> /opt/cni/bin/
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>myk8s-control-plane myk8s-worker myk8s-worker2<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> <span class="nv">$i</span> <span class="nb">ls</span> /opt/cni/bin/<span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done
</span>bridge	flannel  host-local  loopback  portmap	ptp

<span class="c">#</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-A</span> <span class="nt">-owide</span>
<span class="c"># =&gt; NAMESPACE            NAME                                          READY   STATUS    RESTARTS   AGE     IP           NODE                  NOMINATED NODE   READINESS GATES</span>
<span class="c">#    ...</span>
<span class="c">#    kube-system          coredns-7db6d8ff4d-hjmjf                      1/1     Running   0          29m     10.244.1.4   myk8s-worker2         &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    kube-system          coredns-7db6d8ff4d-lfmzj                      1/1     Running   0          29m     10.244.1.3   myk8s-worker2         &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    ...</span>
</code></pre></div></div>

<h3 id="flannel-설치-확인">Flannel 설치 확인</h3>

<ul>
  <li>Flannel 설치 후 coredns가 정상적으로 배포되었습니다. 이제 Flannel이 정상적으로 설치되었는지 확인해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#</span>
<span class="nv">$ </span>kubectl get ds,pod,cm <span class="nt">-n</span> kube-flannel <span class="nt">-owide</span>
<span class="c"># =&gt; &lt;span style="font-weight:bold;"&gt;NAME                             DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE   CONTAINERS     IMAGES                              SELECTOR&lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:gray;"&gt;daemonset.apps/kube-flannel-ds&lt;/span&gt;   &lt;span style="color:teal;"&gt;3&lt;/span&gt;         &lt;span style="color:gray;"&gt;3&lt;/span&gt;         &lt;span style="color:teal;"&gt;3&lt;/span&gt;       &lt;span style="color:gray;"&gt;3&lt;/span&gt;            &lt;span style="color:teal;"&gt;3&lt;/span&gt;           &lt;span style="color:gray;"&gt;&amp;lt;none&amp;gt;&lt;/span&gt;          &lt;span style="color:teal;"&gt;13m&lt;/span&gt;   &lt;span style="color:gray;"&gt;kube-flannel&lt;/span&gt;   &lt;span style="color:teal;"&gt;docker.io/flannel/flannel:v0.25.6&lt;/span&gt;   &lt;span style="color:gray;"&gt;app=flannel&lt;/span&gt;</span>
<span class="c">#    </span>
<span class="c">#    &lt;span style="font-weight:bold;"&gt;NAME                        READY   STATUS    RESTARTS   AGE   IP           NODE                  NOMINATED NODE   READINESS GATES&lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:gray;"&gt;pod/kube-flannel-ds-2l9p6&lt;/span&gt;   &lt;span style="color:teal;"&gt;1/1&lt;/span&gt;     &lt;span style="color:green;"&gt;Running&lt;/span&gt;   &lt;span style="color:teal;"&gt;0&lt;/span&gt;          &lt;span style="color:gray;"&gt;13m&lt;/span&gt;   &lt;span style="color:teal;"&gt;172.20.0.2&lt;/span&gt;   &lt;span style="color:gray;"&gt;myk8s-worker2&lt;/span&gt;         &lt;span style="color:teal;"&gt;&amp;lt;none&amp;gt;&lt;/span&gt;           &lt;span style="color:gray;"&gt;&amp;lt;none&amp;gt;&lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:gray;"&gt;pod/kube-flannel-ds-67ftf&lt;/span&gt;   &lt;span style="color:teal;"&gt;1/1&lt;/span&gt;     &lt;span style="color:green;"&gt;Running&lt;/span&gt;   &lt;span style="color:teal;"&gt;0&lt;/span&gt;          &lt;span style="color:gray;"&gt;13m&lt;/span&gt;   &lt;span style="color:teal;"&gt;172.20.0.3&lt;/span&gt;   &lt;span style="color:gray;"&gt;myk8s-worker&lt;/span&gt;          &lt;span style="color:teal;"&gt;&amp;lt;none&amp;gt;&lt;/span&gt;           &lt;span style="color:gray;"&gt;&amp;lt;none&amp;gt;&lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:gray;"&gt;pod/kube-flannel-ds-87wv8&lt;/span&gt;   &lt;span style="color:teal;"&gt;1/1&lt;/span&gt;     &lt;span style="color:green;"&gt;Running&lt;/span&gt;   &lt;span style="color:teal;"&gt;0&lt;/span&gt;          &lt;span style="color:gray;"&gt;13m&lt;/span&gt;   &lt;span style="color:teal;"&gt;172.20.0.4&lt;/span&gt;   &lt;span style="color:gray;"&gt;myk8s-control-plane&lt;/span&gt;   &lt;span style="color:teal;"&gt;&amp;lt;none&amp;gt;&lt;/span&gt;           &lt;span style="color:gray;"&gt;&amp;lt;none&amp;gt;&lt;/span&gt;</span>
<span class="c">#    </span>
<span class="c">#    &lt;span style="font-weight:bold;"&gt;NAME                         DATA   AGE&lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:gray;"&gt;configmap/kube-flannel-cfg&lt;/span&gt;   &lt;span style="color:teal;"&gt;2&lt;/span&gt;      &lt;span style="color:gray;"&gt;13m&lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:gray;"&gt;configmap/kube-root-ca.crt&lt;/span&gt;   &lt;span style="color:teal;"&gt;1&lt;/span&gt;      &lt;span style="color:gray;"&gt;13m&lt;/span&gt;</span>

<span class="nv">$ </span>kubectl describe cm <span class="nt">-n</span> kube-flannel kube-flannel-cfg

<span class="c"># iptables 정보 확인</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>filter nat mangle raw <span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; IPTables Type : </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane  iptables <span class="nt">-t</span> <span class="nv">$i</span> <span class="nt">-S</span> <span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>filter nat mangle raw <span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; IPTables Type : </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker  iptables <span class="nt">-t</span> <span class="nv">$i</span> <span class="nt">-S</span> <span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>filter nat mangle raw <span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; IPTables Type : </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker2 iptables <span class="nt">-t</span> <span class="nv">$i</span> <span class="nt">-S</span> <span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>

<span class="c"># flannel 정보 확인 : 대역, MTU</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>myk8s-control-plane myk8s-worker myk8s-worker2<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> <span class="nv">$i</span> <span class="nb">cat</span> /run/flannel/subnet.env <span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; &gt;&gt; node myk8s-control-plane &lt;&lt;</span>
<span class="c">#    FLANNEL_NETWORK=10.244.0.0/16</span>
<span class="c">#    FLANNEL_SUBNET=10.244.0.1/24</span>
<span class="c">#    FLANNEL_MTU=1450</span>
<span class="c">#    FLANNEL_IPMASQ=true</span>
<span class="c">#    ...</span>

<span class="c"># 노드마다 할당된 dedicated subnet (podCIDR) 확인</span>
<span class="nv">$ </span>kubectl get nodes <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.items[*].spec.podCIDR}'</span> <span class="p">;</span><span class="nb">echo</span>
<span class="c"># =&gt; 10.244.0.0/24 10.244.2.0/24 10.244.1.0/24</span>

<span class="c"># 노드 정보 중 flannel 관련 정보 확인 : VXLAN 모드 정보와, VTEP 정보(노드 IP, VtepMac) 를 확인</span>
<span class="nv">$ </span>kubectl describe node | <span class="nb">grep</span> <span class="nt">-A3</span> Annotations
<span class="c"># =&gt; Annotations:        flannel.alpha.coreos.com/backend-data: {&amp;quot;VNI&amp;quot;:1,&amp;quot;VtepMAC&amp;quot;:&amp;quot;6e:0e:72:1e:72:1d&amp;quot;}</span>
<span class="c">#                        flannel.alpha.coreos.com/backend-type: vxlan</span>
<span class="c">#                        flannel.alpha.coreos.com/kube-subnet-manager: true</span>
<span class="c">#                        flannel.alpha.coreos.com/public-ip: 172.20.0.4</span>
<span class="c">#    --</span>
<span class="c">#    Annotations:        flannel.alpha.coreos.com/backend-data: {&amp;quot;VNI&amp;quot;:1,&amp;quot;VtepMAC&amp;quot;:&amp;quot;ca:de:a3:b8:65:d8&amp;quot;}</span>
<span class="c">#                        flannel.alpha.coreos.com/backend-type: vxlan</span>
<span class="c">#                        flannel.alpha.coreos.com/kube-subnet-manager: true</span>
<span class="c">#                        flannel.alpha.coreos.com/public-ip: 172.20.0.3</span>
<span class="c">#    --</span>
<span class="c">#    Annotations:        flannel.alpha.coreos.com/backend-data: {&amp;quot;VNI&amp;quot;:1,&amp;quot;VtepMAC&amp;quot;:&amp;quot;1e:1e:b1:15:9e:d3&amp;quot;}</span>
<span class="c">#                        flannel.alpha.coreos.com/backend-type: vxlan</span>
<span class="c">#                        flannel.alpha.coreos.com/kube-subnet-manager: true</span>
<span class="c">#                        flannel.alpha.coreos.com/public-ip: 172.20.0.2</span>

<span class="c"># 각 노드(?) 마다 bash 진입 후 아래 기본 정보 확인 : 먼저 worker 부터 bash 진입 후 확인하자</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker        bash
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker2       bash
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane bash
<span class="nt">----------------------------------------</span>
<span class="c"># 호스트 네트워크 NS와 flannel, kube-proxy 컨테이너의 네트워크 NS 비교 =&gt; 모두 동일한 NS를 가집니다. </span>
<span class="nv">$ </span>lsns <span class="nt">-p</span> 1
<span class="c"># =&gt;         NS TYPE   NPROCS PID USER COMMAND</span>
<span class="c">#    ...</span>
<span class="c">#    4026532344 net        12   1 root /sbin/init</span>
<span class="nv">$ </span>lsns <span class="nt">-p</span> <span class="si">$(</span>pgrep flanneld<span class="si">)</span>
<span class="c"># =&gt;         NS TYPE   NPROCS   PID USER  COMMAND</span>
<span class="c">#    ...</span>
<span class="c">#    4026532344 net        12     1 root  /sbin/init</span>
<span class="nv">$ </span>lsns <span class="nt">-p</span> <span class="si">$(</span>pgrep kube-proxy<span class="si">)</span>
<span class="c"># =&gt;         NS TYPE   NPROCS   PID USER  COMMAND</span>
<span class="c">#    ...</span>
<span class="c">#    4026532344 net        12     1 root  /sbin/init</span>

<span class="c"># 기본 네트워크 정보 확인</span>
<span class="nv">$ </span>ip <span class="nt">-c</span> <span class="nt">-br</span> addr
<span class="c"># =&gt; lo               UNKNOWN        127.0.0.1/8 ::1/128</span>
<span class="c">#    flannel.1        UNKNOWN        10.244.2.0/32</span>
<span class="c">#    eth0@if37        UP             172.20.0.3/16 fc00:f853:ccd:e793::3/64 fe80::42:acff:fe14:3/64</span>
<span class="nv">$ </span>ip <span class="nt">-c</span> <span class="nb">link</span> | <span class="nb">grep</span> <span class="nt">-E</span> <span class="s1">'flannel|cni|veth'</span> <span class="nt">-A1</span>
<span class="c"># =&gt; 4: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default</span>
<span class="c">#       link/ether ca:de:a3:b8:65:d8 brd ff:ff:ff:ff:ff:ff</span>
<span class="nv">$ </span>ip <span class="nt">-c</span> addr
<span class="nv">$ </span>ip <span class="nt">-c</span> <span class="nt">-d</span> addr show cni0     <span class="c"># 네트워크 네임스페이스 격리 파드가 1개 이상 배치 시 확인됨</span>
<span class="c"># =&gt; (공백)</span>

<span class="c"># 현재 네트워크 네임스페이스에 격리된 파드가 없어서 cni0 인터페이스가 없습니다.</span>

<span class="nv">$ </span>ip <span class="nt">-c</span> <span class="nt">-d</span> addr show flannel.1
<span class="c"># =&gt; 4: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN group default</span>
<span class="c">#        link/ether ca:de:a3:b8:65:d8 brd ff:ff:ff:ff:ff:ff promiscuity 0 minmtu 68 maxmtu 65535</span>
<span class="c">#        vxlan id 1 local 172.20.0.3 dev eth0 srcport 0 0 dstport 8472 nolearning ttl auto ageing 300 udpcsum noudp6zerocsumtx noudp6zerocsumrx numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535</span>
<span class="c">#        inet 10.244.2.0/32 scope global flannel.1</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
    
<span class="nv">$ </span>brctl show
<span class="c"># =&gt; (공백)</span>

<span class="c"># 라우팅 정보 확인 : 다른 노드의 파드 대역(podCIDR)의 라우팅 정보가 업데이트되어 있음을 확인		</span>
<span class="nv">$ </span>ip <span class="nt">-c</span> route
<span class="c"># =&gt; default via 172.20.0.1 dev eth0</span>
<span class="c">#    10.244.0.0/24 via 10.244.0.0 dev flannel.1 onlink</span>
<span class="c">#    10.244.1.0/24 via 10.244.1.0 dev flannel.1 onlink</span>
<span class="c">#    172.20.0.0/16 dev eth0 proto kernel scope link src 172.20.0.3</span>

<span class="c"># flannel.1 인터페이스를 통한 ARP 테이블 정보 확인 : 다른 노드의 flannel.1 IP와 MAC 정보를 확인</span>
<span class="nv">$ </span>ip <span class="nt">-c</span> neigh show dev flannel.1
<span class="c"># =&gt; 10.244.1.0 lladdr 1e:1e:b1:15:9e:d3 PERMANENT</span>
<span class="c">#    10.244.0.0 lladdr 6e:0e:72:1e:72:1d PERMANENT</span>

<span class="c"># 브리지 fdb 정보에서 해당 MAC 주소와 통신 시 각 노드의 enp0s8 </span>
<span class="nv">$ </span>bridge fdb show dev flannel.1
<span class="c"># =&gt; 6e:0e:72:1e:72:1d dst 172.20.0.4 self permanent</span>
<span class="c">#    1e:1e:b1:15:9e:d3 dst 172.20.0.2 self permanent</span>

<span class="c"># 다른 노드의 flannel.1 인터페이스로 ping 통신 : VXLAN 오버레이를 통해서 통신</span>
<span class="nv">$ </span>ping <span class="nt">-c</span> 1 10.244.0.0
<span class="c"># =&gt; ...</span>
<span class="c">#    1 packets transmitted, 1 received, 0% packet loss, time 0ms</span>
<span class="nv">$ </span>ping <span class="nt">-c</span> 1 10.244.1.0
<span class="c"># =&gt; ...</span>
<span class="c">#    1 packets transmitted, 1 received, 0% packet loss, time 0ms</span>
<span class="nv">$ </span>ping <span class="nt">-c</span> 1 10.244.2.0
<span class="c"># =&gt; ...</span>
<span class="c">#    1 packets transmitted, 1 received, 0% packet loss, time 0ms</span>

<span class="c"># 다른 노드와 VXLAN을 통해서 잘 통신 됩니다.</span>

<span class="c"># iptables 필터 테이블 정보 확인 : 파드의 10.244.0.0/16 대역 끼리는 모든 노드에서 전달이 가능</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> filter <span class="nt">-S</span> | <span class="nb">grep </span>10.244.0.0
<span class="c"># =&gt; -A FLANNEL-FWD -s 10.244.0.0/16 -m comment --comment "flanneld forward" -j ACCEPT</span>
<span class="c">#    -A FLANNEL-FWD -d 10.244.0.0/16 -m comment --comment "flanneld forward" -j ACCEPT</span>

<span class="c"># iptables NAT 테이블 정보 확인 : 10.244.0.0/16 대역 끼리 통신은 마스커레이딩 없이 통신을 하며,</span>
<span class="c"># 10.244.0.0/16 대역에서 동일 대역(10.244.0.0/16)과 멀티캐스트 대역(224.0.0.0/4) 를 제외한 나머지 (외부) 통신 시에는 마스커레이딩을 수행</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span> | <span class="nb">grep</span> <span class="s1">'flanneld masq'</span> | <span class="nb">grep</span> <span class="nt">-v</span> <span class="s1">'! -s'</span>
<span class="c"># =&gt; -A POSTROUTING -m comment --comment "flanneld masq" -j FLANNEL-POSTRTG</span>
<span class="c">#    -A FLANNEL-POSTRTG -m mark --mark 0x4000/0x4000 -m comment --comment "flanneld masq" -j RETURN</span>
<span class="c">#    -A FLANNEL-POSTRTG -s 10.244.2.0/24 -d 10.244.0.0/16 -m comment --comment "flanneld masq" -j RETURN</span>
<span class="c">#    -A FLANNEL-POSTRTG -s 10.244.0.0/16 -d 10.244.2.0/24 -m comment --comment "flanneld masq" -j RETURN</span>
<span class="c">#    -A FLANNEL-POSTRTG -s 10.244.0.0/16 ! -d 224.0.0.0/4 -m comment --comment "flanneld masq" -j MASQUERADE --random-fully</span>

<span class="nt">----------------------------------------</span>
</code></pre></div></div>

<ul>
  <li>파드 2개를 생성해서 CNI 네트워크 브리지의 정보를  확인해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># [터미널1,2] 워커 노드1,2 - 모니터링</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker  bash
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker2 bash
<span class="nt">-----------------------------</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s2">"ip link | egrep 'cni|veth' ;echo; brctl show cni0"</span>
<span class="nt">-----------------------------</span>

<span class="c"># [터미널3] cat &amp; here document 명령 조합으로 즉석(?) 리소스 생성</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl create -f -
apiVersion: v1
kind: Pod
metadata:
  name: pod-1
  labels:
    app: pod
spec:
  nodeSelector:
    kubernetes.io/hostname: myk8s-worker
  containers:
  - name: netshoot-pod
    image: nicolaka/netshoot
    command: ["tail"]
    args: ["-f", "/dev/null"]
  terminationGracePeriodSeconds: 0
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-2
  labels:
    app: pod
spec:
  nodeSelector:
    kubernetes.io/hostname: myk8s-worker2
  containers:
  - name: netshoot-pod
    image: nicolaka/netshoot
    command: ["tail"]
    args: ["-f", "/dev/null"]
  terminationGracePeriodSeconds: 0
</span><span class="no">EOF

</span><span class="c"># 파드 확인 : IP 확인</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-o</span> wide
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w2/20240907_kans_w2_13.png" alt="img.png" /></p>

<ul>
  <li>CNI 가 없었는데 파드 패포후 CNI 네트워크 브리지가 생성되는 것을 확인할 수 있습니다. 정보를 확인해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker  bash
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker2 bash
<span class="nt">-----------------------------</span>
<span class="c"># 브리지 정보 확인</span>
<span class="nv">$ </span>brctl show cni0

<span class="c"># 브리지 연결 링크(veth) 확인</span>
<span class="nv">$ </span>bridge <span class="nb">link</span>

<span class="c"># 브리지 VLAN 정보 확인</span>
<span class="nv">$ </span>bridge vlan

<span class="c"># cbr(custom bridge) 정보 : kubenet CNI의 bridge - 링크</span>
<span class="nv">$ </span>tree /var/lib/cni/networks/cbr0

<span class="c"># 네트워크 관련 정보들 확인</span>
<span class="nv">$ </span>ip <span class="nt">-c</span> addr | <span class="nb">grep </span>veth <span class="nt">-A3</span>
<span class="nt">-----------------------------</span>
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w2/20240907_kans_w2_14.png" alt="img.png" /></p>

<h3 id="통신-흐름-이해">통신 흐름 이해</h3>

<ul>
  <li>Flannel 기반으로 네트워크를 구축할 때는 다음과 같이 세가지의 통신 시나리오가 생길 수 있습니다.
    <ol>
      <li>동일 노드에서 파드간 통신하는 경우
  <img src="/assets/2024/kans-3th/w2/20240907_kans_w2_15.png" alt="img.png" class="image-center" /></li>
      <li>파드에서 외부와 통신하는 경우
  <img src="/assets/2024/kans-3th/w2/20240907_kans_w2_16.png" alt="img_1.png" class="image-center" /></li>
      <li>서로다른 노드에서 파드간 통신하는 경우
  <img src="/assets/2024/kans-3th/w2/20240907_kans_w2_17.png" alt="img_2.png" class="image-center" /></li>
    </ol>
  </li>
</ul>

<p>파드 간 통신, 서로다른 노드의 파드간 통신, 외부 통신 여부에 대해 살펴보고, 통신이 일어나는 상황의 패킷을 캡처하면서 Flannel network에 대해 이해해 보았습니다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> pod-1 <span class="nt">--</span> zsh
<span class="nt">-----------------------------</span>
<span class="nv">$ </span>ip <span class="nt">-c</span> addr show eth0

<span class="c"># GW IP는 어떤 인터페이스인가? =&gt; cni0</span>
<span class="nv">$ </span>ip <span class="nt">-c</span> route
<span class="nv">$ </span>route <span class="nt">-n</span>
<span class="nv">$ </span>ping <span class="nt">-c</span> 1 &lt;GW IP&gt;
<span class="nv">$ </span>ping <span class="nt">-c</span> 1 &lt;pod-2 IP&gt;  <span class="c"># 다른 노드에 배포된 파드 통신 확인</span>
<span class="nv">$ </span>ping <span class="nt">-c</span> 1 8.8.8.8     <span class="c"># 외부 인터넷 IP   접속 확인</span>
<span class="nv">$ </span>curl <span class="nt">-s</span> wttr.in/Seoul <span class="c"># 외부 인터넷 도메인 접속 확인</span>
<span class="nv">$ </span>ip <span class="nt">-c</span> neigh
<span class="nv">$ </span><span class="nb">exit</span>
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w2/20240907_kans_w2_18.png" alt="img.png" /></p>

<ul>
  <li>서로다른 노드간 파드의 통신, 외부와의 통신 등이 모두 잘 통신 되는것을 확인할 수 있습니다.</li>
  <li>이번에는 각 노드의 cni0에서 패킷 캡쳐를 진행해 보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># [터미널1,2] 워커 노드1,2</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker  bash
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker2 bash
<span class="nt">-----------------------------</span>
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> cni0 <span class="nt">-nn</span> icmp
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> flannel.1 <span class="nt">-nn</span> icmp
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> eth0 <span class="nt">-nn</span> icmp
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> eth0 <span class="nt">-nn</span> udp port 8472 <span class="nt">-w</span> /root/vxlan.pcap 
<span class="c"># CTRL+C 취소 후 확인 : ls -l /root/vxlan.pcap</span>

<span class="nv">$ </span>conntrack <span class="nt">-L</span> | <span class="nb">grep</span> <span class="nt">-i</span> icmp
<span class="nt">-----------------------------</span>

<span class="c"># [터미널3]</span>
<span class="nv">$ </span>docker <span class="nb">cp </span>myk8s-worker:/root/vxlan.pcap <span class="nb">.</span>
<span class="nv">$ </span>wireshark vxlan.pcap
</code></pre></div></div>

<ul>
  <li>Pod-1 =&gt; Pod-2 (cni0 관점)
    <ul>
      <li>브릿지를 통해 각각 오가는 패킷이 잘 보입니다.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/2024/kans-3th/w2/20240907_kans_w2_19.png" alt="img.png" /></p>

<ul>
  <li>Pod-1 =&gt; 외부 (cni0 관점)
    <ul>
      <li>같은 노드에서는 패킷이 외부로 나갔다 오는것이 잘 보입니다.</li>
      <li>하지만 다른 노드 (worker2)에서는 외부와 오가는 패킷이 보이지 않습니다.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/2024/kans-3th/w2/20240907_kans_w2_20.png" alt="img.png" /></p>

<ul>
  <li>Pod-1 =&gt; Pod-2 (flannel.1 관점)
    <ul>
      <li>flannel.1을 통해 각각 오가는 패킷이 잘 보입니다.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/2024/kans-3th/w2/20240907_kans_w2_21.png" alt="img.png" /></p>

<ul>
  <li>Pod-1 -&gt; 외부 (flannel.1 관점)
    <ul>
      <li>앞선 그림에서 보았듯 외부와 통신시 VTEP인 flannel.1을 거치지 않고 cni0에서 호스트의 eth0로 바로 나가기 때문에 패킷이 캡쳐되지 않습니다.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/2024/kans-3th/w2/20240907_kans_w2_22.png" alt="img.png" /></p>

<ul>
  <li>Pod 1 -&gt; Pod 2 (eth0 관점)
    <ul>
      <li>eth0에서 봤을때는 캡쳐가 되지 않습니다. 이유는 flannel.1을 통해 tcp 패킷이 캡슐화 되어 udp 8472로 eth0를 통해 전달되기 때문에
<code class="language-plaintext highlighter-rouge">-nn icmp</code> 옵션으로 icmp 패킷 (ping)만 캡쳐할때는 보이지 않습니다.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/2024/kans-3th/w2/20240907_kans_w2_23.png" alt="img.png" /></p>

<ul>
  <li>Pod 1 -&gt; Pod 2 (eth0 관점, udp port 8472 덤프)
    <ul>
      <li>udp 패킷을 캡쳐하여 wireshark로 확인해보겠습니다.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/2024/kans-3th/w2/20240907_kans_w2_25.png" alt="img.png" /></p>

<ul>
  <li>udp 8472 포트를 통해 icmp (ping)이 오가는 것을 확인할 수 있습니다. (8472는 VXLAN 포트가 아니기 때문에 옵션에서 VXLAN을 지정해야 보입니다.)</li>
</ul>

<p><img src="/assets/2024/kans-3th/w2/20240907_kans_w2_26.png" alt="img_1.png" /></p>

<ul>
  <li>Pod 1 -&gt; 8.8.8.8(외부) (eth0 관점)
    <ul>
      <li>eth0에서 외부와 통신하는 패킷이 같은 노드에서는 보이고, 다른 노드에서는 안 보입니다.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/2024/kans-3th/w2/20240907_kans_w2_24.png" alt="img.png" /></p>

<hr />

<h2 id="마치며">마치며</h2>

<p>이번주에도 많은 내용들을 스터디해보았습니다. 
KIND를 알게되어서 참 좋았던것 같습니다. 
기존에 사내 교육을 위해서 kubernetes를 설치하려면 갖은 어려움이 있었는데
docker만 있으면 간단하게 설치가 가능하다는 것이 참 좋은것 같습니다.</p>

<p>막연하게 알고 있었던 노드간의 파드의 통신에 대해서 알게되었고,
쿠버네티스의 운영중 네트워크 장애에 대해 1주차 스터디와 이번 스터디를 통해
자신감이 조금 생겼습니다.
남은 스터디도 열심히해서 쿠버네티스의 네트워크에 대해 조금 아는 사람이 되어보겠습니다.</p>]]></content><author><name></name></author><category term="kans" /><category term="kubernetes," /><category term="network," /><category term="linux" /><summary type="html"><![CDATA[지난주에 이어 이번주에는 쿠버네티스에대해 간략하게 알아보고 KIND, PAUSE 컨테이너와 Flannel CNI에 대해 알아보겠습니다.]]></summary></entry><entry><title type="html">[KANS 3기] 컨테이너 격리</title><link href="https://sweetlittlebird.github.io/posts/2024-08-27-KANS-Study-Week1/" rel="alternate" type="text/html" title="[KANS 3기] 컨테이너 격리" /><published>2024-08-27T22:50:18+09:00</published><updated>2024-08-27T22:50:18+09:00</updated><id>https://sweetlittlebird.github.io/posts/KANS%20Study%20-%20Week1</id><content type="html" xml:base="https://sweetlittlebird.github.io/posts/2024-08-27-KANS-Study-Week1/"><![CDATA[<h2 id="들어가며">들어가며</h2>

<p>지난 테라폼 스터디에 이어 이번 주 부터 KANS 스터디를 시작하게 되었습니다!
KANS는 <strong>K</strong>ubernetes <strong>A</strong>dvanced <strong>N</strong>etworking <strong>S</strong>tudy의 줄임말로 쿠버네티스 네트워킹에 대한 심도있게 공부하는 스터디입니다.
이번 스터디도 과제할 걱정도 :sweat: 되지만 재미있을것 같아 기대됩니다. :smile:</p>

<p>첫 주 스터디도 컨테이너 격리와 리눅스 네트워크에 대해 많은것을 배웠고 이 자리에 정리해보려고 합니다. 
이번 스터디도 다들 완주하기를 기도하며 스터디 정리를 시작해 보겠습니다.</p>

<h2 id="도커-소개">도커 소개</h2>

<h3 id="도커란-무엇인가">도커란 무엇인가?</h3>

<ul>
  <li>
    <p>도커(Docker)는 컨테이너(Container)라고 불리는 가상실행 환경을 제공하고, 
그 가상환경에서 유용한 어플리케이션을 실행할 수 있게 해주는 오픈소스 플랫폼입니다.</p>
  </li>
  <li>
    <p>컨테이너라는 이름의 기원
컨테이너라는 이름은 배에 화물을 실을때 사용하는 그 컨테이너에서 왔습니다.
<img src="/assets/2024/kans-3th/w1/20240831_kans_w1_1.png" alt="컨테이너선" />
과거에 컨테이너가 발명되기 이전에는 짐의 부피와 모양이 제각각이라서, 화물을 적재하기도 어렵고 
파도가 쳐서 배가 흔들릴때 짐이 이리 저리 움직여서 파손되는 경우가 많았습니다.</p>

    <p>이 문제를 해결하기 위해 Malcom McLean이라는 분이 발명한것이 직육면체의 바로 컨테이너입니다.<br />
<img src="/assets/2024/kans-3th/w1/20240831_kans_w1_2.png" alt="Shipping Container" />
직육면체이기 때문에 적재가 쉽고, 파도가 치더라도 안정적으로 화물을 운반할 수 있었습니다. 
또한 크고 작은 물건도 컨테이너 안에 넣어서 운반할 수 있어서 화물의 종류에 상관없이 효율적으로 운반할 수 있었습니다.</p>

    <p>이 개념을 컴퓨팅에 도입한것이 컨테이너입니다. 
기존에는 각 리눅스 버전마다, glibc냐 musl이냐, debian 기반이냐 redhat 기반이냐 등등 
프로그램을 배포할때 환경을 맞춰야 하는것이 많았습니다. 그 뿐만아니라 각종 라이브러리들도 설치해야 하고
심지어 프로그램 마다 필요한 라이브러리 버전이 다를때도 있었습니다.</p>

    <p>이러한 문제를 해결하기위해 도커라는 컨테이너를 이용한 가상화 기술이 등장하게 되었습니다.
도커 컨테이너 이미지에는 프로그램 실행에 필요한 모든것이 포함되어 있기 때문에
마치 컨테이너에 화물을 싣듯이 프로그램을 배포할 수 있게 되었습니다. 
도커의 로고가 컨테이너를 싣고 있는 배를 형상화한것도 이러한 의미에서 나온것입니다.</p>

    <p><img src="/assets/2024/kans-3th/w1/20240831_kans_w1_3.png" alt="Docker Logo" class="image-center" /></p>
  </li>
  <li>
    <p>컨테이너 이외에도 가상 머신(Virtual Machine)이라는 기술이 있습니다. 
가상 머신은 하이퍼바이저(Hypervisor)를 이용하여 호스트 OS 위에 게스트 OS를 올리는 방식으로 가상화를 구현합니다.
가상 머신은 게스트 OS를 올리기 때문에 무겁습니다. 
반면 컨테이너는 호스트 OS의 커널을 공유하기 때문에 가볍습니다.
<img src="/assets/2024/kans-3th/w1/20240831_kans_w1_4.png" alt="img.png" /></p>
  </li>
</ul>

<h3 id="컨테이너와-가상-머신">컨테이너와 가상 머신</h3>

<p><img src="/assets/2024/kans-3th/w1/20240831_kans_w1_5.png" alt="가상머신과 컨테이너 비교" /></p>

<ul>
  <li>가상머신은 호스트 OS 위에 하이퍼바이저를 두고 하드웨어 일부(또는 전부)를 가상화하고, 그 위에 게스트 OS를 올립니다. 즉, <strong>하드웨어 레벨의 가상화</strong>를 지원합니다.</li>
  <li>컨테이너는 하드웨어 가상화와 게스트 OS 없이, 호스트의 리눅스 커널을 공유하여 바로 프로세스를 실행합니다. 단, 각종 라이브러리와 사용자 환경(User Land)는 컨테이너 단위로 패키징되어 <strong>OS 레벨의 가상화</strong>를 지원한다 할 수 있습니다.</li>
  <li>따라서 컨테이너는 가상머신보다 가볍고 빠르며, 낮은 격리(Weak Isolation) 수준을 가집니다.</li>
  <li>가상머신은 게스트 OS를 올리기 때문에 무겁고 느리지만, 높은 격리(Strong Isolation) 수준을 가집니다.
    <ul>
      <li>낮은 격리 수준을 보완하기 위해 리눅스의 pivot-root, namespace, cgroup 등의 기능들을 활용함으로써 프로세스 단위의 격리 환경과 리소스 제어를 제공합니다.
<img src="/assets/2024/kans-3th/w1/20240831_kans_w1_6.png" alt="img.png" class="image-center" /></li>
    </ul>
  </li>
</ul>

<h3 id="도커-아키텍쳐">도커 아키텍쳐</h3>

<p><img src="/assets/2024/kans-3th/w1/20240831_kans_w1_7.png" alt="도커 아키텍쳐" />
<a href="https://docs.docker.com/get-started/overview/#docker-architecture">https://docs.docker.com/get-started/overview/#docker-architecture</a></p>

<hr />

<h2 id="도커-기본-사용">도커 기본 사용</h2>

<h3 id="도커-설치-및-확인">도커 설치 및 확인</h3>

<ul>
  <li>도커 설치
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 방법1. debian 계열 리눅스에서 패키지 매니저로 설치</span>
<span class="nv">$ </span><span class="nb">sudo </span>apt-get update
<span class="nv">$ </span><span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> docker.io
  
<span class="c"># 방법2. 공식 사이트에서 설치</span>
<span class="nv">$ </span><span class="nb">sudo</span> <span class="nt">-i</span>
<span class="nv">$ </span>curl <span class="nt">-fsSL</span> https://get.docker.com | sh
</code></pre></div>    </div>
  </li>
  <li>기본정보 확인
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 도커 정보 확인 : Client 와 Server , Storage Driver(overlay2), Cgroup Version(2), Default Runtime(runc)</span>
<span class="nv">$ </span><span class="nb">sudo </span>docker info
<span class="c">#    Client:</span>
<span class="c">#     Context:    default</span>
<span class="c">#     Debug Mode: false</span>
<span class="c">#    </span>
<span class="c">#    Server:</span>
<span class="c">#     Containers: 0</span>
<span class="c">#     ...</span>
<span class="c">#     Server Version: 20.10.25+dfsg1</span>
<span class="c">#     Storage Driver: overlay2</span>
<span class="c">#     ...</span>
<span class="c">#     Cgroup Driver: systemd</span>
<span class="c">#     Cgroup Version: 2</span>
<span class="c">#     ...</span>
<span class="c">#     containerd version: 1.6.24~ds1-2</span>
<span class="c">#     runc version: 1.1.12+ds1-5</span>
<span class="c">#     ...</span>
  
<span class="nv">$ </span><span class="nb">sudo </span>docker version
<span class="c"># =&gt; Client:</span>
<span class="c">#     Version:           20.10.25+dfsg1</span>
<span class="c">#     API version:       1.41</span>
<span class="c">#     Go version:        go1.22.3</span>
<span class="c">#     Git commit:        b82b9f3</span>
<span class="c">#     Built:             Tue May  7 10:33:18 2024</span>
<span class="c">#     OS/Arch:           linux/amd64</span>
<span class="c">#     Context:           default</span>
<span class="c">#     Experimental:      true</span>
<span class="c">#    </span>
<span class="c">#    Server:</span>
<span class="c">#     Engine:</span>
<span class="c">#      Version:          20.10.25+dfsg1</span>
<span class="c">#      API version:      1.41 (minimum version 1.12)</span>
<span class="c">#      Go version:       go1.22.3</span>
<span class="c">#      Git commit:       5df983c</span>
<span class="c">#      Built:            Tue May  7 10:33:18 2024</span>
<span class="c">#      OS/Arch:          linux/amd64</span>
<span class="c">#      Experimental:     false</span>
<span class="c">#    ...  </span>
  
<span class="c"># 도커 서비스 상태 확인</span>
<span class="nv">$ </span><span class="nb">sudo </span>systemctl status docker <span class="nt">-l</span> <span class="nt">--no-pager</span>
  
<span class="c"># 모든 서비스의 상태 표시</span>
<span class="nv">$ </span>systemctl list-units <span class="nt">--type</span><span class="o">=</span>service
  
<span class="c"># 도커 루트 디렉터리 확인 : Docker Root Dir(/var/lib/docker)</span>
<span class="nv">$ </span><span class="nb">sudo </span>tree <span class="nt">-L</span> 3 /var/lib/docker
<span class="c"># =&gt; /var/lib/docker</span>
<span class="c">#    |-- buildkit</span>
<span class="c">#    |   ...</span>
<span class="c">#    |-- containers</span>
<span class="c">#    |-- image</span>
<span class="c">#    |   `-- overlay2</span>
<span class="c">#    |       |-- distribution</span>
<span class="c">#    |       |-- imagedb</span>
<span class="c">#    |       |-- layerdb</span>
<span class="c">#    |       `-- repositories.json</span>
<span class="c">#    |-- network</span>
<span class="c">#    |   `-- files</span>
<span class="c">#    |       `-- local-kv.db</span>
<span class="c">#    |-- overlay2</span>
<span class="c">#    |   ...</span>
<span class="c">#    `-- volumes</span>
<span class="c">#        |-- backingFsBlockDev</span>
<span class="c">#        `-- metadata.db</span>
<span class="c">#    </span>
<span class="c">#    24 directories, 8 files</span>
</code></pre></div>    </div>
  </li>
  <li>네트워크 정보 확인
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 프로세스 확인 - 셸변수</span>
<span class="nv">$ </span>ps <span class="nt">-ef</span>      <span class="c"># 프로세스 목록 보기</span>
<span class="nv">$ </span>pstree <span class="nt">-p</span>   <span class="c"># 프로세스 트리로 보기</span>
  
<span class="nv">$ </span><span class="nb">df</span> <span class="nt">-hT</span>    <span class="c"># 디스크 사용량 확인</span>

<span class="c"># 네트워크 정보 확인. 도커에서 사용하는 docker0 네트워크가 추가되어있고 현재 DOWN 상태입니다.</span>
<span class="c"># 컨테이너가 있으면 UP 상태로 변경됩니다.  </span>
<span class="nv">$ </span>ip <span class="nt">-br</span> <span class="nt">-c</span> addr
<span class="c"># =&gt; &lt;span style="color:teal;"&gt;lo               &lt;/span&gt;UNKNOWN        &lt;span style="color:purple;"&gt;127.0.0.1&lt;/span&gt;/8 &lt;span style="color:blue;"&gt;::1&lt;/span&gt;/128 </span>
<span class="c">#    &lt;span style="color:teal;"&gt;eth0             &lt;/span&gt;&lt;span style="color:green;"&gt;UP             &lt;/span&gt;&lt;span style="color:purple;"&gt;10.10.10.109&lt;/span&gt;/24 &lt;span style="color:purple;"&gt;10.10.10.51&lt;/span&gt;/24 &lt;span style="color:blue;"&gt;fe80::a70d:8639:be6:671e&lt;/span&gt;/64</span>
<span class="c">#    &lt;span style="color:teal;"&gt;docker0          &lt;/span&gt;&lt;span style="color:red;"&gt;DOWN           &lt;/span&gt;&lt;span style="color:purple;"&gt;172.17.0.1&lt;/span&gt;/16 &lt;span style="color:blue;"&gt;fe80::42:57ff:fe56:997c&lt;/span&gt;/64</span>
<span class="nv">$ </span>ip <span class="nt">-c</span> addr
<span class="nv">$ </span>ip <span class="nt">-c</span> <span class="nb">link</span>
<span class="nv">$ </span>ip <span class="nt">-br</span> <span class="nt">-c</span> <span class="nb">link</span>
<span class="nv">$ </span>ip <span class="nt">-c</span> route
  
<span class="c"># 이더넷 브릿지 정보 확인</span>
<span class="nv">$ </span>brctl show
<span class="c"># =&gt; bridge name	bridge id		STP enabled	interfaces</span>
<span class="c">#    docker0		8000.02425756997c	no</span>
  
<span class="c"># iptables 정책 확인</span>
<span class="c"># FORWARD 정책이 DROP으로 설정되어 있고, </span>
<span class="c"># docker0에서 docker0 혹은 외부로 전달되는 패킷은 허용되어 있습니다.</span>
<span class="nv">$ </span><span class="nb">sudo </span>iptables <span class="nt">-t</span> filter <span class="nt">-S</span>
<span class="c"># =&gt; -P INPUT ACCEPT</span>
<span class="c">#    &lt;span style="color: red;"&gt;-P FORWARD DROP&lt;/span&gt;</span>
<span class="c">#    -P OUTPUT ACCEPT</span>
<span class="c">#    -N DOCKER</span>
<span class="c">#    -N DOCKER-ISOLATION-STAGE-1</span>
<span class="c">#    -N DOCKER-ISOLATION-STAGE-2</span>
<span class="c">#    -N DOCKER-USER</span>
<span class="c">#    -A FORWARD -j DOCKER-USER</span>
<span class="c">#    -A FORWARD -j DOCKER-ISOLATION-STAGE-1</span>
<span class="c">#    -A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT</span>
<span class="c">#    -A FORWARD -o docker0 -j DOCKER</span>
<span class="c">#    &lt;span style="color: red;"&gt;-A FORWARD -i docker0 ! -o docker0 -j ACCEPT&lt;/span&gt;</span>
<span class="c">#    &lt;span style="color: red;"&gt;-A FORWARD -i docker0 -o docker0 -j ACCEPT&lt;/span&gt;</span>
<span class="c">#    -A DOCKER-ISOLATION-STAGE-1 -i docker0 ! -o docker0 -j DOCKER-ISOLATION-STAGE-2</span>
<span class="c">#    -A DOCKER-ISOLATION-STAGE-1 -j RETURN</span>
<span class="c">#    -A DOCKER-ISOLATION-STAGE-2 -o docker0 -j DROP</span>
<span class="c">#    -A DOCKER-ISOLATION-STAGE-2 -j RETURN</span>
<span class="c">#    -A DOCKER-USER -j RETURN</span>

<span class="c"># NAT POSTROUTING에 172.17.0.0/16에서 외부로 전달시 MASQUERADE (SNAT) 정책이 설정되어 있습니다.</span>
<span class="nv">$ </span><span class="nb">sudo </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span>
<span class="c"># =&gt; -P PREROUTING ACCEPT</span>
<span class="c">#    -P INPUT ACCEPT</span>
<span class="c">#    -P OUTPUT ACCEPT</span>
<span class="c">#    -P POSTROUTING ACCEPT</span>
<span class="c">#    -N DOCKER</span>
<span class="c">#    -A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER</span>
<span class="c">#    -A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER</span>
<span class="c">#    &lt;span style="color: red;"&gt;-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE&lt;/span&gt;</span>
<span class="c">#    -A DOCKER -i docker0 -j RETURN</span>
</code></pre></div>    </div>
  </li>
</ul>

<h3 id="도커를-비-root-유저로-관리하기">도커를 비 root 유저로 관리하기</h3>

<p>도커는 기본적으로 root로 관리할 수 있습니다. 
root가 아닌 유저로 docker 명령을 실행하면 다음과 같은 에러가 발생합니다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">whoami</span>
<span class="c"># =&gt; kali</span>

<span class="nv">$ </span>docker info
<span class="c"># =&gt; ...</span>
<span class="c">#    Server:</span>
<span class="c">#    ERROR: permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get &amp;quot;http://%2Fvar%2Frun%2Fdocker.sock/v1.24/info&amp;quot;: dial unix /var/run/docker.sock: connect: permission denied</span>
</code></pre></div></div>

<p>하지만, 다음의 방법 처럼 현재 사용자를 docker 그룹에 추가하면, root가 아닌 일반 유저로도 관리할 수 있습니다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">whoami</span> 
<span class="c"># =&gt; kali</span>

<span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$USER</span>
<span class="c"># =&gt; kali</span>

<span class="c"># 도커 그룹 추가</span>
<span class="nv">$ </span><span class="nb">sudo </span>usermod <span class="nt">-aG</span> docker <span class="nv">$USER</span>

<span class="c"># 그룹 확인</span>
<span class="nv">$ </span><span class="nb">groups</span> 
<span class="c"># =&gt; adm ... kaboxer</span>

<span class="c"># 로그아웃</span>
<span class="nb">exit</span> 

<span class="c"># ssh 재접속 후 확인</span>
<span class="nv">$ </span><span class="nb">groups</span> 
<span class="c"># =&gt; adm ... kaboxer docker</span>

<span class="nv">$ </span>docker info
<span class="c"># =&gt; Client:</span>
<span class="c">#     Context:    default</span>
<span class="c">#     Debug Mode: false</span>
<span class="c">#    </span>
<span class="c">#    Server:</span>
<span class="c">#     Containers: 0</span>
<span class="c">#     ...</span>
<span class="c">#     Cgroup Version: 2</span>
<span class="c">#     ...</span>
<span class="c">#     Default Runtime: runc</span>
<span class="c">#     Init Binary: docker-init</span>
<span class="c">#     containerd version: 1.6.24~ds1-2</span>
<span class="c">#     runc version: 1.1.12+ds1-5</span>
<span class="c">#     ...    </span>

<span class="c"># 컨테이너 확인</span>
<span class="nv">$ </span>docker run <span class="nt">--rm</span> hello-world
<span class="c"># =&gt; Hello from Docker!</span>
<span class="c">#    This message shows that your installation appears to be working correctly.</span>
<span class="c">#    ...</span>

<span class="c"># 실행중인 도커 컨테이너 확인</span>
<span class="nv">$ </span>docker ps
<span class="c"># 전체 도커 컨테이너 확인</span>
<span class="nv">$ </span>docker ps <span class="nt">-a</span>
<span class="c"># 이미지 목록 확인</span>
<span class="nv">$ </span>docker images
<span class="c"># =&gt; REPOSITORY    TAG       IMAGE ID       CREATED         SIZE</span>
<span class="c">#    hello-world   latest    d2c94e258dcb   16 months ago   13.3kB</span>

<span class="c"># 도커 컨테이너 삭제</span>
<span class="nv">$ </span>docker ps <span class="nt">-aq</span>
<span class="nv">$ </span>docker <span class="nb">rm</span> <span class="nt">-f</span> <span class="si">$(</span>docker ps <span class="nt">-aq</span><span class="si">)</span>
<span class="nv">$ </span>docker ps <span class="nt">-a</span>
</code></pre></div></div>

<h3 id="컨테이너가-host의-docker-socket-file-공유로-도커-실행">컨테이너가 host의 docker socket file 공유로 도커 실행</h3>

<ul>
  <li>
    <p>도커 컨테이너를 GUI로 관리할 수 있는 툴인 <a href="https://www.portainer.io/">portainer</a>처럼 도커 컨테이너가 호스트의 도커 소켓 파일을 공유하여 도커를 관리하는데 사용 할 수 있습니다.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 도커 컨테이너 실행</span>
<span class="nv">$ </span>docker run <span class="nt">-d</span> <span class="nt">-p</span> 9000:9000 <span class="nt">-v</span> /var/run/docker.sock:/var/run/docker.sock portainer/portainer-ce
<span class="nv">$ </span>docker ps
<span class="c"># =&gt; CONTAINER ID   IMAGE                    COMMAND        CREATED         STATUS         PORTS                                                           NAMES</span>
<span class="c">#    1495728fd014   portainer/portainer-ce   &amp;quot;/portainer&amp;quot;   2 minutes ago   Up 2 minutes   8000/tcp, 9443/tcp, 0.0.0.0:9000-&amp;gt;9000/tcp, :::9000-&amp;gt;9000/tcp   wizardly_ride</span>
  
</code></pre></div>    </div>

    <p><code class="language-plaintext highlighter-rouge">-v</code> 옵션으로 호스트의 도커 소켓 파일을 컨테이너의 도커 소켓 파일로 공유하면 아래와 같이 도커 컨테이너에서 호스트의 도커를 관리할 수 있습니다.</p>

    <p><img src="/assets/2024/kans-3th/w1/20240831_kans_w1_8.png" alt="소켓 공유를 통해 portainer 사용" /></p>
  </li>
  <li>
    <p>또한 Jenkins 같은 CI/CD 툴을 사용할때도 도커 소켓 파일을 공유하여 도커 기반 워커를 사용할 수도 있습니다.</p>
  </li>
</ul>

<h3 id="cpu-아키텍쳐">CPU 아키텍쳐</h3>

<ul>
  <li>도커 허브에 등록된 이미지들은 CPU 아키텍쳐별로 이미지를 제공하는데, <strong>호스트의 CPU 아키텍쳐와 다른 이미지는 동작할 수 없습니다.</strong></li>
  <li>아래와 같이 docker hub에서는 지원 CPU 아키텍쳐별로 필터링하는 기능을 제공하니, 특정 아키텍쳐의 이미지가 필요한 경우 사용할 수 있습니다.
<img src="/assets/2024/kans-3th/w1/20240831_kans_w1_9.png" alt="img.png" /></li>
  <li>또한 도커이미지 페이지의 Tags 탭에서 태그의 지원하는 아키텍쳐를 확인할 수 있습니다.
<img src="../../../assets/2024/kans-3th/w1/20240831_kans_w1_10.png" alt="20240831_kans_w1_10.png" /></li>
  <li>현재 리눅스의 CPU 아키텍쳐를 확인 해보겠습니다.
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>lscpu
<span class="c"># =&gt; Architecture:                       x86_64</span>
<span class="c">#    CPU op-mode(s):                     32-bit, 64-bit</span>
<span class="c">#    ...</span>
</code></pre></div>    </div>
    <p>사용중인 CPU 아키텍쳐는 x86_64 입니다.</p>
  </li>
  <li>현재 CPU 아키텍쳐와는 다른 아키텍쳐의 이미지를 설치해서 실패하는것을 확인해 보겠습니다.
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># arm64 실행 실패</span>
<span class="nv">$ </span>docker run <span class="nt">--rm</span> <span class="nt">-it</span> arm64v8/ubuntu bash
<span class="c"># =&gt; WARNING: The requested image's platform (linux/arm64/v8) does not match the detected host platform (linux/amd64) and no specific platform was requested</span>
<span class="c">#    exec /usr/bin/bash: exec format error</span>
  
<span class="c"># riscv64 실행 실패</span>
<span class="nv">$ </span>docker run <span class="nt">--rm</span> <span class="nt">-it</span> riscv64/ubuntu bash
<span class="c"># =&gt; WARNING: The requested image's platform (linux/riscv64) does not match the detected host platform (linux/amd64) and no specific platform was requested</span>
<span class="c">#    exec /usr/bin/bash: exec format error</span>
</code></pre></div>    </div>
  </li>
</ul>

<hr />

<h2 id="컨테이너-격리">컨테이너 격리</h2>

<ul>
  <li>docker는 리눅스의 프로세스 격리 기술을 활용하는데, 프로세스 격리 기술은 chroot에서 부터 cgroup, namespace 등을 거쳐 발전하고 있습니다.</li>
  <li>주요 격리 기술들을 실습해보며 이해해보겠습니다.
<img src="/assets/2024/kans-3th/w1/20240831_kans_w1_11.png" alt="img.png" />
<a href="https://speakerdeck.com/kakao/ige-dwaeyo-dokeo-eobsi-keonteineo-mandeulgi?slide=200">https://speakerdeck.com/kakao/ige-dwaeyo-dokeo-eobsi-keonteineo-mandeulgi?slide=200</a></li>
</ul>

<h3 id="chroot">chroot</h3>

<ul>
  <li>chroot는 리눅스의 프로세스 격리 기술 중 하나로, 프로세스가 접근할 수 있는 파일 시스템의 루트 디렉터리를 변경하는 기술입니다.</li>
  <li>1979년에 처음 등장했으며, 한계가 뚜렷하지만 다양한 목적으로 현재도 현역으로 사용되고 있습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 관리자 전환</span>
<span class="nv">$ </span><span class="nb">sudo</span> <span class="nt">-i</span>
<span class="nv">$ </span><span class="nb">whoami</span>
<span class="c"># =&gt; root</span>

<span class="nv">$ </span><span class="nb">cd</span> /tmp
<span class="nv">$ </span><span class="nb">mkdir </span>myroot

<span class="c"># chroot 실행 (chroot [새 루트] [명령])</span>
<span class="nv">$ </span><span class="nb">chroot </span>myroot /bin/bash
<span class="c"># =&gt; chroot: failed to run command ‘/bin/bash’: No such file or directory</span>
</code></pre></div></div>

<ul>
  <li>/tmp/myroot 로 chroot하려니 bash가 없어서 실행이 되지 않습니다. bash를 복사해 넣어보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># bash를 실행하는데 필요한 라이브러리를 확인하겠습니다.</span>
<span class="nv">$ </span>ldd /bin/bash
<span class="c"># =&gt; linux-vdso.so.1 (0x00007fffecfa8000)</span>
<span class="c">#    libtinfo.so.6 =&gt; /lib/x86_64-linux-gnu/libtinfo.so.6 (0x00007fbfe6a4f000)</span>
<span class="c">#    libc.so.6 =&gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007fbfe686a000)</span>
<span class="c">#    /lib64/ld-linux-x86-64.so.2 (0x00007fbfe6be0000)</span>

<span class="nv">$ </span><span class="nb">mkdir</span> <span class="nt">-p</span> myroot/bin
<span class="nv">$ </span><span class="nb">cp</span> /bin/bash myroot/bin
<span class="nv">$ </span><span class="nb">mkdir</span> <span class="nt">-p</span> myroot/<span class="o">{</span>lib64,lib/x86_64-linux-gnu<span class="o">}</span>
<span class="nv">$ </span><span class="nb">cp</span> /lib/x86_64-linux-gnu/libtinfo.so.6 myroot/lib/x86_64-linux-gnu
<span class="nv">$ </span><span class="nb">cp</span> /lib/x86_64-linux-gnu/libc.so.6 myroot/lib/x86_64-linux-gnu
<span class="nv">$ </span><span class="nb">cp</span> /lib64/ld-linux-x86-64.so.2 myroot/lib64
<span class="nv">$ </span>tree myroot
<span class="c"># =&gt; myroot</span>
<span class="c">#    |-- bin</span>
<span class="c">#    |   `-- bash</span>
<span class="c">#    |-- lib</span>
<span class="c">#    |   `-- x86_64-linux-gnu</span>
<span class="c">#    |       |-- libc.so.6</span>
<span class="c">#    |       `-- libtinfo.so.6</span>
<span class="c">#    `-- lib64</span>
<span class="c">#        `-- ld-linux-x86-64.so.2</span>
<span class="c">#    </span>
<span class="c">#    5 directories, 4 files</span>

<span class="nv">$ </span><span class="nb">chroot </span>myroot /bin/bash
<span class="c"># =&gt; bash-5.2# </span>
<span class="c"># bash와 bash에 필요한 라이브러리를 넣어주니 chroot로 실행할 수 있게 되었습니다.</span>
<span class="c"># ls를 실행해보겠습니다.</span>
<span class="nv">$ </span><span class="nb">ls</span>
<span class="c"># =&gt; bash: ls: command not found</span>
<span class="c"># ls가 없어서 실행이 되지 않습니다. ls를 넣기위해 chroot에서 나오겠습니다.</span>
<span class="nv">$ </span><span class="nb">exit</span>

<span class="c"># ls 위치 확인</span>
<span class="nv">$ </span>whereis <span class="nb">ls</span>
<span class="c"># =&gt; ls: /usr/bin/ls /usr/share/man/man1/ls.1.gz</span>
<span class="nv">$ </span>ldd /usr/bin/ls
<span class="c"># ldd로 확인된 라이브러리를 포함해 ls를 myroot에 넣어보겠습니다.</span>
<span class="nv">$ </span><span class="nb">cp</span> /usr/bin/ls myroot/bin
<span class="nv">$ </span><span class="nb">cp</span> /lib/x86_64-linux-gnu/libselinux.so.1 myroot/lib/x86_64-linux-gnu
<span class="nv">$ </span><span class="nb">cp</span> /lib/x86_64-linux-gnu/libpcre2-8.so.0 myroot/lib/x86_64-linux-gnu

<span class="nv">$ </span><span class="nb">chroot </span>myroot /bin/bash
<span class="c"># ls시 /tmp/myroot에 있는 파일들을 확인할 수 있습니다.</span>
<span class="nv">$ </span><span class="nb">ls</span>
<span class="c"># =&gt; bin  lib  lib64</span>
<span class="c"># 현재 디렉터리 확인시 / 로 되어있습니다. 이 처럼 chroot로 인해 루트 디렉터리가 변경되었습니다. </span>
<span class="nv">$ </span><span class="nb">pwd</span>
<span class="c"># =&gt; /</span>
<span class="nv">$ </span><span class="nb">cd</span> ../../..
<span class="nv">$ </span><span class="nb">ls</span>
<span class="c"># =&gt; bin lib lib64</span>

<span class="c"># chroot를 종료 합니다.</span>
<span class="nv">$ </span><span class="nb">exit</span>
</code></pre></div></div>

<ul>
  <li>이 작업을 반복하면 거의 모든 프로그램을 chroot로 실행할 수 있습니다. 하지만 /proc, /dev 등의 가상 디렉터리는 다음의 방법으로 넣어주어야 합니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 다음 동작은 chroot 밖의 호스트에서 실행해야 합니다.</span>
<span class="c"># mount 할 디렉터리 만들어주기</span>
<span class="nv">$ </span><span class="nb">mkdir</span> <span class="nt">-p</span> myroot/<span class="o">{</span>proc,dev<span class="o">}</span>

<span class="c"># /proc, /dev 마운트</span>
<span class="nv">$ </span>mount <span class="nt">-t</span> proc none myroot/proc
<span class="nv">$ </span>mount <span class="nt">-o</span> <span class="nb">bind</span> /dev myroot/dev

<span class="c"># /proc 확인을 위해 ps도 chroot 환경에 넣어보겠습니다.</span>
<span class="nv">$ </span><span class="nb">cp</span> /usr/bin/ps myroot/bin
<span class="nv">$ </span><span class="nb">cp</span> /lib/x86_64-linux-gnu/<span class="o">{</span>libproc2.so.0,libc.so.6,libsystemd.so.0,libcap.so.2,libgcrypt.so.20,liblz4.so.1,liblzma.so.5,libzstd.so.1,libgpg-error.so.0<span class="o">}</span> myroot/lib/x86_64-linux-gnu/ 
<span class="nv">$ </span><span class="nb">cp</span> /lib64/ld-linux-x86-64.so.2 myroot/lib64/ 

<span class="nv">$ </span><span class="nb">chroot </span>myroot /bin/bash
<span class="nv">$ </span><span class="nb">ls</span> /proc
<span class="nv">$ </span>ps
<span class="c"># =&gt;    PID TTY          TIME CMD</span>
<span class="c">#    729517 ?        00:00:00 sudo</span>
<span class="c">#    741301 ?        00:00:00 bash</span>
<span class="c">#    741310 ?        00:00:00 ps</span>

<span class="c"># chroot 종료</span>
<span class="nv">$ </span><span class="nb">exit</span>

<span class="c"># 마운트 해제</span>
<span class="nv">$ </span>mount <span class="nt">-t</span> proc
<span class="c"># =&gt; proc on /proc type proc (rw,nosuid,nodev,noexec,relatime)</span>
<span class="c">#    none on /tmp/myroot/proc type proc (rw,relatime)</span>
<span class="nv">$ </span>umount myroot/proc
<span class="nv">$ </span>umount myroot/dev
<span class="nv">$ </span>mount <span class="nt">-t</span> proc
<span class="c"># =&gt; proc on /proc type proc (rw,nosuid,nodev,noexec,relatime)</span>
</code></pre></div></div>

<ul>
  <li>도커 컨테이너 이미지를 추출하여 chroot로 실행해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">mkdir </span>nginx-root

<span class="c"># nginx 컨테이너 이미지에서 파일들을 추출하여 nginx-root에 넣어줍니다.</span>
<span class="nv">$ </span>docker <span class="nb">export</span> <span class="si">$(</span>docker create nginx<span class="si">)</span> | <span class="nb">tar</span> <span class="nt">-C</span> nginx-root <span class="nt">-xvf</span> -
<span class="nv">$ </span>docker images

<span class="nv">$ </span>tree <span class="nt">-L</span> 2 nginx-root

<span class="c"># chroot로 nginx-root를 루트 디렉터리로 변경합니다.</span>
<span class="nv">$ </span><span class="nb">chroot </span>nginx-root /bin/bash
<span class="c"># nginx를 실행해봅니다.</span>
<span class="nv">$ </span>nginx <span class="nt">-g</span> <span class="s1">'daemon off;'</span>

<span class="c"># [터미널2] 터미널을 하나더 열고 nginx 동작 여부를 확인합니다.</span>
<span class="nv">$ </span>ps <span class="nt">-f</span> <span class="nt">-C</span> nginx
<span class="nv">$ </span>curl localhost
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w1/20240831_kans_w1_12.png" alt="img.png" /></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 생성된 docker 컨테이너를 확인합니다. docker create nginx로 인해 컨테이너가 생겨져있습니다.</span>
<span class="nv">$ </span>docker ps <span class="nt">-a</span>
<span class="c"># =&gt; CONTAINER ID   IMAGE     COMMAND                  CREATED              STATUS    PORTS     NAMES</span>
<span class="c">#    0b506af00006   nginx     "/docker-entrypoint.…"   About a minute ago   Created             gifted_rosalind</span>

<span class="c"># 사용하지 않는 도커이미지를 지워줍니다.</span>
<span class="nv">$ </span>docker <span class="nb">rm </span>0b5
</code></pre></div></div>

<ul>
  <li>아쉽게도 chroot는 탈옥이 가능하다고 합니다. 다음 코드를 컴파일하여 탈옥을 시도해보겠습니다.</li>
</ul>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;sys/stat.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;unistd.h&gt;</span><span class="cp">
</span>
<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
  <span class="n">mkdir</span><span class="p">(</span><span class="s">".out"</span><span class="p">,</span> <span class="mo">0755</span><span class="p">);</span>
  <span class="n">chroot</span><span class="p">(</span><span class="s">".out"</span><span class="p">);</span>
  <span class="n">chdir</span><span class="p">(</span><span class="s">"../../../../../"</span><span class="p">);</span>
  <span class="n">chroot</span><span class="p">(</span><span class="s">"."</span><span class="p">);</span>

  <span class="k">return</span> <span class="n">execl</span><span class="p">(</span><span class="s">"/bin/sh"</span><span class="p">,</span> <span class="s">"-i"</span><span class="p">,</span> <span class="nb">NULL</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 컴파일</span>
<span class="nv">$ </span>gcc <span class="nt">-o</span> myroot/escape_chroot escape_chroot.c
<span class="nv">$ </span>file myroot/escape_chroot
<span class="c"># =&gt; myroot/escape_chroot: ELF 64-bit LSB pie executable, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, BuildID[sha1]=5a40e26463d1015f870c7f1b9db9be159727c250, for GNU/Linux 3.2.0, not stripped</span>

<span class="c"># chroot 실행</span>
<span class="nv">$ </span><span class="nb">chroot </span>myroot /bin/bash
<span class="nv">$ </span><span class="nb">ls</span>
<span class="nv">$ </span><span class="nb">cd</span> ../../
<span class="nv">$ </span><span class="nb">cd</span> ../../
<span class="nv">$ </span><span class="nb">ls</span>
<span class="c"># 일반적인 방법으로는 myroot에서 벗어날 수 없었습니다.</span>

<span class="c"># escape_chroot 실행해서 탈옥해보겠습니다.</span>
<span class="nv">$ </span>./escape_chroot
<span class="nv">$ </span><span class="nb">ls</span> /
<span class="c"># 탈옥이 잘 되었습니다.</span>

<span class="c"># 종료</span>
<span class="nv">$ </span><span class="nb">exit</span>
<span class="nv">$ </span><span class="nb">exit</span>
</code></pre></div></div>

<h3 id="마운트-네임스페이스--pivot_root">마운트 네임스페이스 + pivot_root</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">pivot_root</code>는 루트 파일 시스템을 변경하는 시스템 콜로, 루트 디렉터리를 변경하는 chroot와 달리 루트 파일 시스템을 별도의 디렉터리로 이동시킬 수 있습니다.</li>
  <li>아래의 그림에서 처럼 /tmp/new_root가 있고 /tmp/new_root/put_old 디렉터리가 있는 경우, <code class="language-plaintext highlighter-rouge">pivot_root /tmp/new_root /tmp_new_root/put_old</code>를 하면 
/tmp/new_root가 루트 디렉터리로 변경되고, 원래의 루트 /는 /tmp/new_root/put_old로 이동됩니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w1/20240831_kans_w1_13.png" alt="img.png" />
<a href="https://speakerdeck.com/kakao/ige-dwaeyo-dokeo-eobsi-keonteineo-mandeulgi?slide=80">https://speakerdeck.com/kakao/ige-dwaeyo-dokeo-eobsi-keonteineo-mandeulgi?slide=80</a></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">pivot_root</code>를 사용하려면 <code class="language-plaintext highlighter-rouge">unshare</code> 명령을 통해 마운트 네임스페이스를 만들어야 합니다. 마운트 네임스페이스는 리눅스 커널에서 제공하는 기능으로, 프로세스가 마운트 정보를 독립적으로 가질 수 있게 해줍니다.</li>
  <li>또한 다음과 같은 제약사항이 적용 됩니다.
    <ul>
      <li>new_root와 put_old가 디렉터리여야 한다.</li>
      <li>new_root와 put_old가 현재 루트와 같은 마운트 상에 있어선 안 된다.</li>
      <li>put_old가 new_root와 같거나 그 아래에 있어야 한다. 즉, put_old가 가리키는 경로명 앞에 “/..”를 0개 이상 붙여서 new_root와 같은 디렉터리가 나와야 한다.</li>
      <li>new_root가 마운트 지점의 경로여야 하되, “/”일 수 없다. 마운트 지점이 아닌 경우에는 그 경로를 스스로에게 바인드 마운트 해서 마운트 지점으로 바꿀 수 있다.</li>
      <li>new_root의 부모 마운트 및 현재 작업 디렉터리의 부모 마운트의 전파 유형이 MS_SHARED여선 안 된다. 마찬가지로 put_old가 기존 마운트 지점인 경우 그 전파 유형이 MS_SHARED여선 안 된다. 이 제약은 pivot_root()로 인해 다른 마운트 네임스페이스로 어떤 변화도 전파되지 않게 한다.</li>
      <li>현재 루트 디렉터리가 마운트 지점이어야 한다.</li>
    </ul>
  </li>
  <li>실습을 통해 마운트 네임스페이스와 pivot_root를 알아보겠습니다.</li>
</ul>

<h4 id="실습">실습</h4>

<ul>
  <li>먼저 pivot_root로 root 디렉터리로 만들 /tmp/new_root를 만들어보겠습니다.</li>
  <li>위의 제약사항 중 new_root와 put_old가 현재 루트와 같은 마운트 상에 있어서는 안 되기 때문에 new_root를 tmpfs 로 마운트 하겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">mkdir</span> /tmp/new_root
<span class="c"># 마운트</span>
<span class="nv">$ </span>mount <span class="nt">-t</span> tmpfs tmpfs /tmp/new_root 
<span class="c"># 기존 루트를 이동시킬 /tmp/new_root/put_old 디렉터리를 만들기</span>
<span class="nv">$ </span><span class="nb">mkdir</span> /tmp/new_root/put_old
<span class="c"># /bin, /lib, /lib64 등 chroot 실습때 사용했던 /tmp/myroot를 /tmp/new_root 로 복사해서 재사용합니다.</span>
<span class="nv">$ </span><span class="nb">cp</span> <span class="nt">-rv</span> /tmp/myroot/<span class="k">*</span> /tmp/new_root 

<span class="nv">$ </span>mount <span class="nt">-t</span> proc proc /tmp/new_root/proc

<span class="c"># unshare 해서 마운트 네임스페이스를 만들어줍니다.</span>
<span class="nv">$ </span>unshare <span class="nt">--mount</span> /bin/bash 

<span class="nv">$ </span><span class="nb">cd</span> /tmp/new_root

<span class="c"># pivot_root를 실행</span>
<span class="nv">$ </span>pivot_root <span class="nb">.</span> put_old

<span class="c"># 새로운 루트로 이동되었습니다.</span>

<span class="c"># 새 루트 확인</span>
<span class="nv">$ </span><span class="nb">ls</span> / 
<span class="c"># =&gt; bin  dev  escape_chroot  lib  lib64  proc  put_old</span>

<span class="c"># 기존 루트 확인</span>
<span class="nv">$ </span><span class="nb">ls</span> /put_old
<span class="c"># =&gt; bin   dev  home        lib32  lost+found   mnt     proc  run   srv	  sys       usr  vmlinuz</span>
<span class="c">#    boot  etc  initrd.img  lib	   lib64        media   opt   root  sbin  swapfile  tmp  var  </span>
</code></pre></div></div>

<ul>
  <li>새로운 루트로 이동되었지만, 기존 루트에 있는 파일들을 삭제하거나 이동하지 않았기 때문에 /put_old로 기존 루트에 있는 파일들을 확인할 수 있습니다.</li>
  <li>하지만 umount를 사용하면 /put_old와 기존 루트의 연결을 끊어서 기존 루트를 숨길 수 있습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>umount <span class="nt">-l</span> /put_old

<span class="nv">$ </span><span class="nb">ls</span> /put_old
<span class="c"># =&gt; (공백)</span>
</code></pre></div></div>

<ul>
  <li>escape_root를 통해 탈옥을 시도해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 탈옥 시도</span>
<span class="nv">$ </span>/escape_chroot
<span class="nv">$ </span><span class="nb">ls</span> /
<span class="c"># =&gt; bin  dev  escape_chroot  lib  lib64  proc  put_old</span>
<span class="nv">$ </span><span class="nb">cd</span> ../../..
<span class="nv">$ </span>/escape_chroot
<span class="nv">$ </span><span class="nb">ls</span> /
<span class="c"># =&gt; bin  dev  escape_chroot  lib  lib64  proc  put_old</span>
</code></pre></div></div>

<ul>
  <li>chroot와 달리 pivot_root는 탈옥이 불가능하고 훨씬 안전한것 같습니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w1/20240831_kans_w1_14.png" alt="img.png" /></p>

<h3 id="네임스페이스-namespace">네임스페이스 (namespace)</h3>

<ul>
  <li>여기에서의 네임스페이스는 쿠버네티스 등의 네임스페이스와는 다른,
리눅스 커널에서 제공하는 프로세스 격리 기술로, 프로세스가 각종 자원을 격리하여 사용할 수 있게 해줍니다.</li>
  <li>주요 네임스페이스의 유형은 아래와 같습니다.
    <ul>
      <li>Mount Namespace (2002년 도입)
        <ul>
          <li>pivot_root 예제에서 처럼 마운트 정보를 격리합니다.</li>
          <li>즉, 서로 다른 네임스페이스가 독립적으로 파일 시스템을 마운트 할 수 있습니다.</li>
        </ul>
      </li>
      <li>UTS Namespace (2006년 도입)
        <ul>
          <li>호스트 이름과 NIS 도메인 이름을 격리합니다. 각 네임스페이스는 자체 호스트 이름과 NIS 도메인 이름을 가질 수 있고,
이를 통해 호스트 이름을 변경하더라도 다른 네임스페이스에 영향을 주지 않습니다.</li>
        </ul>
      </li>
      <li>IPC Namespace (2006년 도입)
        <ul>
          <li>POSIX 메시지 큐, 세마포어, 공유 메모리 같은 IPC 리소스를 격리합니다.</li>
          <li>이를 통해 서로 다른 네임스페이스는 독립적으로 System V IPC 객체와 POSIX 메시지 큐를 사용할 수 있습니다.</li>
        </ul>
      </li>
      <li>PID Namespace (2008년 도입)
        <ul>
          <li>프로세스 ID를 격리합니다. 각 네임스페이스는 자체 PID를 가질 수 있으며 자체적인 PID 1을 가질 수 있습니다.</li>
          <li>프로세스 ID가 1인것은 시스템 시작시에 최초로 실행된 것이며 이를 init 프로세스라고 합니다. 이 프로세스가 종료되면 
시스템이 종료되거나 다시 부팅됩니다. 도커 컨테이너 실행시 실행되는 프로그램이 PID가 1이고, 해당 프로그램이 종료되며 
컨테이너도 종료되는게 이때문입니다.</li>
        </ul>
      </li>
      <li>Network Namespace (2009년 도입)
        <ul>
          <li>네트워크 인터페이스, IP 주소, 라우팅 테이블, 방화벽 규칙 등 네트워크 리소스를 격리합니다.</li>
          <li>각 네임스페이스는 자체 네트워크 인터페이스, IP 주소, 라우팅 테이블, 방화벽 규칙을 가질 수 있습니다.</li>
        </ul>
      </li>
      <li>USER Namespace (2012년 도입)
        <ul>
          <li>사용자 ID와 그룹 ID를 격리합니다. 각 네임스페이스는 자체 사용자 ID와 그룹 ID를 가질 수 있습니다.</li>
          <li>이를 통해 root 권한을 가진 사용자도 일반 사용자로 격리하여 사용할 수 있고, 일반 사용자도 root 인것 처럼 보이게 할 수 있습니다.</li>
          <li>실행 중인 도커컨테이너에서는 ps로 확인시 root로 실행 중인데, 호스트에서 ps로 확인시 일반 사용자로 실행 중인것 처럼 보이는것도 이것 때문입니다.</li>
        </ul>
      </li>
      <li>CGROUP Namespace (2016년 도입)
        <ul>
          <li>CGROUP은 프로세스의 그룹으로 CPU, 메모리, 디스크 I/O, 네트워크 등의 자원을 제한하거나 할당할 수 있습니다.</li>
          <li>CGROUP Namespace는 CPU, 메모리 등의 자원을 제한하거나 할당할 수 있는 CGROUP을 격리하는 기능입니다.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="cgroup-를-이용한-자원관리">cgroup 를 이용한 자원관리</h3>

<ul>
  <li>cgroups는 control groups의 줄일말로 리눅스 커널에서 제공하는 자원 제한 및 할당 기능으로, CPU, 메모리, 디스크 I/O, 네트워크 등의 자원을 제한하거나 할당할 수 있습니다.</li>
  <li>프로세스는 실행중인 프로그램의 인스턴스를 의미하며, OS에서는 프로세스를 관리하기 위해 프로세스 ID(PID)를 사용합니다.</li>
  <li>cgroups는 프로세스를 그룹으로 묶어서 자원을 제한하거나 할당할 수 있습니다.</li>
  <li>cgroups는 /sys/fs/cgroup 디렉터리에 마운트되어 있으며, cgroup v1과 cgroup v2가 있습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># cgroup 버전 확인</span>
<span class="nv">$ </span>mount | <span class="nb">grep </span>cgroup
<span class="c"># =&gt; cgroup2 on /sys/fs/cgroup type cgroup2 (rw,nosuid,nodev,noexec,relatime,nsdelegate,memory_recursiveprot)</span>
</code></pre></div></div>

<ul>
  <li>현재 테스트 시스템에는 cgroup v2가 사용되고 있는것을 확인할 수 있습니다.</li>
  <li>cgroup v2는 v1에 비해 자원 계층구조의 가시성이 향상 되었고, memoryQoS 라는 기능이 추가되어 컨테이너에서 OOM(Out Of Memory)이
발생가능성을 줄였습니다. 최신 리눅스 배포판은 보통 cgroup v2를 사용하고 있어서 cgroup v2로 실습을 진행하겠습니다.</li>
  <li>cgroup의 계층 구조는 /sys/fs/cgroup 에서 확인할 수 있습니다.</li>
  <li>/proc는 보았지만 /sys는 눈에 익지 않습니다. 리눅스 커널 3.x 버전에서 생긴것으로 USER SPACE 쪽은 /proc에 KERNEL SPACE 쪽 정보는 /sys에 들어간다고 합니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w1/20240831_kans_w1_23.png" alt="img.png" class="image-center" />
<em class="image-caption">출처 : <a href="https://blog.naver.com/yu3papa/223562337709">https://blog.naver.com/yu3papa/223562337709</a></em></p>

<ul>
  <li>실습을 통해 cgroup의 정보를 확인해 보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>mount <span class="nt">-t</span> cgroup
<span class="nv">$ </span>mount <span class="nt">-t</span> cgroup2
<span class="c"># =&gt; cgroup2 on /sys/fs/cgroup type cgroup2 (rw,nosuid,nodev,noexec,relatime,nsdelegate,memory_recursiveprot)</span>

<span class="nv">$ </span>findmnt <span class="nt">-t</span> cgroup2
<span class="c"># =&gt; TARGET         SOURCE  FSTYPE  OPTIONS</span>
<span class="c">#    /sys/fs/cgroup cgroup2 cgroup2 rw,nosuid,nodev,noexec,relatime,nsdelegate,memory_recursiveprot</span>

<span class="c"># cgroupv1 만 지원 시, cgroup2 출력되지 않음</span>
<span class="nv">$ </span><span class="nb">grep </span>cgroup /proc/filesystems
<span class="c"># =&gt; nodev   cgroup</span>
<span class="c">#    nodev   cgroup2</span>

<span class="nv">$ </span><span class="nb">stat</span> <span class="nt">-fc</span> %T /sys/fs/cgroup/
<span class="c"># =&gt; cgroup2fs</span>

<span class="c"># 터미널2</span>
<span class="nv">$ </span><span class="nb">sleep </span>100000

<span class="c"># 터미널1</span>
<span class="c"># /proc 에 cgroup 정보 확인</span>
<span class="nv">$ </span><span class="nb">cat</span> /proc/cgroups
<span class="nv">$ </span><span class="nb">cat</span> /proc/<span class="si">$(</span>pgrep <span class="nb">sleep</span><span class="si">)</span>/cgroup
<span class="c"># =&gt; 0::/user.slice/user-1000.slice/session-713.scope</span>

<span class="nv">$ </span>tree /proc/<span class="si">$(</span>pgrep <span class="nb">sleep</span><span class="si">)</span> <span class="nt">-L</span> 2
<span class="c"># =&gt; ...</span>
<span class="c">#    |-- &lt;span style="font-weight:bold;color:blue;"&gt;ns&lt;/span&gt;</span>
<span class="c">#    |   |-- &lt;span style="font-weight:bold;color:teal;"&gt;cgroup&lt;/span&gt; -&amp;gt; cgroup:[4026531835]</span>
<span class="c">#    |   |-- &lt;span style="font-weight:bold;color:teal;"&gt;ipc&lt;/span&gt; -&amp;gt; ipc:[4026531839]</span>
<span class="c">#    |   |-- &lt;span style="font-weight:bold;color:teal;"&gt;mnt&lt;/span&gt; -&amp;gt; mnt:[4026531841]</span>
<span class="c">#    |   |-- &lt;span style="font-weight:bold;color:teal;"&gt;net&lt;/span&gt; -&amp;gt; net:[4026531840]</span>
<span class="c">#    ...</span>

<span class="c"># cgroup 목록 확인</span>
<span class="nv">$ </span><span class="nb">ls</span> /sys/fs/cgroup
<span class="nv">$ </span><span class="nb">cat</span> /sys/fs/cgroup/cgroup.controllers
<span class="c"># =&gt; cpuset cpu io memory hugetlb pids rdma misc</span>
<span class="nv">$ </span>tree /sys/fs/cgroup/ <span class="nt">-L</span> 1
<span class="nv">$ </span>tree /sys/fs/cgroup/ <span class="nt">-L</span> 2
<span class="nv">$ </span>tree /sys/fs/cgroup/user.slice <span class="nt">-L</span> 1
<span class="nv">$ </span>tree /sys/fs/cgroup/user.slice/user-1000.slice <span class="nt">-L</span> 1
</code></pre></div></div>

<ul>
  <li>이번에는 cgroup을 이용하여 자원을 제한하는 실습을 진행해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 터미널 2개를 열어서 root 로 실습 하겠습니다.</span>
<span class="nv">$ </span><span class="nb">sudo</span> <span class="nt">-i</span>
<span class="nv">$ </span><span class="nb">whoami</span>
<span class="c"># =&gt; root</span>

<span class="c"># 툴 설치</span>
<span class="nv">$ </span>apt <span class="nb">install</span> <span class="nt">-y</span> cgroup-tools stress htop

<span class="c"># 터미널2</span>
<span class="c"># CPU 사용률 확인을 위해 htop을 실행합니다.</span>
<span class="nv">$ </span>htop

<span class="c"># 터미널1에서 실습 진행</span>

<span class="c"># 1개 CPU 코어에 부하 발생을 위해 stress를 실행합니다.</span>
<span class="nv">$ </span>stress <span class="nt">--cpu</span> 1
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w1/20240831_kans_w1_25.png" alt="img.png" /></p>

<ul>
  <li>CPU 0만 100% 사용중인것을 확인할 수 있습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cd</span> /sys/fs/cgroup
<span class="nv">$ </span><span class="nb">mkdir </span>test_cgroup_parent <span class="o">&amp;&amp;</span> <span class="nb">cd </span>test_cgroup_parent
<span class="nv">$ </span>tree

<span class="c"># 제어가능한 항목 확인</span>
<span class="nv">$ </span><span class="nb">cat </span>cgroup.controllers
<span class="c"># =&gt; cpuset cpu io memory hugetlb pids rdma misc</span>

<span class="c"># cpu를 subtree이 추가하여 컨트롤 할 수 있도록 설정 : +/-(추가/삭제) </span>
<span class="nv">$ </span><span class="nb">cat </span>cgroup.subtree_control
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"+cpu"</span> <span class="o">&gt;&gt;</span> /sys/fs/cgroup/test_cgroup_parent/cgroup.subtree_control

<span class="c"># cpu.max 제한 설정 : 첫 번쨰 값은 허용된 시간(마이크로초) 두 번째 값은 총 기간 길이 &gt; 1/10 실행 설정</span>
<span class="nv">$ </span><span class="nb">echo </span>100000 1000000 <span class="o">&gt;</span> /sys/fs/cgroup/test_cgroup_parent/cpu.max

<span class="c"># test용 자식 디렉토리를 생성하고, pid를 추가하여 제한을 걸어</span>
<span class="nv">$ </span><span class="nb">mkdir </span>test_cgroup_child <span class="o">&amp;&amp;</span> <span class="nb">cd </span>test_cgroup_child
<span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$$</span> <span class="o">&gt;</span> /sys/fs/cgroup/test_cgroup_parent/test_cgroup_child/cgroup.procs
<span class="nv">$ </span><span class="nb">cat</span> /sys/fs/cgroup/test_cgroup_parent/test_cgroup_child/cgroup.procs
<span class="c"># =&gt; 1947587</span>
<span class="c">#    2194781</span>
<span class="nv">$ </span><span class="nb">cat</span> /proc/<span class="nv">$$</span>/cgroup
<span class="c"># =&gt; 0::/test_cgroup_parent/test_cgroup_child</span>

<span class="c"># 부하 발생 확인 : 터미널2에 htop 확인</span>
<span class="nv">$ </span>stress <span class="nt">--cpu</span> 1
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w1/20240831_kans_w1_26.png" alt="img.png" /></p>

<ul>
  <li>cpu.max 제한 설정에서 설정한 대로 (100000/1000000 =&gt; 10%) CPU 사용량이 10%로 제한된것을 확인할 수 있습니다.</li>
  <li>값 수정을 해서 100%로 변경해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 값 수정</span>
<span class="nv">$ </span><span class="nb">echo </span>1000000 1000000 <span class="o">&gt;</span> /sys/fs/cgroup/test_cgroup_parent/cpu.max

<span class="c"># 부하 발생 확인 : 터미널2에 htop 확인</span>
<span class="nv">$ </span>stress <span class="nt">--cpu</span> 1
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w1/20240831_kans_w1_27.png" alt="img.png" /></p>

<ul>
  <li>테스트에 사용한 cgroup 을 삭제하고 실습을 마무리하겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">exit</span>
<span class="nv">$ </span><span class="nb">sudo</span> <span class="nt">-i</span>
<span class="nv">$ </span><span class="nb">rmdir</span> /sys/fs/cgroup/test_cgroup_parent/test_cgroup_child
<span class="nv">$ </span><span class="nb">rmdir</span> /sys/fs/cgroup/test_cgroup_parent
</code></pre></div></div>

<ul>
  <li>이상과 같이 cgroup을 사용하여 cpu 자원을 제한하는것을 실습해 보았습니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w1/20240831_kans_w1_24.png" alt="img.png" class="image-center" /></p>

<hr />

<h2 id="컨테이너-네트워크--iptables">컨테이너 네트워크 &amp; Iptables</h2>

<ul>
  <li>도커는 호스트와 컨테이너간, 컨테이너 간의 네트워크를 앞에서 살펴본 네트워크 네임스페이스를 통해 격리합니다.</li>
  <li>또한 iptables를 통해 네트워크 패킷을 제어하고, 컨테이너 간의 통신을 제어합니다.</li>
  <li>실습을 통해 네트워크 네임스페이스를 통한 격리와 iptables의 사용법에 대해 알아보겠습니다.</li>
</ul>

<h3 id="red--blue-네트워크-네임스페이스-간-통신">Red &lt;=&gt; Blue 네트워크 네임스페이스 간 통신</h3>

<p><img src="/assets/2024/kans-3th/w1/20240831_kans_w1_17.png" alt="img.png" />
<a href="https://www.slideshare.net/slideshow/make-container-withoutdocker6overlaynetwork1/248297122">출처 : 도커없이 컨테이너 만들기</a></p>

<ul>
  <li>먼저 터미널 3개를 열고 모두 관리자로 로그인 하겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo</span> <span class="nt">-i</span>
<span class="nv">$ </span><span class="nb">whoami</span>
<span class="c"># =&gt; root</span>
</code></pre></div></div>

<ul>
  <li>veth (Virtual Ethernet)를 사용하여 Red와 Blue 네트워크 네임스페이스를 만듭니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>ip <span class="nb">link </span>add veth0 <span class="nb">type </span>veth peer name veth1

<span class="c"># veth 생성 확인 (상태 DOWN)</span>
<span class="nv">$ </span>ip <span class="nb">link</span>
<span class="c"># =&gt; 22: &lt;span style="color:teal;"&gt;veth1@veth0: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST,M-DOWN&amp;gt; mtu 1500 qdisc noop state &lt;span style="color:red;"&gt;DOWN &lt;/span&gt;mode DEFAULT group default qlen 1000</span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;9e:74:34:5c:70:ef&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt;</span>
<span class="c">#    23: &lt;span style="color:teal;"&gt;veth0@veth1: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST,M-DOWN&amp;gt; mtu 1500 qdisc noop state &lt;span style="color:red;"&gt;DOWN &lt;/span&gt;mode DEFAULT group default qlen 1000</span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;72:c0:05:36:cd:1b&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt;</span>
<span class="nv">$ </span>ip addr | <span class="nb">grep </span>veth
<span class="c"># =&gt; 22: &lt;span style="color:teal;"&gt;veth1@veth0: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST,M-DOWN&amp;gt; mtu 1500 qdisc noop state &lt;span style="color:red;"&gt;DOWN &lt;/span&gt;group default qlen 1000</span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;9e:74:34:5c:70:ef&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt;</span>
<span class="c">#    23: &lt;span style="color:teal;"&gt;veth0@veth1: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST,M-DOWN&amp;gt; mtu 1500 qdisc noop state &lt;span style="color:red;"&gt;DOWN &lt;/span&gt;group default qlen 1000</span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;72:c0:05:36:cd:1b&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt;</span>

<span class="c"># 네트워크 네임스페이스 생성</span>
<span class="nv">$ </span>ip netns add RED
<span class="nv">$ </span>ip netns add BLUE

<span class="c"># 네트워크 네임스페이스 확인</span>
<span class="nv">$ </span>ip netns
<span class="c"># =&gt; RED</span>
<span class="c">#    BLUE</span>

<span class="c"># veth0와 veth1을 각각 RED와 BLUE 네트워크 네임스페이스로 이동시킵니다.</span>
<span class="nv">$ </span>ip <span class="nb">link set </span>veth0 netns RED  
<span class="nv">$ </span>ip <span class="nb">link set </span>veth1 netns BLUE

<span class="c"># 네트워크 네임스페이스 확인. id 라는것이 추가되었습니다.</span>
<span class="nv">$ </span>ip netns list
<span class="c"># =&gt; RED (id: 0)</span>
<span class="c">#    BLUE (id: 1)</span>

<span class="c"># ip 링크를 확인하면 veth0와 veth1이 각각 RED와 BLUE 네트워크 네임스페이스로 이동되어 기본 명령에서는 보이지 않습니다.</span>
<span class="nv">$ </span>ip <span class="nb">link</span> | <span class="nb">grep</span> <span class="s2">"veth."</span>
<span class="c"># =&gt; (공백)</span>

<span class="c"># ip netns exec [네임스페이스명] [명령] 으로 네트워크 네임스페이스에서 명령을 실행할 수 있습니다.</span>
<span class="nv">$ </span>ip netns <span class="nb">exec </span>RED ip <span class="nb">link</span>
<span class="c"># =&gt; 1: &lt;span style="color:teal;"&gt;lo: &lt;/span&gt;&amp;lt;LOOPBACK&amp;gt; mtu 65536 qdisc noop state &lt;span style="color:red;"&gt;DOWN &lt;/span&gt;mode DEFAULT group default qlen 1000</span>
<span class="c">#        link/loopback &lt;span style="color:olive;"&gt;00:00:00:00:00:00&lt;/span&gt; brd &lt;span style="color:olive;"&gt;00:00:00:00:00:00&lt;/span&gt;</span>
<span class="c">#    23: &lt;span style="color:teal;"&gt;veth0@if22: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST&amp;gt; mtu 1500 qdisc noop state &lt;span style="color:red;"&gt;DOWN &lt;/span&gt;mode DEFAULT group default qlen 1000</span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;72:c0:05:36:cd:1b&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt; link-netns BLUE</span>
<span class="nv">$ </span>ip netns <span class="nb">exec </span>BLUE ip <span class="nb">link</span>
<span class="c"># =&gt; 1: &lt;span style="color:teal;"&gt;lo: &lt;/span&gt;&amp;lt;LOOPBACK&amp;gt; mtu 65536 qdisc noop state &lt;span style="color:red;"&gt;DOWN &lt;/span&gt;mode DEFAULT group default qlen 1000</span>
<span class="c">#        link/loopback &lt;span style="color:olive;"&gt;00:00:00:00:00:00&lt;/span&gt; brd &lt;span style="color:olive;"&gt;00:00:00:00:00:00&lt;/span&gt;</span>
<span class="c">#    22: &lt;span style="color:teal;"&gt;veth1@if23: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST&amp;gt; mtu 1500 qdisc noop state &lt;span style="color:red;"&gt;DOWN &lt;/span&gt;mode DEFAULT group default qlen 1000</span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;9e:74:34:5c:70:ef&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt; link-netns RED</span>

<span class="c"># veth0과 veth1을 활성화 (UP) 시키겠습니다.</span>
<span class="nv">$ </span>ip netns <span class="nb">exec </span>RED ip <span class="nb">link set </span>veth0 up
<span class="c"># veth0의 IP 확인</span>
<span class="nv">$ </span>ip netns <span class="nb">exec </span>RED ip addr
<span class="c"># =&gt; ...</span>
<span class="c">#    23: &lt;span style="color:teal;"&gt;veth0@if22: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state &lt;span style="color:green;"&gt;UP &lt;/span&gt;group default qlen 1000</span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;72:c0:05:36:cd:1b&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt; link-netns BLUE</span>
<span class="c">#        inet6 &lt;span style="color:blue;"&gt;fe80::70c0:5ff:fe36:cd1b&lt;/span&gt;/64 scope link proto kernel_ll </span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="nv">$ </span>ip netns <span class="nb">exec </span>BLUE ip <span class="nb">link set </span>veth1 up
<span class="nv">$ </span>ip netns <span class="nb">exec </span>BLUE ip addr
<span class="c"># =&gt; ...</span>
<span class="c">#    22: &lt;span style="color:teal;"&gt;veth1@if23: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state &lt;span style="color:green;"&gt;UP &lt;/span&gt;group default qlen 1000</span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;9e:74:34:5c:70:ef&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt; link-netns RED</span>
<span class="c">#        inet6 &lt;span style="color:blue;"&gt;fe80::9c74:34ff:fe5c:70ef&lt;/span&gt;/64 scope link proto kernel_ll </span>
<span class="c">#           valid_lft forever preferred_lft forever</span>

<span class="c"># UP 상태로 되었으나 IP가 없습니다. IP를 할당해보겠습니다.</span>
<span class="nv">$ </span>ip netns <span class="nb">exec </span>RED ip addr add 11.11.11.2/24 dev veth0
<span class="nv">$ </span>ip netns <span class="nb">exec </span>BLUE ip addr add 11.11.11.3/24 dev veth1

<span class="c"># IP 를 확인해보겠습니다.</span>
<span class="nv">$ </span>ip netns <span class="nb">exec </span>RED ip addr
<span class="c"># =&gt; ...</span>
<span class="c">#    23: &lt;span style="color:teal;"&gt;veth0@if22: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state &lt;span style="color:green;"&gt;UP &lt;/span&gt;group default qlen 1000</span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;72:c0:05:36:cd:1b&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt; link-netns BLUE</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;11.11.11.2&lt;/span&gt;/24 scope global veth0</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#        inet6 &lt;span style="color:blue;"&gt;fe80::70c0:5ff:fe36:cd1b&lt;/span&gt;/64 scope link proto kernel_ll </span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="nv">$ </span>ip netns <span class="nb">exec </span>BLUE ip addr
<span class="c"># =&gt; ...</span>
<span class="c">#    22: &lt;span style="color:teal;"&gt;veth1@if23: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state &lt;span style="color:green;"&gt;UP &lt;/span&gt;group default qlen 1000</span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;9e:74:34:5c:70:ef&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt; link-netns RED</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;11.11.11.3&lt;/span&gt;/24 scope global veth1</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#        inet6 &lt;span style="color:blue;"&gt;fe80::9c74:34ff:fe5c:70ef&lt;/span&gt;/64 scope link proto kernel_ll </span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
</code></pre></div></div>

<ul>
  <li>이제 Red와 Blue 네트워크 네임스페이스 간의 통신을 테스트 해보겠습니다.</li>
  <li><code class="language-plaintext highlighter-rouge">nsenter</code> 명령을 사용하여 네트워크에 attach 하고, <code class="language-plaintext highlighter-rouge">tcpdump</code>와 <code class="language-plaintext highlighter-rouge">ping</code>을 사용하여 통신을 확인합니다.</li>
  <li><code class="language-plaintext highlighter-rouge">tcpdump</code>는 네트워크 패킷을 캡처하는 명령어로, 패킷을 캡처하여 확인할 수 있고, <code class="language-plaintext highlighter-rouge">ping</code>은 네트워크 상태를 확인하는 명령어입니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>tree /var/run/netns
<span class="c"># =&gt; &lt;span style="font-weight:bold;color:blue;"&gt;/var/run/netns&lt;/span&gt;</span>
<span class="c">#    |-- BLUE</span>
<span class="c">#    `-- RED</span>
<span class="c">#    </span>
<span class="c">#    1 directory, 2 files</span>

<span class="c"># 터미널 1 (RED 11.11.11.2)</span>
<span class="c"># 네트워크 네임스페이스에 attach. </span>
<span class="c"># 이때 --net 옵션을 사용해 앞에서 확인한 /var/run/netns/RED를 사용해 네트워크 네임스페이스에 attach 합니다.</span>
<span class="nv">$ </span>nsenter <span class="nt">--net</span><span class="o">=</span>/var/run/netns/RED
<span class="c"># 이웃하는 IP/ARP 정보 확인</span>
<span class="nv">$ </span>ip neigh
<span class="c"># =&gt; (공백)</span>
<span class="c"># 라우팅 정보, iptables 정보</span>
<span class="nv">$ </span>ip route
<span class="c"># =&gt; &lt;span style="color:purple;"&gt;11.11.11.0/24 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;veth0 &lt;/span&gt;proto kernel scope link src &lt;span style="color:purple;"&gt;11.11.11.2 &lt;/span&gt;</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> filter <span class="nt">-S</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span> 

<span class="c"># 터미널 2 (호스트)</span>
<span class="c"># 네트워크 네임스페이스 상태 확인</span>
<span class="nv">$ </span>lsns <span class="nt">-t</span> net
<span class="c"># =&gt;         NS TYPE NPROCS     PID USER     NETNSID NSFS            COMMAND</span>
<span class="c">#    ...</span>
<span class="c">#    4026532444 net       1 1940569 root           0 /run/netns/RED  -zsh</span>
<span class="c">#    4026532527 net       0         root             /run/netns/BLUE</span>
<span class="c"># 네트워크 정보 확인</span>
<span class="nv">$ </span>ip addr 
<span class="nv">$ </span>ip neigh
<span class="nv">$ </span>ip route
<span class="nv">$ </span>iptables <span class="nt">-t</span> filter <span class="nt">-S</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span> 

<span class="c"># 터미널 3 (BLUE 11.11.11.3)</span>
<span class="nv">$ </span>nsenter <span class="nt">--net</span><span class="o">=</span>/var/run/netns/BLUE
<span class="nv">$ </span>ip neigh
<span class="nv">$ </span>ip route
<span class="nv">$ </span>iptables <span class="nt">-t</span> filter <span class="nt">-S</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span> 

<span class="c"># ping 통신 확인</span>

<span class="c"># 터미널3 (BLUE)</span>
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> veth1
<span class="nv">$ </span>ip <span class="nt">-c</span> neigh
<span class="nv">$ </span><span class="nb">exit</span>

<span class="c"># 터미널1 (RED)</span>
<span class="nv">$ </span>ping 11.11.11.3 <span class="nt">-c</span> 1 
<span class="nv">$ </span>ip <span class="nt">-c</span> neigh
<span class="nv">$ </span><span class="nb">exit</span>

<span class="c"># 네임스페이스 삭제</span>
<span class="nv">$ </span>ip netns del RED
<span class="nv">$ </span>ip netns del BLUE 
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w1/20240831_kans_w1_15.png" alt="img.png" /></p>

<ul>
  <li>위의 캡쳐와 같이 통신이 되어서 tcpdump에 ARP, ICMP 패킷이 잡히는것을 확인할 수 있습니다.</li>
  <li>또한 <code class="language-plaintext highlighter-rouge">ip neigh</code> 명령을 확인했을때 ARP 테이블에 상대방의 IP와 MAC 주소가 등록되어 있는것을 확인할 수 있습니다.</li>
</ul>

<h3 id="red---bridge-br0---blue-네트워크-네임스페이스-간-통신">Red &lt;- Bridge (br0) -&gt; Blue 네트워크 네임스페이스 간 통신</h3>

<p><img src="/assets/2024/kans-3th/w1/20240831_kans_w1_16.png" alt="img.png" />
<a href="https://www.slideshare.net/slideshow/make-container-withoutdocker6overlaynetwork1/248297122">출처 : 도커없이 컨테이너 만들기</a></p>

<ul>
  <li>이전 실습에서는 Red와 Blue를 연결하여 peer 네트워크로 구성하였는데, 
이번에는 각각 독립적인 네트워크로 구성하여 Bridge를 사용하여 Red와 Blue 네트워크 네임스페이스 간의 통신을 확인해보겠습니다.</li>
  <li>왜 Bridge를 두는가 하면, peer 네트워크를 구성할 경우 구성원들 간의 통신을 위해서는 모든 노드가 서로서로 연결되어야 하기 때문입니다.
<img src="/assets/2024/kans-3th/w1/20240831_kans_w1_18.png" alt="img.png" class="image-center" /></li>
  <li>Bridge를 두면 각 노드는 Bridge와만 연결되어 있으면 통신이 가능하므로 효율적입니다.</li>
  <li>실습을 통해 아래의 그림과 같이 격리된 네트워크 네임스페이스를 만들고 브릿지를 통해 통신해보겠습니다.
<img src="/assets/2024/kans-3th/w1/20240831_kans_w1_19.png" alt="img.png" /></li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 터미널 3개를 root 로 엽니다.</span>
<span class="nv">$ </span><span class="nb">sudo</span> <span class="nt">-i</span>
<span class="nv">$ </span><span class="nb">whoami</span>
<span class="c"># =&gt; root</span>

<span class="c"># 네트워크 네임스페이스 및 veth 생성</span>
<span class="nv">$ </span>ip netns add RED
<span class="nv">$ </span>ip <span class="nb">link </span>add reth0 <span class="nb">type </span>veth peer name reth1
<span class="nv">$ </span>ip <span class="nb">link set </span>reth0 netns RED
<span class="nv">$ </span>ip netns add BLUE
<span class="nv">$ </span>ip <span class="nb">link </span>add beth0 <span class="nb">type </span>veth peer name beth1
<span class="nv">$ </span>ip <span class="nb">link set </span>beth0 netns BLUE

<span class="c"># 확인</span>
<span class="nv">$ </span>ip netns list
<span class="nv">$ </span>ip <span class="nb">link</span>
<span class="c"># =&gt; ...</span>
<span class="c">#    26: &lt;span style="color:teal;"&gt;reth1@if27: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST&amp;gt; mtu 1500 qdisc noop state &lt;span style="color:red;"&gt;DOWN &lt;/span&gt;mode DEFAULT group default qlen 1000</span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;9a:1f:bf:6f:fe:64&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt; link-netns RED</span>
<span class="c">#    28: &lt;span style="color:teal;"&gt;beth1@if29: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST&amp;gt; mtu 1500 qdisc noop state &lt;span style="color:red;"&gt;DOWN &lt;/span&gt;mode DEFAULT group default qlen 1000</span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;7e:31:cf:5f:00:8f&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt; link-netns BLUE</span>
<span class="nv">$ </span>ip netns <span class="nb">exec </span>RED ip addr
<span class="c"># =&gt; ...</span>
<span class="c">#    27: &lt;span style="color:teal;"&gt;reth0@if26: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST&amp;gt; mtu 1500 qdisc noop state &lt;span style="color:red;"&gt;DOWN &lt;/span&gt;group default qlen 1000</span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;ea:7f:a0:1f:00:3d&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt; link-netnsid 0</span>
<span class="nv">$ </span>ip netns <span class="nb">exec </span>BLUE ip addr
<span class="c"># =&gt; ...</span>
<span class="c">#    29: &lt;span style="color:teal;"&gt;beth0@if28: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST&amp;gt; mtu 1500 qdisc noop state &lt;span style="color:red;"&gt;DOWN &lt;/span&gt;group default qlen 1000</span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;66:23:89:a6:f7:70&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt; link-netnsid 0</span>

<span class="c"># 브릿지 정보 확인 </span>
<span class="nv">$ </span>brctl show
<span class="c"># =&gt; bridge name	bridge id		STP enabled	interfaces</span>
<span class="c">#    docker0		8000.02425756997c	no		</span>

<span class="c"># br0 브릿지 생성</span>
<span class="nv">$ </span>ip <span class="nb">link </span>add br0 <span class="nb">type </span>bridge

<span class="c"># br0 브릿지 정보 확인</span>
<span class="nv">$ </span>brctl show br0
<span class="c"># =&gt; bridge name	bridge id		STP enabled	interfaces</span>
<span class="c">#    br0		8000.000000000000	no		</span>
<span class="nv">$ </span>brctl showmacs br0
<span class="nv">$ </span>brctl showstp br0

<span class="c"># reth1과 beth1을 br0 브릿지에 연결</span>
<span class="nv">$ </span>ip <span class="nb">link set </span>reth1 master br0
<span class="nv">$ </span>ip <span class="nb">link set </span>beth1 master br0
<span class="nv">$ </span>brctl show br0
<span class="c"># =&gt; bridge name     bridge id               STP enabled     interfaces</span>
<span class="c">#    br0             8000.7e31cf5f008f       no              beth1</span>
<span class="c">#                                                            reth1</span>
<span class="nv">$ </span>brctl showmacs br0
<span class="c"># =&gt; port no mac addr                is local?       ageing timer</span>
<span class="c">#      2     7e:31:cf:5f:00:8f       yes                0.00</span>
<span class="c">#      2     7e:31:cf:5f:00:8f       yes                0.00</span>
<span class="c">#      1     9a:1f:bf:6f:fe:64       yes                0.00</span>
<span class="c">#      1     9a:1f:bf:6f:fe:64       yes                0.00</span>
<span class="nv">$ </span>ip <span class="nt">-br</span> <span class="nb">link</span>
<span class="c"># =&gt; ... </span>
<span class="c">#    &lt;span style="color:teal;"&gt;reth1@if27       &lt;/span&gt;&lt;span style="color:red;"&gt;DOWN           &lt;/span&gt;&lt;span style="color:olive;"&gt;9a:1f:bf:6f:fe:64 &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST&amp;gt; </span>
<span class="c">#    &lt;span style="color:teal;"&gt;beth1@if29       &lt;/span&gt;&lt;span style="color:red;"&gt;DOWN           &lt;/span&gt;&lt;span style="color:olive;"&gt;7e:31:cf:5f:00:8f &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST&amp;gt; </span>
<span class="c">#    &lt;span style="color:teal;"&gt;br0              &lt;/span&gt;&lt;span style="color:red;"&gt;DOWN           &lt;/span&gt;&lt;span style="color:olive;"&gt;7e:31:cf:5f:00:8f &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST&amp;gt; </span>

<span class="c"># reth0과 beth0에 IP 설정 및 활성화(UP) 시키고, reth1, beth1, br0를 활성화(UP) 합니다.</span>
<span class="nv">$ </span>ip netns <span class="nb">exec </span>RED  ip addr add 11.11.11.2/24 dev reth0
<span class="nv">$ </span>ip netns <span class="nb">exec </span>BLUE ip addr add 11.11.11.3/24 dev beth0
<span class="nv">$ </span>ip netns <span class="nb">exec </span>RED  ip <span class="nb">link set </span>reth0 up
<span class="nv">$ </span>ip <span class="nb">link set </span>reth1 up
<span class="nv">$ </span>ip netns <span class="nb">exec </span>BLUE ip <span class="nb">link set </span>beth0 up
<span class="nv">$ </span>ip <span class="nb">link set </span>beth1 up
<span class="nv">$ </span>ip <span class="nb">link set </span>br0 up
<span class="nv">$ </span>ip <span class="nt">-br</span> addr
<span class="c"># =&gt; ... </span>
<span class="c">#    &lt;span style="color:teal;"&gt;reth1@if27       &lt;/span&gt;&lt;span style="color:green;"&gt;UP             &lt;/span&gt;&lt;span style="color:blue;"&gt;fe80::981f:bfff:fe6f:fe64&lt;/span&gt;/64 </span>
<span class="c">#    &lt;span style="color:teal;"&gt;beth1@if29       &lt;/span&gt;&lt;span style="color:green;"&gt;UP             &lt;/span&gt;&lt;span style="color:blue;"&gt;fe80::7c31:cfff:fe5f:8f&lt;/span&gt;/64 </span>
<span class="c">#    &lt;span style="color:teal;"&gt;br0              &lt;/span&gt;&lt;span style="color:green;"&gt;UP             &lt;/span&gt;&lt;span style="color:blue;"&gt;fe80::7c31:cfff:fe5f:8f&lt;/span&gt;/64 </span>
<span class="nv">$ </span>ip netns <span class="nb">exec </span>RED ip <span class="nt">-br</span> addr
<span class="c"># =&gt; ...</span>
<span class="c">#    &lt;span style="color:teal;"&gt;reth0@if26       &lt;/span&gt;&lt;span style="color:green;"&gt;UP             &lt;/span&gt;&lt;span style="color:purple;"&gt;11.11.11.2&lt;/span&gt;/24 &lt;span style="color:blue;"&gt;fe80::e87f:a0ff:fe1f:3d&lt;/span&gt;/64 </span>
<span class="nv">$ </span>ip netns <span class="nb">exec </span>BLUE ip <span class="nt">-br</span> addr
<span class="c"># =&gt; ...</span>
<span class="c">#    &lt;span style="color:teal;"&gt;beth0@if28       &lt;/span&gt;&lt;span style="color:green;"&gt;UP             &lt;/span&gt;&lt;span style="color:purple;"&gt;11.11.11.3&lt;/span&gt;/24 &lt;span style="color:blue;"&gt;fe80::6423:89ff:fea6:f770&lt;/span&gt;/64 </span>

<span class="c"># 터미널1 (RED 11.11.11.2)</span>
<span class="nv">$ </span>nsenter <span class="nt">--net</span><span class="o">=</span>/var/run/netns/RED
<span class="nv">$ </span>ip <span class="nt">-c</span> a<span class="p">;</span><span class="nb">echo</span><span class="p">;</span> ip <span class="nt">-c</span> route<span class="p">;</span><span class="nb">echo</span><span class="p">;</span> ip <span class="nt">-c</span> neigh
<span class="c"># 현재 네트워크 네임스페이스 확인</span>
<span class="nv">$ </span>ip netns identify <span class="nv">$$</span>
<span class="c"># =&gt; RED</span>

<span class="c"># 터미널2 (호스트)</span>
<span class="nv">$ </span>brctl showmacs br0
<span class="nv">$ </span>bridge fdb show
<span class="nv">$ </span>bridge fdb show dev br0

<span class="nv">$ </span>iptables <span class="nt">-t</span> filter <span class="nt">-S</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> filter <span class="nt">-L</span> <span class="nt">-n</span> <span class="nt">-v</span>

<span class="c"># 터미널3 (BLUE 11.11.11.3)</span>
<span class="nv">$ </span>nsenter <span class="nt">--net</span><span class="o">=</span>/var/run/netns/BLUE
<span class="nv">$ </span>ip <span class="nt">-c</span> a<span class="p">;</span><span class="nb">echo</span><span class="p">;</span> ip <span class="nt">-c</span> route<span class="p">;</span><span class="nb">echo</span><span class="p">;</span> ip <span class="nt">-c</span> neigh
<span class="c"># 현재 네트워크 네임스페이스 확인</span>
<span class="nv">$ </span>ip netns identify <span class="nv">$$</span>
<span class="c"># =&gt; BLUE</span>

<span class="c"># 터미널2 (호스트)</span>
<span class="c"># ping 통신 전 사전 설정</span>
<span class="c">## iptables 정보 확인</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> filter <span class="nt">-S</span> | <span class="nb">grep</span> <span class="s1">'\-P'</span>
<span class="c"># =&gt; -P INPUT ACCEPT</span>
<span class="c">#    -P FORWARD DROP</span>
<span class="c">#    -P OUTPUT ACCEPT</span>
<span class="nv">$ </span>iptables <span class="nt">-nvL</span> <span class="nt">-t</span> filter

<span class="c"># 호스트에서 패킷 라우팅 설정 확인 - 0(off), 1(on)</span>
<span class="nv">$ </span><span class="nb">cat</span> /proc/sys/net/ipv4/ip_forward
<span class="c"># =&gt; 1</span>
<span class="c"># 위의 결과가 0인 경우 아래의 명령을 실행</span>
<span class="c"># echo 1 &gt; /proc/sys/net/ipv4/ip_forward</span>

<span class="c"># ping 통신 확인</span>
<span class="c"># 터미널2 (호스트)</span>
<span class="nv">$ </span>tcpdump <span class="nt">-l</span> <span class="nt">-i</span> br0
<span class="c"># =&gt; tcpdump: verbose output suppressed, use -v[v]... for full protocol decode</span>
<span class="c">#    listening on br0, link-type EN10MB (Ethernet), snapshot length 262144 bytes</span>
<span class="c">#    (터미널1에서 ping 실행시)</span>
<span class="c">#    08:40:00.413198 IP 11.11.11.2 &gt; 11.11.11.3: ICMP echo request, id 41028, seq 1, length 64</span>
<span class="c">#    08:40:05.455528 ARP, Request who-has 11.11.11.3 tell 11.11.11.2, length 28</span>
<span class="c">#    08:40:05.455556 ARP, Reply 11.11.11.3 is-at 66:23:89:a6:f7:70 (oui Unknown), length 28</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s1">'iptables -v --numeric --table filter --list FORWARD'</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s1">'iptables -v --numeric --table filter --list FORWARD;echo;iptables -v --numeric --table filter --list DOCKER-USER;echo;iptables -v --numeric --table filter --list DOCKER-ISOLATION-STAGE-1'</span>

<span class="c"># 터미널3 (BLUE 11.11.11.3)</span>
<span class="nv">$ </span>tcpdump <span class="nt">-l</span> <span class="nt">-i</span> beth0

<span class="c"># 터미널1 (RED 11.11.11.2)</span>
<span class="nv">$ </span>ping 11.11.11.3 <span class="nt">-c</span> 1
<span class="c"># =&gt; 실패</span>
</code></pre></div></div>

<p>위의 캡쳐와 같이 브릿지에서는 패킷이 잡히지만, 브릿지를 통해 Blue로 패킷이 전달되지 않는것을 확인할 수 있습니다.
그렇다면 왜 패킷이 전달되지 않을까요? 그것은 <code class="language-plaintext highlighter-rouge">iptables -t filter -S | grep '\-P'</code> 명령을 통해 확인했을때 FORWARD 체인이 DROP으로 설정되어 있기 때문입니다.
패킷이 브릿지를 통해 전달되려면 FORWARD 체인을 통해야 하는데 DROP이면 패킷이 전달되지 않습니다.</p>

<p><img src="/assets/2024/kans-3th/w1/20240831_kans_w1_20.png" alt="img.png" class="image-center" />
<em class="image-caption">Iptables 처리 흐름도 (<a href="https://natnat1.medium.com/iptables-b9ce0602253f">https://natnat1.medium.com/iptables-b9ce0602253f</a>)</em></p>

<ul>
  <li>위의 그림과 같이 iptables는 패킷이 들어오면 PREROUTING 체인을 통해 패킷을 처리하고, FORWARD 체인을 통해 패킷을 전달합니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w1/20240831_kans_w1_21.png" alt="img.png" /></p>

<ul>
  <li>br0 입장에서 살펴보면 위와 같습니다. 그렇다면 11.11.11.2 &lt;=&gt; 11.11.11.3으로 FORWARD를 허용하면 되는데 방법을 살펴보면 아래와 같습니다.
    <ul>
      <li>출발지 11.11.11.2와 11.11.11.3 허용</li>
      <li>도착지 11.11.11.0/24 대역 출발지 허용</li>
      <li>FORWARD 기본 정책을 ACCEPT로 변경</li>
      <li>등등 기타 어떤 방법으로든 11.11.11.2와 11.11.11.3이 FORWARD 체인을 통해 패킷이 전달되도록 설정하면 됩니다.</li>
    </ul>
  </li>
  <li>실습을 통해 iptables를 통해 패킷이 전달되도록 설정해보겠습니다.</li>
  <li>방법1. 11.11.11.2와 11.11.11.3 허용하기
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 터미널2 (호스트)</span>
<span class="c"># 출발지 11.11.11.2 허용하기</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> filter <span class="nt">-A</span> FORWARD <span class="nt">-s</span> 11.11.11.2/32 <span class="nt">-j</span> ACCEPT
<span class="nv">$ </span>iptables <span class="nt">-t</span> filter <span class="nt">-A</span> FORWARD <span class="nt">-s</span> 11.11.11.3/32 <span class="nt">-j</span> ACCEPT
<span class="nv">$ </span>tcpdump <span class="nt">-l</span> <span class="nt">-i</span> br0
  
<span class="c"># 터미널3 (BLUE 11.11.11.3)</span>
<span class="nv">$ </span>tcpdump <span class="nt">-l</span> <span class="nt">-i</span> beth0
<span class="c"># =&gt; tcpdump: verbose output suppressed, use -v[v]... for full protocol decode</span>
<span class="c">#    listening on beth0, link-type EN10MB (Ethernet), snapshot length 262144 bytes</span>
<span class="c">#    10:39:20.225225 IP 11.11.11.2 &gt; 11.11.11.3: ICMP echo request, id 33335, seq 1, length 64</span>
<span class="c">#    10:39:20.225233 IP 11.11.11.3 &gt; 11.11.11.2: ICMP echo reply, id 33335, seq 1, length 64</span>
  
<span class="c"># 터미널1 (RED 11.11.11.2)</span>
<span class="nv">$ </span>ping 11.11.11.3
<span class="c"># =&gt; PING 11.11.11.3 (11.11.11.3) 56(84) bytes of data.</span>
<span class="c">#    64 bytes from 11.11.11.3: icmp_seq=1 ttl=64 time=0.055 ms</span>
<span class="c">#    </span>
<span class="c">#    --- 11.11.11.3 ping statistics ---</span>
<span class="c">#    1 packets transmitted, 1 received, 0% packet loss, time 0ms</span>
<span class="c">#    rtt min/avg/max/mdev = 0.055/0.055/0.055/0.000 ms</span>
  
<span class="c"># 터미널2 (호스트)</span>
<span class="c"># 허용 룰 제거</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> filter <span class="nt">-D</span> FORWARD <span class="nt">-s</span> 11.11.11.2/32 <span class="nt">-j</span> ACCEPT
<span class="nv">$ </span>iptables <span class="nt">-t</span> filter <span class="nt">-D</span> FORWARD <span class="nt">-s</span> 11.11.11.3/32 <span class="nt">-j</span> ACCEPT
</code></pre></div>    </div>

    <p><img src="/assets/2024/kans-3th/w1/20240831_kans_w1_22.png" alt="img.png" /></p>
  </li>
  <li>방법2. 도착지 11.11.11.0/24 대역 허용하기
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 터미널2 (호스트)</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> filter <span class="nt">-A</span> FORWARD <span class="nt">-d</span> 11.11.11.0/24 <span class="nt">-j</span> ACCEPT
<span class="c"># 테스트 후 허용 룰 제거</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> filter <span class="nt">-D</span> FORWARD <span class="nt">-d</span> 11.11.11.0/24 <span class="nt">-j</span> ACCEPT
</code></pre></div>    </div>
  </li>
  <li>방법3. FORWARD 기본 정책을 ACCEPT로 변경하기
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 터미널2 (호스트)</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> filter <span class="nt">-P</span> FORWARD ACCEPT
<span class="c"># 테스트 후 허용 룰 제거</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> filter <span class="nt">-P</span> FORWARD DROP
</code></pre></div>    </div>
  </li>
</ul>

<h3 id="호스트--redblue-네트워크-네임스페이스로-접근하기">호스트 &lt;=&gt; RED/BLUE 네트워크 네임스페이스로 접근하기</h3>

<ul>
  <li>“Red &lt;- Bridge (br0) -&gt; Blue 네트워크 네임스페이스 간 통신”을 실습한 환경에 이어서 실습해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 터미널1 (RED 11.11.11.2)</span>
<span class="nv">$ </span>nsenter <span class="nt">--net</span><span class="o">=</span>/var/run/netns/RED
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> any

<span class="c"># 터미널3를 호스트 네트워크로 변경합니다.</span>
<span class="nv">$ </span><span class="nb">exit</span>
<span class="nv">$ </span>ip netns identify <span class="nv">$$</span>
<span class="c"># =&gt; (공백)</span>
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> br0 <span class="nt">-n</span>

<span class="c"># 터미널2 (호스트)</span>
<span class="nv">$ </span>ping <span class="nt">-c</span> 1 11.11.11.2
<span class="c"># =&gt; 1 packets transmitted, 0 received, 100% packet loss, time 0ms</span>
<span class="c"># (ping 이 실패합니다.)</span>
</code></pre></div></div>

<ul>
  <li>호스트에서 RED (11.11.11.2) 로 ping시 패킷이 전달되지 않는것을 확인할 수 있습니다.</li>
  <li>그 이유는 RED 네트워크로 접근하기 위해서는 br0를 거쳐서 접근해야하는데, br0는 ip가 없기 때문에 패킷이 전달되지 않습니다.</li>
  <li>br0에 ip를 할당하고, RED와 BLUE 네트워크 네임스페이스로 접근해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 터미널2 (호스트)</span>
<span class="nv">$ </span>ip addr add 11.11.11.1/24 dev br0
<span class="nv">$ </span>ip addr
<span class="c"># =&gt; ...</span>
<span class="c">#    30: &lt;span style="color:teal;"&gt;br0: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state &lt;span style="color:green;"&gt;UP &lt;/span&gt;group default qlen 1000</span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;7e:31:cf:5f:00:8f&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt;</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;11.11.11.1&lt;/span&gt;/24 scope global br0</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="nv">$ </span>ping 11.11.11.2 <span class="nt">-c</span> 1
<span class="c"># =&gt; PING 11.11.11.2 (11.11.11.2) 56(84) bytes of data.</span>
<span class="c">#    64 bytes from 11.11.11.2: icmp_seq=1 ttl=64 time=0.044 ms</span>
<span class="c">#    </span>
<span class="c">#    --- 11.11.11.2 ping statistics ---</span>
<span class="c">#    1 packets transmitted, 1 received, 0% packet loss, time 0ms</span>
<span class="c">#    rtt min/avg/max/mdev = 0.044/0.044/0.044/0.000 ms</span>
<span class="nv">$ </span>ping 11.11.11.3 <span class="nt">-c</span> 1
<span class="c"># =&gt; PING 11.11.11.3 (11.11.11.3) 56(84) bytes of data.</span>
<span class="c">#    64 bytes from 11.11.11.3: icmp_seq=1 ttl=64 time=0.052 ms</span>
<span class="c">#    </span>
<span class="c">#    --- 11.11.11.3 ping statistics ---</span>
<span class="c">#    1 packets transmitted, 1 received, 0% packet loss, time 0ms</span>
<span class="c">#    rtt min/avg/max/mdev = 0.052/0.052/0.052/0.000 ms</span>
</code></pre></div></div>

<ul>
  <li>이번에는 RED에서 호스트로 ping이 되는것을 확인해 보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 터미널1 (RED 11.11.11.2)</span>
<span class="c"># br0 에 ping 테스트</span>
<span class="nv">$ </span>ping 11.11.11.1 <span class="nt">-c</span> 1
<span class="c"># =&gt; PING 11.11.11.1 (11.11.11.1) 56(84) bytes of data.</span>
<span class="c">#    64 bytes from 11.11.11.1: icmp_seq=1 ttl=64 time=0.041 ms</span>
<span class="c">#    </span>
<span class="c">#    --- 11.11.11.1 ping statistics ---</span>
<span class="c">#    1 packets transmitted, 1 received, 0% packet loss, time 0ms</span>
<span class="c">#    rtt min/avg/max/mdev = 0.041/0.041/0.041/0.000 ms</span>

<span class="c"># 호스트로 ping 테스트</span>
<span class="nv">$ </span>ping 10.10.10.51 <span class="nt">-c</span> 1
<span class="c"># =&gt; ping: connect: Network is unreachable</span>
</code></pre></div></div>

<ul>
  <li>br0에는 ping 이 성공하는데 호스트로는 Network is unreachable 에러가 발생하는것을 확인할 수 있습니다.</li>
  <li>그 이유는 11.11.11.0/24에서 호스트 네트워크인 10.10.10.0/24로 패킷을 라우팅하는 정보가 없기 때문입니다.</li>
  <li>RED나 BLUE에서 호스트로 패킷을 전달하기 위해서는 br0를 통해야 하는데, RED와 BLUE에 기본 게이트웨이를 br0로 설정하여 테스트해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 터미널1 (RED 11.11.11.2)</span>
<span class="nv">$ </span>ip route add default via 11.11.11.1
<span class="nv">$ </span>ip route
<span class="c"># =&gt; default via 11.11.11.1 dev reth0</span>
<span class="c">#    11.11.11.0/24 dev reth0 proto kernel scope link src 11.11.11.2</span>
<span class="nv">$ </span>ping 10.10.10.51 <span class="nt">-c</span> 1
<span class="c"># =&gt; PING 10.10.10.51 (10.10.10.51) 56(84) bytes of data.</span>
<span class="c">#    64 bytes from 10.10.10.51: icmp_seq=1 ttl=64 time=0.041 ms</span>
<span class="c">#    </span>
<span class="c">#    --- 10.10.10.51 ping statistics ---</span>
<span class="c">#    1 packets transmitted, 1 received, 0% packet loss, time 0ms</span>
<span class="nv">$ </span><span class="nb">exit</span>

<span class="c"># BLUE에서도 동일하게 테스트해보겠습니다</span>

<span class="c"># 터미널1 (BLUE 11.11.11.3)</span>
<span class="nv">$ </span>nsenter <span class="nt">--net</span><span class="o">=</span>/var/run/netns/BLUE
<span class="nv">$ </span>ip netns identify <span class="nv">$$</span>
<span class="c"># =&gt; BLUE</span>
<span class="nv">$ </span>ping 10.10.10.51 <span class="nt">-c</span> 1
<span class="c"># =&gt; ping: connect: Network is unreachable</span>
<span class="nv">$ </span>ip route add default via 11.11.11.1
<span class="nv">$ </span>ip route
<span class="c"># =&gt; default via 11.11.11.1 dev beth0</span>
<span class="c">#    11.11.11.0/24 dev beth0 proto kernel scope link src 11.11.11.3</span>
<span class="nv">$ </span>ping 10.10.10.51 <span class="nt">-c</span> 1
<span class="c"># =&gt; PING 10.10.10.51 (10.10.10.51) 56(84) bytes of data.</span>
<span class="c">#    64 bytes from 10.10.10.51: icmp_seq=1 ttl=64 time=0.049 ms</span>
<span class="c">#    </span>
<span class="c">#    --- 10.10.10.51 ping statistics ---</span>
<span class="c">#    1 packets transmitted, 1 received, 0% packet loss, time 0ms</span>
<span class="c">#    rtt min/avg/max/mdev = 0.049/0.049/0.049/0.000 ms</span>
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">ip route add default via 11.11.11.1</code> 로 기본 게이트웨이를 br0로 설정하고, 호스트로 ping이 되는것을 확인할 수 있습니다.</li>
</ul>

<h3 id="redblue에서-외부-인터넷-통신">RED/BLUE에서 외부 인터넷 통신</h3>

<ul>
  <li>이번에는 RED와 BLUE 네트워크 네임스페이스에서 외부 인터넷으로 통신하는 방법을 실습해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 터미널1 (RED 11.11.11.2)</span>
<span class="nv">$ </span>nsenter <span class="nt">--net</span><span class="o">=</span>/var/run/netns/RED
<span class="nv">$ </span>ping 8.8.8.8 <span class="nt">-c</span> 1
<span class="c"># =&gt; PING 8.8.8.8 (8.8.8.8) 56(84) bytes of data.</span>
<span class="c">#    </span>
<span class="c">#    --- 8.8.8.8 ping statistics ---</span>
<span class="c">#    1 packets transmitted, 0 received, 100% packet loss, time 0ms</span>
</code></pre></div></div>

<ul>
  <li>RED에서 외부 인터넷으로 ping이 되지 않는것을 확인할 수 있습니다.</li>
  <li>RED/BLUE와 같이 호스트 아래의 내부 네트워크에서 외부 인터넷으로 패킷을 전달하기 위해서는
호스트의 IP로 패킷을 전달하고, 응답을 호스트 IP로 받아서 내부 네트워크(RED/BLUE)로 전달해야하는데, 
이러한 과정을 SNAT (Source Network Address Translation) 또는 MASQUERADE라고 합니다.</li>
  <li>nat 테이블의 POSTROUTING 체인에 MASQUERADE 룰을 추가하면 SNAT이 적용되어서 외부 인터넷으로 패킷을 전달할 수 있습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 터미널2 (호스트)</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-A</span> POSTROUTING <span class="nt">-s</span> 11.11.11.0/24 <span class="nt">-j</span> MASQUERADE
<span class="c"># SNAT 통계 모니터링</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s1">'iptables -v --numeric --table nat --list POSTROUTING'</span>
<span class="nv">$ </span>iptables <span class="nt">-nvL</span> <span class="nt">-t</span> nat
<span class="nv">$ </span>conntrack <span class="nt">-L</span> <span class="nt">--src-nat</span>
<span class="c"># =&gt; icmp     1 29 src=11.11.11.2 dst=8.8.8.8 type=8 code=0 id=62779 src=8.8.8.8 dst=10.10.10.109 type=0 code=0 id=62779 mark=0 use=1</span>
<span class="c">#    conntrack v1.4.8 (conntrack-tools): 1 flow entries have been shown.</span>

<span class="c"># 터미널1 (RED 11.11.11.2)</span>
<span class="nv">$ </span>ping 8.8.8.8 <span class="nt">-c</span> 1
<span class="c"># =&gt; PING 8.8.8.8 (8.8.8.8) 56(84) bytes of data.</span>
<span class="c">#    64 bytes from 8.8.8.8: icmp_seq=1 ttl=113 time=26.3 ms</span>
<span class="c">#    </span>
<span class="c">#    --- 8.8.8.8 ping statistics ---</span>
<span class="c">#    1 packets transmitted, 1 received, 0% packet loss, time 0ms</span>
<span class="c">#    rtt min/avg/max/mdev = 26.277/26.277/26.277/0.000 ms</span>
<span class="nv">$ </span><span class="nb">exit</span>

<span class="c"># 터미널1 (BLUE 11.11.11.3)</span>
<span class="nv">$ </span>nsenter <span class="nt">--net</span><span class="o">=</span>/var/run/netns/BLUE
<span class="nv">$ </span>ip route add default via 11.11.11.1
<span class="nv">$ </span>ping 8.8.8.8 <span class="nt">-c</span> 1
<span class="nv">$ </span><span class="nb">exit</span>

<span class="c"># 삭제</span>
<span class="nv">$ </span>ip netns delete RED
<span class="nv">$ </span>ip netns delete BLUE
<span class="nv">$ </span>ip <span class="nb">link </span>delete br0

<span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-D</span> POSTROUTING <span class="nt">-s</span> 11.11.11.0/24 <span class="nt">-j</span> MASQUERADE
</code></pre></div></div>

<ul>
  <li>SNAT 추가한 이후 ping이 잘 되는것을 확인할 수 있었습니다.</li>
</ul>

<hr />

<h2 id="마치며">마치며</h2>

<p>첫주부터 이론과 실습할것이 굉장히 많았습니다. 테라폼 스터디가 순한맛으로 보일 정도입니다. 😅
하지만 그동안 막연하게 알고 있었던 도커 컨테이너의 격리 원리와 리눅스 네트워크와 iptables에 
대해 더 깊게 이해할 수 있어서 좋았습니다.</p>

<p><del>개인적으로 *BSD를 좋아하는데 이 정도면 FreeBSD에서도 BSD만의 docker 같은 에코시스템 구축이 가능할것 같은데
왜 못하고 있는지 의문입니다. 비슷하게 돌릴 수 있는 다양한 시도들은 많은데 흐지부지 되는 이유는 대체 무엇인지..</del></p>

<p>항상 무언가를 배우는것은 즐겁습니다. 다음 스터디도 기대됩니다! :smile:</p>]]></content><author><name></name></author><category term="kans" /><category term="kubernetes," /><category term="network," /><category term="linux" /><summary type="html"><![CDATA[지난 테라폼 스터디에 이어 이번 주 부터 KANS 스터디를 시작하게 되었습니다! KANS는 Kubernetes Advanced Networking Study의 줄임말로 쿠버네티스 네트워킹에 대한 심도있게 공부하는 스터디입니다. 이번 스터디도 과제할 걱정도 되지만 재미있을것 같아 기대됩니다.]]></summary></entry><entry><title type="html">[T101 4기] OpenTofu</title><link href="https://sweetlittlebird.github.io/posts/2024-08-03-T101-Study-Terraform-Week-8/" rel="alternate" type="text/html" title="[T101 4기] OpenTofu" /><published>2024-08-03T16:02:00+09:00</published><updated>2024-08-03T16:02:00+09:00</updated><id>https://sweetlittlebird.github.io/posts/T101%20Study%20-%20Terraform%20Week%208</id><content type="html" xml:base="https://sweetlittlebird.github.io/posts/2024-08-03-T101-Study-Terraform-Week-8/"><![CDATA[<h2 id="들어가며">들어가며</h2>

<p>이번 주가 8주차이자 벌써 마지막주차입니다. 
T101 4기에서 마지막으로 알아볼 주제는 OpenTofu로 Terraform의 
오픈소스 커뮤니티에서 포크한 버전으로 앞으로가 기대되는 프로젝트입니다.
이 블로그 글의 내용은 
<a href="https://product.kyobobook.co.kr/detail/S000202478097">테라폼으로 시작하는 IaC</a>를 참고하였습니다.</p>

<blockquote>
  <p><img src="/assets/2024/t101-4th/20240614_terraform_book.jpg" alt="테라폼으로 시작하는 IaC" /></p>

  <p><a href="https://product.kyobobook.co.kr/detail/S000202478097">테라폼으로 시작하는 IaC</a></p>
</blockquote>

<h2 id="opentofu">OpenTofu</h2>

<h3 id="개요">개요</h3>

<p><img src="/assets/2024/t101-4th/20240803_terraform_w8_opentofu_1.png" alt="OpenTofu 로고" class="image-center" />
<em class="image-caption">OpenTofu 로고</em></p>

<ul>
  <li>OpenTofu는 HashiCorp가 Terraform의 라이센스를 MPL(Mozilla Public License)에서 비오픈 소스 라이센스인 BUSL(Business Source License)로 변경한 것에
반발하여 만들어진 프로젝트입니다. BUSL로 라이센스가 변경되기 전의 소스를 포크하였으며, 초기에는 OpenTF 라는 이름을 사용하다가
OpenTofu로 변경하였습니다.
    <ul>
      <li>Terraform AWS Provider 등은 아직 MPL 버전이어서 OpenTofu에서 사용 가능합니다.</li>
    </ul>
  </li>
  <li>초기에는 Terraform의 오픈소스버전을 유지하기 위한 프로젝트였으나, 현재는 상태파일 암호화나 Backend 블록에서 Variable을 사용할 수 있게 하는 등
본가의 Terraform 에서 조차 지원하지 않는 편리한 기능들을 추가하며 발전해가고 있습니다.</li>
</ul>

<h3 id="특이사항">특이사항</h3>

<ul>
  <li>OpenTofu 1.6.x는 Terraform 1.6.x와 기능적으로 매우 유사하지만, 앞으로 기능추가 등 각각 독자적인 방향으로 발전할 가능성이 있습니다.</li>
  <li>Terraform 1.5.x 및 1.6.x 와 상당 부분 호환되므로 해당 버전을 사용 중이었다면 OpenTofu로 전환하기가 쉽습니다.</li>
  <li>Terraform 1.5.x 까지와는 상태 파일 그대로 호환 될정도입니다.</li>
  <li>OpenTofu는 현재 자체 Provider가 없으며, Terraform의 Provider는 라이센스가 변경되지 않는 이상 최신버전을 사용할 수 없습니다.</li>
  <li>현재 OpenTofu는 Terraform 공급자와 함께 동작할 수 있지만 별도의 레지스트리를 사용합니다.</li>
  <li>OpenTofu 1.7 버전 부터는 프로덕션환경에서 쓸 수 있는 버전으로 알려져있습니다.</li>
</ul>

<p>OpenTofu는 Terraform과 유사하여 바로 설치 후 실습을 진행하겠습니다.</p>

<h3 id="opentofu-설치">OpenTofu 설치</h3>

<p>tenv를 사용하여 OpenTofu를 설치해보겠습니다.
tenv는 tfenv와 유사하나 Terraform만 관리하는 tfenv와는 달리 tenv는  Terraform 외에도 OpenTofu, Terragrunt 등을 설치하고 버전을 관리할 수 있습니다.</p>

<p>각 툴별로 명령과 환경 변수는 아래와 같습니다.</p>

<table>
  <thead>
    <tr>
      <th>툴</th>
      <th>명령어</th>
      <th>환경변수</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>OpenTofu</td>
      <td>tofu</td>
      <td>TOFUENV_</td>
    </tr>
    <tr>
      <td>Terraform</td>
      <td>terraform</td>
      <td>TFENV_</td>
    </tr>
    <tr>
      <td>Terragrunt</td>
      <td>terragrunt</td>
      <td>TG_</td>
    </tr>
    <tr>
      <td>Atmos</td>
      <td>atmos</td>
      <td>ATMOS_</td>
    </tr>
  </tbody>
</table>

<p>MacOS를 기준으로 설치하겠습니다.</p>

<ul>
  <li>tenv 설치
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 먼저 tfenv를 삭제합니다.</span>
<span class="nv">$ </span>brew remove tfenv
  
<span class="c"># tenv를 설치</span>
<span class="nv">$ </span>brew <span class="nb">install </span>tenv
  
<span class="nv">$ </span>tenv <span class="nt">--version</span>
<span class="c"># =&gt; tenv version 2.7.9</span>
  
<span class="nv">$ </span>tenv <span class="nt">-h</span>       <span class="c"># 도움말 보기 </span>
<span class="nv">$ </span>tenv tofu <span class="nt">-h</span>  <span class="c"># OpenTofu 도움말 보기</span>
  
<span class="c"># (옵션) Install shell completion</span>
<span class="nv">$ </span>tenv completion zsh <span class="o">&gt;</span> ~/.tenv.completion.zsh
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"source '~/.tenv.completion.zsh'"</span> <span class="o">&gt;&gt;</span> ~/.zshrc
</code></pre></div>    </div>
  </li>
  <li>OpenTofu 설치
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>tenv tofu list            <span class="c"># 현재 설치된 목록 확인</span>
<span class="nv">$ </span>tenv tofu list-remote     <span class="c"># 설치가능한 목록 확인</span>
  
<span class="c"># 설치</span>
<span class="nv">$ </span>tenv tofu <span class="nb">install </span>1.7.3   <span class="c"># 1.8은 너무 최신이어서 안정적인 1.7.3 버전을 설치합니다.</span>
<span class="c"># =&gt; Installation of OpenTofu 1.7.3 successful</span>
  
<span class="nv">$ </span>tenv tofu list            <span class="c"># 설치된 목록 확인</span>
<span class="c"># =&gt; 1.7.3</span>
<span class="nv">$ </span>tenv tofu use 1.7.3       <span class="c"># 사용할 버전 선택</span>
<span class="nv">$ </span>tenv tofu detect          <span class="c"># 현재 사용하는 버전의 위치를 확인</span>
  
<span class="nv">$ </span>tofu version       <span class="c"># OpenTofu 버전 확인</span>
<span class="c"># =&gt; OpenTofu v1.7.3</span>
<span class="c">#    on darwin_arm64</span>
</code></pre></div>    </div>
  </li>
</ul>

<h3 id="opentofu-17">OpenTofu 1.7</h3>

<h4 id="실습-provider-defined-functions">[실습] Provider-defined functions</h4>

<p>OpenTofu에서 Provider-defined functions을 사용해 보겠습니다.
Functions는 크게 Built-in functions와 Provider-defined functions로 나뉩니다.
Built-in functions는 OpenTofu(Terraform)에서 기본적으로 제공하는 함수이며, Provider-defined functions는 Provider에서 제공하는 함수입니다.
각각 아래의 링크에서 목록을 확인할 수 있습니다.</p>
<ul>
  <li>Built-in functions: <a href="https://opentofu.org/docs/language/functions/">https://opentofu.org/docs/language/functions/</a></li>
  <li>Provider-defined functions: <a href="https://library.tf/providers">https://library.tf/providers</a> 
=&gt; (Provider를 선택하고 Functions 탭에서 확인)</li>
</ul>

<p>이번 실습에서 사용할 함수는 <a href="https://library.tf/providers/northwood-labs/corefunc/latest/docs/functions/str_snake">Provider::corefunc::str_snake</a>으로
문자열을 snake_case로 변환하는 함수입니다.</p>

<ul>
  <li>main.tf 생성
    <div class="language-tf highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># main.tf</span>
<span class="k">terraform</span> <span class="p">{</span>
  <span class="nx">required_providers</span> <span class="p">{</span>
    <span class="nx">corefunc</span> <span class="p">=</span> <span class="p">{</span>
      <span class="nx">source</span> <span class="p">=</span> <span class="s2">"northwood-labs/corefunc"</span>
      <span class="nx">version</span> <span class="p">=</span> <span class="s2">"1.4.0"</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>
  
<span class="k">provider</span> <span class="s2">"corefunc"</span> <span class="p">{</span>
<span class="p">}</span>
  
<span class="k">output</span> <span class="s2">"test"</span> <span class="p">{</span>
  <span class="nx">value</span> <span class="p">=</span> <span class="k">provider</span><span class="err">::</span><span class="nx">corefunc</span><span class="err">::</span><span class="nx">str_snake</span><span class="p">(</span><span class="s2">"Hello world!"</span><span class="p">)</span>
  <span class="c1"># Prints: hello_world</span>
<span class="p">}</span>   
</code></pre></div>    </div>
  </li>
  <li>실행
    <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 초기화 (프로바이더 다운로드 등)</span>
<span class="nv">$ </span>tofu init
  
<span class="c"># 프로바이더 정보 확인</span>
<span class="nv">$ </span>tree .terraform
<span class="c"># =&gt; .terraform</span>
<span class="c">#    └── providers</span>
<span class="c">#        └── registry.opentofu.org</span>
<span class="c">#            └── northwood-labs</span>
<span class="c">#                └── corefunc</span>
<span class="c">#                    ...  </span>
  
<span class="c"># 실행계획 보기</span>
<span class="nv">$ </span>tofu plan
<span class="c"># 적용</span>
<span class="nv">$ </span>tofu apply <span class="nt">-auto-approve</span>
<span class="c"># =&gt; ...</span>
<span class="c">#    Outputs:</span>
<span class="c">#    test = "hello_world"</span>
  
<span class="nv">$ </span>tofu output
<span class="c"># =&gt; test = "hello_world"</span>
<span class="nv">$ </span><span class="nb">cat </span>terraform.tfstate | jq
</code></pre></div>    </div>
    <p>명령만 terraform에서 tofu로 바뀌었을 뿐, 사용법과 결과는 거의 동일한것을 확인할 수 있었습니다.</p>
  </li>
  <li>main.tf 를 수정하여 str_camel 함수를 사용하여 snake_case를 camelCase로 변환해보겠습니다.
    <div class="language-tf highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># main.tf</span>
<span class="k">terraform</span> <span class="p">{</span>
  <span class="nx">required_providers</span> <span class="p">{</span>
    <span class="nx">corefunc</span> <span class="p">=</span> <span class="p">{</span>
      <span class="nx">source</span> <span class="p">=</span> <span class="s2">"northwood-labs/corefunc"</span>
      <span class="nx">version</span> <span class="p">=</span> <span class="s2">"1.4.0"</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>
  
<span class="k">provider</span> <span class="s2">"corefunc"</span> <span class="p">{</span>
<span class="p">}</span>
  
<span class="k">output</span> <span class="s2">"test"</span> <span class="p">{</span>
  <span class="nx">value</span> <span class="p">=</span> <span class="k">provider</span><span class="err">::</span><span class="nx">corefunc</span><span class="err">::</span><span class="nx">str_camel</span><span class="p">(</span><span class="s2">"Hello world!"</span><span class="p">)</span>  <span class="c1"># str_snake =&gt; str_camel로 변경</span>
  <span class="c1"># Prints: hello_world</span>
<span class="p">}</span>   
</code></pre></div>    </div>
  </li>
  <li>실행
    <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>tofu plan
<span class="c"># =&gt; Changes to Outputs:</span>
<span class="c">#    ~ test = "hello_world" -&gt; "helloWorld"</span>
  
<span class="c"># 적용</span>
<span class="nv">$ </span>tofu apply <span class="nt">-auto-approve</span>
<span class="c"># =&gt; ...</span>
<span class="c">#    Outputs:</span>
<span class="c">#    test = "helloWorld"</span>
  
<span class="nv">$ </span><span class="nb">ls</span> <span class="nt">-l</span> terraform.tfstate<span class="k">*</span>
<span class="c"># =&gt; -rw-r--r--  1 user  staff  255  8  3 16:02 terraform.tfstate</span>
<span class="c">#    -rw-r--r--  1 user  staff  256  8  3 16:02 terraform.tfstate.backup</span>
</code></pre></div>    </div>
  </li>
</ul>

<h4 id="실습-loopable-import-blocks">[실습] Loopable import blocks</h4>
<p><a href="https://opentofu.org/docs/v1.7/intro/whats-new/#loopable-import-blocks">관련문서</a></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">import</code> 블록은 `terraform import 명령 처럼 기존 리소스를 가져오는 기능입니다. 차이점은 terraform 선언 파일에서 사용할 수 있다는 것입니다.</li>
  <li>아래는 aws_instance.example 리소스를 가져오는 예제입니다. <code class="language-plaintext highlighter-rouge">import</code> 블록에서 <code class="language-plaintext highlighter-rouge">id</code>를 사용하여 가져올 리소스를 지정합니다.
    <div class="language-tf highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># import 블록 예제</span>
<span class="nx">import</span> <span class="p">{</span>
  <span class="nx">to</span> <span class="p">=</span> <span class="nx">aws_instance</span><span class="p">.</span><span class="nx">example</span>
  <span class="nx">id</span> <span class="p">=</span> <span class="s2">"i-abcd1234"</span>
<span class="p">}</span>
  
<span class="k">resource</span> <span class="s2">"aws_instance"</span> <span class="s2">"example"</span> <span class="p">{</span>
  <span class="nx">name</span> <span class="p">=</span> <span class="s2">"hashi"</span>
  <span class="c1"># (other resource arguments...)</span>
<span class="p">}</span>
</code></pre></div>    </div>
    <p>위의 예제를 수행하면 <code class="language-plaintext highlighter-rouge">aws_instance.example</code>은 마치 <code class="language-plaintext highlighter-rouge">OpenTofu</code>에서 프로비저닝한것 처럼 state 파일에 추가 됩니다.</p>
  </li>
  <li>이번 실습에서는 위의 <code class="language-plaintext highlighter-rouge">import</code> 블록을 Loop를 돌려서 여러개의 리소스를 가져와 보겠습니다.</li>
  <li>먼저 정상적으로 AWS 상에 EC2 인스턴스를 생성하겠습니다.
    <ul>
      <li>main.tf 생성
        <div class="language-tf highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># main.tf</span>
<span class="k">data</span> <span class="s2">"aws_ami"</span> <span class="s2">"ubuntu"</span> <span class="p">{</span>
    <span class="nx">most_recent</span> <span class="p">=</span> <span class="kc">true</span>
    
    <span class="nx">filter</span> <span class="p">{</span>
        <span class="nx">name</span>   <span class="p">=</span> <span class="s2">"name"</span>
        <span class="nx">values</span> <span class="p">=</span> <span class="p">[</span><span class="s2">"ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*"</span><span class="p">]</span>
    <span class="p">}</span>
    
    <span class="nx">filter</span> <span class="p">{</span>
        <span class="nx">name</span>   <span class="p">=</span> <span class="s2">"virtualization-type"</span>
        <span class="nx">values</span> <span class="p">=</span> <span class="p">[</span><span class="s2">"hvm"</span><span class="p">]</span>
    <span class="p">}</span>
    
    <span class="nx">owners</span> <span class="p">=</span> <span class="p">[</span><span class="s2">"099720109477"</span><span class="p">]</span> <span class="c1"># Canonical</span>
<span class="p">}</span>
    
<span class="k">variable</span> <span class="s2">"instance_tags"</span> <span class="p">{</span>
  <span class="nx">type</span> <span class="p">=</span> <span class="nx">list</span><span class="p">(</span><span class="nx">string</span><span class="p">)</span>
  <span class="nx">default</span> <span class="p">=</span> <span class="p">[</span><span class="s2">"web"</span><span class="p">,</span> <span class="s2">"app"</span><span class="p">]</span>
<span class="p">}</span>
    
<span class="k">resource</span> <span class="s2">"aws_instance"</span> <span class="s2">"this"</span> <span class="p">{</span>
  <span class="nx">count</span> <span class="p">=</span> <span class="nx">length</span><span class="p">(</span><span class="kd">var</span><span class="p">.</span><span class="nx">instance_tags</span><span class="p">)</span>
  <span class="nx">ami</span>                    <span class="p">=</span> <span class="k">data</span><span class="p">.</span><span class="nx">aws_ami</span><span class="p">.</span><span class="nx">ubuntu</span><span class="p">.</span><span class="nx">id</span>
  <span class="nx">instance_type</span>          <span class="p">=</span> <span class="s2">"t3.micro"</span>
    
  <span class="nx">tags</span> <span class="p">=</span> <span class="p">{</span>
    <span class="nx">Name</span> <span class="p">=</span> <span class="kd">var</span><span class="p">.</span><span class="nx">instance_tags</span><span class="p">[</span><span class="nx">count</span><span class="p">.</span><span class="nx">index</span><span class="p">]</span>
  <span class="p">}</span>
<span class="p">}</span>    
</code></pre></div>        </div>
      </li>
      <li>실행
        <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 초기화</span>
<span class="nv">$ </span>tofu init
    
<span class="c"># 프로바이더 정보 확인</span>
<span class="nv">$ </span>tree .terraform
<span class="c"># =&gt; .terraform</span>
<span class="c">#    └── providers</span>
<span class="c">#        └── registry.opentofu.org</span>
<span class="c">#            └── hashicorp</span>
<span class="c">#                └── aws</span>
<span class="c">#                    └── 5.60.0</span>
                              
<span class="c"># 적용</span>
<span class="nv">$ </span>tofu apply <span class="nt">-auto-approve</span>
<span class="c"># =&gt; Apply complete! Resources: 2 added, 0 changed, 0 destroyed.</span>
    
<span class="c"># EC2 확인</span>
<span class="nv">$ </span><span class="k">while </span><span class="nb">true</span><span class="p">;</span> <span class="k">do </span>aws ec2 describe-instances <span class="nt">--query</span> <span class="s2">"Reservations[*].Instances[*].{PublicIPAdd:PublicIpAddress,InstanceName:Tags[?Key=='Name']|[0].Value,Status:State.Name}"</span> <span class="nt">--filters</span> <span class="nv">Name</span><span class="o">=</span>instance-state-name,Values<span class="o">=</span>running <span class="nt">--output</span> text <span class="p">;</span> <span class="nb">echo</span> <span class="s2">"------------------------------"</span> <span class="p">;</span> <span class="nb">sleep </span>1<span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; app     3.36.26.23      running</span>
<span class="c">#    web     3.39.248.105    running</span>
    
<span class="c"># 확인</span>
<span class="nv">$ </span>tofu state list
<span class="nv">$ </span>tofu state <span class="nb">ls</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"data.aws_ami.ubuntu"</span> | tofu console
<span class="nv">$ </span>tofu show          
</code></pre></div>        </div>
      </li>
      <li>tfstate 파일을 삭제하여 문제 상황을 만들어보겠습니다.
        <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 문제 상황 발생 : tfstate 파일 삭제</span>
<span class="nv">$ </span><span class="nb">rm</span> <span class="nt">-r</span> .terraform<span class="k">*</span> terraform.tfstate<span class="k">*</span>
    
<span class="c"># EC2 ID 확인 : ID 메모 </span>
<span class="nv">$ </span>aws ec2 describe-instances <span class="nt">--query</span> <span class="s1">'Reservations[*].Instances[*].{InstanceID:InstanceId,PublicIP:PublicIpAddress,Name:Tags[?Key==`Name`]|[0].Value}'</span> <span class="nt">--output</span> json | jq <span class="nt">-r</span> <span class="s1">'.[][] | "\(.InstanceID)\t\(.PublicIP)\t\(.Name)"'</span>
<span class="c"># =&gt; i-051d42d1feafc4d4a     3.36.26.23      app</span>
<span class="c">#    i-0bb0a855d1749a7c2     3.39.248.105    web</span>
</code></pre></div>        </div>
      </li>
      <li>main.tf 파일 수정하여 instance id를 위에서 확인한 ID 값으로 수정합니다.
        <div class="language-tf highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># main.tf</span>
<span class="k">data</span> <span class="s2">"aws_ami"</span> <span class="s2">"ubuntu"</span> <span class="p">{</span>
    <span class="nx">most_recent</span> <span class="p">=</span> <span class="kc">true</span>
    
    <span class="nx">filter</span> <span class="p">{</span>
        <span class="nx">name</span>   <span class="p">=</span> <span class="s2">"name"</span>
        <span class="nx">values</span> <span class="p">=</span> <span class="p">[</span><span class="s2">"ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*"</span><span class="p">]</span>
    <span class="p">}</span>
    
    <span class="nx">filter</span> <span class="p">{</span>
        <span class="nx">name</span>   <span class="p">=</span> <span class="s2">"virtualization-type"</span>
        <span class="nx">values</span> <span class="p">=</span> <span class="p">[</span><span class="s2">"hvm"</span><span class="p">]</span>
    <span class="p">}</span>
    
    <span class="nx">owners</span> <span class="p">=</span> <span class="p">[</span><span class="s2">"099720109477"</span><span class="p">]</span> <span class="c1"># Canonical</span>
<span class="p">}</span>
    
<span class="c1"># 추가 시작</span>
<span class="k">variable</span> <span class="s2">"instance_ids"</span> <span class="p">{</span>                                   
  <span class="nx">type</span> <span class="p">=</span> <span class="nx">list</span><span class="p">(</span><span class="nx">string</span><span class="p">)</span>
  <span class="nx">default</span> <span class="p">=</span> <span class="p">[</span><span class="s2">"i-051d42d1feafc4d4a"</span><span class="p">,</span> <span class="s2">"i-0bb0a855d1749a7c2"</span><span class="p">]</span>
<span class="p">}</span>
<span class="c1"># 추가 종료</span>
    
<span class="k">variable</span> <span class="s2">"instance_tags"</span> <span class="p">{</span>
  <span class="nx">type</span> <span class="p">=</span> <span class="nx">list</span><span class="p">(</span><span class="nx">string</span><span class="p">)</span>
  <span class="nx">default</span> <span class="p">=</span> <span class="p">[</span><span class="s2">"web"</span><span class="p">,</span> <span class="s2">"app"</span><span class="p">]</span>
<span class="p">}</span>
    
<span class="k">resource</span> <span class="s2">"aws_instance"</span> <span class="s2">"this"</span> <span class="p">{</span>
  <span class="nx">count</span> <span class="p">=</span> <span class="nx">length</span><span class="p">(</span><span class="kd">var</span><span class="p">.</span><span class="nx">instance_tags</span><span class="p">)</span>
  <span class="nx">ami</span>                    <span class="p">=</span> <span class="k">data</span><span class="p">.</span><span class="nx">aws_ami</span><span class="p">.</span><span class="nx">ubuntu</span><span class="p">.</span><span class="nx">id</span>
  <span class="nx">instance_type</span>          <span class="p">=</span> <span class="s2">"t3.micro"</span>
    
  <span class="nx">tags</span> <span class="p">=</span> <span class="p">{</span>
    <span class="nx">Name</span> <span class="p">=</span> <span class="kd">var</span><span class="p">.</span><span class="nx">instance_tags</span><span class="p">[</span><span class="nx">count</span><span class="p">.</span><span class="nx">index</span><span class="p">]</span>
  <span class="p">}</span>
<span class="p">}</span>
    
<span class="c1"># 추가 시작</span>
<span class="nx">import</span> <span class="p">{</span>
  <span class="nx">for_each</span> <span class="p">=</span> <span class="p">{</span> <span class="nx">for</span> <span class="nx">idx</span><span class="p">,</span> <span class="nx">item</span> <span class="nx">in</span> <span class="kd">var</span><span class="p">.</span><span class="nx">instance_ids</span> <span class="err">:</span> <span class="nx">idx</span> <span class="p">=</span><span class="err">&gt;</span> <span class="nx">item</span> <span class="p">}</span>
  <span class="nx">to</span> <span class="p">=</span> <span class="nx">aws_instance</span><span class="p">.</span><span class="nx">this</span><span class="p">[</span><span class="nx">tonumber</span><span class="p">(</span><span class="nx">each</span><span class="p">.</span><span class="nx">key</span><span class="p">)]</span>
  <span class="nx">id</span> <span class="p">=</span> <span class="nx">each</span><span class="p">.</span><span class="nx">value</span>
<span class="p">}</span>
<span class="c1"># 추가 종료 </span>
</code></pre></div>        </div>
      </li>
      <li>실행
        <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 초기화 </span>
<span class="nv">$ </span>tofu init
    
<span class="c"># import 실행</span>
<span class="nv">$ </span>tofu apply <span class="nt">-auto-approve</span>
<span class="c"># =&gt; Plan: 2 to import, 0 to add, 2 to change, 0 to destroy.</span>
<span class="c">#    aws_instance.this[1]: Importing... [id=i-0bb0a855d1749a7c2]</span>
<span class="c">#    aws_instance.this[1]: Import complete [id=i-0bb0a855d1749a7c2]</span>
<span class="c">#    aws_instance.this[0]: Importing... [id=i-051d42d1feafc4d4a]</span>
<span class="c">#    aws_instance.this[0]: Import complete [id=i-051d42d1feafc4d4a]</span>
<span class="c">#    aws_instance.this[0]: Modifying... [id=i-051d42d1feafc4d4a]</span>
<span class="c">#    aws_instance.this[1]: Modifying... [id=i-0bb0a855d1749a7c2]</span>
<span class="c">#    aws_instance.this[0]: Modifications complete after 1s [id=i-051d42d1feafc4d4a]</span>
<span class="c">#    aws_instance.this[1]: Modifications complete after 1s [id=i-0bb0a855d1749a7c2]</span>
<span class="c">#    Apply complete! Resources: 2 imported, 0 added, 2 changed, 0 destroyed.</span>
    
<span class="c"># 확인</span>
<span class="nv">$ </span>tofu state <span class="nb">ls</span>
<span class="nv">$ </span>tofu show
    
<span class="c"># 상태파일 확인</span>
<span class="nv">$ </span><span class="nb">cat </span>terraform.tfstate | jq    
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
</ul>

<p>테라폼에서 한번에 하나씩만 import 할 수 있었는데 OpenTofu에서는 여러 건을 한꺼번에 import 할 수 있어 편리한것 같습니다. 
테라폼과 OpenTofu의 경쟁을 통해 더욱 빠르고 강력하게 될것같아 기대됩니다.</p>

<h4 id="실습-state-file-encryption---local">[실습] State file encryption - Local</h4>

<ul>
  <li>OpenTofu에서는 상태파일을 로컬 스토리지 및 백엔드에서 암호화 하는것을 지원합니다. 또한 <code class="language-plaintext highlighter-rouge">terraform_remote_state</code> 데이터 소스와 함께 암호화를 사용할 수도 있습니다.</li>
  <li>사용 형태
    <div class="language-tf highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">terraform</span> <span class="p">{</span>
  <span class="nx">encryption</span> <span class="p">{</span>
    <span class="nx">key_provider</span> <span class="s2">"some_key_provider"</span> <span class="s2">"some_name"</span> <span class="p">{</span>
      <span class="c1"># 키 프로바이더 옵션 지정 </span>
    <span class="p">}</span>
  
    <span class="nx">method</span> <span class="s2">"some_method"</span> <span class="s2">"some_method_name"</span> <span class="p">{</span>
      <span class="c1"># 메쏘드 옵션 지정</span>
      <span class="nx">keys</span> <span class="p">=</span> <span class="nx">key_provider</span><span class="p">.</span><span class="nx">some_key_provider</span><span class="p">.</span><span class="nx">some_name</span>
    <span class="p">}</span>
  
    <span class="nx">state</span> <span class="p">{</span>
      <span class="c1"># 상태데이터 암호화/복호화</span>
      <span class="nx">method</span> <span class="p">=</span> <span class="nx">method</span><span class="p">.</span><span class="nx">some_method</span><span class="p">.</span><span class="nx">some_method_name</span>   
        
      <span class="nx">fallback</span> <span class="p">{</span>     <span class="c1"># (선택사항)</span>
        <span class="c1"># 위의 method가 실패할 경우 사용할 method</span>
        <span class="nx">method</span> <span class="p">=</span> <span class="nx">method</span><span class="p">.</span><span class="nx">some_method</span><span class="p">.</span><span class="nx">old_method_name</span>
      <span class="p">}</span>
    <span class="p">}</span>
  
    <span class="nx">plan</span> <span class="p">{</span>
      <span class="c1"># 계획(plan) 데이터 암호화/복호화</span>
      <span class="nx">method</span> <span class="p">=</span> <span class="nx">method</span><span class="p">.</span><span class="nx">some_method</span><span class="p">.</span><span class="nx">some_method_name</span>
      <span class="nx">fallback</span> <span class="p">{</span>     <span class="c1"># (선택사항)</span>
        <span class="c1"># 위의 method가 실패할 경우 사용할 method</span>
        <span class="nx">method</span> <span class="p">=</span> <span class="nx">method</span><span class="p">.</span><span class="nx">some_method</span><span class="p">.</span><span class="nx">old_method_name</span>
      <span class="p">}</span>
    <span class="p">}</span>
  
    <span class="nx">remote_state_data_sources</span> <span class="p">{</span>
      <span class="c1"># terraform_remote_state 데이터 소스 지정</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div>    </div>
  </li>
  <li>위의 사용 형태를 조금 더 자세하게 알아보겠습니다.
    <div class="language-tf highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">terraform</span> <span class="p">{</span>
  <span class="nx">encryption</span> <span class="p">{</span>
    <span class="c1">## Step 1: 암호화 되지 않은 method:</span>
    <span class="nx">method</span> <span class="s2">"unencrypted"</span> <span class="s2">"migrate"</span> <span class="p">{}</span>
  
    <span class="c1">## Step 2: key provider 지정 - 아래의 예제에서는 pbkdf2 키 프로바이더를 사용하고 리소스 이름을 mykey로 지정합니다.</span>
    <span class="nx">key_provider</span> <span class="s2">"pbkdf2"</span> <span class="s2">"mykey"</span> <span class="p">{</span>
      <span class="c1"># 암호화 키를 지정합니다. (pbkdf2는 16자 이상 지정이 필요합니다.)</span>
      <span class="nx">passphrase</span> <span class="p">=</span> <span class="s2">"correct-horse-battery-staple"</span>
  
      <span class="c1"># 암호화 키를 암호화 method에 맞게 조정합니다. (aes-gcm은 32자로 지정합니다.)</span>
      <span class="nx">key_length</span> <span class="p">=</span> <span class="mi">32</span>
        
      <span class="c1"># Specify the number of iterations (min. 200.000, default: 600.000)</span>
        <span class="c1">## The work factor for PBKDF2 is implemented through an iteration count, which should set differently based on the internal hashing algorithm used.</span>
            <span class="c1">## PBKDF2-HMAC-SHA1: 1,300,000 iterations</span>
            <span class="c1">## PBKDF2-HMAC-SHA256: 600,000 iterations</span>
            <span class="c1">## PBKDF2-HMAC-SHA512: 210,000 iterations</span>
  
      <span class="c1"># PBKDF2에서 사용할 반복 횟수를 지정하십시오 (최소 200,000, 기본값: 600,000)</span>
            <span class="c1">## PBKDF2-HMAC-SHA1: 1,300,000 iterations</span>
            <span class="c1">## PBKDF2-HMAC-SHA256: 600,000 iterations</span>
            <span class="c1">## PBKDF2-HMAC-SHA512: 210,000 iterations</span>
      <span class="nx">iterations</span> <span class="p">=</span> <span class="mi">600000</span>
        
      <span class="c1"># 암호화 salt 길이를 byte로 지정하십시오. (기본값: 32)</span>
      <span class="nx">salt_length</span> <span class="p">=</span> <span class="mi">32</span>
        
      <span class="c1"># 해시함수를 지정하십시오. (sha256 or sha512, default: sha512)</span>
      <span class="nx">hash_function</span> <span class="p">=</span> <span class="s2">"sha512"</span>  
    <span class="p">}</span>
  
    <span class="c1">## Step 3: 암호화 method를 지정하십시오 - 아래의 예제에서는 aes_gcm 암호화 method를 사용하고 리소스 이름을 new_method로 지정합니다.</span>
    <span class="nx">method</span> <span class="s2">"aes_gcm"</span> <span class="s2">"new_method"</span> <span class="p">{</span>
      <span class="c1"># 암호화 키를 지정합니다. 앞에서 만든 my_key 리소스를 지정합니다.</span>
      <span class="nx">keys</span> <span class="p">=</span> <span class="nx">key_provider</span><span class="p">.</span><span class="nx">pbkdf2</span><span class="p">.</span><span class="nx">mykey</span>
    <span class="p">}</span>
  
    <span class="nx">state</span> <span class="p">{</span>
      <span class="c1"># 상태 파일의 암호화 방법을 지정합니다.</span>
  
      <span class="c1">## Step 4: 암호화 method를 지정하십시오.</span>
      <span class="nx">method</span> <span class="p">=</span> <span class="nx">method</span><span class="p">.</span><span class="nx">aes_gcm</span><span class="p">.</span><span class="nx">new_method</span>
  
      <span class="c1">## Step 5: Step 4가 실패할 경우 사용할 "fallback" method를 지정하십시오.</span>
      <span class="nx">fallback</span> <span class="p">{</span>
        <span class="c1">## "unencrypted" method를 사용합니다.</span>
        <span class="nx">method</span> <span class="p">=</span> <span class="nx">method</span><span class="p">.</span><span class="nx">unencrypted</span><span class="p">.</span><span class="nx">migrate</span>
      <span class="p">}</span>
  
      <span class="c1">## Step 6: "tofu apply" 적용</span>
  
      <span class="c1">## Step 7: 암호화를 강제 하고 싶으면 fallback 블록을 삭제하고 아래의 enforce = true 옵션을 추가하십시오.</span>
      <span class="c1"># enforced = true</span>
    <span class="p">}</span>
  
    <span class="c1">## Step 8: 계획(plan)도 암호화 하고 싶다면 Step 4 ~ 7을 반복하십시오.</span>
  <span class="p">}</span>
<span class="p">}</span>  
</code></pre></div>    </div>
  </li>
  <li>암호화 실습
    <div class="language-tf highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># main.tf</span>
  
<span class="k">terraform</span> <span class="p">{</span>
  <span class="nx">encryption</span> <span class="p">{</span>
    <span class="nx">key_provider</span> <span class="s2">"pbkdf2"</span> <span class="s2">"my_passphrase"</span> <span class="p">{</span>
      <span class="c1">## Enter a passphrase here:</span>
      <span class="nx">passphrase</span> <span class="p">=</span> <span class="s2">"ChangeIt_123abcd"</span>
    <span class="p">}</span>
  
    <span class="nx">method</span> <span class="s2">"aes_gcm"</span> <span class="s2">"my_method"</span> <span class="p">{</span>
      <span class="nx">keys</span> <span class="p">=</span> <span class="nx">key_provider</span><span class="p">.</span><span class="nx">pbkdf2</span><span class="p">.</span><span class="nx">my_passphrase</span>
    <span class="p">}</span>
  
    <span class="c1">## Remove this after the migration:</span>
    <span class="nx">method</span> <span class="s2">"unencrypted"</span> <span class="s2">"migration"</span> <span class="p">{</span>
    <span class="p">}</span>
  
    <span class="nx">state</span> <span class="p">{</span>
      <span class="nx">method</span> <span class="p">=</span> <span class="nx">method</span><span class="p">.</span><span class="nx">aes_gcm</span><span class="p">.</span><span class="nx">my_method</span>
  
      <span class="c1">## Remove the fallback block after migration:</span>
      <span class="nx">fallback</span><span class="p">{</span>
        <span class="nx">method</span> <span class="p">=</span> <span class="nx">method</span><span class="p">.</span><span class="nx">unencrypted</span><span class="p">.</span><span class="nx">migration</span>
      <span class="p">}</span>
      <span class="c1">## Enable this after migration:</span>
      <span class="c1">#enforced = true</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div>    </div>
    <ul>
      <li>실행
        <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>tofu init <span class="o">&amp;&amp;</span> tofu apply <span class="nt">-auto-approve</span>
<span class="nv">$ </span>tofu state list 
<span class="nv">$ </span>tofu show 
    
<span class="c"># 상태 파일 암호화 확인</span>
<span class="nv">$ </span><span class="nb">cat </span>terraform.tfstate | jq
<span class="c"># =&gt; {</span>
<span class="c">#      "serial": 1,</span>
<span class="c">#      "lineage": "35fa959f-d819-8d1b-e036-c81f4827cf21",</span>
<span class="c">#      "meta": {</span>
<span class="c">#        "key_provider.pbkdf2.my_passphrase": "eyJzYWx0IjoiRGJXSzhyR3hBSHdQOUoxSmZncTYyaEJFSW9LaVZGNy9GK2JjSmlxQTBiT</span>
<span class="c">#         T0iLCJpdGVyYXRpb25zIjo2MDAwMDAsImhhc2hfZnVuY3Rpb24iOiJzaGE1MTIiLCJrZXlfbGVuZ3RoIjozMn0="</span>
<span class="c">#      },</span>
<span class="c">#      "encrypted_data": "EIuEr7sU9kb37t11Oy2JmbI1F/WFOYPuBjUlVV//IlrXVQYWyhlgw+JXu8m+2cztDGbkNIZ5h/giflO4nCUESI3mSP</span>
<span class="c">#         D8ZGETc80hR/JptGIv03RnKcyYXqzwoFDHS/7D8I4E5/itpBnmWCAXsFUTpoJ/vKySl3DfUrd/KFDKm0Db5RK2BjYywF+BeexpL7l//EZM</span>
<span class="c">#         zvyXkz0Tx85b+7q6SopHwUm1FztxSjqL2yiz1uZlyv5cJgUFHpsYV4geF/geMmDc5Kf9sysRNiJYaEJSrg==",</span>
<span class="c">#      "encryption_version": "v0"</span>
<span class="c">#    }    </span>
</code></pre></div>        </div>
      </li>
      <li>암호화 된 것을 확인할 수 있습니다.</li>
      <li>암호화가 apply 된 이후에는 <code class="language-plaintext highlighter-rouge">method "unencrypt"</code>와 <code class="language-plaintext highlighter-rouge">fallback</code> 블록을 삭제하고, <code class="language-plaintext highlighter-rouge">enforced = true</code> 옵션을 추가하여 암호화를 강제할 수 있습니다.</li>
    </ul>
  </li>
  <li>복호화 실습
    <div class="language-tf highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">terraform</span> <span class="p">{</span>
  <span class="nx">encryption</span> <span class="p">{</span>
    <span class="nx">key_provider</span> <span class="s2">"pbkdf2"</span> <span class="s2">"my_passphrase"</span> <span class="p">{</span>
      <span class="c1">## Enter a passphrase here:</span>
      <span class="nx">passphrase</span> <span class="p">=</span> <span class="s2">"ChangeIt_123abcd"</span>
    <span class="p">}</span>
  
    <span class="nx">method</span> <span class="s2">"aes_gcm"</span> <span class="s2">"my_method"</span> <span class="p">{</span>
      <span class="nx">keys</span> <span class="p">=</span> <span class="nx">key_provider</span><span class="p">.</span><span class="nx">pbkdf2</span><span class="p">.</span><span class="nx">my_passphrase</span>
    <span class="p">}</span>
  
    <span class="c1">## Remove this after the migration:</span>
    <span class="nx">method</span> <span class="s2">"unencrypted"</span> <span class="s2">"migration"</span> <span class="p">{</span>
    <span class="p">}</span>
  
    <span class="nx">state</span> <span class="p">{</span>
      <span class="nx">method</span> <span class="p">=</span> <span class="nx">method</span><span class="p">.</span><span class="nx">unencrypted</span><span class="p">.</span><span class="nx">migration</span>
  
      <span class="c1">## Remove the fallback block after migration:</span>
      <span class="nx">fallback</span><span class="p">{</span>
        <span class="nx">method</span> <span class="p">=</span> <span class="nx">method</span><span class="p">.</span><span class="nx">aes_gcm</span><span class="p">.</span><span class="nx">my_method</span>
      <span class="p">}</span>
      <span class="c1"># Enable this after migration:</span>
      <span class="nx">enforced</span> <span class="p">=</span> <span class="kc">false</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div>    </div>
    <ul>
      <li>실행
        <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>tofu apply <span class="nt">-auto-approve</span>
    
<span class="c"># 상태 파일 복호화 확인</span>
<span class="nv">$ </span><span class="nb">cat </span>terraform.tfstate | jq
<span class="c"># =&gt; {</span>
<span class="c">#      "version": 4,</span>
<span class="c">#      "terraform_version": "1.7.3",</span>
<span class="c">#      "serial": 1,</span>
<span class="c">#      "lineage": "35fa959f-d819-8d1b-e036-c81f4827cf21",</span>
<span class="c">#      "outputs": {},</span>
<span class="c">#      "resources": [],</span>
<span class="c">#      "check_results": null</span>
<span class="c">#    }    </span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>복호화가 정상적으로 되었음을 확인할 수 있습니다.
실습에서는 암호화키를 하드코딩하였지만,
실제 사용할때는 암호화 키를 변수로 입력 받도록 하거나, AWS KMS 와 같이 관리되는 키를 사용하면 보안성을 높일 수 있을것 같습니다.</li>
</ul>

<h4 id="실습-removed-block">[실습] Removed Block</h4>

<ul>
  <li><code class="language-plaintext highlighter-rouge">removed</code> 블록을 사용하면 실제 인프라스트럭쳐에 프로비저닝된 리소스는 삭제하지 않고 상태파일에서만 삭제할 수 있습니다.</li>
  <li>사용 방법
    <ul>
      <li>먼저 리소스를 생성 합니다.
        <div class="language-tf highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">resource</span> <span class="s2">"local_file"</span> <span class="s2">"test"</span> <span class="p">{</span>
  <span class="nx">content</span> <span class="p">=</span> <span class="s2">"Hello world!"</span>
  <span class="nx">filename</span> <span class="p">=</span> <span class="s2">"test.txt"</span>
<span class="p">}</span> 
</code></pre></div>        </div>
      </li>
      <li><code class="language-plaintext highlighter-rouge">removed</code> 블록을 추가하여 상태파일에서만 삭제합니다.
        <div class="language-tf highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">removed</span> <span class="p">{</span>
  <span class="nx">from</span> <span class="p">=</span> <span class="nx">local_file</span><span class="p">.</span><span class="nx">test</span>
<span class="p">}</span>
</code></pre></div>        </div>
      </li>
      <li>실행을 하면 <code class="language-plaintext highlighter-rouge">local_file.test</code> 리소스는 상태 파일에서 삭제 되지만, 프로비저닝된 test.txt는 그대로 남아있음을 확인 할 수 있습니다.</li>
      <li><code class="language-plaintext highlighter-rouge">removed</code> 블록을 apply 한 이후에 <code class="language-plaintext highlighter-rouge">resource</code> 블록을 삭제해도 됩니다.</li>
    </ul>
  </li>
  <li>실습
    <div class="language-tf highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># main.tf</span>
<span class="k">data</span> <span class="s2">"aws_ami"</span> <span class="s2">"ubuntu"</span> <span class="p">{</span>
    <span class="nx">most_recent</span> <span class="p">=</span> <span class="kc">true</span>
  
    <span class="nx">filter</span> <span class="p">{</span>
        <span class="nx">name</span>   <span class="p">=</span> <span class="s2">"name"</span>
        <span class="nx">values</span> <span class="p">=</span> <span class="p">[</span><span class="s2">"ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*"</span><span class="p">]</span>
    <span class="p">}</span>
  
    <span class="nx">filter</span> <span class="p">{</span>
        <span class="nx">name</span>   <span class="p">=</span> <span class="s2">"virtualization-type"</span>
        <span class="nx">values</span> <span class="p">=</span> <span class="p">[</span><span class="s2">"hvm"</span><span class="p">]</span>
    <span class="p">}</span>
  
    <span class="nx">owners</span> <span class="p">=</span> <span class="p">[</span><span class="s2">"099720109477"</span><span class="p">]</span> <span class="c1"># Canonical</span>
<span class="p">}</span>
  
<span class="k">variable</span> <span class="s2">"instance_tags"</span> <span class="p">{</span>
  <span class="nx">type</span> <span class="p">=</span> <span class="nx">list</span><span class="p">(</span><span class="nx">string</span><span class="p">)</span>
  <span class="nx">default</span> <span class="p">=</span> <span class="p">[</span><span class="s2">"web"</span><span class="p">,</span> <span class="s2">"app"</span><span class="p">]</span>
<span class="p">}</span>
  
<span class="k">resource</span> <span class="s2">"aws_instance"</span> <span class="s2">"this"</span> <span class="p">{</span>
  <span class="nx">count</span> <span class="p">=</span> <span class="nx">length</span><span class="p">(</span><span class="kd">var</span><span class="p">.</span><span class="nx">instance_tags</span><span class="p">)</span>
  <span class="nx">ami</span>                    <span class="p">=</span> <span class="k">data</span><span class="p">.</span><span class="nx">aws_ami</span><span class="p">.</span><span class="nx">ubuntu</span><span class="p">.</span><span class="nx">id</span>
  <span class="nx">instance_type</span>          <span class="p">=</span> <span class="s2">"t3.micro"</span>
  
  <span class="nx">tags</span> <span class="p">=</span> <span class="p">{</span>
    <span class="nx">Name</span> <span class="p">=</span> <span class="kd">var</span><span class="p">.</span><span class="nx">instance_tags</span><span class="p">[</span><span class="nx">count</span><span class="p">.</span><span class="nx">index</span><span class="p">]</span>
  <span class="p">}</span>
<span class="p">}</span>
  
<span class="k">resource</span> <span class="s2">"aws_ssm_parameter"</span> <span class="s2">"this"</span> <span class="p">{</span>
  <span class="nx">count</span> <span class="p">=</span> <span class="nx">length</span><span class="p">(</span><span class="kd">var</span><span class="p">.</span><span class="nx">instance_tags</span><span class="p">)</span>
  <span class="nx">name</span>  <span class="p">=</span> <span class="kd">var</span><span class="p">.</span><span class="nx">instance_tags</span><span class="p">[</span><span class="nx">count</span><span class="p">.</span><span class="nx">index</span><span class="p">]</span>
  <span class="nx">type</span>  <span class="p">=</span> <span class="s2">"String"</span>
  <span class="nx">value</span> <span class="p">=</span> <span class="nx">aws_instance</span><span class="p">.</span><span class="nx">this</span><span class="p">[</span><span class="nx">count</span><span class="p">.</span><span class="nx">index</span><span class="p">].</span><span class="nx">id</span>
<span class="p">}</span>    
</code></pre></div>    </div>
    <ul>
      <li>실행
        <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>tofu init <span class="o">&amp;&amp;</span> tofu apply <span class="nt">-auto-approve</span>
<span class="c"># =&gt; Apply complete! Resources: 4 added, 0 changed, 0 destroyed.</span>
    
<span class="nv">$ </span>tree .terraform
<span class="nv">$ </span>tofu state <span class="nb">ls</span>
<span class="nv">$ </span>tofu show
<span class="nv">$ </span>tofu state show <span class="s1">'aws_ssm_parameter.this[0]'</span>
    
<span class="c"># tfstate 파일 확인</span>
<span class="nv">$ </span><span class="nb">cat </span>terraform.tfstate | jq
    
<span class="nv">$ </span><span class="nb">cat </span>terraform.tfstate | jq | <span class="nb">grep</span> <span class="nt">-E</span> <span class="s1">'"i-'</span>
<span class="c"># =&gt; "id": "i-036b9e057c2eb71d8",</span>
<span class="c">#    "id": "i-0f8f05caabc524b47",</span>
<span class="c">#    "value": "i-036b9e057c2eb71d8",</span>
<span class="c">#    "value": "i-0f8f05caabc524b47",    </span>
    
<span class="c"># parameters 정보 확인</span>
<span class="nv">$ </span>aws ssm describe-parameters | jq
<span class="nv">$ </span>aws ssm get-parameter <span class="nt">--name</span> <span class="s2">"web"</span>
<span class="nv">$ </span>aws ssm get-parameter <span class="nt">--name</span> <span class="s2">"web"</span> <span class="nt">--query</span> <span class="s2">"Parameter.Value"</span> <span class="nt">--output</span> text
<span class="c"># =&gt; i-036b9e057c2eb71d8</span>
<span class="nv">$ </span>aws ssm get-parameter <span class="nt">--name</span> <span class="s2">"app"</span>
<span class="nv">$ </span>aws ssm get-parameter <span class="nt">--name</span> <span class="s2">"app"</span> <span class="nt">--query</span> <span class="s2">"Parameter.Value"</span> <span class="nt">--output</span> text
<span class="c"># =&gt; i-0f8f05caabc524b47</span>
    
<span class="c"># EC2 목록 확인</span>
<span class="nv">$ </span>aws ec2 describe-instances <span class="nt">--query</span> <span class="s1">'Reservations[*].Instances[*].{InstanceID:InstanceId,PublicIP:PublicIpAddress,Name:Tags[?Key==`Name`]|[0].Value}'</span> <span class="nt">--output</span> json | jq <span class="nt">-r</span> <span class="s1">'.[][] | "\(.InstanceID)\t\(.PublicIP)\t\(.Name)"'</span>
<span class="c"># =&gt; i-036b9e057c2eb71d8     3.34.132.49     web</span>
<span class="c">#    i-0f8f05caabc524b47     3.35.51.118     app   </span>
</code></pre></div>        </div>
      </li>
      <li><code class="language-plaintext highlighter-rouge">removed</code> 블록 추가하여 ssm parameter와 EC2를 리소스는 그대로 두고 state 파일에서만 삭제하겠습니다.
        <div class="language-tf highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">data</span> <span class="s2">"aws_ami"</span> <span class="s2">"ubuntu"</span> <span class="p">{</span>
    <span class="nx">most_recent</span> <span class="p">=</span> <span class="kc">true</span>
    
    <span class="nx">filter</span> <span class="p">{</span>
        <span class="nx">name</span>   <span class="p">=</span> <span class="s2">"name"</span>
        <span class="nx">values</span> <span class="p">=</span> <span class="p">[</span><span class="s2">"ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*"</span><span class="p">]</span>
    <span class="p">}</span>
    
    <span class="nx">filter</span> <span class="p">{</span>
        <span class="nx">name</span>   <span class="p">=</span> <span class="s2">"virtualization-type"</span>
        <span class="nx">values</span> <span class="p">=</span> <span class="p">[</span><span class="s2">"hvm"</span><span class="p">]</span>
    <span class="p">}</span>
    
    <span class="nx">owners</span> <span class="p">=</span> <span class="p">[</span><span class="s2">"099720109477"</span><span class="p">]</span> <span class="c1"># Canonical</span>
<span class="p">}</span>
    
<span class="k">variable</span> <span class="s2">"instance_tags"</span> <span class="p">{</span>
  <span class="nx">type</span> <span class="p">=</span> <span class="nx">list</span><span class="p">(</span><span class="nx">string</span><span class="p">)</span>
  <span class="nx">default</span> <span class="p">=</span> <span class="p">[</span><span class="s2">"web"</span><span class="p">,</span> <span class="s2">"app"</span><span class="p">]</span>
<span class="p">}</span>
    
<span class="c1">## 리소스 정의 파일에서 주석처리하여 삭제</span>
<span class="c1"># resource "aws_instance" "this" {</span>
<span class="c1">#   count = length(var.instance_tags)</span>
<span class="c1">#   ami                    = data.aws_ami.ubuntu.id</span>
<span class="c1">#   instance_type          = "t3.micro"</span>
<span class="c1">#  </span>
<span class="c1">#   tags = {</span>
<span class="c1">#     Name = var.instance_tags[count.index]</span>
<span class="c1">#   }</span>
<span class="c1"># }</span>
    
<span class="c1">## 리소스 정의 파일에서 주석처리하여 삭제</span>
<span class="c1"># resource "aws_ssm_parameter" "this" {</span>
<span class="c1">#   count = length(var.instance_tags)</span>
<span class="c1">#   name  = var.instance_tags[count.index]</span>
<span class="c1">#   type  = "String"</span>
<span class="c1">#   value = aws_instance.this[count.index].id</span>
<span class="c1"># }    </span>
  
<span class="nx">removed</span> <span class="p">{</span> <span class="c1"># 추가</span>
  <span class="nx">from</span> <span class="p">=</span> <span class="nx">aws_ssm_parameter</span><span class="p">.</span><span class="nx">this</span>
<span class="p">}</span>
  
<span class="nx">removed</span> <span class="p">{</span> <span class="c1"># 추가</span>
  <span class="nx">from</span> <span class="p">=</span> <span class="nx">aws_instance</span><span class="p">.</span><span class="nx">this</span>
<span class="p">}</span>
</code></pre></div>        </div>
      </li>
      <li>실행
        <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>tofu apply <span class="nt">-auto-approve</span>
<span class="c"># =&gt; ...</span>
<span class="c">#    # aws_ssm_parameter.this[0] will be removed from the OpenTofu state but will not be destroyed</span>
<span class="c">#    ...</span>
<span class="c">#    # aws_ssm_parameter.this[1] will be removed from the OpenTofu state but will not be destroyed</span>
<span class="c">#    Apply complete! Resources: 0 added, 0 changed, 0 destroyed.</span>

<span class="c"># tfstate 파일 확인</span>
<span class="nv">$ </span><span class="nb">cat </span>terraform.tfstate | jq
    
<span class="nv">$ </span><span class="nb">cat </span>terraform.tfstate | jq | <span class="nb">grep</span> <span class="nt">-E</span> <span class="s1">'"i-'</span>
<span class="c"># =&gt; &lt;결과 없음&gt;         </span>
</code></pre></div>        </div>
        <p>상태파일에서 삭제된것을 확인할 수 있었습니다. AWS에서 리소스를 확인해보겠습니다.</p>
        <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>aws ssm describe-parameters | jq
<span class="nv">$ </span>aws ssm get-parameter <span class="nt">--name</span> <span class="s2">"web"</span>
<span class="nv">$ </span>aws ssm get-parameter <span class="nt">--name</span> <span class="s2">"web"</span> <span class="nt">--query</span> <span class="s2">"Parameter.Value"</span> <span class="nt">--output</span> text
<span class="c"># =&gt; i-036b9e057c2eb71d8</span>
<span class="nv">$ </span>aws ssm get-parameter <span class="nt">--name</span> <span class="s2">"app"</span>
<span class="nv">$ </span>aws ssm get-parameter <span class="nt">--name</span> <span class="s2">"app"</span> <span class="nt">--query</span> <span class="s2">"Parameter.Value"</span> <span class="nt">--output</span> text
<span class="c"># =&gt; i-0f8f05caabc524b47</span>
    
<span class="nv">$ </span>aws ec2 describe-instances <span class="nt">--query</span> <span class="s1">'Reservations[*].Instances[*].{InstanceID:InstanceId,PublicIP:PublicIpAddress,Name:Tags[?Key==`Name`]|[0].Value}'</span> <span class="nt">--output</span> json | jq <span class="nt">-r</span> <span class="s1">'.[][] | "\(.InstanceID)\t\(.PublicIP)\t\(.Name)"'</span>
<span class="c"># =&gt; i-036b9e057c2eb71d8     3.34.132.49     web</span>
<span class="c">#    i-0f8f05caabc524b47     3.35.51.118     app</span>
</code></pre></div>        </div>
        <p>상태 파일에서는 없어졌지만 AWS에 프로비저닝된 리소스는 남아있는것을 확인할 수 있습니다.</p>
      </li>
      <li>이렇게하여 <code class="language-plaintext highlighter-rouge">removed</code> 블록을 실습해보았습니다. AWS에서 실습한 리소스를 삭제하시거나, 
추가적으로 import 블록을 사용하여 상태파일에 다시 넣은 다음 destroy 하는것을 실습해보는것도 좋을것 같습니다.</li>
    </ul>
  </li>
</ul>

<h4 id="실습-test">[실습] Test</h4>

<ul>
  <li>OpenTofu에서는 <code class="language-plaintext highlighter-rouge">tofu test</code>라는 명령을 통해 구성파일의 내용을 테스트 할 수 있습니다. 실제 인프라를 프로비저닝하고 원하는 조건이 맞는지 확인하는 과정을 거칩니다.
<code class="language-plaintext highlighter-rouge">postcondition</code>과 유사한데 <strong>다른 점</strong>은 테스트 중에 <strong>생성한 리소스는 자동으로 삭제된다</strong>는 것입니다.</li>
  <li>사용 방법
    <ul>
      <li>먼저 구성파일과 같은 디렉터리 또는 <code class="language-plaintext highlighter-rouge">tests</code> 디렉터리 아래에 <code class="language-plaintext highlighter-rouge">*.tftest.hcl</code> 파일을 생성합니다.</li>
      <li><code class="language-plaintext highlighter-rouge">*.tftest.hcl</code> 파일에 테스트 코드를 작성합니다. 또한 테스트를 위한 변수파일 (.tfvars)가 필요하면 해당 파일도 <code class="language-plaintext highlighter-rouge">tests</code> 디렉터리에 만들면 테스트 시에만 사용 됩니다.</li>
      <li><code class="language-plaintext highlighter-rouge">*.tftest.hcl</code> 의 형태는 아래와 같습니다.
        <div class="language-tf highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">run</span> <span class="s2">"test"</span> <span class="p">{</span>
  <span class="nx">assert</span> <span class="p">{</span>
    <span class="nx">condition</span>     <span class="p">=</span> <span class="nx">file</span><span class="p">(</span><span class="nx">local_file</span><span class="p">.</span><span class="nx">test</span><span class="p">.</span><span class="nx">filename</span><span class="p">)</span> <span class="p">==</span> <span class="s2">"Hello world!"</span>
    <span class="nx">error_message</span> <span class="p">=</span> <span class="s2">"Incorrect content in </span><span class="k">${</span><span class="nx">local_file</span><span class="p">.</span><span class="nx">test</span><span class="p">.</span><span class="nx">filename</span><span class="k">}</span><span class="s2">."</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>실습
    <ul>
      <li>main.tf 작성
        <div class="language-tf highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># main.tf</span>
<span class="k">variable</span> <span class="s2">"test"</span> <span class="p">{</span>
  <span class="nx">type</span> <span class="p">=</span> <span class="nx">string</span>
<span class="p">}</span>
    
<span class="k">resource</span> <span class="s2">"local_file"</span> <span class="s2">"this"</span> <span class="p">{</span>
  <span class="nx">filename</span> <span class="p">=</span> <span class="s2">"</span><span class="k">${</span><span class="nx">path</span><span class="p">.</span><span class="k">module}</span><span class="s2">/test.txt"</span>
  <span class="nx">content</span>  <span class="p">=</span> <span class="kd">var</span><span class="p">.</span><span class="nx">test</span>
<span class="p">}</span>     
</code></pre></div>        </div>
      </li>
      <li>tests/main.tftest.hcl 작성
        <div class="language-tf highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># tests/main.tftest.hcl           </span>
<span class="nx">run</span> <span class="s2">"test"</span> <span class="p">{</span>
  <span class="nx">assert</span> <span class="p">{</span>
    <span class="nx">condition</span>     <span class="p">=</span> <span class="nx">file</span><span class="p">(</span><span class="nx">local_file</span><span class="p">.</span><span class="nx">this</span><span class="p">.</span><span class="nx">filename</span><span class="p">)</span> <span class="p">==</span> <span class="kd">var</span><span class="p">.</span><span class="nx">test</span>
    <span class="nx">error_message</span> <span class="p">=</span> <span class="s2">"Incorrect content in file"</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div>        </div>
      </li>
      <li>실행 후 확인
        <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>tofu init
<span class="nv">$ </span>tree .terraform
    
<span class="c"># Test 실행</span>
<span class="nv">$ </span>tofu <span class="nb">test</span>
<span class="c"># =&gt; tests/main.tftest.hcl... fail</span>
<span class="c">#      run "test"... fail</span>
<span class="c">#      │ Error: No value for required variable</span>
</code></pre></div>        </div>
      </li>
      <li>변수가 없다며 테스트가 실패합니다. 변수를 넣어서 다시 실행해 보겠습니다.
        <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 방법1. -var 로 변수를 넣어서 실행</span>
<span class="nv">$ </span>tofu <span class="nb">test</span> <span class="nt">-var</span> <span class="s1">'test=Hello world!'</span>
<span class="c"># =&gt; tests/main.tftest.hcl... pass</span>
<span class="c">#      run "test"... pass</span>
<span class="c">#    Success! 1 passed, 0 failed.</span>
    
<span class="c"># 방법2. tests/terraform.tfvars 파일 작성</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="s1">'test = "T101 4th thank you!"'</span> <span class="o">&gt;</span> tests/terraform.tfvars 
<span class="nv">$ </span>tofu <span class="nb">test</span>
<span class="c"># =&gt; tests/main.tftest.hcl... pass</span>
<span class="c">#      run "test"... pass</span>
<span class="c">#    Success! 1 passed, 0 failed.</span>
    
<span class="nv">$ </span>tofu state list
<span class="c"># =&gt; No state file was found!</span>
<span class="c"># 상태파일이 없다고 나오는데, 테스트를 위해 프로비저닝한 리소스는 자동으로 삭제되어서 별도로 상태파일을 생성하지 않는것 같습니다.</span>

<span class="nv">$ </span><span class="nb">cat </span>test.txt
<span class="c"># =&gt; cat: test.txt: No such file or directory</span>
<span class="c"># 테스트된 파일은 삭제된것을 확인할 수 있습니다.</span>
        
<span class="c"># Apply 확인</span>
<span class="nv">$ </span>tofu apply <span class="nt">-auto-approve</span>
<span class="nv">$ </span>tofu state list
<span class="c"># =&gt; local_file.this    </span>
<span class="nv">$ </span><span class="nb">cat </span>test.txt
<span class="c"># =&gt; T101 4th thank you!</span>
</code></pre></div>        </div>
      </li>
      <li>혹시나 해서 <code class="language-plaintext highlighter-rouge">tofu test</code>를 한번더 실행한 다음 결과를 확인해보겠습니다.
        <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>tofu <span class="nb">test</span>
<span class="nv">$ </span>tofu state list
<span class="c"># =&gt; local_file.this</span>
<span class="nv">$ </span><span class="nb">cat </span>test.txt
<span class="c"># =&gt; cat: test.txt: No such file or directory</span>
</code></pre></div>        </div>
      </li>
      <li>이미 프로비저닝 된 상태에서 test를 할 경우 <strong>상태파일은 남아있지만 실제 리소스(여기서는 파일)은 삭제되</strong>는것 같습니다. 
프로덕션 환경에서 사용시 주의가 필요해보입니다. :sweat_smile:</li>
    </ul>
  </li>
</ul>

<h4 id="실습-terraform을-opentofu-17로-이전하기">[실습] Terraform을 OpenTofu 1.7로 이전하기</h4>

<ul>
  <li>OpenTofu 1.7로 Terraform을 이전하는 방법을 실습해보겠습니다. Terraform 버전은 최소 1.8.2 이상을 사용할것을 권고한다 합니다.
여기에서는 Terraform 1.8.5 버전으로 실습해 보겠습니다.</li>
  <li>Tenv로 Terraform 1.8.5 설치
    <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>tenv tf <span class="nb">install </span>1.8.5 
<span class="c"># =&gt; ...</span>
<span class="c">#    Installation of Terraform 1.8.5 successful</span>
<span class="nv">$ </span>tenv tf list
<span class="c"># =&gt;   1.8.5 (never used)</span>
<span class="nv">$ </span>tenv tf detect
</code></pre></div>    </div>
  </li>
  <li>main.tf 파일 작성
    <div class="language-tf highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># main.tf</span>
<span class="k">data</span> <span class="s2">"aws_ami"</span> <span class="s2">"ubuntu"</span> <span class="p">{</span>
    <span class="nx">most_recent</span> <span class="p">=</span> <span class="kc">true</span>
  
    <span class="nx">filter</span> <span class="p">{</span>
        <span class="nx">name</span>   <span class="p">=</span> <span class="s2">"name"</span>
        <span class="nx">values</span> <span class="p">=</span> <span class="p">[</span><span class="s2">"ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*"</span><span class="p">]</span>
    <span class="p">}</span>
  
    <span class="nx">filter</span> <span class="p">{</span>
        <span class="nx">name</span>   <span class="p">=</span> <span class="s2">"virtualization-type"</span>
        <span class="nx">values</span> <span class="p">=</span> <span class="p">[</span><span class="s2">"hvm"</span><span class="p">]</span>
    <span class="p">}</span>
  
    <span class="nx">owners</span> <span class="p">=</span> <span class="p">[</span><span class="s2">"099720109477"</span><span class="p">]</span> <span class="c1"># Canonical</span>
<span class="p">}</span>
  
<span class="k">variable</span> <span class="s2">"instance_tags"</span> <span class="p">{</span>
  <span class="nx">type</span> <span class="p">=</span> <span class="nx">list</span><span class="p">(</span><span class="nx">string</span><span class="p">)</span>
  <span class="nx">default</span> <span class="p">=</span> <span class="p">[</span><span class="s2">"web"</span><span class="p">,</span> <span class="s2">"app"</span><span class="p">]</span>
<span class="p">}</span>
  
<span class="k">resource</span> <span class="s2">"aws_instance"</span> <span class="s2">"this"</span> <span class="p">{</span>
  <span class="nx">count</span> <span class="p">=</span> <span class="nx">length</span><span class="p">(</span><span class="kd">var</span><span class="p">.</span><span class="nx">instance_tags</span><span class="p">)</span>
  <span class="nx">ami</span>                    <span class="p">=</span> <span class="k">data</span><span class="p">.</span><span class="nx">aws_ami</span><span class="p">.</span><span class="nx">ubuntu</span><span class="p">.</span><span class="nx">id</span>
  <span class="nx">instance_type</span>          <span class="p">=</span> <span class="s2">"t3.micro"</span>
  
  <span class="nx">tags</span> <span class="p">=</span> <span class="p">{</span>
    <span class="nx">Name</span> <span class="p">=</span> <span class="kd">var</span><span class="p">.</span><span class="nx">instance_tags</span><span class="p">[</span><span class="nx">count</span><span class="p">.</span><span class="nx">index</span><span class="p">]</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div>    </div>
  </li>
  <li>실행 후 확인
    <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 초기화</span>
<span class="nv">$ </span>terraform init
  
<span class="c"># 프로바이더 정보 확인</span>
<span class="nv">$ </span>tree .terraform 
<span class="c"># =&gt; .terraform</span>
<span class="c">#    └── providers</span>
<span class="c">#        └── registry.terraform.io</span>
<span class="c">#            └── hashicorp</span>
<span class="c">#                └── aws</span>
<span class="c">#                    └── 5.61.0  </span>
                                 
<span class="c"># 적용</span>
<span class="nv">$ </span>terraform apply <span class="nt">-auto-approve</span>
<span class="c"># =&gt; Apply complete! Resources: 2 added, 0 changed, 0 destroyed.</span>
  
<span class="c"># EC2 확인</span>
<span class="nv">$ </span><span class="k">while </span><span class="nb">true</span><span class="p">;</span> <span class="k">do </span>aws ec2 describe-instances <span class="nt">--query</span> <span class="s2">"Reservations[*].Instances[*].{PublicIPAdd:PublicIpAddress,InstanceName:Tags[?Key=='Name']|[0].Value,Status:State.Name}"</span> <span class="nt">--filters</span> <span class="nv">Name</span><span class="o">=</span>instance-state-name,Values<span class="o">=</span>running <span class="nt">--output</span> text <span class="p">;</span> <span class="nb">echo</span> <span class="s2">"------------------------------"</span> <span class="p">;</span> <span class="nb">sleep </span>1<span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; app     3.38.165.56     running</span>
<span class="c">#    web     43.203.241.233  running</span>
  
<span class="c"># 상태 확인</span>
<span class="nv">$ </span>terraform state list
<span class="c"># =&gt; data.aws_ami.ubuntu</span>
<span class="c">#    aws_instance.this[0]</span>
<span class="c">#    aws_instance.this[1]</span>
  
<span class="c"># tfstate 파일 확인</span>
<span class="nv">$ </span><span class="nb">cat </span>terraform.tfstate | jq
</code></pre></div>    </div>
  </li>
  <li>마이그레이션
    <ul>
      <li>현재 최신 상태여부 확인
        <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>terraform plan 
<span class="c"># =&gt; No changes. Your infrastructure matches the configuration.</span>
</code></pre></div>        </div>
      </li>
      <li>tfstate 파일과 구성 파일 백업
        <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cp </span>terraform.tfstate terraform.tfstate.bak
<span class="nv">$ </span><span class="nb">cp </span>main.tf main.tf.bak
</code></pre></div>        </div>
      </li>
      <li>Terraform과 OpenTofu 간의 차이로 인한 수정이 불가피한 코드 변경
        <ul>
          <li>S3 Backend 사용시 다음의 변경이 필요합니다.
            <ul>
              <li><code class="language-plaintext highlighter-rouge">skip_s3_checksum</code> 옵션을 사용중이라면 OpenTofu는 필요로 하지 않기 때문에 삭제합니다.</li>
              <li><code class="language-plaintext highlighter-rouge">endpoints</code> =&gt; <code class="language-plaintext highlighter-rouge">sso</code> 옵션을 사용중이거나 <code class="language-plaintext highlighter-rouge">AWS_ENDPOINT_URL</code> 환경 변수를 사용중이라면 삭제합니다.</li>
            </ul>
          </li>
          <li><code class="language-plaintext highlighter-rouge">removed</code> 블록 삭제 : OpenTofu의 <code class="language-plaintext highlighter-rouge">removed</code> 블록과는 동작 방식 차이가 있기 때문에 삭제합니다.</li>
        </ul>
      </li>
      <li>OpenTofu 초기화 및 Plan
        <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 초기화</span>
<span class="nv">$ </span>tofu init
<span class="nv">$ </span>tree .terraform <span class="nt">-L</span> 5 
<span class="c"># =&gt; .terraform</span>
<span class="c">#    └── providers</span>
<span class="c">#        ├── registry.opentofu.org</span>
<span class="c">#        │   └── hashicorp</span>
<span class="c">#        │       └── aws</span>
<span class="c">#        │           └── 5.61.0</span>
<span class="c">#        └── registry.terraform.io</span>
<span class="c">#            └── hashicorp</span>
<span class="c">#                └── aws</span>
<span class="c">#                    └── 5.61.0</span>
    
<span class="c"># Plan</span>
<span class="nv">$ </span>tofu plan
<span class="c"># =&gt; No changes. Your infrastructure matches the configuration.</span>
</code></pre></div>        </div>
      </li>
      <li>정상적으로 Terraform을 OpenTofu로 마이그레이션 완료한것 같습니다. <code class="language-plaintext highlighter-rouge">.terraform</code> 디렉터리에는 Terraform용 프로바이더와 OpenTofu용 프로바이더 둘 다 존재하는것을 확인할 수 있습니다.</li>
      <li>이제 실제 적용해 보겠습니다.
        <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>tofu apply <span class="nt">-auto-approve</span>
<span class="c"># =&gt; No changes. Your infrastructure matches the configuration.</span>
<span class="c">#    OpenTofu has compared your real infrastructure against your configuration and found no differences, so no changes are needed.</span>
<span class="c">#    Apply complete! Resources: 0 added, 0 changed, 0 destroyed.</span>
    
<span class="c"># EC2 인스턴스에 신규 태그를 추가 후 apply 해보겠습니다.</span>
<span class="nv">$ </span>vi main.tf 
...
tags <span class="o">=</span> <span class="o">{</span>
  Name <span class="o">=</span> var.instance_tags[count.index]
  T101 <span class="o">=</span> <span class="s2">"end"</span>
<span class="o">}</span>
...
    
<span class="c"># 적용</span>
<span class="nv">$ </span>tofu apply <span class="nt">-auto-approve</span>
</code></pre></div>        </div>
        <p><img src="/assets/2024/t101-4th/20240803_terraform_w8_opentofu_2.png" alt="img.png" /></p>
      </li>
    </ul>
  </li>
  <li>실습 리소스 삭제
    <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>tofu destroy <span class="nt">-auto-approve</span>
</code></pre></div>    </div>
  </li>
</ul>

<h3 id="opentofu-18">OpenTofu 1.8</h3>

<ul>
  <li>OpenTofu 1.8이 지난 7월 29일 출시 되었습니다. 자세한 소식은 아래의 블로그에서 확인할 수 있습니다.
    <ul>
      <li><a href="https://opentofu.org/blog/opentofu-1-8-0/">OpenTofu 1.8 릴리즈 블로그</a></li>
      <li><a href="https://github.com/opentofu/opentofu/releases/tag/v1.8.0">OpenTofu 1.8 Github</a></li>
    </ul>
  </li>
  <li>주요 변경점은 아래와 같습니다.
    <ul>
      <li>variable과 locals를 module 소스와 backend 설정에 사용할 수 있게 되었습니다.</li>
      <li><code class="language-plaintext highlighter-rouge">.tofu</code> 확장자가 추가되었습니다. <code class="language-plaintext highlighter-rouge">.tf</code> 파일에 추가적으로 OpenTofu 전용 기능을 사용할 수 있습니다.</li>
      <li>테스트 프레임워크에서 사용할 수 있는 <code class="language-plaintext highlighter-rouge">override_resource</code>, <code class="language-plaintext highlighter-rouge">override_data</code>, <code class="language-plaintext highlighter-rouge">override_module</code>,
<code class="language-plaintext highlighter-rouge">mock_resource</code>, <code class="language-plaintext highlighter-rouge">mock_data</code>, <code class="language-plaintext highlighter-rouge">mock_module</code> 등이 추가되었습니다.</li>
      <li>Deprecation : <code class="language-plaintext highlighter-rouge">use_legacy_workflow</code> 가 S3 backend 설정에서 제거되었습니다.</li>
    </ul>
  </li>
  <li>1.8.0 사용 설정
    <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>tenv tofu list
<span class="c"># =&gt; * 1.7.3</span>
<span class="nv">$ </span>tenv tofu list-remote 
<span class="c"># =&gt; ...</span>
<span class="c">#    1.7.3 (installed)</span>
<span class="c">#    ...</span>
<span class="c">#    1.8.0 </span>
  
<span class="c"># 설치</span>
<span class="nv">$ </span>tenv tofu <span class="nb">install </span>1.8.0
<span class="nv">$ </span>tenv tofu list
<span class="nv">$ </span>tenv tofu use 1.8.0
<span class="nv">$ </span>tenv tofu detect
  
<span class="c"># 버전 확인</span>
<span class="nv">$ </span>tofu <span class="nt">-h</span>
<span class="nv">$ </span>tofu version
<span class="c"># =&gt; OpenTofu v1.8.0</span>
<span class="c">#    on darwin_arm64</span>
<span class="c">#    + provider registry.opentofu.org/hashicorp/aws v5.61.0</span>
</code></pre></div>    </div>
  </li>
</ul>

<h4 id="실습-early-variablelocals-evaluation">[실습] Early variable/locals evaluation</h4>

<p><img src="/assets/2024/t101-4th/20240803_terraform_w8_opentofu_3.png" alt="img.png" />
기존에는 variable을 Backend 설정이나 mobule 블록, 암호화 설정 등에서 사용하지 못하였는데 OpenTofu 1.8부터는 사용할 
수 있게 되었습니다. 실습을 통해 알아보겠습니다.</p>

<ul>
  <li>AWS S3 버킷 생성 : Backend State 저장용도
    <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>aws s3 mb s3://&lt;생성할 고유한 S3 버킷명&gt; <span class="nt">--region</span> ap-northeast-2 
<span class="nv">$ </span>aws s3 mb s3://t101-4th-tofu-1-8 <span class="nt">--region</span> ap-northeast-2
  
<span class="c"># 버킷 확인</span>
<span class="nv">$ </span>aws s3 <span class="nb">ls</span> 
</code></pre></div>    </div>
  </li>
  <li>main.tf 생성
    <div class="language-tf highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># main.tofu</span>
<span class="k">variable</span> <span class="s2">"s3bucket_myname"</span> <span class="p">{</span>
 <span class="nx">type</span> <span class="p">=</span> <span class="nx">string</span>
 <span class="nx">default</span> <span class="p">=</span> <span class="s2">"t101-4th-tofu-1-8"</span> <span class="c1"># 생성한 S3 버킷명</span>
<span class="p">}</span>
  
<span class="k">terraform</span> <span class="p">{</span>
  <span class="nx">backend</span> <span class="s2">"s3"</span> <span class="p">{</span>
    <span class="nx">bucket</span> <span class="p">=</span> <span class="kd">var</span><span class="p">.</span><span class="nx">s3bucket_myname</span>
    <span class="nx">key</span> <span class="p">=</span> <span class="s2">"terraform.tfstate"</span>
    <span class="nx">region</span> <span class="p">=</span> <span class="s2">"ap-northeast-2"</span>
    <span class="nx">encrypt</span> <span class="p">=</span> <span class="kc">true</span>
  <span class="p">}</span>
<span class="p">}</span>
  
<span class="k">data</span> <span class="s2">"aws_ami"</span> <span class="s2">"ubuntu"</span> <span class="p">{</span>
    <span class="nx">most_recent</span> <span class="p">=</span> <span class="kc">true</span>
  
    <span class="nx">filter</span> <span class="p">{</span>
        <span class="nx">name</span>   <span class="p">=</span> <span class="s2">"name"</span>
        <span class="nx">values</span> <span class="p">=</span> <span class="p">[</span><span class="s2">"ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*"</span><span class="p">]</span>
    <span class="p">}</span>
  
    <span class="nx">filter</span> <span class="p">{</span>
        <span class="nx">name</span>   <span class="p">=</span> <span class="s2">"virtualization-type"</span>
        <span class="nx">values</span> <span class="p">=</span> <span class="p">[</span><span class="s2">"hvm"</span><span class="p">]</span>
    <span class="p">}</span>
  
    <span class="nx">owners</span> <span class="p">=</span> <span class="p">[</span><span class="s2">"099720109477"</span><span class="p">]</span> <span class="c1"># Canonical</span>
<span class="p">}</span>
  
<span class="k">resource</span> <span class="s2">"aws_instance"</span> <span class="s2">"this"</span> <span class="p">{</span>
  <span class="nx">ami</span>                    <span class="p">=</span> <span class="k">data</span><span class="p">.</span><span class="nx">aws_ami</span><span class="p">.</span><span class="nx">ubuntu</span><span class="p">.</span><span class="nx">id</span>
  <span class="nx">instance_type</span>          <span class="p">=</span> <span class="s2">"t3.micro"</span>
  
  <span class="nx">tags</span> <span class="p">=</span> <span class="p">{</span>
    <span class="nx">Name</span> <span class="p">=</span> <span class="s2">"final-labs"</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div>    </div>
  </li>
  <li>초기화 및 프로바이더 확인
    <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 테라폼으로 확인</span>
<span class="nv">$ </span>terraform init
<span class="c"># =&gt; Initializing the backend...</span>
<span class="c">#    │ Error: Variables not allowed</span>
<span class="c">#    │   on main.tf line 8, in terraform:</span>
<span class="c">#    │    8:     bucket = var.s3bucket_myname</span>
<span class="c">#    │ Variables may not be used here.</span>
  
<span class="c"># OpenTofu 1.7.x로 확인</span>
<span class="nv">$ </span>tenv tofu use 1.7.3
<span class="nv">$ </span>tofu version
<span class="c"># =&gt; OpenTofu v1.7.3</span>
<span class="c">#    on darwin_arm64</span>
<span class="nv">$ </span>tofu init
<span class="c"># =&gt; Initializing the backend...</span>
<span class="c">#    │ Error: Variables not allowed</span>
<span class="c">#    │   on main.tf line 8, in terraform:</span>
<span class="c">#    │    8:     bucket = var.s3bucket_myname</span>
<span class="c">#    │ Variables may not be used here.</span>
  
<span class="c"># OpenTofu 1.8.0로 확인</span>
<span class="nv">$ </span>tenv tofu use 1.8.0
<span class="nv">$ </span>
<span class="c"># =&gt; OpenTofu v1.8.0</span>
<span class="c">#    on darwin_arm64</span>
<span class="nv">$ </span>tofu init
<span class="c"># =&gt; Initializing the backend...    </span>
<span class="c">#    ...</span>
<span class="c">#    OpenTofu has been successfully initialized!</span>
</code></pre></div>    </div>
    <p>Terraform이나 OpenTofu 1.7.x에서는 Backend 설정에 variable을 사용할 수 없었지만, OpenTofu 1.8.0에서는 사용할 수 있게 된것을 확인하였습니다.
이제 적용을 해서 실제 동작하는지 확인해보겠습니다.</p>
  </li>
  <li>실행
    <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>tofu apply <span class="nt">-auto-approve</span>
<span class="c"># =&gt; Apply complete! Resources: 1 added, 0 changed, 0 destroyed.</span>
  
<span class="c"># 확인</span>
<span class="nv">$ </span>aws s3 <span class="nb">ls </span>s3://t101-4th-tofu-1-8
<span class="c"># =&gt; 2024-08-03 21:04:56       4890 terraform.tfstate</span>
<span class="nv">$ </span><span class="nb">cat</span> .terraform/terraform.tfstate | <span class="nb">grep </span>bucket
<span class="c"># =&gt; "bucket": "t101-4th-tofu-1-8",  </span>
</code></pre></div>    </div>
    <p>잘 동작하여 상태파일이 S3 Backend에 잘 저장되었음을 확인할 수 있습니다.</p>
  </li>
  <li>실습 리소스 삭제
    <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 리소스 삭제</span>
<span class="nv">$ </span>tofu apply <span class="nt">-destroy</span> <span class="nt">-auto-approve</span>
  
<span class="c"># 상태파일은 아직 존재 합니다.</span>
<span class="nv">$ </span>aws s3 <span class="nb">ls </span>s3://t101-4th-tofu-1-8
  
<span class="c"># 상태파일을 삭제하고</span>
<span class="nv">$ </span>aws s3 <span class="nb">rm </span>s3://t101-4th-tofu-1-8 <span class="nt">--recursive</span>
<span class="c"># 버킷을 삭제합니다.</span>
<span class="nv">$ </span>aws s3 rb s3://t101-4th-tofu-1-8
  
<span class="c"># 버킷이 삭제되었음을 확인합니다.</span>
<span class="nv">$ </span>aws s3 <span class="nb">ls</span> 
</code></pre></div>    </div>
  </li>
</ul>

<h2 id="마치며">마치며</h2>

<p>이렇게 OpenTofu에 대하여 알아보았습니다.
Terraform이 계속 오픈소스 커뮤니티의 사랑을 받았었다면 좋았겠지만,
그러지 못하여 OpenTofu가 나온것을 보면 다소 씁쓸합니다.
아무쪼록 경쟁을 통해 둘 다 발전하는 모습을 보여주었으면 좋겠습니다.</p>

<p>이번 테라폼 기초 입문 실습 스터디 4기 과정을 통해 IaC가 무엇인지와
Terraform 사용 방법 등 많은것을 배울 수 있었습니다.
가장 큰 결실은 블로그를 만들었다는 것입니다!
회사 팀장님의 추천에 얼떨결에 시작하였고,
매주 블로그 글 쓰기가 벅차서 몇 번이고 포기 할뻔 했지만 
무사히 마칠 수 있어서 감개무량합니다. (이번주도 무사히 지나가야 하긴 합니다. :sweat_smile:)
앞으로도 꾸준히 배우고 누군가에게 도움이 될 수 있도록 
블로그에 기록하며 성장해 나가겠습니다.</p>

<p>함께 고생한 Gasida님을 비롯한 T101 4기 멤버분들도 모두 고생 많으셨습니다.
특히 매주 실습을 준비하고, 강의를 진행해주신 Gasida님과 조력자 분들께 감사드립니다.
다음 스터디도 참여할 수 있다면 또 열심히 달려보겠습니다.
다들 건강하시고 행복한 모습으로 다시 뵙기를 고대합니다! :smile:</p>]]></content><author><name></name></author><category term="terraform" /><category term="terraform," /><category term="cloud," /><category term="aws" /><summary type="html"><![CDATA[이번 주가 8주차이자 벌써 마지막주차입니다. T101 4기에서 마지막으로 알아볼 주제는 OpenTofu로 Terraform의 오픈소스 커뮤니티에서 포크한 버전으로 앞으로가 기대되는 프로젝트입니다.]]></summary></entry><entry><title type="html">[T101 4기] 테라폼으로 AWS EKS 배포</title><link href="https://sweetlittlebird.github.io/posts/2024-07-27-T101-Study-Terraform-Week-7/" rel="alternate" type="text/html" title="[T101 4기] 테라폼으로 AWS EKS 배포" /><published>2024-07-27T16:02:00+09:00</published><updated>2024-07-27T16:02:00+09:00</updated><id>https://sweetlittlebird.github.io/posts/T101%20Study%20-%20Terraform%20Week%207</id><content type="html" xml:base="https://sweetlittlebird.github.io/posts/2024-07-27-T101-Study-Terraform-Week-7/"><![CDATA[<h2 id="들어가며">들어가며</h2>

<p>이번 주에는 테라폼으로 AWS EKS 배포를 하는것을
<a href="https://product.kyobobook.co.kr/detail/S000202478097">테라폼으로 시작하는 IaC</a>를 통해 알아 보겠습니다.</p>

<blockquote>
  <p><img src="/assets/2024/t101-4th/20240614_terraform_book.jpg" alt="테라폼으로 시작하는 IaC" /></p>

  <p><a href="https://product.kyobobook.co.kr/detail/S000202478097">테라폼으로 시작하는 IaC</a></p>
</blockquote>

<h2 id="amazon-eks-blueprints-for-terraform">Amazon EKS Blueprints for Terraform</h2>

<h3 id="개요">개요</h3>

<ul>
  <li><a href="https://aws-ia.github.io/terraform-aws-eks-blueprints/">Amazon EKS BluePrints 소개 Link</a> / <a href="https://aws-ia.github.io/terraform-aws-eks-blueprints/faq/">FAQ Link</a></li>
  <li>Amazon EKS Blueprints for Terraform은 Terraform으로 Amazon EKS 클러스터를 구축하는 패턴들의 모음이며, Amazon EKS를 도입하는것이 얼마나 빠르고 쉬운지 보여줍니다.</li>
  <li>사용 방법은 Amazon EKS Blueprints를 <strong>참고해서</strong> 원하는 패턴을 선택하고 Terraform을 사용해서 클러스터를 구축하는 방법과, 필요한 스니펫들을 <strong>복사해서</strong> 사용할 수 있습니다.</li>
  <li>주의할 점
    <ul>
      <li>Amazon EKS Blueprints는 <strong>AWS CloudFormation</strong>을 사용하는 것이 아니라 <strong>Terraform</strong>을 사용합니다.</li>
      <li>있는 그대로 사용되는것을 권장하지 않으며, <strong>사용자의 환경에 맞게 수정</strong>해서 사용하는것을 권장합니다.</li>
      <li>패턴과 스니펫은 Terraform 모듈로 설계되지 않았으며, 모듈화를 하려면는 사용자가 직접 수행해야 합니다.</li>
      <li>Local 블록을 주로 사용하며, 특정 정보가 필요한 경우 (Route 53 호스트 영역 ID 등)만 Variable 블록을 사용합니다.</li>
      <li>복잡성을 줄이기위해 variable과 outputs가 최대한 사용되지 않았습니다.</li>
    </ul>
  </li>
</ul>

<h3 id="실습-eks-fargate-및-karpenter-배포">[실습] EKS Fargate 및 Karpenter 배포</h3>

<p>먼저 <code class="language-plaintext highlighter-rouge">EKS</code>와 <code class="language-plaintext highlighter-rouge">Fargate</code>, <code class="language-plaintext highlighter-rouge">Karpenter</code>에 대해 간략하게 알아보겠습니다.</p>

<p><img src="/assets/2024/t101-4th/20240727_terraform_w7_eks_karpenter_1.png" alt="EKS 소개" />
<code class="language-plaintext highlighter-rouge">EKS</code>는 Elastic Kubernetes Service의 약자로, AWS에서 제공하는 <strong>관리형 Kubernetes 서비스</strong>입니다.
EKS를 사용하면 컨테이너 예약, 가용성 관리, 클러스터의 데이터 저장 및 다른 주요 작업을 담당하는 Kubernetes 컨트롤 플레인의
가용성과 확장성을 쉽게 관리할 수 있습니다. EKS는 컨테이너를 EC2 상에서 운영하거나 Fargate를 통해 
탄력적으로 서버리스 방식으로 운영할 수도 있습니다.
<a href="https://aws.amazon.com/ko/eks/">AWS EKS 소개</a></p>

<p><img src="/assets/2024/t101-4th/20240727_terraform_w7_eks_karpenter_2.png" alt="Fargate와 기존 EC2 방식의 컨테이너 배포와 비교" />
<code class="language-plaintext highlighter-rouge">Fargate</code>는 AWS에서 제공하는 <strong>서버리스 컴퓨팅 엔진</strong>으로, 컨테이너를 별도의 <strong>서버 관리 없이, 컨테이너를 사용한 만큼만</strong> 비용을 지불할 수 있는 서비스입니다. <a href="https://aws.amazon.com/ko/fargate/">AWS Fargate 소개</a></p>

<p><img src="/assets/2024/t101-4th/20240727_terraform_w7_eks_karpenter_36.png" alt="Karpenter의 동작 방식 소개" />
<code class="language-plaintext highlighter-rouge">Karpenter</code>는 Kubernetes 클러스터에 동작하는 Node를 최적화하여 효율성과 비용을 최적화하는 오픈소스 프로젝트입니다.
Karpenter는 Node의 리소스 부족으로 스케쥴 되지 못한 pod들을 Node를 추가하여 스케쥴링하는 Autoscaling 기능 뿐만 아니라,
각각 Node들을 조각모음 하듯이 더 크고 저렴한 하나의 노드로 교체하는 등의 전략을 통해 비용을 최적화합니다.
<a href="https://aws.amazon.com/ko/blogs/containers/introducing-karpenter/">Karpenter 소개</a></p>

<ul>
  <li>이번에 실습할 패턴은 <code class="language-plaintext highlighter-rouge">Karpenter</code>를 통하여 Fargate에 서버리스 클러스터를 프로비저닝하는 것으로 먼저 EKS와 Fargate를 배포하고, 그 다음에 Karpenter를 배포하는 것을 실습해보겠습니다.</li>
</ul>

<h4 id="사전준비">사전준비</h4>
<ul>
  <li>awscli, terraform, kubectl, helm 등 설치
    <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 설치된 파일 버전 확인</span>
<span class="nv">$ </span>aws <span class="nt">--version</span>
<span class="c"># =&gt; aws-cli/2.17.14 Python/3.11.9 Darwin/22.6.0 source/arm64</span>
<span class="nv">$ </span>terraform <span class="nt">--version</span>
<span class="c"># =&gt; Terraform v1.8.5</span>
<span class="nv">$ </span>kubectl version <span class="nt">--client</span><span class="o">=</span><span class="nb">true</span>
<span class="c"># =&gt; Client Version: version.Info{Major:"1", Minor:"22", GitVersion:"v1.22.5", GitCommit:"5c99e2ac2ff9a3c549d9ca665e7bc05a3e18f07e", GitTreeState:"clean", BuildDate:"2021-12-16T08:38:33Z", GoVersion:"go1.16.12", Compiler:"gc", Platform:"darwin/arm64"}</span>
<span class="nv">$ </span>helm version 
<span class="c"># =&gt; version.BuildInfo{Version:"v3.15.3", GitCommit:"3bb50bbbdd9c946ba9989fbe4fb4104766302a64", GitTreeState:"clean", GoVersion:"go1.22.5"}</span>
</code></pre></div>    </div>
  </li>
  <li>코드 준비 - <a href="https://github.com/aws-ia/terraform-aws-eks-blueprints">Github</a>
    <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>git clone https://github.com/aws-ia/terraform-aws-eks-blueprints
<span class="nv">$ </span><span class="nb">cd </span>terraform-aws-eks-blueprints/patterns/karpenter
<span class="nv">$ </span>tree 
</code></pre></div>    </div>
    <ul>
      <li>versions.tf
        <div class="language-tf highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">terraform</span> <span class="p">{</span>
  <span class="nx">required_version</span> <span class="p">=</span> <span class="s2">"&gt;= 1.3"</span>
    
  <span class="nx">required_providers</span> <span class="p">{</span>
    <span class="nx">aws</span> <span class="p">=</span> <span class="p">{</span>
      <span class="nx">source</span>  <span class="p">=</span> <span class="s2">"hashicorp/aws"</span>
      <span class="nx">version</span> <span class="p">=</span> <span class="s2">"&gt;= 5.34"</span>
    <span class="p">}</span>
    <span class="nx">helm</span> <span class="p">=</span> <span class="p">{</span>
      <span class="nx">source</span>  <span class="p">=</span> <span class="s2">"hashicorp/helm"</span>
      <span class="nx">version</span> <span class="p">=</span> <span class="s2">"&gt;= 2.9"</span>
    <span class="p">}</span>
    <span class="nx">kubernetes</span> <span class="p">=</span> <span class="p">{</span>
      <span class="nx">source</span>  <span class="p">=</span> <span class="s2">"hashicorp/kubernetes"</span>
      <span class="nx">version</span> <span class="p">=</span> <span class="s2">"&gt;= 2.20"</span>
    <span class="p">}</span>
  <span class="p">}</span>
    
  <span class="c1"># ##  Used for end-to-end testing on project; update to suit your needs</span>
  <span class="c1"># backend "s3" {</span>
  <span class="c1">#   bucket = "terraform-ssp-github-actions-state"</span>
  <span class="c1">#   region = "us-west-2"</span>
  <span class="c1">#   key    = "e2e/karpenter/terraform.tfstate"</span>
  <span class="c1"># }</span>
<span class="p">}</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>main.tf</p>

        <div class="language-tf highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">provider</span> <span class="s2">"aws"</span> <span class="p">{</span>
  <span class="nx">region</span> <span class="p">=</span> <span class="kd">local</span><span class="p">.</span><span class="nx">region</span>
<span class="p">}</span>
    
<span class="c1"># Required for public ECR where Karpenter artifacts are hosted</span>
<span class="k">provider</span> <span class="s2">"aws"</span> <span class="p">{</span>
  <span class="nx">region</span> <span class="p">=</span> <span class="s2">"us-east-1"</span>
  <span class="nx">alias</span>  <span class="p">=</span> <span class="s2">"virginia"</span>
<span class="p">}</span>
    
<span class="k">provider</span> <span class="s2">"kubernetes"</span> <span class="p">{</span>
  <span class="nx">host</span>                   <span class="p">=</span> <span class="k">module</span><span class="p">.</span><span class="nx">eks</span><span class="p">.</span><span class="nx">cluster_endpoint</span>
  <span class="nx">cluster_ca_certificate</span> <span class="p">=</span> <span class="nx">base64decode</span><span class="p">(</span><span class="k">module</span><span class="p">.</span><span class="nx">eks</span><span class="p">.</span><span class="nx">cluster_certificate_authority_data</span><span class="p">)</span>
    
  <span class="nx">exec</span> <span class="p">{</span>
    <span class="nx">api_version</span> <span class="p">=</span> <span class="s2">"client.authentication.k8s.io/v1beta1"</span>
    <span class="nx">command</span>     <span class="p">=</span> <span class="s2">"aws"</span>
    <span class="c1"># This requires the awscli to be installed locally where Terraform is executed</span>
    <span class="nx">args</span> <span class="p">=</span> <span class="p">[</span><span class="s2">"eks"</span><span class="p">,</span> <span class="s2">"get-token"</span><span class="p">,</span> <span class="s2">"--cluster-name"</span><span class="p">,</span> <span class="k">module</span><span class="p">.</span><span class="nx">eks</span><span class="p">.</span><span class="nx">cluster_name</span><span class="p">]</span>
  <span class="p">}</span>
<span class="p">}</span>
    
<span class="k">provider</span> <span class="s2">"helm"</span> <span class="p">{</span>
  <span class="nx">kubernetes</span> <span class="p">{</span>
    <span class="nx">host</span>                   <span class="p">=</span> <span class="k">module</span><span class="p">.</span><span class="nx">eks</span><span class="p">.</span><span class="nx">cluster_endpoint</span>
    <span class="nx">cluster_ca_certificate</span> <span class="p">=</span> <span class="nx">base64decode</span><span class="p">(</span><span class="k">module</span><span class="p">.</span><span class="nx">eks</span><span class="p">.</span><span class="nx">cluster_certificate_authority_data</span><span class="p">)</span>
    
    <span class="nx">exec</span> <span class="p">{</span>
      <span class="nx">api_version</span> <span class="p">=</span> <span class="s2">"client.authentication.k8s.io/v1beta1"</span>
      <span class="nx">command</span>     <span class="p">=</span> <span class="s2">"aws"</span>
      <span class="c1"># This requires the awscli to be installed locally where Terraform is executed</span>
      <span class="nx">args</span> <span class="p">=</span> <span class="p">[</span><span class="s2">"eks"</span><span class="p">,</span> <span class="s2">"get-token"</span><span class="p">,</span> <span class="s2">"--cluster-name"</span><span class="p">,</span> <span class="k">module</span><span class="p">.</span><span class="nx">eks</span><span class="p">.</span><span class="nx">cluster_name</span><span class="p">]</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>
    
<span class="k">data</span> <span class="s2">"aws_ecrpublic_authorization_token"</span> <span class="s2">"token"</span> <span class="p">{</span>
  <span class="k">provider</span> <span class="p">=</span> <span class="nx">aws</span><span class="p">.</span><span class="nx">virginia</span>
<span class="p">}</span>
    
<span class="k">data</span> <span class="s2">"aws_availability_zones"</span> <span class="s2">"available"</span> <span class="p">{}</span>
    
<span class="nx">locals</span> <span class="p">{</span>
  <span class="c1"># name   = "ex-${basename(path.cwd)}"</span>
  <span class="nx">name</span>   <span class="p">=</span> <span class="s2">"t101-</span><span class="k">${</span><span class="nx">basename</span><span class="p">(</span><span class="nx">path</span><span class="p">.</span><span class="nx">cwd</span><span class="p">)</span><span class="k">}</span><span class="s2">"</span>
  <span class="c1"># region = "us-west-2"</span>
  <span class="nx">region</span> <span class="p">=</span> <span class="s2">"ap-northeast-2"</span>
    
  <span class="nx">vpc_cidr</span> <span class="p">=</span> <span class="s2">"10.10.0.0/16"</span>
  <span class="nx">azs</span>      <span class="p">=</span> <span class="nx">slice</span><span class="p">(</span><span class="k">data</span><span class="p">.</span><span class="nx">aws_availability_zones</span><span class="p">.</span><span class="nx">available</span><span class="p">.</span><span class="nx">names</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    
  <span class="nx">tags</span> <span class="p">=</span> <span class="p">{</span>
    <span class="nx">Blueprint</span>  <span class="p">=</span> <span class="kd">local</span><span class="p">.</span><span class="nx">name</span>
    <span class="nx">GithubRepo</span> <span class="p">=</span> <span class="s2">"github.com/aws-ia/terraform-aws-eks-blueprints"</span>
  <span class="p">}</span>
<span class="p">}</span>
    
<span class="c1">################################################################################</span>
<span class="c1"># Cluster</span>
<span class="c1">################################################################################</span>
    
<span class="k">module</span> <span class="s2">"eks"</span> <span class="p">{</span>
  <span class="nx">source</span>  <span class="p">=</span> <span class="s2">"terraform-aws-modules/eks/aws"</span>
  <span class="nx">version</span> <span class="p">=</span> <span class="s2">"~&gt; 20.11"</span>
    
  <span class="nx">cluster_name</span>                   <span class="p">=</span> <span class="kd">local</span><span class="p">.</span><span class="nx">name</span>
  <span class="nx">cluster_version</span>                <span class="p">=</span> <span class="s2">"1.30"</span>
  <span class="nx">cluster_endpoint_public_access</span> <span class="p">=</span> <span class="kc">true</span>
    
  <span class="nx">vpc_id</span>     <span class="p">=</span> <span class="k">module</span><span class="p">.</span><span class="nx">vpc</span><span class="p">.</span><span class="nx">vpc_id</span>
  <span class="nx">subnet_ids</span> <span class="p">=</span> <span class="k">module</span><span class="p">.</span><span class="nx">vpc</span><span class="p">.</span><span class="nx">private_subnets</span>
    
  <span class="c1"># Fargate profiles use the cluster primary security group so these are not utilized</span>
  <span class="nx">create_cluster_security_group</span> <span class="p">=</span> <span class="kc">false</span>
  <span class="nx">create_node_security_group</span>    <span class="p">=</span> <span class="kc">false</span>
    
  <span class="nx">enable_cluster_creator_admin_permissions</span> <span class="p">=</span> <span class="kc">true</span>
    
  <span class="nx">fargate_profiles</span> <span class="p">=</span> <span class="p">{</span>
    <span class="nx">karpenter</span> <span class="p">=</span> <span class="p">{</span>
      <span class="nx">selectors</span> <span class="p">=</span> <span class="p">[</span>
        <span class="p">{</span> <span class="nx">namespace</span> <span class="p">=</span> <span class="s2">"karpenter"</span> <span class="p">}</span>
      <span class="p">]</span>
    <span class="p">}</span>
    <span class="nx">kube_system</span> <span class="p">=</span> <span class="p">{</span>
      <span class="nx">name</span> <span class="p">=</span> <span class="s2">"kube-system"</span>
      <span class="nx">selectors</span> <span class="p">=</span> <span class="p">[</span>
        <span class="p">{</span> <span class="nx">namespace</span> <span class="p">=</span> <span class="s2">"kube-system"</span> <span class="p">}</span>
      <span class="p">]</span>
    <span class="p">}</span>
  <span class="p">}</span>
    
  <span class="nx">tags</span> <span class="p">=</span> <span class="nx">merge</span><span class="p">(</span><span class="kd">local</span><span class="p">.</span><span class="nx">tags</span><span class="p">,</span> <span class="p">{</span>
    <span class="c1"># NOTE - if creating multiple security groups with this module, only tag the</span>
    <span class="c1"># security group that Karpenter should utilize with the following tag</span>
    <span class="c1"># (i.e. - at most, only one security group should have this tag in your account)</span>
    <span class="s2">"karpenter.sh/discovery"</span> <span class="p">=</span> <span class="kd">local</span><span class="p">.</span><span class="nx">name</span>
  <span class="p">})</span>
<span class="p">}</span>
    
<span class="c1">################################################################################</span>
<span class="c1"># EKS Blueprints Addons</span>
<span class="c1">################################################################################</span>
    
<span class="k">module</span> <span class="s2">"eks_blueprints_addons"</span> <span class="p">{</span>
  <span class="nx">source</span>  <span class="p">=</span> <span class="s2">"aws-ia/eks-blueprints-addons/aws"</span>
  <span class="nx">version</span> <span class="p">=</span> <span class="s2">"~&gt; 1.16"</span>
    
  <span class="nx">cluster_name</span>      <span class="p">=</span> <span class="k">module</span><span class="p">.</span><span class="nx">eks</span><span class="p">.</span><span class="nx">cluster_name</span>
  <span class="nx">cluster_endpoint</span>  <span class="p">=</span> <span class="k">module</span><span class="p">.</span><span class="nx">eks</span><span class="p">.</span><span class="nx">cluster_endpoint</span>
  <span class="nx">cluster_version</span>   <span class="p">=</span> <span class="k">module</span><span class="p">.</span><span class="nx">eks</span><span class="p">.</span><span class="nx">cluster_version</span>
  <span class="nx">oidc_provider_arn</span> <span class="p">=</span> <span class="k">module</span><span class="p">.</span><span class="nx">eks</span><span class="p">.</span><span class="nx">oidc_provider_arn</span>
    
  <span class="c1"># We want to wait for the Fargate profiles to be deployed first</span>
  <span class="nx">create_delay_dependencies</span> <span class="p">=</span> <span class="p">[</span><span class="nx">for</span> <span class="nx">prof</span> <span class="nx">in</span> <span class="k">module</span><span class="p">.</span><span class="nx">eks</span><span class="p">.</span><span class="nx">fargate_profiles</span> <span class="err">:</span> <span class="nx">prof</span><span class="p">.</span><span class="nx">fargate_profile_arn</span><span class="p">]</span>
    
  <span class="nx">eks_addons</span> <span class="p">=</span> <span class="p">{</span>
    <span class="nx">coredns</span> <span class="p">=</span> <span class="p">{</span>
      <span class="nx">configuration_values</span> <span class="p">=</span> <span class="nx">jsonencode</span><span class="p">({</span>
        <span class="nx">computeType</span> <span class="p">=</span> <span class="s2">"Fargate"</span>
        <span class="c1"># Ensure that the we fully utilize the minimum amount of resources that are supplied by</span>
        <span class="c1"># Fargate https://docs.aws.amazon.com/eks/latest/userguide/fargate-pod-configuration.html</span>
        <span class="c1"># Fargate adds 256 MB to each pod's memory reservation for the required Kubernetes</span>
        <span class="c1"># components (kubelet, kube-proxy, and containerd). Fargate rounds up to the following</span>
        <span class="c1"># compute configuration that most closely matches the sum of vCPU and memory requests in</span>
        <span class="c1"># order to ensure pods always have the resources that they need to run.</span>
        <span class="nx">resources</span> <span class="p">=</span> <span class="p">{</span>
          <span class="nx">limits</span> <span class="p">=</span> <span class="p">{</span>
            <span class="nx">cpu</span> <span class="p">=</span> <span class="s2">"0.25"</span>
            <span class="c1"># We are targeting the smallest Task size of 512Mb, so we subtract 256Mb from the</span>
            <span class="c1"># request/limit to ensure we can fit within that task</span>
            <span class="nx">memory</span> <span class="p">=</span> <span class="s2">"256M"</span>
          <span class="p">}</span>
          <span class="nx">requests</span> <span class="p">=</span> <span class="p">{</span>
            <span class="nx">cpu</span> <span class="p">=</span> <span class="s2">"0.25"</span>
            <span class="c1"># We are targeting the smallest Task size of 512Mb, so we subtract 256Mb from the</span>
            <span class="c1"># request/limit to ensure we can fit within that task</span>
            <span class="nx">memory</span> <span class="p">=</span> <span class="s2">"256M"</span>
          <span class="p">}</span>
        <span class="p">}</span>
      <span class="p">})</span>
    <span class="p">}</span>
    <span class="nx">vpc-cni</span>    <span class="p">=</span> <span class="p">{}</span>
    <span class="nx">kube-proxy</span> <span class="p">=</span> <span class="p">{}</span>
  <span class="p">}</span>
    
  <span class="nx">enable_karpenter</span> <span class="p">=</span> <span class="kc">true</span>
    
  <span class="nx">karpenter</span> <span class="p">=</span> <span class="p">{</span>
    <span class="nx">repository_username</span> <span class="p">=</span> <span class="k">data</span><span class="p">.</span><span class="nx">aws_ecrpublic_authorization_token</span><span class="p">.</span><span class="nx">token</span><span class="p">.</span><span class="nx">user_name</span>
    <span class="nx">repository_password</span> <span class="p">=</span> <span class="k">data</span><span class="p">.</span><span class="nx">aws_ecrpublic_authorization_token</span><span class="p">.</span><span class="nx">token</span><span class="p">.</span><span class="nx">password</span>
  <span class="p">}</span>
    
  <span class="nx">karpenter_node</span> <span class="p">=</span> <span class="p">{</span>
    <span class="c1"># Use static name so that it matches what is defined in `karpenter.yaml` example manifest</span>
    <span class="nx">iam_role_use_name_prefix</span> <span class="p">=</span> <span class="kc">false</span>
  <span class="p">}</span>
    
  <span class="nx">tags</span> <span class="p">=</span> <span class="kd">local</span><span class="p">.</span><span class="nx">tags</span>
<span class="p">}</span>
    
<span class="k">resource</span> <span class="s2">"aws_eks_access_entry"</span> <span class="s2">"karpenter_node_access_entry"</span> <span class="p">{</span>
  <span class="nx">cluster_name</span>      <span class="p">=</span> <span class="k">module</span><span class="p">.</span><span class="nx">eks</span><span class="p">.</span><span class="nx">cluster_name</span>
  <span class="nx">principal_arn</span>     <span class="p">=</span> <span class="k">module</span><span class="p">.</span><span class="nx">eks_blueprints_addons</span><span class="p">.</span><span class="nx">karpenter</span><span class="p">.</span><span class="nx">node_iam_role_arn</span>
  <span class="nx">kubernetes_groups</span> <span class="p">=</span> <span class="p">[]</span>
  <span class="nx">type</span>              <span class="p">=</span> <span class="s2">"EC2_LINUX"</span>
<span class="p">}</span>
    
<span class="c1">################################################################################</span>
<span class="c1"># Supporting Resources</span>
<span class="c1">################################################################################</span>
    
<span class="k">module</span> <span class="s2">"vpc"</span> <span class="p">{</span>
  <span class="nx">source</span>  <span class="p">=</span> <span class="s2">"terraform-aws-modules/vpc/aws"</span>
  <span class="nx">version</span> <span class="p">=</span> <span class="s2">"~&gt; 5.0"</span>
    
  <span class="nx">name</span> <span class="p">=</span> <span class="kd">local</span><span class="p">.</span><span class="nx">name</span>
  <span class="nx">cidr</span> <span class="p">=</span> <span class="kd">local</span><span class="p">.</span><span class="nx">vpc_cidr</span>
    
  <span class="nx">azs</span>             <span class="p">=</span> <span class="kd">local</span><span class="p">.</span><span class="nx">azs</span>
  <span class="nx">private_subnets</span> <span class="p">=</span> <span class="p">[</span><span class="nx">for</span> <span class="nx">k</span><span class="p">,</span> <span class="nx">v</span> <span class="nx">in</span> <span class="kd">local</span><span class="p">.</span><span class="nx">azs</span> <span class="err">:</span> <span class="nx">cidrsubnet</span><span class="p">(</span><span class="kd">local</span><span class="p">.</span><span class="nx">vpc_cidr</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="nx">k</span><span class="p">)]</span>
  <span class="nx">public_subnets</span>  <span class="p">=</span> <span class="p">[</span><span class="nx">for</span> <span class="nx">k</span><span class="p">,</span> <span class="nx">v</span> <span class="nx">in</span> <span class="kd">local</span><span class="p">.</span><span class="nx">azs</span> <span class="err">:</span> <span class="nx">cidrsubnet</span><span class="p">(</span><span class="kd">local</span><span class="p">.</span><span class="nx">vpc_cidr</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="nx">k</span> <span class="err">+</span> <span class="mi">48</span><span class="p">)]</span>
    
  <span class="nx">enable_nat_gateway</span> <span class="p">=</span> <span class="kc">true</span>
  <span class="nx">single_nat_gateway</span> <span class="p">=</span> <span class="kc">true</span>
    
  <span class="nx">public_subnet_tags</span> <span class="p">=</span> <span class="p">{</span>
    <span class="s2">"kubernetes.io/role/elb"</span> <span class="p">=</span> <span class="mi">1</span>
  <span class="p">}</span>
    
  <span class="nx">private_subnet_tags</span> <span class="p">=</span> <span class="p">{</span>
    <span class="s2">"kubernetes.io/role/internal-elb"</span> <span class="p">=</span> <span class="mi">1</span>
    <span class="c1"># Tags subnets for Karpenter auto-discovery</span>
    <span class="s2">"karpenter.sh/discovery"</span> <span class="p">=</span> <span class="kd">local</span><span class="p">.</span><span class="nx">name</span>
  <span class="p">}</span>
    
  <span class="nx">tags</span> <span class="p">=</span> <span class="kd">local</span><span class="p">.</span><span class="nx">tags</span>
<span class="p">}</span>
</code></pre></div>        </div>
      </li>
      <li>output.tf
        <div class="language-tf highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">output</span> <span class="s2">"configure_kubectl"</span> <span class="p">{</span>
  <span class="nx">description</span> <span class="p">=</span> <span class="s2">"Configure kubectl: make sure you're logged in with the correct AWS profile and run the following command to update your kubeconfig"</span>
  <span class="nx">value</span>       <span class="p">=</span> <span class="s2">"aws eks --region </span><span class="k">${</span><span class="kd">local</span><span class="p">.</span><span class="nx">region</span><span class="k">}</span><span class="s2"> update-kubeconfig --name </span><span class="k">${module</span><span class="p">.</span><span class="nx">eks</span><span class="p">.</span><span class="nx">cluster_name</span><span class="k">}</span><span class="s2">"</span>
<span class="p">}</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>init
    <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>terraform init
<span class="nv">$ </span>tree .terraform 
<span class="nv">$ </span><span class="nb">cat</span> .terraform/modules/modules.json | jq
<span class="nv">$ </span>tree .terraform/providers/registry.terraform.io/hashicorp <span class="nt">-L</span> 2
</code></pre></div>    </div>
  </li>
</ul>

<h4 id="vpc-배포">VPC 배포</h4>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c"># VPC 배포전 VPC 확인</span>
  <span class="nv">$ </span>aws ec2 describe-vpcs <span class="nt">--filter</span> <span class="s1">'Name=isDefault,Values=false'</span> <span class="nt">--output</span> yaml
  <span class="c"># =&gt; Vpcs: []</span>
  
  <span class="c"># vpc 배포</span>
  <span class="nv">$ </span>terraform apply <span class="nt">-target</span><span class="o">=</span><span class="s2">"module.vpc"</span> <span class="nt">-auto-approve</span>
  
  <span class="c"># 배포 확인</span>
  <span class="nv">$ </span>terraform state list
  <span class="nv">$ </span>terraform show
  
  <span class="c"># VPC 정보 확인</span>
  <span class="nv">$ </span>aws ec2 describe-vpcs <span class="nt">--filter</span> <span class="s1">'Name=isDefault,Values=false'</span> <span class="nt">--output</span> yaml
  <span class="c"># =&gt; Vpcs:</span>
  <span class="c">#    - CidrBlock: 10.10.0.0/16</span>
  <span class="c">#      CidrBlockAssociationSet:</span>
  <span class="c">#      - AssociationId: vpc-cidr</span>
  <span class="c">#    ...</span>
  
  <span class="c"># 상세정보 확인</span>
  <span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"data.aws_availability_zones.available"</span> | terraform console
  <span class="c"># =&gt; {</span>
  <span class="c">#      "all_availability_zones" = tobool(null)</span>
  <span class="c">#      "exclude_names" = toset(null) /* of string */</span>
  <span class="c">#      "exclude_zone_ids" = toset(null) /* of string */</span>
  <span class="c">#      "filter" = toset(null) /* of object */</span>
  <span class="c">#      "group_names" = toset([</span>
  <span class="c">#        "ap-northeast-2",</span>
  <span class="c">#      ])</span>
  <span class="c">#      "id" = "ap-northeast-2"</span>
  <span class="c">#      "names" = tolist([</span>
  <span class="c">#        "ap-northeast-2a",</span>
  <span class="c">#        "ap-northeast-2b",</span>
  <span class="c">#        "ap-northeast-2c",</span>
  <span class="c">#        "ap-northeast-2d",</span>
  <span class="c">#      ])</span>
  <span class="c">#      "state" = tostring(null)</span>
  <span class="c">#      "timeouts" = null /* object */</span>
  <span class="c">#      "zone_ids" = tolist([</span>
  <span class="c">#        "apne2-az1",</span>
  <span class="c">#        "apne2-az2",</span>
  <span class="c">#        "apne2-az3",</span>
  <span class="c">#        "apne2-az4",</span>
  <span class="c">#      ])</span>
  <span class="c">#    }</span>
  
  <span class="nv">$ </span>terraform state show <span class="s1">'module.vpc.aws_vpc.this[0]'</span>
  <span class="nv">$ VPCID</span><span class="o">=</span><span class="si">$(</span><span class="nb">echo</span> <span class="s1">'module.vpc.vpc_id'</span> | terraform console<span class="si">)</span>
  <span class="nv">$ </span>aws ec2 describe-subnets <span class="nt">--filters</span> <span class="s2">"Name=vpc-id,Values=</span><span class="nv">$VPCID</span><span class="s2">"</span> | jq
  <span class="nv">$ </span>aws ec2 describe-subnets <span class="nt">--filters</span> <span class="s2">"Name=vpc-id,Values=</span><span class="nv">$VPCID</span><span class="s2">"</span> <span class="nt">--output</span> text

  <span class="c"># public 서브넷과 private 서브넷 CIDR 확인</span>
  <span class="c">## private_subnets = [for k, v in local.azs : cidrsubnet(local.vpc_cidr, 4, k)]</span>
  <span class="c">## public_subnets  = [for k, v in local.azs : cidrsubnet(local.vpc_cidr, 8, k + 48)]</span>
  <span class="nv">$ </span>terraform state show <span class="s1">'module.vpc.aws_subnet.public[0]'</span>
  <span class="nv">$ </span>terraform state show <span class="s1">'module.vpc.aws_subnet.private[0]'</span>
</code></pre></div></div>

<p><img src="/assets/2024/t101-4th/20240727_terraform_w7_eks_karpenter_3.png" alt="VPC 배포 결과" class="image-center" />
<em class="image-caption">VPC 배포 결과</em></p>

<h4 id="eks-및-fargate-배포">EKS 및 Fargate 배포</h4>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c"># EKS 배포</span>
  <span class="nv">$ </span>terraform apply <span class="nt">-target</span><span class="o">=</span><span class="s2">"module.eks"</span> <span class="nt">-auto-approve</span>
  <span class="c"># =&gt; Apply complete! Resources: 24 added, 0 changed, 0 destroyed.</span>
  <span class="c">#    Outputs:</span>
  <span class="c">#    configure_kubectl = "aws eks --region ap-northeast-2 update-kubeconfig --name t101-karpenter"</span>
  
  <span class="c"># 배포 확인</span>
  <span class="nv">$ </span>terraform state list
  <span class="c"># =&gt; data.aws_availability_zones.available</span>
  <span class="c">#    module.eks.data.aws_caller_identity.current</span>
  <span class="c">#    module.eks.data.aws_iam_policy_document.assume_role_policy[0]</span>
  <span class="c">#    module.eks.data.aws_iam_session_context.current</span>
  <span class="c">#    module.eks.data.aws_partition.current</span>
  <span class="c">#    module.eks.data.tls_certificate.this[0]</span>
  <span class="c">#    module.eks.aws_cloudwatch_log_group.this[0]</span>
  <span class="c">#    module.eks.aws_ec2_tag.cluster_primary_security_group["Blueprint"]</span>
  <span class="c">#    module.eks.aws_ec2_tag.cluster_primary_security_group["GithubRepo"]</span>
  <span class="c">#    module.eks.aws_ec2_tag.cluster_primary_security_group["karpenter.sh/discovery"]</span>
  <span class="c">#    module.eks.aws_eks_access_entry.this["cluster_creator"]</span>
  <span class="c">#    module.eks.aws_eks_access_policy_association.this["cluster_creator_admin"]</span>
  <span class="c">#    module.eks.aws_eks_cluster.this[0]</span>
  <span class="c">#    module.eks.aws_iam_openid_connect_provider.oidc_provider[0]</span>
  <span class="c">#    module.eks.aws_iam_policy.cluster_encryption[0]</span>
  <span class="c">#    module.eks.aws_iam_role.this[0]</span>
  <span class="c">#    module.eks.aws_iam_role_policy_attachment.cluster_encryption[0]</span>
  <span class="c">#    module.eks.aws_iam_role_policy_attachment.this["AmazonEKSClusterPolicy"]</span>
  <span class="c">#    module.eks.aws_iam_role_policy_attachment.this["AmazonEKSVPCResourceController"]</span>
  <span class="c">#    module.eks.time_sleep.this[0]</span>
  <span class="c">#    module.vpc.aws_default_network_acl.this[0]</span>
  <span class="c">#    module.vpc.aws_default_route_table.default[0]</span>
  <span class="c">#    module.vpc.aws_default_security_group.this[0]</span>
  <span class="c">#    module.vpc.aws_eip.nat[0]</span>
  <span class="c">#    module.vpc.aws_internet_gateway.this[0]</span>
  <span class="c">#    module.vpc.aws_nat_gateway.this[0]</span>
  <span class="c">#    module.vpc.aws_route.private_nat_gateway[0]</span>
  <span class="c">#    module.vpc.aws_route.public_internet_gateway[0]</span>
  <span class="c">#    module.vpc.aws_route_table.private[0]</span>
  <span class="c">#    module.vpc.aws_route_table.public[0]</span>
  <span class="c">#    module.vpc.aws_route_table_association.private[0]</span>
  <span class="c">#    module.vpc.aws_route_table_association.private[1]</span>
  <span class="c">#    module.vpc.aws_route_table_association.private[2]</span>
  <span class="c">#    module.vpc.aws_route_table_association.public[0]</span>
  <span class="c">#    module.vpc.aws_route_table_association.public[1]</span>
  <span class="c">#    module.vpc.aws_route_table_association.public[2]</span>
  <span class="c">#    module.vpc.aws_subnet.private[0]</span>
  <span class="c">#    module.vpc.aws_subnet.private[1]</span>
  <span class="c">#    module.vpc.aws_subnet.private[2]</span>
  <span class="c">#    module.vpc.aws_subnet.public[0]</span>
  <span class="c">#    module.vpc.aws_subnet.public[1]</span>
  <span class="c">#    module.vpc.aws_subnet.public[2]</span>
  <span class="c">#    module.vpc.aws_vpc.this[0]</span>
  <span class="c">#    module.eks.module.fargate_profile["karpenter"].data.aws_caller_identity.current</span>
  <span class="c">#    module.eks.module.fargate_profile["karpenter"].data.aws_iam_policy_document.assume_role_policy[0]</span>
  <span class="c">#    module.eks.module.fargate_profile["karpenter"].data.aws_partition.current</span>
  <span class="c">#    module.eks.module.fargate_profile["karpenter"].data.aws_region.current</span>
  <span class="c">#    module.eks.module.fargate_profile["karpenter"].aws_eks_fargate_profile.this[0]</span>
  <span class="c">#    module.eks.module.fargate_profile["karpenter"].aws_iam_role.this[0]</span>
  <span class="c">#    module.eks.module.fargate_profile["karpenter"].aws_iam_role_policy_attachment.this["AmazonEKSFargatePodExecutionRolePolicy"]</span>
  <span class="c">#    module.eks.module.fargate_profile["karpenter"].aws_iam_role_policy_attachment.this["AmazonEKS_CNI_Policy"]</span>
  <span class="c">#    module.eks.module.fargate_profile["kube_system"].data.aws_caller_identity.current</span>
  <span class="c">#    module.eks.module.fargate_profile["kube_system"].data.aws_iam_policy_document.assume_role_policy[0]</span>
  <span class="c">#    module.eks.module.fargate_profile["kube_system"].data.aws_partition.current</span>
  <span class="c">#    module.eks.module.fargate_profile["kube_system"].data.aws_region.current</span>
  <span class="c">#    module.eks.module.fargate_profile["kube_system"].aws_eks_fargate_profile.this[0]</span>
  <span class="c">#    module.eks.module.fargate_profile["kube_system"].aws_iam_role.this[0]</span>
  <span class="c">#    module.eks.module.fargate_profile["kube_system"].aws_iam_role_policy_attachment.this["AmazonEKSFargatePodExecutionRolePolicy"]</span>
  <span class="c">#    module.eks.module.fargate_profile["kube_system"].aws_iam_role_policy_attachment.this["AmazonEKS_CNI_Policy"]</span>
  <span class="c">#    module.eks.module.kms.data.aws_caller_identity.current[0]</span>
  <span class="c">#    module.eks.module.kms.data.aws_iam_policy_document.this[0]</span>
  <span class="c">#    module.eks.module.kms.data.aws_partition.current[0]</span>
  <span class="c">#    module.eks.module.kms.aws_kms_alias.this["cluster"]</span>
  <span class="c">#    module.eks.module.kms.aws_kms_key.this[0]</span>
  
  <span class="nv">$ </span>terraform output
  <span class="c"># =&gt; configure_kubectl = "aws eks --region ap-northeast-2 update-kubeconfig --name t101-karpenter"</span>
  
  <span class="c"># kubectl 설정</span>
  <span class="nv">$ </span>aws eks <span class="nt">--region</span> ap-northeast-2 update-kubeconfig <span class="nt">--name</span> t101-karpenter
  <span class="nv">$ </span><span class="nb">cat</span> ~/.kube/config
    
  <span class="c"># k8s 노드, 파드 정보 확인</span>
  <span class="nv">$ </span>kubectl cluster-info
  <span class="c"># =&gt; Kubernetes control plane is running at https://D858CEEA85279742B1B4738D555C8602.sk1.ap-northeast-2.eks.amazonaws.com</span>
  <span class="c">#    CoreDNS is running at https://D858CEEA85279742B1B4738D555C8602.sk1.ap-northeast-2.eks.amazonaws.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy</span>
  <span class="c">#    To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.</span>
  <span class="nv">$ </span>kubectl get node
  <span class="c"># =&gt; No resources found</span>
  <span class="nv">$ </span>kubectl get pod <span class="nt">-A</span>
  <span class="c"># =&gt; NAMESPACE     NAME                      READY   STATUS    RESTARTS   AGE</span>
  <span class="c">#    kube-system   coredns-5b9dfbf96-24qbm   0/1     Pending   0          22m</span>
  <span class="c">#    kube-system   coredns-5b9dfbf96-8q225   0/1     Pending   0          22m</span>
  <span class="nv">$ </span>kubectl describe pod coredns-5b9dfbf96-24 <span class="nt">-n</span> kube-system
  <span class="c"># =&gt; ...</span>
  <span class="c">#    Type     Reason            Age                    From               Message</span>
  <span class="c">#    ----     ------            ----                   ----               -------</span>
  <span class="c">#    Warning  FailedScheduling  3m22s (x124 over 23m)  default-scheduler  no nodes available to schedule pods</span>
  
  <span class="c"># 상세 정보 확인</span>
  <span class="nv">$ </span>terraform show
  <span class="nv">$ </span>terraform state list
  <span class="nv">$ </span>terraform state show <span class="s1">'module.eks.data.aws_caller_identity.current'</span>
  <span class="nv">$ </span>terraform state show <span class="s1">'module.eks.data.aws_iam_session_context.current'</span>
   
  <span class="nv">$ </span>terraform state show <span class="s1">'module.eks.aws_eks_cluster.this[0]'</span>
  <span class="nv">$ </span>terraform state show <span class="s1">'module.eks.data.tls_certificate.this[0]'</span>
  <span class="nv">$ </span>terraform state show <span class="s1">'module.eks.aws_cloudwatch_log_group.this[0]'</span>
  <span class="nv">$ </span>terraform state show <span class="s1">'module.eks.aws_eks_access_entry.this["cluster_creator"]'</span>
  <span class="nv">$ </span>terraform state show <span class="s1">'module.eks.aws_iam_openid_connect_provider.oidc_provider[0]'</span>
  <span class="nv">$ </span>terraform state show <span class="s1">'module.eks.data.aws_partition.current'</span>
  <span class="nv">$ </span>terraform state show <span class="s1">'module.eks.aws_iam_policy.cluster_encryption[0]'</span>
  <span class="nv">$ </span>terraform state show <span class="s1">'module.eks.aws_iam_role.this[0]'</span>
  
  <span class="nv">$ </span>terraform state show <span class="s1">'module.eks.time_sleep.this[0]'</span>
  <span class="nv">$ </span>terraform state show <span class="s1">'module.eks.module.kms.aws_kms_key.this[0]'</span>
  <span class="nv">$ </span>terraform state show <span class="s1">'module.eks.module.fargate_profile["kube_system"].aws_eks_fargate_profile.this[0]'</span>
  <span class="nv">$ </span>terraform state show <span class="s1">'module.eks.module.fargate_profile["karpenter"].aws_eks_fargate_profile.this[0]'</span>
</code></pre></div></div>

<p><img src="/assets/2024/t101-4th/20240727_terraform_w7_eks_karpenter_4.png" alt="EKS 배포 결과" class="image-center" />
<em class="image-caption">EKS 배포 결과</em></p>

<p><img src="/assets/2024/t101-4th/20240727_terraform_w7_eks_karpenter_5.png" alt="EKS 배포 결과 (Secret 암호화 관련 설정)" class="image-center" />
<em class="image-caption">EKS 배포 결과 (Secret 암호화 관련) 설정</em></p>

<p><img src="/assets/2024/t101-4th/20240727_terraform_w7_eks_karpenter_6.png" alt="EKS 배포 결과 (Secret 암호화에 쓰이는 KMS 정보)" class="image-center" />
<em class="image-caption">EKS 배포 결과 (Secret 암호화에 쓰이는 KMS 정보)</em></p>

<p><img src="/assets/2024/t101-4th/20240727_terraform_w7_eks_karpenter_8.png" alt="EKS 배포 결과 (네트워킹 정보)" class="image-center" />
<em class="image-caption">EKS 배포 결과 (네트워킹 정보)</em></p>

<p><img src="/assets/2024/t101-4th/20240727_terraform_w7_eks_karpenter_9.png" alt="EKS 배포 결과 (접근 IAM 정보)" class="image-center" />
<em class="image-caption">EKS 배포 결과 (접근 IAM 정보)</em></p>

<p><img src="/assets/2024/t101-4th/20240727_terraform_w7_eks_karpenter_10.png" alt="EKS 배포 결과 (로그 설정)" class="image-center" />
<em class="image-caption">EKS 배포 결과 (로그 설정)</em></p>

<p><img src="/assets/2024/t101-4th/20240727_terraform_w7_eks_karpenter_11.png" alt="EKS 배포 결과 (Cloud Watch를 통한 로그 관리)" class="image-center" />
<em class="image-caption">EKS 배포 결과 (Cloud Watch를 통한 로그 관리)</em></p>

<p><img src="/assets/2024/t101-4th/20240727_terraform_w7_eks_karpenter_7.png" alt="Fargate 배포 결과" class="image-center" />
<em class="image-caption">Fargate 배포 결과</em></p>

<h4 id="karpenter-및-기타-addon-배포">Karpenter 및 기타 addon 배포</h4>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c"># 배포</span>
  <span class="nv">$ </span>terraform apply <span class="nt">-auto-approve</span>
  
  <span class="c"># 확인</span>
  <span class="nv">$ </span>terraform state list
  <span class="c"># =&gt; data.aws_ecrpublic_authorization_token.token</span>
  <span class="c">#    aws_eks_access_entry.karpenter_node_access_entry</span>
  <span class="c">#    ...</span>
  <span class="c">#    module.eks_blueprints_addons.data.aws_caller_identity.current</span>
  <span class="c">#    module.eks_blueprints_addons.data.aws_eks_addon_version.this["coredns"]</span>
  <span class="c">#    module.eks_blueprints_addons.data.aws_eks_addon_version.this["kube-proxy"]</span>
  <span class="c">#    module.eks_blueprints_addons.data.aws_eks_addon_version.this["vpc-cni"]</span>
  <span class="c">#    module.eks_blueprints_addons.data.aws_iam_policy_document.karpenter[0]</span>
  <span class="c">#    module.eks_blueprints_addons.data.aws_iam_policy_document.karpenter_assume_role[0]</span>
  <span class="c">#    module.eks_blueprints_addons.data.aws_partition.current</span>
  <span class="c">#    module.eks_blueprints_addons.data.aws_region.current</span>
  <span class="c">#    module.eks_blueprints_addons.aws_cloudwatch_event_rule.karpenter["health_event"]</span>
  <span class="c">#    module.eks_blueprints_addons.aws_cloudwatch_event_rule.karpenter["instance_rebalance"]</span>
  <span class="c">#    module.eks_blueprints_addons.aws_cloudwatch_event_rule.karpenter["instance_state_change"]</span>
  <span class="c">#    module.eks_blueprints_addons.aws_cloudwatch_event_rule.karpenter["spot_interupt"]</span>
  <span class="c">#    module.eks_blueprints_addons.aws_cloudwatch_event_target.karpenter["health_event"]</span>
  <span class="c">#    module.eks_blueprints_addons.aws_cloudwatch_event_target.karpenter["instance_rebalance"]</span>
  <span class="c">#    module.eks_blueprints_addons.aws_cloudwatch_event_target.karpenter["instance_state_change"]</span>
  <span class="c">#    module.eks_blueprints_addons.aws_cloudwatch_event_target.karpenter["spot_interupt"]</span>
  <span class="c">#    module.eks_blueprints_addons.aws_eks_addon.this["coredns"]</span>
  <span class="c">#    module.eks_blueprints_addons.aws_eks_addon.this["kube-proxy"]</span>
  <span class="c">#    module.eks_blueprints_addons.aws_eks_addon.this["vpc-cni"]</span>
  <span class="c">#    module.eks_blueprints_addons.aws_iam_instance_profile.karpenter[0]</span>
  <span class="c">#    module.eks_blueprints_addons.aws_iam_role.karpenter[0]</span>
  <span class="c">#    module.eks_blueprints_addons.aws_iam_role_policy_attachment.karpenter["AmazonEC2ContainerRegistryReadOnly"]</span>
  <span class="c">#    module.eks_blueprints_addons.aws_iam_role_policy_attachment.karpenter["AmazonEKSWorkerNodePolicy"]</span>
  <span class="c">#    module.eks_blueprints_addons.aws_iam_role_policy_attachment.karpenter["AmazonEKS_CNI_Policy"]</span>
  <span class="c">#    module.eks_blueprints_addons.time_sleep.this</span>
  <span class="c">#    ...</span>
  <span class="c">#    module.eks_blueprints_addons.module.karpenter.data.aws_caller_identity.current[0]</span>
  <span class="c">#    module.eks_blueprints_addons.module.karpenter.data.aws_iam_policy_document.assume[0]</span>
  <span class="c">#    module.eks_blueprints_addons.module.karpenter.data.aws_iam_policy_document.this[0]</span>
  <span class="c">#    module.eks_blueprints_addons.module.karpenter.data.aws_partition.current[0]</span>
  <span class="c">#    module.eks_blueprints_addons.module.karpenter.aws_iam_policy.this[0]</span>
  <span class="c">#    module.eks_blueprints_addons.module.karpenter.aws_iam_role.this[0]</span>
  <span class="c">#    module.eks_blueprints_addons.module.karpenter.aws_iam_role_policy_attachment.this[0]</span>
  <span class="c">#    module.eks_blueprints_addons.module.karpenter.helm_release.this[0]</span>
  <span class="c">#    module.eks_blueprints_addons.module.karpenter_sqs.data.aws_iam_policy_document.this[0]</span>
  <span class="c">#    module.eks_blueprints_addons.module.karpenter_sqs.aws_sqs_queue.this[0]</span>
  <span class="c">#    module.eks_blueprints_addons.module.karpenter_sqs.aws_sqs_queue_policy.this[0]</span>
  
  <span class="nv">$ </span>terraform show

  <span class="c"># k8s 노드, 파드 정보 확인</span>
  <span class="nv">$ </span>kubectl cluster-info
  <span class="c"># =&gt; Kubernetes control plane is running at https://D858CEEA85279742B1B4738D555C8602.sk1.ap-northeast-2.eks.amazonaws.com</span>
  <span class="c">#    CoreDNS is running at https://D858CEEA85279742B1B4738D555C8602.sk1.ap-northeast-2.eks.amazonaws.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy</span>
  <span class="c">#    </span>
  <span class="c">#    To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.</span>
  
  <span class="nv">$ </span>kubectl get nodes <span class="nt">-L</span> node.kubernetes.io/instance-type <span class="nt">-L</span> topology.kubernetes.io/zone
  <span class="c"># =&gt; NAME                                                      STATUS   ROLES    AGE     VERSION               INSTANCE-TYPE   ZONE</span>
  <span class="c">#    fargate-ip-10-10-18-179.ap-northeast-2.compute.internal   Ready    &lt;none&gt;   4m58s   v1.30.0-eks-404b9c6                   ap-northeast-2b</span>
  <span class="c">#    fargate-ip-10-10-26-139.ap-northeast-2.compute.internal   Ready    &lt;none&gt;   4m58s   v1.30.0-eks-404b9c6                   ap-northeast-2b</span>
  <span class="c">#    fargate-ip-10-10-41-162.ap-northeast-2.compute.internal   Ready    &lt;none&gt;   4m51s   v1.30.0-eks-404b9c6                   ap-northeast-2c</span>
  <span class="c">#    fargate-ip-10-10-44-218.ap-northeast-2.compute.internal   Ready    &lt;none&gt;   4m57s   v1.30.0-eks-404b9c6                   ap-northeast-2c</span>
   
  <span class="c"># fargate를 프로비저닝하기 전에는 노드가 없었으나, </span>
  <span class="c"># fargate 프로비저닝 후 노드가 생성되었음을 확인할 수 있습니다.</span>
  
  <span class="nv">$ </span>kubectl get node <span class="nt">-owide</span>  
  <span class="c"># =&gt; NAME                                                      STATUS   ROLES    AGE     VERSION               INTERNAL-IP    EXTERNAL-IP   OS-IMAGE         KERNEL-VERSION                  CONTAINER-RUNTIME</span>
  <span class="c">#    fargate-ip-10-10-18-179.ap-northeast-2.compute.internal   Ready    &lt;none&gt;   6m45s   v1.30.0-eks-404b9c6   10.10.18.179   &lt;none&gt;        Amazon Linux 2   5.10.220-209.867.amzn2.x86_64   containerd://1.7.11</span>
  <span class="c">#    fargate-ip-10-10-26-139.ap-northeast-2.compute.internal   Ready    &lt;none&gt;   6m45s   v1.30.0-eks-404b9c6   10.10.26.139   &lt;none&gt;        Amazon Linux 2   5.10.220-209.867.amzn2.x86_64   containerd://1.7.11</span>
  <span class="c">#    fargate-ip-10-10-41-162.ap-northeast-2.compute.internal   Ready    &lt;none&gt;   6m38s   v1.30.0-eks-404b9c6   10.10.41.162   &lt;none&gt;        Amazon Linux 2   5.10.220-209.867.amzn2.x86_64   containerd://1.7.11</span>
  <span class="c">#    fargate-ip-10-10-44-218.ap-northeast-2.compute.internal   Ready    &lt;none&gt;   6m44s   v1.30.0-eks-404b9c6   10.10.44.218   &lt;none&gt;        Amazon Linux 2   5.10.220-209.867.amzn2.x86_64   containerd://1.7.11</span>
  
  <span class="nv">$ </span>kubectl get pod <span class="nt">-A</span> <span class="nt">-owide</span>        
  <span class="c"># =&gt; NAMESPACE     NAME                         READY   STATUS    RESTARTS   AGE     IP             NODE                                                      NOMINATED NODE   READINESS GATES</span>
  <span class="c">#    karpenter     karpenter-7c5786cbc4-q5k7x   1/1     Running   0          8m42s   10.10.18.179   fargate-ip-10-10-18-179.ap-northeast-2.compute.internal   &lt;none&gt;           &lt;none&gt;</span>
  <span class="c">#    karpenter     karpenter-7c5786cbc4-vxlrq   1/1     Running   0          8m42s   10.10.26.139   fargate-ip-10-10-26-139.ap-northeast-2.compute.internal   &lt;none&gt;           &lt;none&gt;</span>
  <span class="c">#    kube-system   coredns-86dcddd859-2x2gf     1/1     Running   0          8m43s   10.10.41.162   fargate-ip-10-10-41-162.ap-northeast-2.compute.internal   &lt;none&gt;           &lt;none&gt;</span>
  <span class="c">#    kube-system   coredns-86dcddd859-xsdw5     1/1     Running   0          8m43s   10.10.44.218   fargate-ip-10-10-44-218.ap-northeast-2.compute.internal   &lt;none&gt;           &lt;none&gt;  </span>
  
  <span class="c"># node가 없어서 pod가 Pending 상태였던 것이, </span>
  <span class="c"># fargate로 프로비저닝되면서 fargate computing node가 생겨서 Running 상태로 변경되었음을 확인할 수 있습니다.</span>
                            
  <span class="c"># helm chart 확인</span>
  <span class="nv">$ </span>helm list <span class="nt">-n</span> karpenter
  <span class="c"># =&gt; NAME            NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                   APP VERSION</span>
  <span class="c">#    karpenter       karpenter       1               2024-07-25 23:54:59.19597 +0900 KST     deployed        karpenter-0.35.0        0.35.0</span>
  
  <span class="c"># 시크릿 확인 : kms로 암호 처리됨</span>
  <span class="nv">$ </span>kubectl get secret <span class="nt">-n</span> karpenter
  <span class="c"># =&gt; NAME                              TYPE                 DATA   AGE</span>
  <span class="c">#    sh.helm.release.v1.karpenter.v1   helm.sh/release.v1   1      12m</span>
  <span class="nv">$ </span>kubectl get secret <span class="nt">-n</span> karpenter sh.helm.release.v1.karpenter.v1 <span class="nt">-o</span> json | jq

  <span class="c"># 상세 정보 확인</span>
  <span class="nv">$ </span>terraform state list
  <span class="nv">$ </span>terraform state show <span class="s1">'data.aws_ecrpublic_authorization_token.token'</span>
  <span class="nv">$ </span>terraform state show <span class="s1">'aws_eks_access_entry.karpenter_node_access_entry'</span>
  <span class="nv">$ </span>terraform state show <span class="s1">'module.eks_blueprints_addons.data.aws_caller_identity.current'</span>
  <span class="nv">$ </span>terraform state show <span class="s1">'module.eks_blueprints_addons.data.aws_eks_addon_version.this["coredns"]'</span>
  <span class="nv">$ </span>terraform state show <span class="s1">'module.eks_blueprints_addons.aws_cloudwatch_event_rule.karpenter["health_event"]'</span>
  <span class="nv">$ </span>terraform state show <span class="s1">'module.eks_blueprints_addons.aws_cloudwatch_event_target.karpenter["health_event"]'</span>
  <span class="nv">$ </span>terraform state show <span class="s1">'module.eks_blueprints_addons.aws_eks_addon.this["coredns"]'</span>
  <span class="nv">$ </span>terraform state show <span class="s1">'module.eks_blueprints_addons.aws_iam_role.karpenter[0]'</span>
  <span class="nv">$ </span>terraform state show <span class="s1">'module.eks_blueprints_addons.aws_iam_instance_profile.karpenter[0]'</span>
  <span class="nv">$ </span>terraform state show <span class="s1">'module.eks_blueprints_addons.module.karpenter.data.aws_iam_policy_document.this[0]'</span>
  <span class="nv">$ </span>terraform state show <span class="s1">'module.eks_blueprints_addons.module.karpenter.data.aws_iam_policy_document.assume[0]'</span>
  <span class="nv">$ </span>terraform state show <span class="s1">'module.eks_blueprints_addons.module.karpenter.aws_iam_policy.this[0]'</span>
  <span class="nv">$ </span>terraform state show <span class="s1">'module.eks_blueprints_addons.module.karpenter.helm_release.this[0]'</span>
  <span class="nv">$ </span>terraform state show <span class="s1">'module.eks_blueprints_addons.module.karpenter_sqs.aws_sqs_queue.this[0]'</span>
  <span class="nv">$ </span>terraform state show <span class="s1">'module.eks_blueprints_addons.module.karpenter_sqs.aws_sqs_queue_policy.this[0]'</span>
</code></pre></div></div>

<p><img src="/assets/2024/t101-4th/20240727_terraform_w7_eks_karpenter_12.png" alt="Karpenter 배포 결과" class="image-center" />
<em class="image-caption">Karpenter 배포 결과</em></p>

<p><img src="/assets/2024/t101-4th/20240727_terraform_w7_eks_karpenter_13.png" alt="Addon 설치 결과" class="image-center" />
<em class="image-caption">Addon 설치 결과</em></p>

<p><img src="/assets/2024/t101-4th/20240727_terraform_w7_eks_karpenter_14.png" alt="Karpenter 배포 결과 (접근 IAM 정보)" class="image-center" />
<em class="image-caption">Karpenter 배포 결과 (접근 IAM 정보)</em></p>

<p><img src="/assets/2024/t101-4th/20240727_terraform_w7_eks_karpenter_15.png" alt="EC2 목록 (아직은 EC2 인스턴스가 없음)" class="image-center" />
<em class="image-caption">EC2 목록 (아직은 EC2 인스턴스가 없음)</em></p>

<p><img src="/assets/2024/t101-4th/20240727_terraform_w7_eks_karpenter_16.png" alt="Karpenter의 Autoscaling 관련 룰" class="image-center" />
<em class="image-caption">Karpenter의 Autoscaling 관련 룰</em></p>

<p><img src="/assets/2024/t101-4th/20240727_terraform_w7_eks_karpenter_35.png" alt="Karpenter의 SQS Queue" />
<em class="image-caption">Karpenter의 SQS Queue</em></p>

<ul>
  <li>
    <p>eks-node-viewer 설치 및 사용법
<img src="/assets/2024/t101-4th/20240727_terraform_w7_eks_karpenter_17.png" alt="eks-node-viewer" class="image-center" />
<em class="image-caption">eks-node-viewer</em></p>

    <ul>
      <li>노드 할당 가능 용량과 요청 request 리소스 표시해줍니다.</li>
      <li>실제 파드의 리소스 사용량과는 차이가 있으니 참고용으로만 사용해야 할것입니다.</li>
      <li>설치
        <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># MacOS 기준 설치</span>
<span class="nv">$ </span>brew tap aws/tap
<span class="nv">$ </span>brew <span class="nb">install </span>eks-node-viewer
</code></pre></div>        </div>
      </li>
      <li>사용
        <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># [신규 터미널] Display both CPU and Memory Usage*</span>
eks-node-viewer <span class="nt">--resources</span> cpu,memory
    
<span class="c"># Standard usage</span>
eks-node-viewer
    
<span class="c"># Karpenter nodes only</span>
eks-node-viewer <span class="nt">--node-selector</span> karpenter.sh/nodepool
    
<span class="c"># Display extra labels, i.e. AZ</span>
eks-node-viewer <span class="nt">--extra-labels</span> topology.kubernetes.io/zone
    
<span class="c"># Sort by CPU usage in descending order</span>
eks-node-viewer <span class="nt">--node-sort</span><span class="o">=</span>eks-node-viewer/node-cpu-usage<span class="o">=</span>dsc
    
<span class="c"># Specify a particular AWS profile and region</span>
<span class="nv">AWS_PROFILE</span><span class="o">=</span>myprofile <span class="nv">AWS_REGION</span><span class="o">=</span>us-west-2
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>
    <p>kube-ops-view 설치 및 사용법
<img src="/assets/2024/t101-4th/20240727_terraform_w7_eks_karpenter_20.png" alt="kube-ops-view" class="image-center" />
<em class="image-caption">kube-ops-view</em></p>

    <ul>
      <li>노드의 파드 상태정보를 웹페이지에서 실시간으로 출력해줍니다.</li>
      <li>설치 및 사용
        <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>helm repo add geek-cookbook https://geek-cookbook.github.io/charts/
<span class="c"># =&gt; "geek-cookbook" has been added to your repositories</span>
    
<span class="nv">$ </span>helm <span class="nb">install </span>kube-ops-view geek-cookbook/kube-ops-view <span class="nt">--version</span> 1.2.2 <span class="nt">--set</span> env.TZ<span class="o">=</span><span class="s2">"Asia/Seoul"</span> <span class="nt">--namespace</span> kube-system

<span class="c"># 포트포워딩</span>
<span class="nv">$ </span>kubectl port-forward deployment/kube-ops-view <span class="nt">-n</span> kube-system 8080:8080 &amp;

<span class="c"># 접속 주소 확인 : 각각 1배, 1.5배, 3배 크기</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"KUBE-OPS-VIEW URL = http://localhost:8080"</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"KUBE-OPS-VIEW URL = http://localhost:8080/#scale=1.5"</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"KUBE-OPS-VIEW URL = http://localhost:8080/#scale=3"</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
</ul>

<h3 id="실습-karpenter-on-eks-fargate">[실습] Karpenter on EKS Fargate</h3>

<p>이번에는 <code class="language-plaintext highlighter-rouge">Karpenter</code>를 사용해 앞서 설치한 EKS와 Fargate로 구성한 클러스터를 관리해보겠습니다.</p>

<p><code class="language-plaintext highlighter-rouge">Karpenter</code>는 AWS에서 제공하는 오픈소스 프로젝트로, Kubernetes 클러스터를 관리하는데 사용되는 <strong>오토스케일링 프로비저닝 엔진</strong>입니다. Node Auto Scaling과 비슷하나
AWS API를 통해 빠르고 <strong>효율적으로 워커노드를 스케일링</strong>하며, <strong>높은 가용성</strong>, <strong>비용최적화</strong>를 제공합니다. <a href="https://karpenter.sh/">Karpenter 소개</a></p>

<p>먼저 replicas가 0인 pod를 생성하고, replica 수를 늘려서 <code class="language-plaintext highlighter-rouge">Karpenter</code>를 이용하여 autoscaling을 진행해보겠습니다.</p>

<ul>
  <li>karpenter.yml 생성
    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># karpenter.yml</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">karpenter.k8s.aws/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">EC2NodeClass</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">default</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">amiFamily</span><span class="pi">:</span> <span class="s">AL2</span>
  <span class="na">role</span><span class="pi">:</span> <span class="s">karpenter-t101-karpenter</span>
  <span class="na">subnetSelectorTerms</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">tags</span><span class="pi">:</span>
        <span class="na">karpenter.sh/discovery</span><span class="pi">:</span> <span class="s">t101-karpenter</span>
  <span class="na">securityGroupSelectorTerms</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">tags</span><span class="pi">:</span>
        <span class="na">karpenter.sh/discovery</span><span class="pi">:</span> <span class="s">t101-karpenter</span>
  <span class="na">tags</span><span class="pi">:</span>
    <span class="na">karpenter.sh/discovery</span><span class="pi">:</span> <span class="s">t101-karpenter</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">karpenter.sh/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">NodePool</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">default</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">nodeClassRef</span><span class="pi">:</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">default</span>
      <span class="na">requirements</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s2">"</span><span class="s">karpenter.k8s.aws/instance-category"</span>
          <span class="na">operator</span><span class="pi">:</span> <span class="s">In</span>
          <span class="na">values</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">c"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">m"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">r"</span><span class="pi">]</span>
        <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s2">"</span><span class="s">karpenter.k8s.aws/instance-cpu"</span>
          <span class="na">operator</span><span class="pi">:</span> <span class="s">In</span>
          <span class="na">values</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">4"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">8"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">16"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">32"</span><span class="pi">]</span>
        <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s2">"</span><span class="s">karpenter.k8s.aws/instance-hypervisor"</span>
          <span class="na">operator</span><span class="pi">:</span> <span class="s">In</span>
          <span class="na">values</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">nitro"</span><span class="pi">]</span>
        <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s2">"</span><span class="s">karpenter.k8s.aws/instance-generation"</span>
          <span class="na">operator</span><span class="pi">:</span> <span class="s">Gt</span>
          <span class="na">values</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">2"</span><span class="pi">]</span>
  <span class="na">limits</span><span class="pi">:</span>
    <span class="na">cpu</span><span class="pi">:</span> <span class="m">1000</span>
  <span class="na">disruption</span><span class="pi">:</span>
    <span class="na">consolidationPolicy</span><span class="pi">:</span> <span class="s">WhenEmpty</span>
    <span class="na">consolidateAfter</span><span class="pi">:</span> <span class="s">30s</span>   
</code></pre></div>    </div>
  </li>
  <li>example.yml 생성
    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">inflate</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">0</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">inflate</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">app</span><span class="pi">:</span> <span class="s">inflate</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
      <span class="na">containers</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">inflate</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">public.ecr.aws/eks-distro/kubernetes/pause:3.7</span>
          <span class="na">resources</span><span class="pi">:</span>
            <span class="na">requests</span><span class="pi">:</span>
              <span class="na">cpu</span><span class="pi">:</span> <span class="s">1</span>     
</code></pre></div>    </div>
  </li>
  <li>karpenter 실행
    <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl apply <span class="nt">-f</span> karpenter.yml                           
<span class="c"># =&gt; ec2nodeclass.karpenter.k8s.aws/default created</span>
<span class="c">#    nodepool.karpenter.sh/default created</span>
    
<span class="c"># 확인</span>
<span class="nv">$ </span>kubectl get ec2nodeclass,nodepool
<span class="c"># =&gt; NAME                                     AGE</span>
<span class="c">#    ec2nodeclass.karpenter.k8s.aws/default   98s</span>
<span class="c">#    </span>
<span class="c">#    NAME                            NODECLASS</span>
<span class="c">#    nodepool.karpenter.sh/default   default</span>
    
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> example.yml
    
<span class="nv">$ </span>kubectl get deploy
<span class="c"># =&gt; NAME      READY   UP-TO-DATE   AVAILABLE   AGE</span>
<span class="c">#    inflate   0/0     0            0           11s</span>
    
<span class="nv">$ </span>karpenter 컨트롤러 로그 확인
<span class="nv">$ </span>kubectl logs <span class="nt">-f</span> <span class="nt">-n</span> karpenter <span class="nt">-l</span> app.kubernetes.io/name<span class="o">=</span>karpenter <span class="nt">-c</span> controller

<span class="c"># karpenter를 이용한 autoscaling 확인</span>
<span class="nv">$ </span>kubectl scale deployment inflate <span class="nt">--replicas</span><span class="o">=</span>3 <span class="o">&amp;&amp;</span> kubectl get pod <span class="nt">-w</span>
<span class="c"># =&gt; deployment.apps/inflate scaled</span>
<span class="c">#    NAME                       READY   STATUS    RESTARTS   AGE</span>
<span class="c">#    inflate-66fb68585c-8wnvs   0/1     Pending   0          0s</span>
<span class="c">#    inflate-66fb68585c-p5j7d   0/1     Pending   0          0s</span>
<span class="c">#    inflate-66fb68585c-sdqzz   0/1     Pending   0          0s</span>
</code></pre></div>    </div>

    <p><img src="/assets/2024/t101-4th/20240727_terraform_w7_eks_karpenter_25.png" alt="autoscaling 이 진행되지 않고 대기중인 모습" class="image-center" />
<em class="image-caption">autoscaling 이 진행되지 않고 대기중인 모습</em></p>

    <ul>
      <li>위의 그림처럼 scaling 후 pending 상태로 오래 대기중이어서 log를 확인해보겠습니다.
        <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># karpenter 컨트롤러 로그 확인</span>
<span class="nv">$ </span>kubectl logs <span class="nt">-f</span> <span class="nt">-n</span> karpenter <span class="nt">-l</span> app.kubernetes.io/name<span class="o">=</span>karpenter <span class="nt">-c</span> controller
<span class="c"># =&gt; {</span>
<span class="c">#      "level":"ERROR","time":"2024-07-25T15:36:58.791Z",</span>
<span class="c">#      ...</span>
<span class="c">#      "error":"launching nodeclaim, creating instance, with fleet error(s), </span>
<span class="c">#               AuthFailure.ServiceLinkedRoleCreationNotPermitted: </span>
<span class="c">#               The provided credentials do not have permission to </span>
<span class="c">#               create the service-linked role for EC2 Spot Instances."</span>
<span class="c">#    }</span>
</code></pre></div>        </div>
        <ul>
          <li>현재 ServiceLinkedRole 관련 기능이 없어서 생기는 문제 인것 같습니다. 아래의 명령을 실행하면 role이 생성되면서 오토 스케일링이 진행됩니다.
            <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>aws iam create-service-linked-role <span class="nt">--aws-service-name</span> spot.amazonaws.com
</code></pre></div>            </div>
          </li>
        </ul>

        <p><img src="/assets/2024/t101-4th/20240727_terraform_w7_eks_karpenter_26.png" alt="정상적으로 Autoscaling 된 결과" class="image-center" />
<em class="image-caption image-caption--img-shadow">정상적으로 Autoscaling 된 결과</em></p>

        <p><img src="/assets/2024/t101-4th/20240727_terraform_w7_eks_karpenter_30.png" alt="정상적으로 Autoscaling 된 결과 (Pod 상세 정보)" class="image-center" />
<em class="image-caption image-caption--img-shadow">정상적으로 Autoscaling 된 결과 (Pod 상세 정보)</em></p>

        <ul>
          <li>위의 그림들 처럼 정상적으로 autoscaling이 진행되고, 5개 =&gt; 6개로 Node 가 추가되고 pod가 생성되어 실행되는 것을 확인할 수 있습니다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>karpenter 삭제
    <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#</span>
<span class="nv">$ </span>kubectl get nodes <span class="nt">-L</span> karpenter.sh/nodepool <span class="nt">-L</span> node.kubernetes.io/instance-type <span class="nt">-L</span> topology.kubernetes.io/zone <span class="nt">-L</span> karpenter.sh/capacity-type
<span class="c"># =&gt; NAME                                                      STATUS   ROLES    AGE    VERSION               NODEPOOL   INSTANCE-TYPE   ZONE              CAPACITY-TYPE</span>
<span class="c">#    fargate-ip-10-10-18-179.ap-northeast-2.compute.internal   Ready    &lt;none&gt;   53m    v1.30.0-eks-404b9c6                              ap-northeast-2b</span>
<span class="c">#    fargate-ip-10-10-25-19.ap-northeast-2.compute.internal    Ready    &lt;none&gt;   30m    v1.30.0-eks-404b9c6                              ap-northeast-2b</span>
<span class="c">#    fargate-ip-10-10-26-139.ap-northeast-2.compute.internal   Ready    &lt;none&gt;   53m    v1.30.0-eks-404b9c6                              ap-northeast-2b</span>
<span class="c">#    fargate-ip-10-10-41-162.ap-northeast-2.compute.internal   Ready    &lt;none&gt;   53m    v1.30.0-eks-404b9c6                              ap-northeast-2c</span>
<span class="c">#    fargate-ip-10-10-44-218.ap-northeast-2.compute.internal   Ready    &lt;none&gt;   53m    v1.30.0-eks-404b9c6                              ap-northeast-2c</span>
<span class="c">#    ip-10-10-30-139.ap-northeast-2.compute.internal           Ready    &lt;none&gt;   2m3s   v1.30.0-eks-036c24b   default    c6gn.xlarge     ap-northeast-2b   spot</span>

<span class="c">#</span>
<span class="nv">$ </span>kubectl get nodeclaims
<span class="c"># =&gt; default-mkbpj   c6gn.xlarge   ap-northeast-2b   ip-10-10-30-139.ap-northeast-2.compute.internal   True    18m</span>
<span class="nv">$ </span>kubectl get nodeclaims <span class="nt">-o</span> yaml | kubectl neat
    
<span class="c"># deploy 삭제 &gt;&gt; 노드 변화 확인 해보기!</span>
<span class="nv">$ </span>kubectl delete <span class="nt">-f</span> example.yml
<span class="c"># =&gt; deployment.apps "inflate" deleted</span>
    
<span class="c"># karpenter 정책 삭제</span>
<span class="nv">$ </span>kubectl delete <span class="nt">-f</span> karpenter.yml
<span class="c"># =&gt; ec2nodeclass.karpenter.k8s.aws "default" deleted</span>
<span class="c">#    nodepool.karpenter.sh "default" deleted</span>
</code></pre></div>    </div>
  </li>
  <li>삭제
    <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># kube-ops-view 삭제</span>
<span class="nv">$ </span>helm uninstall kube-ops-view <span class="nt">-n</span> kube-system
    
<span class="c"># addon &amp; karpenter helm 삭제</span>
<span class="nv">$ </span>terraform destroy <span class="nt">-target</span><span class="o">=</span><span class="s2">"module.eks_blueprints_addons"</span> <span class="nt">-auto-approve</span>
<span class="c"># =&gt; Destroy complete! Resources: 24 destroyed.</span>
    
<span class="c"># EKS 삭제</span>
<span class="nv">$ </span>terraform destroy <span class="nt">-target</span><span class="o">=</span><span class="s2">"module.eks"</span> <span class="nt">-auto-approve</span>
<span class="c"># =&gt; Destroy complete! Resources: 24 destroyed.</span>
    
<span class="c"># VPC 삭제 : vpc 삭제가 잘 안될 경우 aws 콘솔에서 vpc 수동 삭제 -&gt; vnic 등 남아 있을 경우 해당 vnic 강제 삭제</span>
<span class="nv">$ </span>terraform destroy <span class="nt">-auto-approve</span>
<span class="c"># =&gt; Destroy complete! Resources: 23 destroyed.</span>
    
<span class="c"># VPC 삭제 확인</span>
<span class="nv">$ </span>aws ec2 describe-vpcs <span class="nt">--filter</span> <span class="s1">'Name=isDefault,Values=false'</span> <span class="nt">--output</span> yaml
<span class="c"># =&gt; Vpcs: []</span>
    
<span class="c"># 잘 삭제되었습니다.</span>
    
<span class="c"># kubeconfig 삭제</span>
<span class="nv">$ </span><span class="nb">rm</span> <span class="nt">-rf</span> ~/.kube/config
</code></pre></div>    </div>
  </li>
</ul>

<h2 id="eks-workshop">EKS Workshop</h2>

<ul>
  <li>이번에는 AWS에서 제공하는 EKS Workshop에서 소개하는 방법으로 EKS를 배포해보겠습니다.
    <ul>
      <li>관련링크 :
<a href="https://eksworkshop.com/docs/introduction/">EKS Workshop 소개</a>, 
<a href="https://github.com/aws-samples/eks-workshop-v2">EKS Workshop Github</a>,
<a href="https://www.youtube.com/watch?v=E956xeOt050">유튜브</a></li>
    </ul>
  </li>
</ul>

<h3 id="실습-eks-배포">[실습] EKS 배포</h3>

<h4 id="사전준비-1">사전준비</h4>

<ul>
  <li>awscli, terraform, kubectl 설치</li>
  <li>코드 준비 <a href="https://github.com/aws-samples/eks-workshop-v2">Github</a>
    <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>git clone https://github.com/aws-samples/eks-workshop-v2
<span class="nv">$ </span><span class="nb">cd </span>eks-workshop-v2/cluster/terraform
</code></pre></div>    </div>
    <ul>
      <li>providers.tf
        <div class="language-tf highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">provider</span> <span class="s2">"aws"</span> <span class="p">{</span>
  <span class="nx">default_tags</span> <span class="p">{</span>
    <span class="nx">tags</span> <span class="p">=</span> <span class="kd">local</span><span class="p">.</span><span class="nx">tags</span>
  <span class="p">}</span>
<span class="p">}</span>
    
<span class="k">terraform</span> <span class="p">{</span>
  <span class="nx">required_providers</span> <span class="p">{</span>
    <span class="nx">aws</span> <span class="p">=</span> <span class="p">{</span>
      <span class="nx">source</span>  <span class="p">=</span> <span class="s2">"hashicorp/aws"</span>
      <span class="nx">version</span> <span class="p">=</span> <span class="s2">"&gt;= 4.67.0"</span>
    <span class="p">}</span>
  <span class="p">}</span>
    
  <span class="nx">required_version</span> <span class="p">=</span> <span class="s2">"&gt;= 1.4.2"</span>
<span class="p">}</span> 
</code></pre></div>        </div>
      </li>
      <li>variables.tf - cluster 이름, cluster 버전, ami 버전, vpc cidr 변경 가능
        <div class="language-tf highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">variable</span> <span class="s2">"cluster_name"</span> <span class="p">{</span>
  <span class="nx">description</span> <span class="p">=</span> <span class="s2">"Name of the EKS cluster"</span>
  <span class="nx">type</span>        <span class="p">=</span> <span class="nx">string</span>
  <span class="nx">default</span>     <span class="p">=</span> <span class="s2">"t101-eks-workshop"</span>
<span class="p">}</span>
    
<span class="k">variable</span> <span class="s2">"cluster_version"</span> <span class="p">{</span>
  <span class="nx">description</span> <span class="p">=</span> <span class="s2">"EKS cluster version."</span>
  <span class="nx">type</span>        <span class="p">=</span> <span class="nx">string</span>
  <span class="nx">default</span>     <span class="p">=</span> <span class="s2">"1.30"</span>
<span class="p">}</span>
    
<span class="k">variable</span> <span class="s2">"ami_release_version"</span> <span class="p">{</span>
  <span class="nx">description</span> <span class="p">=</span> <span class="s2">"Default EKS AMI release version for node groups"</span>
  <span class="nx">type</span>        <span class="p">=</span> <span class="nx">string</span>
  <span class="nx">default</span>     <span class="p">=</span> <span class="s2">"1.30.0-20240625"</span>
<span class="p">}</span>
    
<span class="k">variable</span> <span class="s2">"vpc_cidr"</span> <span class="p">{</span>
  <span class="nx">description</span> <span class="p">=</span> <span class="s2">"Defines the CIDR block used on Amazon VPC created for Amazon EKS."</span>
  <span class="nx">type</span>        <span class="p">=</span> <span class="nx">string</span>
  <span class="nx">default</span>     <span class="p">=</span> <span class="s2">"10.42.0.0/16"</span>
<span class="p">}</span>
</code></pre></div>        </div>
      </li>
      <li>vpc.tf
        <div class="language-tf highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">locals</span> <span class="p">{</span>
  <span class="nx">private_subnets</span> <span class="p">=</span> <span class="p">[</span><span class="nx">for</span> <span class="nx">k</span><span class="p">,</span> <span class="nx">v</span> <span class="nx">in</span> <span class="kd">local</span><span class="p">.</span><span class="nx">azs</span> <span class="err">:</span> <span class="nx">cidrsubnet</span><span class="p">(</span><span class="kd">var</span><span class="p">.</span><span class="nx">vpc_cidr</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="nx">k</span> <span class="err">+</span> <span class="mi">3</span><span class="p">)]</span>
  <span class="nx">public_subnets</span>  <span class="p">=</span> <span class="p">[</span><span class="nx">for</span> <span class="nx">k</span><span class="p">,</span> <span class="nx">v</span> <span class="nx">in</span> <span class="kd">local</span><span class="p">.</span><span class="nx">azs</span> <span class="err">:</span> <span class="nx">cidrsubnet</span><span class="p">(</span><span class="kd">var</span><span class="p">.</span><span class="nx">vpc_cidr</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="nx">k</span><span class="p">)]</span>
  <span class="nx">azs</span>             <span class="p">=</span> <span class="nx">slice</span><span class="p">(</span><span class="k">data</span><span class="p">.</span><span class="nx">aws_availability_zones</span><span class="p">.</span><span class="nx">available</span><span class="p">.</span><span class="nx">names</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="p">}</span>
    
<span class="k">data</span> <span class="s2">"aws_availability_zones"</span> <span class="s2">"available"</span> <span class="p">{</span>
  <span class="nx">state</span> <span class="p">=</span> <span class="s2">"available"</span>
<span class="p">}</span>
    
<span class="k">module</span> <span class="s2">"vpc"</span> <span class="p">{</span>
  <span class="nx">source</span>  <span class="p">=</span> <span class="s2">"terraform-aws-modules/vpc/aws"</span>
  <span class="nx">version</span> <span class="p">=</span> <span class="s2">"~&gt; 5.1"</span>
    
  <span class="nx">name</span> <span class="p">=</span> <span class="kd">var</span><span class="p">.</span><span class="nx">cluster_name</span>
  <span class="nx">cidr</span> <span class="p">=</span> <span class="kd">var</span><span class="p">.</span><span class="nx">vpc_cidr</span>
    
  <span class="nx">azs</span>                   <span class="p">=</span> <span class="kd">local</span><span class="p">.</span><span class="nx">azs</span>
  <span class="nx">public_subnets</span>        <span class="p">=</span> <span class="kd">local</span><span class="p">.</span><span class="nx">public_subnets</span>
  <span class="nx">private_subnets</span>       <span class="p">=</span> <span class="kd">local</span><span class="p">.</span><span class="nx">private_subnets</span>
  <span class="nx">public_subnet_suffix</span>  <span class="p">=</span> <span class="s2">"SubnetPublic"</span>
  <span class="nx">private_subnet_suffix</span> <span class="p">=</span> <span class="s2">"SubnetPrivate"</span>
    
  <span class="nx">enable_nat_gateway</span>   <span class="p">=</span> <span class="kc">true</span>
  <span class="nx">create_igw</span>           <span class="p">=</span> <span class="kc">true</span>
  <span class="nx">enable_dns_hostnames</span> <span class="p">=</span> <span class="kc">true</span>
  <span class="nx">single_nat_gateway</span>   <span class="p">=</span> <span class="kc">true</span>
    
  <span class="c1"># Manage so we can name</span>
  <span class="nx">manage_default_network_acl</span>    <span class="p">=</span> <span class="kc">true</span>
  <span class="nx">default_network_acl_tags</span>      <span class="p">=</span> <span class="p">{</span> <span class="nx">Name</span> <span class="p">=</span> <span class="s2">"</span><span class="k">${</span><span class="kd">var</span><span class="p">.</span><span class="nx">cluster_name</span><span class="k">}</span><span class="s2">-default"</span> <span class="p">}</span>
  <span class="nx">manage_default_route_table</span>    <span class="p">=</span> <span class="kc">true</span>
  <span class="nx">default_route_table_tags</span>      <span class="p">=</span> <span class="p">{</span> <span class="nx">Name</span> <span class="p">=</span> <span class="s2">"</span><span class="k">${</span><span class="kd">var</span><span class="p">.</span><span class="nx">cluster_name</span><span class="k">}</span><span class="s2">-default"</span> <span class="p">}</span>
  <span class="nx">manage_default_security_group</span> <span class="p">=</span> <span class="kc">true</span>
  <span class="nx">default_security_group_tags</span>   <span class="p">=</span> <span class="p">{</span> <span class="nx">Name</span> <span class="p">=</span> <span class="s2">"</span><span class="k">${</span><span class="kd">var</span><span class="p">.</span><span class="nx">cluster_name</span><span class="k">}</span><span class="s2">-default"</span> <span class="p">}</span>
    
  <span class="nx">public_subnet_tags</span> <span class="p">=</span> <span class="nx">merge</span><span class="p">(</span><span class="kd">local</span><span class="p">.</span><span class="nx">tags</span><span class="p">,</span> <span class="p">{</span>
    <span class="s2">"kubernetes.io/role/elb"</span> <span class="p">=</span> <span class="s2">"1"</span>
  <span class="p">})</span>
  <span class="nx">private_subnet_tags</span> <span class="p">=</span> <span class="nx">merge</span><span class="p">(</span><span class="kd">local</span><span class="p">.</span><span class="nx">tags</span><span class="p">,</span> <span class="p">{</span>
    <span class="s2">"karpenter.sh/discovery"</span> <span class="p">=</span> <span class="kd">var</span><span class="p">.</span><span class="nx">cluster_name</span>
  <span class="p">})</span>
    
  <span class="nx">tags</span> <span class="p">=</span> <span class="kd">local</span><span class="p">.</span><span class="nx">tags</span>
<span class="p">}</span>
</code></pre></div>        </div>
      </li>
      <li>main.tf
        <div class="language-tf highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">locals</span> <span class="p">{</span>
  <span class="nx">tags</span> <span class="p">=</span> <span class="p">{</span>
    <span class="nx">created-by</span> <span class="p">=</span> <span class="s2">"eks-workshop-v2"</span>
    <span class="nx">study</span>      <span class="p">=</span> <span class="s2">"t101"</span>             <span class="c1"># 태그 추가</span>
    <span class="nx">for</span>        <span class="p">=</span> <span class="s2">"Sweet Little Bird"</span>
    <span class="nx">env</span>        <span class="p">=</span> <span class="kd">var</span><span class="p">.</span><span class="nx">cluster_name</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div>        </div>
      </li>
      <li>eks.tf
        <div class="language-tf highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">module</span> <span class="s2">"eks"</span> <span class="p">{</span>
  <span class="nx">source</span>  <span class="p">=</span> <span class="s2">"terraform-aws-modules/eks/aws"</span>
  <span class="nx">version</span> <span class="p">=</span> <span class="s2">"~&gt; 20.0"</span>
    
  <span class="nx">cluster_name</span>                   <span class="p">=</span> <span class="kd">var</span><span class="p">.</span><span class="nx">cluster_name</span>
  <span class="nx">cluster_version</span>                <span class="p">=</span> <span class="kd">var</span><span class="p">.</span><span class="nx">cluster_version</span>
  <span class="nx">cluster_endpoint_public_access</span> <span class="p">=</span> <span class="kc">true</span>
    
  <span class="nx">cluster_addons</span> <span class="p">=</span> <span class="p">{</span>
    <span class="nx">vpc-cni</span> <span class="p">=</span> <span class="p">{</span>
      <span class="nx">before_compute</span> <span class="p">=</span> <span class="kc">true</span>
      <span class="nx">most_recent</span>    <span class="p">=</span> <span class="kc">true</span>
      <span class="nx">configuration_values</span> <span class="p">=</span> <span class="nx">jsonencode</span><span class="p">({</span>
        <span class="nx">env</span> <span class="p">=</span> <span class="p">{</span>
          <span class="nx">ENABLE_POD_ENI</span>                    <span class="p">=</span> <span class="s2">"true"</span>
          <span class="nx">ENABLE_PREFIX_DELEGATION</span>          <span class="p">=</span> <span class="s2">"true"</span>
          <span class="nx">POD_SECURITY_GROUP_ENFORCING_MODE</span> <span class="p">=</span> <span class="s2">"standard"</span>
        <span class="p">}</span>
        <span class="nx">nodeAgent</span> <span class="p">=</span> <span class="p">{</span>
          <span class="nx">enablePolicyEventLogs</span> <span class="p">=</span> <span class="s2">"true"</span>
        <span class="p">}</span>
        <span class="nx">enableNetworkPolicy</span> <span class="p">=</span> <span class="s2">"true"</span>
      <span class="p">})</span>
    <span class="p">}</span>
  <span class="p">}</span>
    
  <span class="nx">vpc_id</span>     <span class="p">=</span> <span class="k">module</span><span class="p">.</span><span class="nx">vpc</span><span class="p">.</span><span class="nx">vpc_id</span>
  <span class="nx">subnet_ids</span> <span class="p">=</span> <span class="k">module</span><span class="p">.</span><span class="nx">vpc</span><span class="p">.</span><span class="nx">private_subnets</span>
    
  <span class="nx">create_cluster_security_group</span> <span class="p">=</span> <span class="kc">false</span>
  <span class="nx">create_node_security_group</span>    <span class="p">=</span> <span class="kc">false</span>
    
  <span class="nx">eks_managed_node_groups</span> <span class="p">=</span> <span class="p">{</span>
    <span class="nx">default</span> <span class="p">=</span> <span class="p">{</span>
      <span class="nx">instance_types</span>       <span class="p">=</span> <span class="p">[</span><span class="s2">"m5.large"</span><span class="p">]</span>
      <span class="nx">force_update_version</span> <span class="p">=</span> <span class="kc">true</span>
      <span class="nx">release_version</span>      <span class="p">=</span> <span class="kd">var</span><span class="p">.</span><span class="nx">ami_release_version</span>
    
      <span class="nx">min_size</span>     <span class="p">=</span> <span class="mi">3</span>
      <span class="nx">max_size</span>     <span class="p">=</span> <span class="mi">6</span>
      <span class="nx">desired_size</span> <span class="p">=</span> <span class="mi">3</span>
    
      <span class="nx">update_config</span> <span class="p">=</span> <span class="p">{</span>
        <span class="nx">max_unavailable_percentage</span> <span class="p">=</span> <span class="mi">50</span>
      <span class="p">}</span>
    
      <span class="nx">labels</span> <span class="p">=</span> <span class="p">{</span>
        <span class="nx">workshop-default</span> <span class="p">=</span> <span class="s2">"yes"</span>
      <span class="p">}</span>
    <span class="p">}</span>
  <span class="p">}</span>
    
  <span class="nx">tags</span> <span class="p">=</span> <span class="nx">merge</span><span class="p">(</span><span class="kd">local</span><span class="p">.</span><span class="nx">tags</span><span class="p">,</span> <span class="p">{</span>
    <span class="s2">"karpenter.sh/discovery"</span> <span class="p">=</span> <span class="kd">var</span><span class="p">.</span><span class="nx">cluster_name</span>
  <span class="p">})</span>
<span class="p">}</span>    
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
</ul>

<h4 id="terraform-init">terraform init</h4>

<p>테라폼을 사용하기위해 초기화하고, 설치된 모듈 및 프로바이더 정보를 확인해보겠습니다.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>terraform init
<span class="nv">$ </span>tree .terraform

<span class="c"># 설치된 모듈 확인</span>
<span class="nv">$ </span><span class="nb">cat</span> .terraform/modules/modules.json | jq

<span class="c"># 설치된 프로바이더 및 버전 확인</span>
<span class="nv">$ </span>tree .terraform/providers/registry.terraform.io/hashicorp <span class="nt">-L</span> 2
<span class="c"># =&gt; .terraform/providers/registry.terraform.io/hashicorp</span>
<span class="c">#    ├── aws</span>
<span class="c">#    │   └── 5.60.0</span>
<span class="c">#    ├── cloudinit</span>
<span class="c">#    │   └── 2.3.4</span>
<span class="c">#    ├── null</span>
<span class="c">#    │   └── 3.2.2</span>
<span class="c">#    ├── time</span>
<span class="c">#    │   └── 0.12.0</span>
<span class="c">#    └── tls</span>
<span class="c">#        └── 4.0.5</span>
</code></pre></div></div>

<h4 id="vpc-배포-1">VPC 배포</h4>

<p>VPC를 먼저 배포하고 관련 정보를 확인해보겠습니다.
EKS와 한꺼번에 배포해도 되지만 진행 상황을 확인하기 위해 VPC를 먼저 배포해보겠습니다.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 설치전 VPC 정보 확인</span>
<span class="nv">$ </span>aws ec2 describe-vpcs <span class="nt">--filter</span> <span class="s1">'Name=isDefault,Values=false'</span> <span class="nt">--output</span> yaml
<span class="c"># =&gt; Vpcs: []     # 현재 VPC가 없습니다.</span>

<span class="c"># VPC 배포</span>
<span class="nv">$ </span>terraform apply <span class="nt">-target</span><span class="o">=</span><span class="s2">"module.vpc"</span> <span class="nt">-auto-approve</span>
<span class="c"># =&gt; Apply complete! Resources: 23 added, 0 changed, 0 destroyed.</span>

<span class="c"># 배포 확인</span>
<span class="nv">$ </span>terraform state list
<span class="nv">$ </span>terraform show
<span class="nv">$ </span>aws ec2 describe-vpcs <span class="nt">--filter</span> <span class="s1">'Name=isDefault,Values=false'</span> <span class="nt">--output</span> yaml
<span class="c"># =&gt; Vpcs:</span>
<span class="c">#    - CidrBlock: 10.42.0.0/16</span>
<span class="c">#      CidrBlockAssociationSet:</span>
<span class="c">#      - AssociationId: vpc-cidr-assoc-0c375c03935938c89</span>
<span class="c">#        CidrBlock: 10.42.0.0/16</span>
<span class="c">#      ...</span>
<span class="c">#      State: available</span>
<span class="c">#      Tags:</span>
<span class="c">#      ...</span>
<span class="c">#      - Key: study                   # 추가한 태그 확인    </span>
<span class="c">#        Value: t101</span>
<span class="c">#      - Key: Name</span>
<span class="c">#        Value: t101-eks-workshop</span>
<span class="c">#      VpcId: vpc-0247fb591c49eab40</span>

<span class="c"># 사용가능한 가용성 존 확인 </span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"data.aws_availability_zones.available"</span> | terraform console

<span class="c"># VPC/subnet 상세 정보 및 tag 확인</span>
<span class="nv">$ </span>terraform state show <span class="s1">'module.vpc.aws_vpc.this[0]'</span>

<span class="nv">$ VPCID</span><span class="o">=</span><span class="si">$(</span><span class="nb">echo </span>module.vpc.vpc_id | terraform console<span class="si">)</span>   <span class="c"># 현재 VPC ID를 $VPCID 변수에 저장</span>
<span class="c"># VPC 의 Subnet 상세 정보 확인 (json)</span>
<span class="nv">$ </span>aws ec2 describe-subnets <span class="nt">--filters</span> <span class="s2">"Name=vpc-id,Values=</span><span class="nv">$VPCID</span><span class="s2">"</span> | jq
<span class="c"># VPC 의 Subnet 상세 정보 확인 (텍스트)</span>
<span class="nv">$ </span>aws ec2 describe-subnets <span class="nt">--filters</span> <span class="s2">"Name=vpc-id,Values=</span><span class="nv">$VPCID</span><span class="s2">"</span> <span class="nt">--output</span> text

<span class="c"># public 서브넷과 private 서브넷 CIDR 확인</span>
<span class="nv">$ </span>terraform state show <span class="s1">'module.vpc.aws_subnet.public[0]'</span>
<span class="nv">$ </span>terraform state show <span class="s1">'module.vpc.aws_subnet.private[0]'</span>
</code></pre></div></div>

<p><img src="/assets/2024/t101-4th/20240727_terraform_w7_eks_karpenter_37.png" alt="VPC 배포 결과 (subnet)" class="image-center" />
<em class="image-caption">VPC 배포 결과 (subnet)</em></p>

<p><img src="/assets/2024/t101-4th/20240727_terraform_w7_eks_karpenter_38.png" alt="VPC 배포 결과 (subnet 태그 정보)" class="image-center" />
<em class="image-caption">VPC 배포 결과 (subnet 태그 정보)</em></p>

<h4 id="eks-배포">EKS 배포</h4>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># EKS 배포</span>
<span class="nv">$ </span>terraform apply <span class="nt">-auto-approve</span> 
<span class="c"># =&gt; Apply complete! Resources: 24 added, 0 changed, 0 destroyed.</span>

<span class="c"># EKS 배포 확인</span>
<span class="nv">$ </span>terraform state list
<span class="c"># =&gt; data.aws_availability_zones.available</span>
<span class="c">#    module.eks.data.aws_caller_identity.current</span>
<span class="c">#    module.eks.data.aws_eks_addon_version.this["vpc-cni"]</span>
<span class="c">#    module.eks.data.aws_iam_policy_document.assume_role_policy[0]</span>
<span class="c">#    module.eks.data.aws_iam_session_context.current</span>
<span class="c">#    module.eks.data.aws_partition.current</span>
<span class="c">#    module.eks.data.tls_certificate.this[0]</span>
<span class="c">#    module.eks.aws_cloudwatch_log_group.this[0]</span>
<span class="c">#    module.eks.aws_ec2_tag.cluster_primary_security_group["created-by"]</span>
<span class="c">#    module.eks.aws_ec2_tag.cluster_primary_security_group["env"]</span>
<span class="c">#    module.eks.aws_ec2_tag.cluster_primary_security_group["karpenter.sh/discovery"]</span>
<span class="c">#    module.eks.aws_ec2_tag.cluster_primary_security_group["study"]</span>
<span class="c">#    module.eks.aws_eks_addon.before_compute["vpc-cni"]</span>
<span class="c">#    module.eks.aws_eks_cluster.this[0]</span>
<span class="c">#    module.eks.aws_iam_openid_connect_provider.oidc_provider[0]</span>
<span class="c">#    module.eks.aws_iam_policy.cluster_encryption[0]</span>
<span class="c">#    module.eks.aws_iam_role.this[0]</span>
<span class="c">#    ...</span>
<span class="c">#    module.eks.module.kms.data.aws_caller_identity.current[0]</span>
<span class="c">#    module.eks.module.kms.data.aws_iam_policy_document.this[0]</span>
<span class="c">#    module.eks.module.kms.data.aws_partition.current[0]</span>
<span class="c">#    module.eks.module.kms.aws_kms_alias.this["cluster"]</span>
<span class="c">#    module.eks.module.kms.aws_kms_key.this[0]</span>
<span class="c">#    module.eks.module.eks_managed_node_group["default"].module.user_data.null_resource.validate_cluster_service_cidr</span>

<span class="c"># EKS 자격 증명 설정 및 확인</span>
<span class="c">## aws eks --region &lt;REGION&gt; update-kubeconfig --name &lt;CLUSTER_NAME&gt; --alias &lt;CLUSTER_NAME&gt;</span>
<span class="nv">$ </span>aws eks <span class="nt">--region</span> ap-northeast-2 update-kubeconfig <span class="nt">--name</span> t101-eks-workshop
<span class="nv">$ </span><span class="nb">cat</span> ~/.kube/config

<span class="c"># k8s 클러스터 정보 확인</span>
<span class="nv">$ </span>kubectl cluster-info
<span class="c"># =&gt; error: You must be logged in to the server (Unauthorized)</span>
</code></pre></div></div>

<p>정상적으로 배포하였고 EKS 자격증명을 설정하였음에도 kubectl 사용시 권한이 없다고 나옵니다.</p>

<p><img src="/assets/2024/t101-4th/20240727_terraform_w7_eks_karpenter_40.png" alt="EKS 배포 결과 (IAM 권한 없음)" class="image-center" /></p>

<p>또한 EKS 콘솔에서도 권한이 없다고 나옵니다.
이는 현재 사용중인 IAM이 해당 k8s 클러스터에 대한 권한이 없기 때문입니다.
권한을 부여하는 작업을 진행하고 계속 배포상태를 확인해보겠습니다.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># EKS 관리용 IAM User 의 access entry 생성</span>
<span class="nv">$ ACCOUNT_ID</span><span class="o">=</span><span class="si">$(</span>aws sts get-caller-identity <span class="nt">--query</span> <span class="s1">'Account'</span> <span class="nt">--output</span> text<span class="si">)</span>
<span class="c"># $ MYIAMUSER=&lt;각자 자신의 IAM User&gt;</span>
<span class="nv">$ MYIAMUSER</span><span class="o">=</span>admin
<span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$ACCOUNT_ID</span> <span class="nv">$MYIAMUSER</span>
<span class="nv">$ </span>aws eks create-access-entry <span class="nt">--cluster-name</span> t101-eks-workshop <span class="nt">--principal-arn</span> arn:aws:iam::<span class="k">${</span><span class="nv">ACCOUNT_ID</span><span class="k">}</span>:user/<span class="k">${</span><span class="nv">MYIAMUSER</span><span class="k">}</span>
<span class="nv">$ </span>aws eks list-access-entries <span class="nt">--cluster-name</span> t101-eks-workshop

<span class="c"># EKS 관리용 IAM User에 AmazonEKSClusterAdminPolicy 연동</span>
<span class="nv">$ </span>aws eks associate-access-policy <span class="nt">--cluster-name</span> t101-eks-workshop <span class="nt">--principal-arn</span> arn:aws:iam::<span class="k">${</span><span class="nv">ACCOUNT_ID</span><span class="k">}</span>:user/<span class="k">${</span><span class="nv">MYIAMUSER</span><span class="k">}</span> <span class="se">\</span>
  <span class="nt">--policy-arn</span> arn:aws:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy <span class="nt">--access-scope</span> <span class="nb">type</span><span class="o">=</span>cluster

<span class="nv">$ </span>aws eks list-associated-access-policies <span class="nt">--cluster-name</span> t101-eks-workshop <span class="nt">--principal-arn</span> arn:aws:iam::<span class="k">${</span><span class="nv">ACCOUNT_ID</span><span class="k">}</span>:user/<span class="k">${</span><span class="nv">MYIAMUSER</span><span class="k">}</span> | jq
<span class="nv">$ </span>aws eks describe-access-entry <span class="nt">--cluster-name</span> t101-eks-workshop <span class="nt">--principal-arn</span> arn:aws:iam::<span class="k">${</span><span class="nv">ACCOUNT_ID</span><span class="k">}</span>:user/<span class="k">${</span><span class="nv">MYIAMUSER</span><span class="k">}</span> | jq
 
<span class="c"># (참고) context name 변경</span>
<span class="nv">$ </span>kubectl config rename-context <span class="s2">"arn:aws:eks:ap-northeast-2:</span><span class="si">$(</span>aws sts get-caller-identity <span class="nt">--query</span> <span class="s1">'Account'</span> <span class="nt">--output</span> text<span class="si">)</span><span class="s2">:cluster/t101-eks-workshop"</span> <span class="s2">"T101-Lab"</span>
<span class="c"># =&gt; Context "arn:aws:eks:ap-northeast-2:123456:cluster/t101-eks-workshop" renamed to "T101-Lab".</span>

<span class="c"># k8s 클러스터, 노드, 파드 정보 확인 - 이제 정상적으로 조회 됩니다.</span>
<span class="nv">$ </span>kubectl cluster-info
<span class="c"># =&gt; Kubernetes control plane is running at https://79B209921C6987FCA6A542FDE1272C2E.gr7.ap-northeast-2.eks.amazonaws.com</span>
<span class="c">#    CoreDNS is running at https://79B209921C6987FCA6A542FDE1272C2E.gr7.ap-northeast-2.eks.amazonaws.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy</span>

<span class="nv">$ </span>kubectl get node
<span class="nv">$ </span>kubectl get nodes <span class="nt">-L</span> node.kubernetes.io/instance-type <span class="nt">-L</span> topology.kubernetes.io/zone
<span class="c"># =&gt; NAME                                               STATUS   ROLES    AGE   VERSION               INSTANCE-TYPE   ZONE</span>
<span class="c">#    ip-10-42-103-44.ap-northeast-2.compute.internal    Ready    &lt;none&gt;   16m   v1.30.0-eks-036c24b   m5.large        ap-northeast-2a</span>
<span class="c">#    ip-10-42-153-180.ap-northeast-2.compute.internal   Ready    &lt;none&gt;   16m   v1.30.0-eks-036c24b   m5.large        ap-northeast-2b</span>
<span class="c">#    ip-10-42-171-220.ap-northeast-2.compute.internal   Ready    &lt;none&gt;   16m   v1.30.0-eks-036c24b   m5.large        ap-northeast-2c</span>

<span class="nv">$ </span>kubectl get pod <span class="nt">-A</span>
<span class="c"># =&gt; NAMESPACE     NAME                      READY   STATUS    RESTARTS   AGE</span>
<span class="c">#    kube-system   aws-node-2jnd5            2/2     Running   0          16m</span>
<span class="c">#    kube-system   aws-node-ftb8g            2/2     Running   0          16m</span>
<span class="c">#    kube-system   aws-node-gn6nx            2/2     Running   0          16m</span>
<span class="c">#    kube-system   coredns-5b9dfbf96-f8bfr   1/1     Running   0          19m</span>
<span class="c">#    kube-system   coredns-5b9dfbf96-rtwfs   1/1     Running   0          19m</span>
<span class="c">#    kube-system   kube-proxy-5zkxj          1/1     Running   0          16m</span>
<span class="c">#    kube-system   kube-proxy-n645w          1/1     Running   0          16m</span>
<span class="c">#    kube-system   kube-proxy-rcbvk          1/1     Running   0          16m</span>
 
<span class="c"># 상세 정보 확인</span>
<span class="nv">$ </span>terraform show
<span class="nv">$ </span>terraform state list
<span class="nv">$ </span>terraform state show <span class="s1">'module.eks.aws_ec2_tag.cluster_primary_security_group["study"]'</span>
<span class="nv">$ </span>terraform state show <span class="s1">'module.eks.aws_eks_addon.before_compute["vpc-cni"]'</span>
<span class="nv">$ </span>terraform state show <span class="s1">'module.eks.aws_eks_cluster.this[0]'</span>
<span class="nv">$ </span>terraform state show <span class="s1">'module.eks.aws_iam_openid_connect_provider.oidc_provider[0]'</span>
<span class="nv">$ </span>terraform state show <span class="s1">'module.eks.aws_iam_policy.cluster_encryption[0]'</span>
<span class="nv">$ </span>terraform state show <span class="s1">'module.eks.time_sleep.this[0]'</span>
<span class="nv">$ </span>terraform state show <span class="s1">'module.eks.module.eks_managed_node_group["default"].aws_eks_node_group.this[0]'</span>
<span class="nv">$ </span>terraform state show <span class="s1">'module.eks.module.eks_managed_node_group["default"].aws_iam_role.this[0]'</span>
<span class="nv">$ </span>terraform state show <span class="s1">'module.eks.module.eks_managed_node_group["default"].aws_launch_template.this[0]'</span>
<span class="nv">$ </span>terraform state show <span class="s1">'module.eks.module.eks_managed_node_group["default"].module.user_data.null_resource.validate_cluster_service_cidr'</span>
<span class="nv">$ </span>terraform state show <span class="s1">'module.eks.module.kms.aws_kms_key.this[0]'</span>
<span class="nv">$ </span>terraform state show <span class="s1">'module.eks.module.kms.aws_kms_alias.this["cluster"]'</span>
</code></pre></div></div>

<p><img src="/assets/2024/t101-4th/20240727_terraform_w7_eks_karpenter_41.png" alt="EKS 배포 결과 (정상)" class="image-center" /></p>

<h4 id="kube-ops-view-설치-및-사용">kube-ops-view 설치 및 사용</h4>
<p>앞서 설치해보았던 kube-ops-view를 이번 kubernetes 클러스터에도 설치해보겠습니다.</p>

<p><img src="/assets/2024/t101-4th/20240727_terraform_w7_eks_karpenter_42.png" alt="kube-ops-view" class="image-center" />
  <em class="image-caption image-caption--img-shadow">kube-ops-view</em></p>

<ul>
  <li>노드의 파드 상태정보를 웹페이지에서 실시간으로 출력해줍니다.</li>
  <li>설치 및 사용
    <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>helm repo add geek-cookbook https://geek-cookbook.github.io/charts/
<span class="c"># =&gt; "geek-cookbook" has been added to your repositories</span>
    
<span class="nv">$ </span>helm <span class="nb">install </span>kube-ops-view geek-cookbook/kube-ops-view <span class="nt">--version</span> 1.2.2 <span class="nt">--set</span> env.TZ<span class="o">=</span><span class="s2">"Asia/Seoul"</span> <span class="nt">--namespace</span> kube-system

<span class="c"># 포트포워딩</span>
<span class="nv">$ </span>kubectl port-forward deployment/kube-ops-view <span class="nt">-n</span> kube-system 8080:8080 &amp;

<span class="c"># 접속 주소 확인 : 각각 1배, 1.5배, 3배 크기</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"KUBE-OPS-VIEW URL = http://localhost:8080"</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"KUBE-OPS-VIEW URL = http://localhost:8080/#scale=1.5"</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"KUBE-OPS-VIEW URL = http://localhost:8080/#scale=3"</span>
</code></pre></div>    </div>
  </li>
</ul>

<h4 id="pod-배포">Pod 배포</h4>

<p>EKS Blueprint에서 제공하는 예제를 이용하여 Pod를 배포해보겠습니다.</p>

<ul>
  <li>example.yml 생성
    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">inflate</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">0</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">inflate</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">app</span><span class="pi">:</span> <span class="s">inflate</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
      <span class="na">containers</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">inflate</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">public.ecr.aws/eks-distro/kubernetes/pause:3.7</span>
          <span class="na">resources</span><span class="pi">:</span>
            <span class="na">requests</span><span class="pi">:</span>
              <span class="na">cpu</span><span class="pi">:</span> <span class="s">1</span>     
</code></pre></div>    </div>
  </li>
  <li>Pod 배포 실습
    <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl create <span class="nt">-f</span> example.yml
<span class="c"># =&gt; deployment.apps/inflate created</span>
  
<span class="nv">$ </span>kubectl get deploy   <span class="c"># replicas가 0이어서 생성된 pod가 없습니다.</span>
<span class="c"># =&gt; NAME      READY   UP-TO-DATE   AVAILABLE   AGE</span>
<span class="c">#    inflate   0/0     0            0           16s</span>
  
<span class="c"># 3개로 scale 해보겠습니다.</span>
<span class="nv">$ </span>kubectl scale deployment inflate <span class="nt">--replicas</span><span class="o">=</span>3 <span class="o">&amp;&amp;</span> kubectl get pod <span class="nt">-w</span>
<span class="c"># =&gt; deployment.apps/inflate scaled</span>
<span class="c">#    NAME                       READY   STATUS              RESTARTS   AGE</span>
<span class="c">#    inflate-66fb68585c-8j7wr   0/1     ContainerCreating   0          1s</span>
<span class="c">#    inflate-66fb68585c-j6bh4   0/1     ContainerCreating   0          1s</span>
<span class="c">#    inflate-66fb68585c-sqwmc   0/1     ContainerCreating   0          1s</span>
<span class="c">#    ... 잠시후 ...</span>
<span class="c">#    inflate-66fb68585c-8j7wr   1/1     Running             0          4s</span>
<span class="c">#    inflate-66fb68585c-j6bh4   1/1     Running             0          5s</span>
<span class="c">#    inflate-66fb68585c-sqwmc   1/1     Running             0          5s</span>
</code></pre></div>    </div>
    <p><img src="/assets/2024/t101-4th/20240727_terraform_w7_eks_karpenter_43.png" alt="Pod 배포 결과" class="image-center" />
<em class="image-caption image-caption--img-shadow">Pod 배포 결과</em></p>
  </li>
</ul>

<h4 id="eks-클러스터-및-vpc-삭제">EKS 클러스터 및 VPC 삭제</h4>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># kube-ops-view 삭제</span>
<span class="nv">$ </span>helm uninstall kube-ops-view <span class="nt">-n</span> kube-system
<span class="c"># =&gt; release "kube-ops-view" uninstalled</span>

<span class="c"># 삭제 : vpc 삭제가 잘 안될 경우 aws 콘솔에서 vpc 수동 삭제 -&gt; vnic 등 남아 있을 경우 해당 vnic 강제 삭제 : 9분 소요</span>
<span class="nv">$ </span>terraform destroy <span class="nt">-auto-approve</span>
<span class="c"># =&gt; Destroy complete! Resources: 47 destroyed.</span>

<span class="c"># VPC 삭제 확인</span>
<span class="nv">$ </span>aws ec2 describe-vpcs <span class="nt">--filter</span> <span class="s1">'Name=isDefault,Values=false'</span> <span class="nt">--output</span> yaml
<span class="c"># =&gt; Vpcs: []</span>

<span class="c"># kubeconfig 삭제</span>
<span class="nv">$ </span><span class="nb">rm</span> <span class="nt">-rf</span> ~/.kube/config
</code></pre></div></div>

<h2 id="마치며">마치며</h2>

<p>이번 주에는 EKS Blueprint를 이용한 방법과 EKS Workshop을 이용한 방법을 통해 Terraform을 활용하여 EKS 배포를 진행해보았습니다.
이번 실습을 통해 놀란점은 <strong>EKS 배포가 참 복잡하고 많은 AWS 기능을 사용해야 하는구나</strong> 하는것과
<strong>Terraform을 활용하면 이렇게 쉽게 클러스터를 배포</strong> 할 수 있구나 하는것입니다.</p>

<p>아무리 복잡한 배포 과정이 필요하더라도 <strong>Terraform 모듈을 잘 만들어두면</strong>
이후에는 <strong>누구라도 쉽게</strong> 몇 줄의 명령어 만으로 배포가 가능하다는 것을 느꼈고
Terraform의 강력함을 느꼈습니다.</p>

<p>이번 실습을 거치면서 AWS와 Terraform에 대해 조금더 알게 되었고, 한 발자국 더 가까워진 느낌입니다.
좋은 실습 기회를 주신 가시다 님과 Terraform 101 스터디를 진행하신 분들께 감사드립니다.
다음 스터디가 마지막이어서 아쉽기도 하면서 기대도 됩니다. 마지막까지 열심히 달려보겠습니다.</p>]]></content><author><name></name></author><category term="terraform" /><category term="terraform," /><category term="cloud," /><category term="aws" /><summary type="html"><![CDATA[이번 주에는 테라폼으로 AWS EKS 배포를 하는것을 테라폼으로 시작하는 IaC를 통해 알아 보겠습니다.]]></summary></entry><entry><title type="html">[T101 4기] Runner</title><link href="https://sweetlittlebird.github.io/posts/2024-07-13-T101-Study-Terraform-Week-5b/" rel="alternate" type="text/html" title="[T101 4기] Runner" /><published>2024-07-13T02:08:00+09:00</published><updated>2024-07-13T02:08:00+09:00</updated><id>https://sweetlittlebird.github.io/posts/T101%20Study%20-%20Terraform%20Week%205b</id><content type="html" xml:base="https://sweetlittlebird.github.io/posts/2024-07-13-T101-Study-Terraform-Week-5b/"><![CDATA[<h2 id="들어가며">들어가며</h2>

<p>이번 주에는 <code class="language-plaintext highlighter-rouge">Module</code>과 <code class="language-plaintext highlighter-rouge">Runner</code>에 대해
<a href="https://product.kyobobook.co.kr/detail/S000202478097">테라폼으로 시작하는 IaC</a>를 통해 알아 보는 중입니다.
계속해서 Terraform Runner에 대해 알아보겠습니다.</p>

<blockquote>
  <p><img src="/assets/2024/t101-4th/20240614_terraform_book.jpg" alt="테라폼으로 시작하는 IaC" /></p>

  <p><a href="https://product.kyobobook.co.kr/detail/S000202478097">테라폼으로 시작하는 IaC</a></p>
</blockquote>

<h2 id="terraform-runner">Terraform Runner</h2>

<ul>
  <li>Terraform Runner는 Terraform을 사용하여 인프라를 관리하는 방법 중 하나로
인프라스트럭처 코드의 배포를 간소화하기 위해 사용됩니다.</li>
  <li>특히 GitOps 등을 통한 CI/CD 파이프라인 내에서 자주 사용되며,
코드 변경 사항을 자동으로 감지하고 적용하여 인프라의 상태를 최신 상태로 유지하는 데 도움을 줍니다. 
이를 통해 개발자와 운영팀은 인프라 관리를 더 효율적이고 안정적으로 수행할 수 있습니다.</li>
  <li>참고 링크
    <ul>
      <li>국내
        <ul>
          <li>Terraform을 GitOps 방식으로 사용하기 위한 도구 선택 - <a href="https://nangman14.tistory.com/109">Link</a></li>
          <li>Atlantis 란? (Terraform Pull Request Automation) - <a href="https://kim-dragon.tistory.com/253">Link</a></li>
          <li>펫프렌즈가 DevOps 조직없이 인프라를 관리하는 방법 - <a href="https://techblog.pet-friends.co.kr/%ED%8E%AB%ED%94%84%EB%A0%8C%EC%A6%88%EA%B0%80-devops-%EC%A1%B0%EC%A7%81%EC%97%86%EC%9D%B4-%EC%9D%B8%ED%94%84%EB%9D%BC%EB%A5%BC-%EA%B4%80%EB%A6%AC%ED%95%98%EB%8A%94-%EB%B0%A9%EB%B2%95-ce27449bc201">Link</a></li>
          <li>Atlantis 배포하기 - <a href="https://devops-art-factory.gitbook.io/devops-workshop/terraform/atlantis/atlantis-main">Link</a></li>
          <li>Github에서 Atlantis를 이용한  Terraform 협업 환경 구축 - <a href="https://isn-t.tistory.com/46">Link</a> <a href="https://github.com/TAEKnical/Terraform_study/">Github</a></li>
          <li>Jenkins + terraform + tfsec 로 정적 분석 자동화 구성 하기 - <a href="https://devocean.sk.com/blog/techBoardDetail.do?ID=163872">Link</a></li>
        </ul>
      </li>
      <li>해외
        <ul>
          <li>install-atlantis-docker-container-in-amazon-linux-integrate - <a href="https://www.easydeploy.io/blog/install-atlantis-docker-container-in-amazon-linux-integrate/">Link</a> <a href="https://github.com/easydeploy-cloud/atlantis-blog">Github</a></li>
          <li>Hacking Atlantis - <a href="https://www.youtube.com/watch?v=S1-vGf_ao1s&amp;t=896s">Youtube</a></li>
          <li>[CNCF] How Breaking up Our Infrastructure Monorepos Saved Us from Pul… Donnie Laughton - <a href="https://www.youtube.com/watch?v=5dAUDpwZTQ8">Youtube</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Terraform Runner는 다수의 종류가 있는데 그 중에서 Atlantis를 사용해 보겠습니다.</li>
</ul>

<h3 id="atlantis-란">Atlantis 란?</h3>

<ul>
  <li><strong>Atlantis</strong>는 <strong>Terraform Pull Request Automation</strong> 도구로,
Terraform 코드를 GitOps 방식으로 관리할 때 사용됩니다. Terraform Pull Request Automation란 개발자와 운영자가 <strong>풀 리퀘스트에서 직접 terraform plan과 apply를 실행할 수 있게</strong> 하여
Terraform 워크플로우의 협업과 안전성을 개선합니다.</li>
  <li>이는 개발자가 자격 증명 없이 Terraform을 작성하고 적용할 수 있도록 돕고, 효과적인 협업 문제를 해결합니다. 자세한 내용은 다음 링크에서 확인할 수 있습니다. <a href="https://www.runatlantis.io/">Link</a></li>
  <li>대략적인 동작 방식은 아래와 같습니다.
<img src="/assets/2024/t101-4th/20240711_terraform_w5_atlantis_1.png" alt="img.png" class="image-center" />
<em class="image-caption">출처 : <a href="https://isn-t.tistory.com/46">https://isn-t.tistory.com/46</a></em></li>
</ul>

<h3 id="atlantis-설치">Atlantis 설치</h3>

<ul>
  <li>Atlantis는 다양한 방법으로 설치할 수 있습니다. 이 중에서 Github를 사용하고 Docker를 사용한 설치 방법을 소개합니다.</li>
  <li>설치 순서
    <ol>
      <li>github 에 repository 생성합니다. (private/public 관계 없음)</li>
      <li>Personal Access Token을 생성합니다.
        <ul>
          <li>프로필 사진 클릭 &gt; Settings &gt; Developer settings &gt; Personal access tokens &gt; Fine-grained tokens &gt; Generate new token</li>
          <li>이름 부여</li>
          <li>Only select repositories 선택</li>
          <li>권한을 줄 저장소 선택 (1. 에서 만든 repository)</li>
          <li>Repository Permissions의 Contents 에 Read &amp; write 선택</li>
          <li>Generate token 버튼 클릭</li>
          <li>생성된 토큰을 복사합니다.</li>
        </ul>
      </li>
      <li>공인 IP가 있는 서버에 Docker를 설치합니다. (공유기로 포트포워딩 해도 좋습니다.)
        <ul>
          <li>Docker 설치 방법은 다양하게 있습니다. <a href="https://docs.docker.com/get-docker/">Docker 설치</a> Docker가 설치 되었다고 가정하고 진행하겠습니다.</li>
        </ul>
      </li>
      <li>Docker에서 atlantis 실행
        <ul>
          <li>docker-compose.yml 작성
            <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">services</span><span class="pi">:</span>
  <span class="na">atlantis</span><span class="pi">:</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">ghcr.io/runatlantis/atlantis:v0.28-alpine</span>
    <span class="na">restart</span><span class="pi">:</span> <span class="s">always</span>
    <span class="na">ports</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s2">"</span><span class="s">4141:4141"</span>
    <span class="na">environment</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">ATLANTIS_GH_WEBHOOK_SECRET=&lt;GITHUB연동용 랜덤값&gt;</span>
      <span class="pi">-</span> <span class="s">ATLANTIS_GH_USER=&lt;GITHUB 아이디&gt;</span>
      <span class="pi">-</span> <span class="s">ATLANTIS_GH_TOKEN=&lt;2.에서 생성한 토큰&gt;</span>
      <span class="pi">-</span> <span class="s">ATLANTIS_REPO_ALLOWLIST=&lt;1.에서 생성한 repository 이름&gt;</span>
</code></pre></div>            </div>
          </li>
          <li>docker-compose 실행
            <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker compose up <span class="nt">-d</span>
</code></pre></div>            </div>
          </li>
        </ul>
      </li>
      <li>Atlantis 접속 테스트
        <ul>
          <li>브라우저에서 <code class="language-plaintext highlighter-rouge">http://&lt;서버 IP&gt;:4141</code>로 접속하여 확인합니다.</li>
          <li>Atlantis가 정상적으로 실행되었다면 아래와 같은 화면이 나타납니다.
 <img src="/assets/2024/t101-4th/20240711_terraform_w5_atlantis_2.png" alt="img.png" /></li>
        </ul>
      </li>
      <li>Github에 Webhook 설정
        <ul>
          <li>1.에서 생성한 저장소에 접속합니다.</li>
          <li>Settings &gt; Webhooks &gt; Add webhook 클릭합니다.</li>
          <li>Payload URL에 앞서 접속 테스트한 <code class="language-plaintext highlighter-rouge">http://&lt;서버 IP&gt;:4141/events</code> 를 기입합니다.
            <ul>
              <li><code class="language-plaintext highlighter-rouge">/events</code> 로 끝나는지 반드시 확인합니다. 특히 405 에러가 나면 더욱 그러합니다.</li>
            </ul>
          </li>
          <li>Content type에 <code class="language-plaintext highlighter-rouge">application/json</code> 을 선택합니다.</li>
          <li>Secret에 4.에서 등록한 <code class="language-plaintext highlighter-rouge">&lt;GITHUB연동용 랜덤값&gt;</code>을 기입합니다.</li>
          <li>Let me select individual events를 선택합니다.</li>
          <li>다음 항목들을 체크합니다.
            <ul>
              <li>Pull request reviews</li>
              <li>Pushes</li>
              <li>Issue comments</li>
              <li>Pull requests</li>
            </ul>
          </li>
          <li>Active를 체크합니다.</li>
          <li>Add webhook 버튼을 클릭합니다.</li>
        </ul>
      </li>
    </ol>
  </li>
  <li>설치과정이 완료되었습니다. 이제 Terraform 코드를 작성하고 PR을 생성하여 Atlantis를 통해 Terraform 코드를 관리해 보겠습니다.</li>
</ul>

<h3 id="atlantis-실습">Atlantis 실습</h3>

<h4 id="작업-1-null-provider">작업 1. null provider</h4>

<ul>
  <li>
    <p>Local에서 git 코드 작업</p>

    <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># git clone</span>
<span class="nv">$ </span>git clone https://github.com/sweetlittlebird/terraform-atlantis-test <span class="o">&amp;&amp;</span> <span class="nb">cd </span>terraform-atlantis-test <span class="o">&amp;&amp;</span> tree
  
<span class="c"># feature branch 생성</span>
<span class="nv">$ </span>git branch <span class="nb">test</span> <span class="o">&amp;&amp;</span> git checkout <span class="nb">test</span> <span class="o">&amp;&amp;</span> git branch
  
<span class="c"># main.tf 파일 작성</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="s1">'resource "null_resource" "example" {}'</span> <span class="o">&gt;</span> main.tf
  
<span class="c"># add commit push</span>
<span class="nv">$ </span>git add main.tf <span class="o">&amp;&amp;</span> git commit <span class="nt">-m</span> <span class="s2">"add main.tf"</span> <span class="o">&amp;&amp;</span> git push origin <span class="nb">test</span> 
</code></pre></div>    </div>
  </li>
  <li>
    <p>Github PR 생성 =&gt; Atlantis 확인</p>
    <ul>
      <li>서버에서 다음의 명령어로 모니터링합니다.
        <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> docker-atlantis-1 watch <span class="nt">-n</span> 1 tree /home/atlantis/.atlantis
</code></pre></div>        </div>
      </li>
      <li>Github 에서 Compare &amp; Pull request를 클릭합니다.
<img src="/assets/2024/t101-4th/20240711_terraform_w5_atlantis_3.png" alt="img.png" /></li>
      <li>Create pull request : title ( create null resource )
<img src="/assets/2024/t101-4th/20240711_terraform_w5_atlantis_4.png" alt="img.png" /></li>
      <li>plan 자동 수행 확인
<img src="/assets/2024/t101-4th/20240711_terraform_w5_atlantis_5.png" alt="img.png" />
<img src="/assets/2024/t101-4th/20240711_terraform_w5_atlantis_6.png" alt="img.png" /></li>
      <li>서버 모니터링 결과 init 과 plan 이 실행된것을 확인 할 수 있습니다.
        <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> docker-atlantis-1 watch <span class="nt">-n</span> 1 tree /home/atlantis/.atlantis    
<span class="c"># =&gt; /home/atlantis/.atlantis</span>
<span class="c">#    ├── atlantis.db</span>
<span class="c">#    ├── bin</span>
<span class="c">#    ├── plugin-cache</span>
<span class="c">#    │   └── registry.terraform.io</span>
<span class="c">#    │       └── hashicorp</span>
<span class="c">#    │           └── null</span>
<span class="c">#    │               └── 3.2.2</span>
<span class="c">#    │                   └── linux_amd64</span>
<span class="c">#    │                       └── terraform-provider-null_v3.2.2_x5</span>
<span class="c">#    └── repos</span>
<span class="c">#        └── sweetlittlebird</span>
<span class="c">#            └── terraform-atlantis-test</span>
<span class="c">#                └── 2</span>
<span class="c">#                    └── default</span>
<span class="c">#                        ├── default.tfplan</span>
<span class="c">#                        └── main.tf                                        </span>
    
<span class="c"># github repo에서 코드를 가져온것을 확인                          </span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> docker-atlantis-1 <span class="nb">cat</span> /home/atlantis/.atlantis/repos/sweetlittlebird/terraform-atlantis-test/2/default/main.tf
</code></pre></div>        </div>
      </li>
      <li>PR 코멘트에 이것저것 명령을 넣어봅니다.
<img src="/assets/2024/t101-4th/20240711_terraform_w5_atlantis_7.png" alt="20240711_terraform_w5_atlantis_7.png" />
        <ul>
          <li><code class="language-plaintext highlighter-rouge">atlantis help</code>는 응답이 오지만 <code class="language-plaintext highlighter-rouge">cat /etc/passwd</code>는 반응이 없습니다.</li>
        </ul>
      </li>
      <li>PR 코멘트에 명령을 조합해서 속이려고 해도 되지 않습니다.
<img src="/assets/2024/t101-4th/20240711_terraform_w5_atlantis_8.png" alt="20240711_terraform_w5_atlantis_8.png" /></li>
      <li>이제 apply 해보겠습니다.
        <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>atlantis plan <span class="nt">-d</span> <span class="nb">.</span>
atlantis apply <span class="nt">-d</span> <span class="nb">.</span>
</code></pre></div>        </div>
        <p><img src="/assets/2024/t101-4th/20240711_terraform_w5_atlantis_9.png" alt="20240711_terraform_w5_atlantis_9.png" /></p>
      </li>
      <li>아틀란티스 웹에서 확인해보겠습니다.
<img src="/assets/2024/t101-4th/20240711_terraform_w5_atlantis_10.png" alt="img.png" /></li>
      <li>plan과 apply가 잘 된 것을 확인할 수 있습니다.</li>
      <li>만약 Locks에 항목이 있고, 어떤 의도에서든 plan을 다시 실행하고 싶다면 클릭해서 <strong>Discard Plan &amp; Unlock</strong> 하십시오. 그렇지 않으면 Lock 된 plan으로 apply 됩니다. pull request를 merge 하면 Lock 이 해제됩니다.</li>
    </ul>
  </li>
</ul>

<h5 id="atlantis의-auto-plan-manual-plan-apply">atlantis의 Auto plan, Manual plan, Apply</h5>

<ul>
  <li>auto plan
    <ul>
      <li>Atlantis는 기본적으로 자동으로 plan을 실행합니다.</li>
    </ul>
  </li>
  <li>manual plan
    <ul>
      <li>variable의 값을 넘기거나 하고 싶다면 수동 plan 실행이 필요합니다.</li>
      <li>PR 코멘트에 다음 명령들을 실행하면 수동 plan이 실행됩니다. (단, Lock이 걸려있지 않아야 합니다.)
        <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>atlantis plan <span class="nt">-d</span> <span class="nb">dir</span>      <span class="c"># 디렉터리</span>
atlantis plan <span class="nt">-w</span> staging  <span class="c"># 워크스페이스</span>
atlantis plan <span class="nt">--</span> <span class="nt">-target</span><span class="o">=</span>resource <span class="nt">-var</span> <span class="s1">'foo=bar'</span>  <span class="c"># -var로 variable 값 넘기기</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>apply
    <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>atlantis apply             <span class="c"># 루트 디렉터리에서 실행</span>
atlantis apply <span class="nt">-d</span> <span class="nb">dir</span>      <span class="c"># 디렉터리</span>
atlantis apply <span class="nt">-w</span> staging  <span class="c"># 워크스페이스</span>
</code></pre></div>    </div>
  </li>
</ul>

<h4 id="작업-2-aws-iam-user-생성">작업 2. aws iam user 생성</h4>

<ul>
  <li>AWS S3 버킷 생성 : Terraform Backend state 저장용으로 사용합니다.
    <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>aws s3 <span class="nb">ls</span> 
  
<span class="c"># aws s3 mb s3://&lt;유일한 사용할 S3 버킷명&gt; --region ap-northeast-2</span>
aws s3 mb s3://sweetlittlebird-terraform-state <span class="nt">--region</span> ap-northeast-2 <span class="c"># S3 버킷 생성</span>
  
aws s3 <span class="nb">ls</span>  <span class="c"># 생성 확인</span>
</code></pre></div>    </div>
  </li>
  <li>Local에서 Git 코드 작업
    <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># feature branch 생성</span>
<span class="nv">$ </span>git branch iam <span class="o">&amp;&amp;</span> git checkout iam <span class="o">&amp;&amp;</span> git branch
  
<span class="c"># 디렉터리 생성</span>
<span class="nv">$ </span><span class="nb">mkdir </span>iam <span class="o">&amp;&amp;</span> <span class="nb">cd </span>iam
  
<span class="c"># main.tf 파일 작성</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> &gt; main.tf
terraform {
  backend "s3" {
    bucket = "sweetlittlebird-terraform-state"  # 앞서 생성한 S3 버킷명
    key    = "terraform.tfstate"
    region = "ap-northeast-2"
  }
}
         
provider "aws" {
  region = "ap-southeast-1"
}  
  
resource "aws_iam_user" "myuser" {
  name = "t101user"
}
</span><span class="no">EOF
  
</span><span class="c"># add commit push</span>
git add main.tf <span class="o">&amp;&amp;</span> git commit <span class="nt">-m</span> <span class="s2">"add main.tf"</span> <span class="o">&amp;&amp;</span> git push origin iam
</code></pre></div>    </div>
  </li>
  <li>Github에서 PR 생성 및 Atlantis 확인
    <ul>
      <li>서버에서 다음의 명령어로 모니터링합니다.
        <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> docker-atlantis-1 watch <span class="nt">-n</span> 1 tree /home/atlantis/.atlantis
</code></pre></div>        </div>
      </li>
      <li>Github 에서 Compare &amp; Pull request를 클릭합니다.</li>
      <li>Create pull request : title ( create iam user )</li>
      <li>plan 자동 수행 확인</li>
      <li>서버 모니터링 결과 init 과 plan 이 실행된것을 확인 할 수 있습니다.
        <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> docker-atlantis-1 watch <span class="nt">-n</span> 1 tree /home/atlantis/.atlantis
    
<span class="c"># =&gt; /home/atlantis/.atlantis</span>
<span class="c">#    ...</span>
<span class="c">#    └── repos</span>
<span class="c">#        └── sweetlittlebird</span>
<span class="c">#            └── terraform-atlantis-test</span>
<span class="c">#                └── 3</span>
<span class="c">#                    └── default</span>
<span class="c">#                        ├── iam</span>
<span class="c">#                        │   ├── default.tfplan</span>
<span class="c">#                        │   └── main.tf</span>
<span class="c">#                        └── main.tf</span>
    
<span class="c"># github repo에서 코드를 가져온것을 확인                          </span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> docker-atlantis-1 <span class="nb">cat</span> /home/atlantis/.atlantis/repos/sweetlittlebird/terraform-atlantis-test/3/default/iam/main.tf
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>S3 버킷 확인
    <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>aws s3 <span class="nb">ls </span>s3://sweetlittlebird-terraform-state
<span class="c"># =&gt; 2024-07-XX 00:00:00        180 terraform.tfstate  </span>
</code></pre></div>    </div>
  </li>
  <li>apply 하기
    <ul>
      <li>PR 코멘트에 아래의 명령을 넣어 apply 합니다.
        <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>atlantis plan <span class="nt">-d</span> iam
    
atlantis apply <span class="nt">-d</span> iam
</code></pre></div>        </div>
        <p><img src="/assets/2024/t101-4th/20240711_terraform_w5_atlantis_12.png" alt="img.png" /></p>
      </li>
    </ul>
  </li>
  <li>S3 버킷 확인 - apply 하면서 적용된 최종 상태값을 저장하여 크기가 커진것을 확인할 수 있습니다.
    <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>aws s3 <span class="nb">ls </span>s3://sweetlittlebird-terraform-state
<span class="c"># =&gt; 2024-07-14 02:12:56        862 terraform.tfstate</span>
</code></pre></div>    </div>
  </li>
  <li>Atlantis 웹에서 apply 됨을 확인합니다.
<img src="/assets/2024/t101-4th/20240711_terraform_w5_atlantis_11.png" alt="img.png" /></li>
  <li>Merge Pull request =&gt; Confirm merge
    <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> docker-atlantis-1 watch <span class="nt">-n</span> 1 tree /home/atlantis/.atlantis
  
<span class="c"># =&gt; /home/atlantis/.atlantis</span>
<span class="c">#    ...</span>
<span class="c">#    └── repos</span>
<span class="c">#        └── sweetlittlebird</span>
<span class="c">#            └── terraform-atlantis-test</span>
</code></pre></div>    </div>
    <p>terraform-atlantis-test 이하의 디렉터리와 파일들이 정리되어 삭제된것을 확인할 수 있습니다.</p>
  </li>
  <li>Merge request된 사항을 Local git main 에서 pull 받아서 확인합니다.
    <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>git checkout main
<span class="nv">$ </span>git pull origin main
<span class="nv">$ </span><span class="nb">cd</span> ..
<span class="nv">$ </span>tree
</code></pre></div>    </div>
  </li>
</ul>

<h4 id="작업-3-작업-2에서-생성한-리소스-삭제">작업 3. 작업 2에서 생성한 리소스 삭제</h4>
<ul>
  <li>Local에서 Git 코드 작업
    <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># feature branch 생성</span>
git branch deleteiam <span class="o">&amp;&amp;</span> git checkout deleteiam <span class="o">&amp;&amp;</span> git branch
  
<span class="c"># 디렉터리 생성</span>
<span class="nb">mkdir </span>deleteiam <span class="o">&amp;&amp;</span> <span class="nb">cd </span>deleteiam
  
<span class="c"># main.tf 파일 작성</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> &gt; main.tf
terraform {
  backend "s3" {
    bucket = "sweetlittlebird-terraform-state"  # 앞서 생성한 S3 버킷명
    key    = "terraform.tfstate"
    region = "ap-northeast-2"
  }
}
</span><span class="no">EOF
  
</span><span class="c"># add commit push</span>
git add main.tf <span class="o">&amp;&amp;</span> git commit <span class="nt">-m</span> <span class="s2">"add main.tf"</span> <span class="o">&amp;&amp;</span> git push origin deleteiam
</code></pre></div>    </div>
  </li>
  <li>Github에서 PR 생성 및 Atlantis 확인
    <ul>
      <li>서버에서 다음의 명령어로 모니터링합니다.
        <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> docker-atlantis-1 watch <span class="nt">-n</span> 1 tree /home/atlantis/.atlantis
</code></pre></div>        </div>
      </li>
      <li>Github 에서 Compare &amp; Pull request를 클릭합니다.</li>
      <li>Create pull request : title ( delete iam user )</li>
      <li>plan 자동 수행 확인
<img src="/assets/2024/t101-4th/20240711_terraform_w5_atlantis_13.png" alt="20240711_terraform_w5_atlantis_13.png" /></li>
      <li>서버 모니터링 결과 init 과 plan 이 실행된것을 확인 할 수 있습니다.
        <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> docker-atlantis-1 watch <span class="nt">-n</span> 1 tree /home/atlantis/.atlantis
    
<span class="c"># =&gt; /home/atlantis/.atlantis</span>
<span class="c">#    ...</span>
<span class="c">#    └── repos</span>
<span class="c">#        └── sweetlittlebird</span>
<span class="c">#            └── terraform-atlantis-test</span>
<span class="c">#                └── 4</span>
<span class="c">#                    └── default</span>
<span class="c">#                        ├── deleteiam</span>
<span class="c">#                        │   ├── default.tfplan</span>
<span class="c">#                        │   └── main.tf</span>
<span class="c">#                        ├── iam</span>
<span class="c">#                        │   └── main.tf</span>
<span class="c">#                        └── main.tf</span>
    
<span class="c"># github repo에서 코드를 가져온것을 확인                          </span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> docker-atlantis-1 <span class="nb">cat</span> /home/atlantis/.atlantis/repos/sweetlittlebird/terraform-atlantis-test/4/default/deleteiam/main.tf
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>S3 버킷 확인
    <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>aws s3 <span class="nb">ls </span>s3://sweetlittlebird-terraform-state
<span class="c"># =&gt; 2024-07-14 02:12:56        862 terraform.tfstate</span>
</code></pre></div>    </div>
  </li>
  <li>apply 하기
    <ul>
      <li>PR 코멘트에 아래의 명령을 넣어 apply 합니다.
        <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>atlantis apply <span class="nt">-d</span> deleteiam
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>AWS 콘솔에서 IAM이 삭제됨을 확인합니다.
<img src="/assets/2024/t101-4th/20240711_terraform_w5_atlantis_15.png" alt="20240711_terraform_w5_atlantis_15.png" /></li>
  <li>S3 버킷 확인 - apply 하면서 리소스가 삭제되어 상태파일 크기가 줄어든것을 확인할 수 있습니다.
    <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>aws s3 <span class="nb">ls </span>s3://sweetlittlebird-terraform-state
<span class="c"># =&gt; 2024-07-14 02:35:09        180 terraform.tfstate</span>
</code></pre></div>    </div>
  </li>
  <li>Atlantis 웹에서 apply 됨을 확인합니다.
<img src="/assets/2024/t101-4th/20240711_terraform_w5_atlantis_14.png" alt="img.png" /></li>
  <li>Merge Pull request =&gt; Confirm merge
    <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> docker-atlantis-1 watch <span class="nt">-n</span> 1 tree /home/atlantis/.atlantis
  
<span class="c"># =&gt; /home/atlantis/.atlantis</span>
<span class="c">#    ...</span>
<span class="c">#    └── repos</span>
<span class="c">#        └── sweetlittlebird</span>
<span class="c">#            └── terraform-atlantis-test</span>
</code></pre></div>    </div>
    <p>terraform-atlantis-test 이하의 디렉터리와 파일들이 정리되어 삭제된것을 확인할 수 있습니다.</p>
  </li>
  <li>Merge request된 사항을 Local git main 에서 pull 받아서 확인합니다.
    <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>git checkout main <span class="o">&amp;&amp;</span> git pull <span class="o">&amp;&amp;</span> <span class="nb">cd</span> .. <span class="o">&amp;&amp;</span> tree
</code></pre></div>    </div>
  </li>
</ul>

<h2 id="마치며">마치며</h2>

<p>이상과 같이 Atlantis를 사용하여 Terraform 코드를 관리하는 방법을 알아보았습니다.
팀으로 작업을 할 때 참 유용한 기능인것 같습니다. 
또한 이번주 스터디를 통해 Terraform의 생태계가 참 다양하고 크구나 하는것을 느꼈습니다.</p>

<p>공부할 수록 입문하기는 쉽지만 마스터하기는 어렵다는것을 느낍니다.
—애초에 마스터한다라는 개념이 존재하는지 모르겠습니다—
계속 공부하고 적용하고 반복하는것이 중요한것 같다라는것을 
이번 T101 4기 스터디를 통해 다시 한번 느낍니다.</p>

<p>기본 상태에서는 누구나 Atlantis 페이지에 접속해서 이력을 확인할 수 있어서
염려되는데 해당 부분에 대해  보완할 방법을 조금더 찾아봐야 할것 같습니다.
이번 주 수업도 기대됩니다. 완주까지 화이팅입니다.</p>]]></content><author><name></name></author><category term="terraform" /><category term="terraform," /><category term="cloud," /><category term="aws" /><summary type="html"><![CDATA[이번 주에는 Module과 Runner에 대해 테라폼으로 시작하는 IaC를 통해 알아 보는 중입니다. 계속해서 Terraform Runner에 대해 알아보겠습니다.]]></summary></entry></feed>