<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="ko"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://sweetlittlebird.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://sweetlittlebird.github.io/" rel="alternate" type="text/html" hreflang="ko" /><updated>2024-12-08T01:12:51+09:00</updated><id>https://sweetlittlebird.github.io/feed.xml</id><title type="html">Sweet Little Bird</title><subtitle>공부 기록과 개발 이야기를 담은 블로그입니다.</subtitle><entry><title type="html">[CI/CD] Jenkins CI/CD + Docker</title><link href="https://sweetlittlebird.github.io/posts/2024-12-08-CICD-Lite-Week1/" rel="alternate" type="text/html" title="[CI/CD] Jenkins CI/CD + Docker" /><published>2024-12-08T00:50:18+09:00</published><updated>2024-12-08T00:50:18+09:00</updated><id>https://sweetlittlebird.github.io/posts/CICD%20Lite%20-%20Week1</id><content type="html" xml:base="https://sweetlittlebird.github.io/posts/2024-12-08-CICD-Lite-Week1/"><![CDATA[<h2 id="들어가며">들어가며</h2>

<p>스터디 시간이 다시 돌아왔습니다. ^^ 이번 스터디는 3주차의 다소 짧은 스터디로 CI/CD에 관련해서 진행됩니다.
즐거운 연말 다시 한번 과제로 달려보겠습니다. :laughing:</p>

<p>이번 주에는 Jenkins와 Git, Docker를 이용하여 CI/CD 파이프라인을 구축해보겠습니다.</p>

<h2 id="jenkins-cicd--docker">Jenkins CI/CD + Docker</h2>

<h3 id="cicd란">CI/CD란?</h3>

<p><img src="/assets/2024/cicd-lite/w1/20241205_cicd_lite_w1_1.png" alt="img.png" class="image-center" />
<em class="image-caption">CI/CD 파이프라인 (출처:<a href="https://blog.devgenius.io/what-is-ci-cd-concept-375cb226cf3d">Dev Genius</a>)</em></p>

<ul>
  <li>CI/CD는 Continuous Integration(지속적 통합)과 Continuous Deployment(지속적 배포)의 약자로 소프트웨어 개발의 계획단계에서 부터 배포/운영까지 전 과정에 걸쳐
자동화된 프로세스를 통해 소프트웨어를 빠르게, 안정적으로 배포할 수 있도록 하는 방법론입니다.</li>
  <li>CI와 CD로 나눠서 살펴보겠습니다.
    <ul>
      <li>CI : 여러 개발자들이 작성한 코드를 하나로 통합하는 코드의 통합을 지속적으로 진행하는 것을 의미합니다.
        <ul>
          <li>CI의 단계 : 계획 -&gt; 코딩 -&gt; 빌드 -&gt; 테스트 -&gt; 패키징</li>
        </ul>
      </li>
      <li>CD : CI를 통해 빌드된 결과물을 배포하고 운영하고, 모니터링을 통해 개선할 점을 파악하는 과정을 지속적으로 진행하는 것을 의미합니다.
        <ul>
          <li>CD의 단계 : 배포 -&gt; 운영 -&gt; 모니터링 -&gt; 피드백</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>CI/CD는 위의 그림과 같이 다양한 툴들로 구성이 되며 이번주에는 Jenkins와 Git, Docker를 이용하여 CI/CD 파이프라인을 구축해보겠습니다.</li>
</ul>

<h3 id="컨테이너를-이용한-어플리케이션-개발">컨테이너를 이용한 어플리케이션 개발</h3>

<p>CI/CD 파이프라인을 구축하기 위해 Docker를 이용하여 어플리케이션을 컨테이너화하겠습니다.</p>

<h4 id="ruby로-특정-문자열-출력하는-간단한-어플리케이션-만들기">ruby로 특정 문자열 출력하는 간단한 어플리케이션 만들기</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="nv">$ </span><span class="nb">mkdir </span>1.1 <span class="o">&amp;&amp;</span> <span class="nb">cd </span>1.1
  <span class="nv">$ </span><span class="nb">echo</span> <span class="s1">'puts "Hello Docker!"'</span> <span class="o">&gt;</span> hello.rb
  
  <span class="nv">$ </span><span class="nb">cat</span> <span class="o">&gt;</span> Dockerfile <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
  FROM ruby:3.3
  COPY hello.rb /app/
  WORKDIR /app
  CMD ["ruby", "hello.rb"]
</span><span class="no">  EOF
  
</span>  <span class="c"># 이미지 빌드</span>
  <span class="nv">$ </span>docker build <span class="nt">-t</span> hello <span class="nb">.</span>
  <span class="c"># =&gt; [+] Building 36.6s (9/9) FINISHED</span>
  <span class="nv">$ </span>docker image <span class="nb">ls</span> <span class="nt">-f</span> <span class="nv">reference</span><span class="o">=</span>hello
  <span class="c"># =&gt; REPOSITORY   TAG       IMAGE ID       CREATED          SIZE</span>
  <span class="c">#    hello        latest    d0c55f1ebe18   35 seconds ago   1GB</span>
  
  <span class="c"># 실행</span>
  <span class="nv">$ </span>docker run hello
  <span class="c"># =&gt; Hello Docker!</span>
</code></pre></div></div>

<ul>
  <li>
    <p>코드 수정</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 코드 수정</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"puts 'Hello CloudNet@'"</span> <span class="o">&gt;</span> hello.rb
  
<span class="c"># 컨테이너 이미지 빌드</span>
<span class="nv">$ </span>docker build <span class="nb">.</span> <span class="nt">-t</span> hello:1
<span class="nv">$ </span>docker image <span class="nb">ls</span> <span class="nt">-f</span> <span class="nv">reference</span><span class="o">=</span>hello
<span class="c"># =&gt; REPOSITORY   TAG       IMAGE ID       CREATED         SIZE</span>
<span class="c">#    hello        1         7fe4f428d492   3 seconds ago   1GB</span>
<span class="c">#    hello        latest    d0c55f1ebe18   3 minutes ago   1GB  </span>
<span class="c"># &lt;span style="color: green;"&gt;👉 Latest 태그는 IMAGE ID를 통해 아직 이전의 이미지를 갖고 있는 것을 확인할 수 있습니다.&lt;/span&gt;</span>
  
<span class="c"># 1번 태그에 추가적으로 latest 태그를 붙여보겠습니다.</span>
<span class="nv">$ </span>docker tag hello:1 hello:latest
<span class="nv">$ </span>docker image <span class="nb">ls</span> <span class="nt">-f</span> <span class="nv">reference</span><span class="o">=</span>hello
<span class="c"># =&gt; REPOSITORY   TAG       IMAGE ID       CREATED          SIZE</span>
<span class="c">#    hello        1         7fe4f428d492   36 seconds ago   1GB</span>
<span class="c">#    hello        latest    7fe4f428d492   36 seconds ago   1GB</span>
<span class="c"># &lt;span style="color: green;"&gt;👉 Latest 태그도 동일한 IMAGE ID를 갖게되었습니다.&lt;/span&gt;</span>
  
<span class="c"># 컨테이너 실행</span>
<span class="nv">$ </span>docker run <span class="nt">--rm</span> hello:1
<span class="c"># =&gt; Hello CloudNet@</span>
<span class="nv">$ </span>docker run <span class="nt">--rm</span> hello
<span class="c"># =&gt; Hello CloudNet@</span>
</code></pre></div>    </div>
  </li>
</ul>

<h4 id="compiling-code-in-docker">Compiling code in Docker</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c"># 코드 작성</span>
  <span class="nv">$ </span><span class="nb">mkdir </span>1.2 <span class="o">&amp;&amp;</span> <span class="nb">cd </span>1.2
  
  <span class="nv">$ </span><span class="nb">cat</span> <span class="o">&gt;</span> Hello.java <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
  class Hello {
      public static void main(String[] args) {
          System.out.println("Hello Docker");
      }
  }
</span><span class="no">  EOF
  
</span>  <span class="nv">$ </span><span class="nb">cat</span> <span class="o">&gt;</span> Dockerfile <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
  FROM openjdk
  COPY . /app
  WORKDIR /app
  RUN javac Hello.java    # 컴파일
  CMD java Hello
</span><span class="no">  EOF
  
</span>  <span class="c"># 컨테이너 이미지 빌드</span>
  <span class="nv">$ </span>docker pull openjdk
  <span class="nv">$ </span>docker build <span class="nb">.</span> <span class="nt">-t</span> hello:2 <span class="nt">-t</span> hello:latest
  <span class="c"># =&gt; [+] Building 0.8s (9/9) FINISHED</span>
  <span class="nv">$ </span>docker image <span class="nb">ls</span> <span class="nt">-f</span> <span class="nv">reference</span><span class="o">=</span>hello
  <span class="c"># =&gt; REPOSITORY   TAG       IMAGE ID       CREATED          SIZE</span>
  <span class="c">#    hello        2         ba37ddf45c26   10 seconds ago   487MB</span>
  <span class="c">#    hello        latest    ba37ddf45c26   10 seconds ago   487MB</span>
  <span class="c">#    hello        1         7fe4f428d492   9 minutes ago    1GB</span>
  
  <span class="c"># 컨테이너 실행</span>
  <span class="nv">$ </span>docker run <span class="nt">--rm</span> hello:2
  <span class="c"># =&gt; Hello Docker</span>
  <span class="nv">$ </span>docker run <span class="nt">--rm</span> hello
  <span class="c"># =&gt; Hello Docker</span>
  
  <span class="c"># 컨테이너 이미지 내부에 파일 목록을 보면 어떤가요? 꼭 필요한 파일만 있는가요? 보안적으로 어떨까요?</span>
  <span class="nv">$ </span>docker run <span class="nt">--rm</span> hello <span class="nb">ls</span> <span class="nt">-l</span>
  <span class="c"># =&gt; -rw-r--r-- 1 root root  89 Dec  5 15:04 Dockerfile</span>
  <span class="c">#    -rw-r--r-- 1 root root 416 Dec  5 15:05 Hello.class</span>
  <span class="c">#    -rw-r--r-- 1 root root 111 Dec  5 15:04 Hello.java</span>
  <span class="c"># &lt;span style="color: green;"&gt;👉 꼭 필요한 파일 외에도 Dockerfile, *.java 파일, java 컴파일러 등이 있습니다.&lt;/span&gt;</span>
  <span class="c"># &lt;span style="color: green;"&gt;   이 파일들은 정보 유출이나 공격 대상이 될 수 있기 때문에 컨테이너 이미지에 없어야 합니다.&lt;/span&gt;</span>
  
  <span class="c"># RUN 컴파일 시 소스코드와 java 컴파일러(javac)가 포함되어 있음. 실제 애플리케이션 실행에 필요 없음. </span>
  <span class="nv">$ </span>docker run <span class="nt">--rm</span> hello javac <span class="nt">--help</span>
  <span class="c"># =&gt; Usage: javac &amp;lt;options&amp;gt; &amp;lt;source files&amp;gt;</span>
  <span class="c">#    where possible options include:</span>
  <span class="c">#      @&amp;lt;filename&amp;gt;                  Read options and filenames from file</span>
  <span class="c">#    ...</span>
</code></pre></div></div>

<h4 id="멀티-스테이지-빌드">멀티 스테이지 빌드</h4>
<ul>
  <li>멀티 스테이지 빌드는 빌드를 여러 단계로 나누어서 진행하는 방법입니다.</li>
  <li>각 단계마다 필요한 환경을 구성하여 빌드를 진행하고, 최종적으로 필요한 파일만을 추출하여 불필요한 파일들이 제외된
가볍고 안전한 이미지를 생성할 수 있습니다.
  <img src="/assets/2024/cicd-lite/w1/20241205_cicd_lite_w1_2.png" alt="img.png" class="image-center" />
  <em class="image-caption">멀티 스테이지 빌드 동작</em></li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c"># 코드 작성</span>
  <span class="nv">$ </span><span class="nb">mkdir </span>1.3 <span class="o">&amp;&amp;</span> <span class="nb">cd </span>1.3
  
  <span class="nv">$ </span><span class="nb">cat</span> <span class="o">&gt;</span> Hello.java <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
  class Hello {
      public static void main(String[] args) {
          System.out.println("Hello Multistage container build");
      }
  }
</span><span class="no">  EOF
  
</span>  <span class="nv">$ </span><span class="nb">cat</span> <span class="o">&gt;</span> Dockerfile <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
  FROM openjdk:11 AS buildstage
  COPY . /app
  WORKDIR /app
  RUN javac Hello.java
  
  FROM openjdk:11-jre-slim
  COPY --from=buildstage /app/Hello.class /app/
  WORKDIR /app
  CMD java Hello
</span><span class="no">  EOF
  
</span>  <span class="c"># 컨테이너 이미지 빌드 : 용량 비교 해보자!</span>
  <span class="nv">$ </span>docker build <span class="nb">.</span> <span class="nt">-t</span> hello:3 <span class="nt">-t</span> hello:latest
  <span class="nv">$ </span>docker image <span class="nb">ls</span> <span class="nt">-f</span> <span class="nv">reference</span><span class="o">=</span>hello
  <span class="c"># =&gt; REPOSITORY   TAG       IMAGE ID       CREATED          SIZE</span>
  <span class="c">#    hello        3         4a058414ac44   11 seconds ago   216MB</span>
  <span class="c">#    hello        latest    4a058414ac44   11 seconds ago   216MB</span>
  <span class="c">#    hello        2         ba37ddf45c26   24 minutes ago   487MB</span>
  <span class="c">#    ...</span>
  <span class="c"># &lt;span style="color: green;"&gt;👉 Compiler가 없는 가벼운 jre 이미지를 사용하여 컨테이너 이미지 크기도 줄어든 것을 확인할 수 있습니다.&lt;/span&gt;</span>
  
  <span class="c"># 컨테이너 실행</span>
  <span class="nv">$ </span>docker run <span class="nt">--rm</span> hello:3
  <span class="c"># =&gt; Hello Multistage container build</span>
  <span class="nv">$ </span>docker run <span class="nt">--rm</span> hello
  <span class="c"># =&gt; Hello Multistage container build</span>
  
  <span class="c"># 컨테이너 이미지 내부에 파일 목록을 보면 어떤가요?</span>
  <span class="nv">$ </span>docker run <span class="nt">--rm</span> hello <span class="nb">ls</span> <span class="nt">-l</span>
  <span class="c"># =&gt; total 4</span>
  <span class="c">#    -rw-r--r-- 1 root root 436 Dec  5 15:26 Hello.class</span>
  <span class="nv">$ </span>docker run <span class="nt">--rm</span> hello javac <span class="nt">--help</span>
  <span class="c"># =&gt; docker: Error response from daemon: OCI runtime create failed: container_linux.go:380: starting container process caused: exec: &amp;quot;javac&amp;quot;: executable file not found in $PATH: unknown.</span>
  <span class="c"># &lt;span style="color: green;"&gt;👉 javac가 없어서 이전보다 안전한것을 알 수 있습니다.&lt;/span&gt;</span>
</code></pre></div></div>

<h4 id="jib로-자바-컨테이너-빌드">Jib로 자바 컨테이너 빌드</h4>
<p><a href="https://cloud.google.com/java/getting-started/jib?hl=ko">문서</a>, 
  <a href="https://colevelup.tistory.com/53">관련 블로그1</a>,
  <a href="https://jh-labs.tistory.com/509">관련 블로그2</a></p>
<ul>
  <li>Jib는 Google에서 만든 오픈소스 도구로, Java 어플리케이션을 컨테이너 이미지로 빌드하는 도구입니다.</li>
  <li>Jib는 docker 없이 컨테이너 이미지를 빌드할 수 있으며, 빌드 속도가 빠르고, 이미지 크기가 작아서 배포가 용이합니다.</li>
  <li>Maven 또는 Gradle 플러그인으로 사용할 수 있습니다.</li>
  <li>기존의 Docker 이미지 빌드 흐름은 다음과 같습니다.
<img src="/assets/2024/cicd-lite/w1/20241205_cicd_lite_w1_3.png" alt="img.png" /></li>
  <li>Jib는 다음과 같이 빌드 흐름이 간소화됩니다.
<img src="/assets/2024/cicd-lite/w1/20241205_cicd_lite_w1_4.png" alt="img.png" />
    <ul>
      <li>빌드와 동시에 이미지가 만들어지고 저장소에 푸시까지 가능합니다.</li>
      <li>Jenkins 등의 CI 서버에 Docker가 없어도 컨테이너 이미지를 빌드할 수 있습니다.</li>
      <li>이미지 레이어 캐싱을 통해 빌드 속도가 빠릅니다.</li>
      <li>이미지 크기가 작아서 배포가 용이합니다.</li>
    </ul>
  </li>
</ul>

<h4 id="어플리케이션-서버-컨테이너화-하기">어플리케이션 서버 컨테이너화 하기</h4>

<ul>
  <li>데모를 위해 HTTP 웹 어플리케이션을 컨테이너화 해보겠습니다.</li>
  <li>다음은 ruby 언어로 작성한 기본적인 웹서버로, 현재 날짜와 시간을 표시합니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">mkdir </span>1.4 <span class="o">&amp;&amp;</span> <span class="nb">cd </span>1.4

<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&gt;</span> app.rb <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
require 'sinatra'

get '/' do
  "Hello, World! The time is #{Time.now}"
end
</span><span class="no">EOF

</span><span class="nv">$ </span><span class="nb">cat</span> <span class="o">&gt;</span> Dockerfile <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
FROM ruby:3.3
RUN gem install sinatra rackup puma
COPY app.rb /app/
WORKDIR /app
CMD ["ruby", "app.rb", "-o", "0.0.0.0"]
</span><span class="no">EOF

</span><span class="c"># 컨테이너 이미지 빌드</span>

<span class="nv">$ </span>docker build <span class="nb">.</span> <span class="nt">-t</span> timeserver:1 <span class="o">&amp;&amp;</span> docker tag timeserver:1 timeserver:latest
<span class="nv">$ </span>docker image <span class="nb">ls</span> <span class="nt">-f</span> <span class="nv">reference</span><span class="o">=</span>timeserver
<span class="c"># =&gt; REPOSITORY   TAG       IMAGE ID       CREATED          SIZE</span>
<span class="c">#    timeserver   1         6393669e5e68   12 seconds ago   1GB</span>
<span class="c">#    timeserver   latest    6393669e5e68   12 seconds ago   1GB</span>

<span class="c"># 컨테이너 실행</span>
<span class="nv">$ </span>docker run <span class="nt">-d</span> <span class="nt">-p</span> 8080:4567 <span class="nt">--name</span><span class="o">=</span>timeserver timeserver

<span class="c"># 컨테이너 접속 및 로그 확인</span>
<span class="nv">$ </span>curl http://localhost:8080
<span class="c"># =&gt; Hello, World! The time is 2024-10-01 15:28:18 +0000</span>
<span class="nv">$ </span>docker logs timeserver
<span class="c"># =&gt; Puma starting in single mode...</span>
<span class="c">#    * Puma version: 6.5.0 (&amp;quot;Sky's Version&amp;quot;)</span>
<span class="c">#    == Sinatra (v4.1.1) has taken the stage on 4567 for development with backup from Puma</span>
<span class="c">#    * Ruby version: ruby 3.3.6 (2024-10-01 revision 75015d4c1f) [aarch64-linux]</span>
<span class="c">#    *  Min threads: 0</span>
<span class="c">#    *  Max threads: 5</span>
<span class="c">#    *  Environment: development</span>
<span class="c">#    *          PID: 1</span>
<span class="c">#    * Listening on http://0.0.0.0:4567</span>
<span class="c">#    Use Ctrl-C to stop</span>
<span class="c">#    172.17.0.1 - - [01/Oct/2024:15:28:07 +0000] &amp;quot;GET / HTTP/1.1&amp;quot; 200 51 0.0048</span>

<span class="c"># 컨테이너 이미지 내부에 파일 확인</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> timeserver <span class="nb">ls</span> <span class="nt">-l</span>
<span class="c"># =&gt; total 4</span>
<span class="c">#    -rw-r--r-- 1 root root 76 Dec  6 15:20 app.rb</span>
</code></pre></div></div>

<ul>
  <li>도커 컨테이너 내부의 소스코드를 수정해서 반영되는지 확인해보겠습니다.</li>
</ul>

<p><img src="/assets/2024/cicd-lite/w1/20241205_cicd_lite_w1_5.png" alt="20241205_cicd_lite_w1_5.png" class="image-center" />
<em class="image-caption">vscode에 docker 확장 설치</em></p>

<p><img src="/assets/2024/cicd-lite/w1/20241205_cicd_lite_w1_6.png" alt="20241205_cicd_lite_w1_6.png" class="image-center" />
<em class="image-caption">timeserver 컨테이너 내부의 app.rb 파일 수정</em></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 컨테이너 이미지 내부에 app.rb 파일 수정 후 반영 확인 : VSCODE 경우 docker 확장프로그램 활용</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> timeserver <span class="nb">cat </span>app.rb
<span class="c"># =&gt; require 'sinatra'</span>
<span class="c">#    </span>
<span class="c">#    get '/' do</span>
<span class="c">#      &amp;quot;Hello, World! 😀 The time is #{Time.now}&amp;quot;</span>
<span class="c">#    end</span>

<span class="c"># 컨테이너 접속 후 확인 </span>
<span class="nv">$ </span>curl http://localhost:8080
<span class="c"># =&gt; Hello, World! The time is 2024-10-01 15:45:09 +0000%</span>
<span class="c"># &lt;span style="color: green;"&gt;👉 수정 사항이 반영되지 않았습니다!&lt;/span&gt;</span>

<span class="c"># 컨테이너 삭제</span>
<span class="nv">$ </span>docker <span class="nb">rm</span> <span class="nt">-f</span> timeserver
</code></pre></div></div>

<ul>
  <li>어플리케이션 수정해서 테스트</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sed</span> <span class="nt">-i</span> <span class="nt">-e</span> <span class="s1">'s#World!#World! 😀 Hello CloudNeta Study!#'</span> app.rb
<span class="nv">$ </span><span class="nb">cat </span>app.rb
<span class="c"># =&gt; require 'sinatra'</span>
<span class="c">#    </span>
<span class="c">#    get '/' do</span>
<span class="c">#      &amp;quot;Hello, World! 😀 Hello CloudNeta Study! The time is #{Time.now}&amp;quot;</span>
<span class="c">#    end</span>

<span class="c"># 컨테이너 이미지 빌드</span>
<span class="nv">$ </span>docker build <span class="nb">.</span> <span class="nt">-t</span> timeserver:2 <span class="nt">-t</span> timeserver:latest
<span class="nv">$ </span>docker image <span class="nb">ls</span> <span class="nt">-f</span> <span class="nv">reference</span><span class="o">=</span>timeserver
<span class="c"># =&gt; REPOSITORY   TAG       IMAGE ID       CREATED          SIZE</span>
<span class="c">#    timeserver   2         80f230757e3c   2 seconds ago    1.01GB</span>
<span class="c">#    timeserver   latest    80f230757e3c   2 seconds ago    1.01GB</span>
<span class="c">#    timeserver   1         c7055ab70155   27 minutes ago   1.01GB</span>

<span class="c"># 컨테이너 실행</span>
<span class="nv">$ </span>docker run <span class="nt">-d</span> <span class="nt">-p</span> 8080:4567 <span class="nt">--name</span><span class="o">=</span>timeserver timeserver
<span class="c"># =&gt; 278108d26b3998c8281add75b631d59a9d44abd4eb3e4f173b0b156d66e5da75</span>

<span class="c"># 컨테이너 접속 및 로그 확인</span>
<span class="nv">$ </span>curl http://localhost:8080
<span class="c"># =&gt; Hello, World! 😀 Hello CloudNeta Study! The time is 2024-10-01 15:55:19 +0000</span>

<span class="c"># 컨테이너 삭제</span>
<span class="nv">$ </span>docker <span class="nb">rm</span> <span class="nt">-f</span> timeserver
</code></pre></div></div>

<h4 id="로컬-개발을-위한-방안">로컬 개발을 위한 방안</h4>

<ul>
  <li>소스를 수정할 때마다 위와 같이 컨테이너 이미지를 빌드하고 실행하는 것은 번거롭습니다.</li>
  <li>이를 편리하게 하기 위해서 로컬 폴더와 컨테이너의 앱 소스를 매핑하고, 코드 내용을 동적으로 반영해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">mkdir </span>1.5 <span class="o">&amp;&amp;</span> <span class="nb">cd </span>1.5

<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&gt;</span> app.rb <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
require 'sinatra/base'

class App &lt; Sinatra::Base
  get '/' do
    "Hello, World! The time is #{Time.now}"
  end
end
</span><span class="no">EOF

</span><span class="nv">$ </span><span class="nb">cat</span> <span class="o">&gt;</span> config.ru <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
require 'rack/unreloader'
require 'sinatra'
Unreloader = Rack::Unreloader.new{App}
Unreloader.require './app.rb'

run Unreloader
</span><span class="no">EOF

</span><span class="nv">$ </span><span class="nb">cat</span> <span class="o">&gt;</span> Dockerfile <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
FROM ruby:3.3
RUN gem install sinatra rackup puma </span><span class="se">\</span><span class="sh">
    rack-unreloader # 소스코드 변경시 자동으로 반영하기위한 툴
COPY app.rb config.ru /app/
WORKDIR /app
CMD ["rackup", "--host", "0.0.0.0"]
</span><span class="no">EOF

</span><span class="nv">$ </span><span class="nb">cat</span> <span class="o">&gt;</span> docker-compose.yaml <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
services:
  frontend:
    build: .
    command: rackup --host 0.0.0.0
    volumes:
      - type: bind
        source: .
        target: /app
    ports:
      - "8080:9292"
</span><span class="no">EOF

</span><span class="c"># 도커 컴포즈로 컨테이너 빌드</span>
<span class="nv">$ </span>docker compose build 
<span class="c"># 도커 컴포즈로 컨테이너 실행 </span>
<span class="nv">$ </span>docker compose up <span class="nt">-d</span>
<span class="c"># =&gt; [+] Running 1/1</span>
<span class="c">#     ⠿ Container 15-frontend-1  Started 0.4s</span>

<span class="c"># 컴포즈로 실행 시 이미지와 컨테이너 네이밍 규칙을 알아보자!</span>
<span class="nv">$ </span>docker compose ps
<span class="c"># =&gt; NAME                COMMAND                  SERVICE             STATUS              PORTS</span>
<span class="c">#    15-frontend-1       &amp;quot;rackup --host 0.0.0…&amp;quot;   frontend            running             0.0.0.0:8080-&amp;gt;9292/tcp</span>
<span class="c"># &lt;span style="color: green;"&gt;👉 컴포즈로 실행시 현재 디렉터리 이름에서 특수문자를 제외한 것에 컨테이너 이름에 "-1"를 붙이는듯 합니다.&lt;/span&gt;</span>

<span class="nv">$ </span>docker compose images
<span class="c"># =&gt; Container           Repository          Tag                 Image Id            Size</span>
<span class="c">#    15-frontend-1       15_frontend         latest              4aa2b680f319        1.01GB</span>

<span class="c">#</span>
<span class="nv">$ </span>curl http://localhost:8080
<span class="c"># =&gt; Hello, World! The time is 2024-10-01 05:55:18 +0000!!!!!%</span>
<span class="nv">$ </span>docker compose logs
<span class="c"># =&gt; 15-frontend-1  | Puma starting in single mode...</span>
<span class="c">#    15-frontend-1  | * Puma version: 6.5.0 (&amp;quot;Sky's Version&amp;quot;)</span>
<span class="c">#    15-frontend-1  | * Ruby version: ruby 3.3.6 (2024-10-01 revision 75015d4c1f) [aarch64-linux]</span>
<span class="c">#    15-frontend-1  | *  Min threads: 0</span>
<span class="c">#    15-frontend-1  | *  Max threads: 5</span>
<span class="c">#    15-frontend-1  | *  Environment: development</span>
<span class="c">#    15-frontend-1  | *          PID: 1</span>
<span class="c">#    15-frontend-1  | * Listening on http://0.0.0.0:9292</span>
<span class="c">#    15-frontend-1  | Use Ctrl-C to stop</span>
<span class="c">#    15-frontend-1  | 172.23.0.1 - - [01/Oct/2024:03:58:23 +0000] &amp;quot;GET / HTTP/1.1&amp;quot; 200 51 0.0153</span>
</code></pre></div></div>

<ul>
  <li>소스코드 수정 후 반영 확인</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cat </span>app.rb
<span class="c"># =&gt; require 'sinatra/base'</span>
<span class="c">#    </span>
<span class="c">#    class App &amp;lt; Sinatra::Base</span>
<span class="c">#      get '/' do</span>
<span class="c">#        &amp;quot;Hello, World! The time is #{Time.now}&amp;quot;</span>
<span class="c">#      end</span>
<span class="c">#    end</span>

<span class="c"># 소스코드 수정</span>
<span class="nv">$ </span><span class="nb">sed</span> <span class="nt">-i</span> <span class="nt">-e</span> <span class="s1">'s#World!#World! 😀 Hello CloudNeta Study!#'</span> app.rb
<span class="nv">$ </span><span class="nb">cat </span>app.rb
<span class="c"># =&gt; require 'sinatra/base'</span>
<span class="c">#    </span>
<span class="c">#    class App &amp;lt; Sinatra::Base</span>
<span class="c">#      get '/' do</span>
<span class="c">#        &amp;quot;Hello, World! 😀 Hello CloudNeta Study! The time is #{Time.now}!!!!!&amp;quot;</span>
<span class="c">#      end</span>
<span class="c">#    end</span>

<span class="nv">$ </span>curl http://localhost:8080
<span class="c"># =&gt; Hello, World! 😀 Hello CloudNeta Study! The time is 2024-10-01 05:57:05 +0000!!!!!</span>
<span class="c"># &lt;span style="color: green;"&gt;👉 변경사항이 잘 반영되었습니다. :)&lt;/span&gt;</span>

<span class="c"># 컨테이너 중지 및 삭제</span>
<span class="nv">$ </span>docker compose down
</code></pre></div></div>

<h3 id="cicd-실습환경-구성">CI/CD 실습환경 구성</h3>

<ul>
  <li>Jenkins와 Gitlab을 이용하여 CI/CD 파이프라인을 구축해보겠습니다.</li>
  <li>Jenkins는 Docker에 설치해서 사용하고 Gitlab은 <a href="https://www.gitlab.com">gitlab.com</a>를 사용하겠습니다.</li>
</ul>

<h4 id="jenkins-소개">Jenkins 소개</h4>

<p><img src="/assets/2024/cicd-lite/w1/20241205_cicd_lite_w1_7.png" alt="img.png" class="image-center" /></p>
<ul>
  <li>Jenkins는 오픈소스 CI/CD 도구로, 빌드, 테스트, 배포 등의 작업을 자동화할 수 있습니다.</li>
  <li>Jenkins는 CI/CD라는 용어가 있기 전부터 사용되던 도구로, CI/CD에 국한되지 않고 다양한 작업을 자동화할 수 있습니다.</li>
  <li>주요 기능은 다음과 같습니다.
    <ol>
      <li><strong>확장성</strong> : 다양한 플러그인 생태계를 가지고 있어 기능을 확장할 수 있습니다. Git, Docker, Kubernetes 등 다양한 도구 및 플랫폼과 통합할 수 있습니다.</li>
      <li><strong>분산 빌드</strong> : 분산 빌드를 지원하여 여러 머신에서 작업을 실행할 수 있습니다. 이를 통해 부하를 분산시키고 빌드 속도를 높일 수 있습니다.</li>
      <li><strong>자동화된 테스트</strong> : 테스트 실행을 자동화하여 코드 품질에 대한 즉각적인 피드백을 제공합니다. 다양한 테스트 프레임워크 및 도구를 지원합니다.</li>
      <li><strong>코드 파이프라인</strong> : Jenkinsfile을 사용하여 빌드, 테스트, 배포 파이프라인을 코드로 정의할 수 있습니다. 이를 통해 버전 관리와 협업이 용이합니다.</li>
      <li><strong>지속적 통합 및 지속적 배포 (CI/CD)</strong> : 코드 변경 사항을 통합하고, 애플리케이션을 빌드하고, 테스트를 실행하고, 배포하는 과정을 자동화합니다. 이를 통해 일관되고 신뢰할 수 있는 배포 프로세스를 보장합니다.</li>
    </ol>
  </li>
  <li>흔히 사용되는 CI/CD 워크플로우는 다음과 같습니다.
    <ol>
      <li><strong>최신 코드 가져오기</strong> : 개발을 위해 중앙 코드 리포지터리에서 로컬 시스템으로 애플리케이션의 최신 코드를 가져</li>
      <li><strong>단위 테스트 구현과 실행</strong> : 코드 작성 전 단위 테스트 케이스를 먼저 작성</li>
      <li><strong>코드 개발</strong> : 실패한 테스트 케이스를 성공으로 바꾸면서 코드 개발</li>
      <li><strong>단위 테스트 케이스 재실행</strong> : 단위 테스트 케이스 실행 시 통과(성공!)</li>
      <li><strong>코드 푸시와 병합</strong> : 개발 소스 코드를 중앙 리포지터리로 푸시하고, 코드 병합</li>
      <li><strong>코드 병합 후 컴파일</strong> : 변경 함수 코드가 병함되면 전체 애플리케이션이 컴파일된다</li>
      <li><strong>병합된 코드에서 테스트 실행</strong> : 개별 테스트뿐만 아니라 전체 통합 테스트를 실행하여 문제 없는지 확인</li>
      <li><strong>아티팩트 배포</strong> : 애플리케이션을 빌드하고, 애플리케이션 서버의 프로덕션 환경에 배포</li>
      <li><strong>배포 애플리케이션의 E-E 테스트 실행</strong> : 셀레늄 Selenium과 같은 User Interface 자동화 도구를 통해 애플리케이션의 전체 워크플로가 정상 동작하는지 확인하는 종단간 End-to-End 테스트를 실행.</li>
    </ol>
  </li>
  <li>이러한 워크플로우를 코드 커밋/푸시와 같은 이벤트가 발생할 때 자동으로 실행되도록 설정할 수 있습니다.</li>
</ul>

<h5 id="jenkins-컨테이너에서-호스트에-도커-데몬-사용-설정">Jenkins 컨테이너에서 호스트에 도커 데몬 사용 설정</h5>

<ul>
  <li>컨테이너에서 도커를 사용하기 위해서는 DinD(Docker in Docker)를 사용하여 컨테이너 안에서 도커를 실행하거나
DooD(Docker outside of Docker)를 사용하여 호스트의 도커 데몬을 사용할 수 있습니다.</li>
  <li>이번에는 Docker outside of Docker를 사용하여 호스트의 도커 데몬을 사용하겠습니다.
<img src="/assets/2024/cicd-lite/w1/20241205_cicd_lite_w1_8.png" alt="img.png" class="image-center" />
<em class="image-caption">DinD와 DooD 구조 비교</em></li>
</ul>

<h4 id="jenkins-컨테이너-실행-및-설정">Jenkins 컨테이너 실행 및 설정</h4>

<ul>
  <li>먼저 Jenkins 컨테이너를 실행하겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 작업 디렉토리 생성 후 이동</span>
<span class="nv">$ </span><span class="nb">mkdir </span>cicd-labs <span class="o">&amp;&amp;</span> <span class="nb">cd </span>cicd-labs

<span class="c"># </span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOT</span><span class="sh"> &gt; docker-compose.yaml
services:
  jenkins:
    container_name: jenkins
    image: jenkins/jenkins
    restart: unless-stopped
    networks:
      - cicd-network
    ports:
      - "8080:8080"
      - "50000:50000"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - jenkins_home:/var/jenkins_home
volumes:
  jenkins_home:
networks:
  cicd-network:
    driver: bridge
</span><span class="no">EOT

</span><span class="c"># 배포</span>
<span class="nv">$ </span>docker compose up <span class="nt">-d</span>
<span class="c"># =&gt; [+] Running 13/13</span>
<span class="c">#     ⠿ jenkins Pulled                                                                                                                                                      13.7s</span>
<span class="c">#    [+] Running 3/3</span>
<span class="c">#     ⠿ Network cicd-labs_cicd-network   Created                                                                                                                             0.0s</span>
<span class="c">#     ⠿ Volume &amp;quot;cicd-labs_jenkins_home&amp;quot;  Created                                                                                                                             0.0s</span>
<span class="c">#     ⠿ Container jenkins                Started</span>
<span class="nv">$ </span>docker compose ps
<span class="c"># =&gt; NAME                COMMAND                  SERVICE             STATUS              PORTS</span>
<span class="c">#    jenkins             &amp;quot;/usr/bin/tini -- /u…&amp;quot;   jenkins             running             0.0.0.0:8080-&amp;gt;8080/tcp, 0.0.0.0:50000-&amp;gt;50000/tcp</span>

<span class="c"># 기본 정보 확인</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>jenkins <span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; container : </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker compose <span class="nb">exec</span> <span class="nv">$i</span> sh <span class="nt">-c</span> <span class="s2">"whoami &amp;&amp; pwd"</span><span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; &amp;gt;&amp;gt; container : jenkins &amp;lt;&amp;lt;</span>
<span class="c">#    jenkins</span>
<span class="c">#    /</span>

<span class="c"># 도커를 이용하여 컨테이너로 접속</span>
<span class="nv">$ </span>docker compose <span class="nb">exec </span>jenkins bash
<span class="nv">$ </span><span class="nb">exit</span>
</code></pre></div></div>

<ul>
  <li>Jenkins 초기 설정을 진행하겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 초기 비밀번호 확인</span>
<span class="nv">$ </span>docker compose <span class="nb">exec </span>jenkins <span class="nb">cat</span> /var/jenkins_home/secrets/initialAdminPassword
<span class="c"># =&gt; dcc757c8c4f14fc09795ed0440baf157</span>

<span class="c"># 브라우저에서 접속하여 초기 비밀번호 입력후 설정 진행 : 계정 / 암호 입력 &gt;&gt; admin / qwe123</span>
<span class="nv">$ </span>open http://localhost:8080
</code></pre></div></div>

<ul>
  <li>jenkins 컨테이너에서 호스트의 도커 데몬을 사용하기 위해 설정을 진행하겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># jenkins 컨테이너에서 호스트의 도커 데몬을 사용하기 위한 설정</span>
<span class="nv">$ </span>docker compose <span class="nb">exec</span> <span class="nt">--privileged</span> <span class="nt">-u</span> root jenkins bash
<span class="nt">-----------------------------------------------------</span>
<span class="nv">$ </span><span class="nb">id</span>
<span class="c"># =&gt; uid=0(root) gid=0(root) groups=0(root)</span>

<span class="nv">$ </span>curl <span class="nt">-fsSL</span> https://download.docker.com/linux/debian/gpg <span class="nt">-o</span> /etc/apt/keyrings/docker.asc
<span class="nv">$ </span><span class="nb">chmod </span>a+r /etc/apt/keyrings/docker.asc
<span class="nv">$ </span><span class="nb">echo</span> <span class="se">\</span>
  <span class="s2">"deb [arch=</span><span class="si">$(</span>dpkg <span class="nt">--print-architecture</span><span class="si">)</span><span class="s2"> signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/debian </span><span class="se">\</span><span class="s2">
  </span><span class="si">$(</span><span class="nb">.</span> /etc/os-release <span class="o">&amp;&amp;</span> <span class="nb">echo</span> <span class="s2">"</span><span class="nv">$VERSION_CODENAME</span><span class="s2">"</span><span class="si">)</span><span class="s2"> stable"</span> | <span class="se">\</span>
  <span class="nb">tee</span> /etc/apt/sources.list.d/docker.list <span class="o">&gt;</span> /dev/null
<span class="nv">$ </span>apt-get update <span class="o">&amp;&amp;</span> apt <span class="nb">install </span>docker-ce-cli curl tree jq <span class="nt">-y</span>

<span class="nv">$ </span>docker info
<span class="c"># =&gt; Client: Docker Engine - Community</span>
<span class="c">#     Version:    27.3.1</span>
<span class="c">#     Context:    default</span>
<span class="c">#     Debug Mode: false</span>
<span class="c">#     Plugins:</span>
<span class="c">#      buildx: Docker Buildx (Docker Inc.)</span>
<span class="c">#        Version:  v0.17.1</span>
<span class="c">#        Path:     /usr/libexec/docker/cli-plugins/docker-buildx</span>
<span class="c">#      compose: Docker Compose (Docker Inc.)</span>
<span class="c">#        Version:  v2.29.7</span>
<span class="c">#        Path:     /usr/libexec/docker/cli-plugins/docker-compose</span>
<span class="c">#    </span>
<span class="c">#    Server:</span>
<span class="c">#     Containers: 27</span>
<span class="c">#      Running: 3</span>
<span class="c">#      Paused: 0</span>
<span class="c">#      Stopped: 24</span>
<span class="c">#     Images: 37</span>
<span class="c">#     Server Version: 20.10.12</span>
<span class="c">#     Storage Driver: overlay2</span>
<span class="c">#      Backing Filesystem: extfs</span>
<span class="c">#      Supports d_type: true</span>
<span class="c">#    ...</span>
<span class="nv">$ </span>docker ps
<span class="c"># =&gt; CONTAINER ID   IMAGE                  COMMAND                  CREATED          STATUS          PORTS                                              NAMES</span>
<span class="c">#    06409fe2219f   jenkins/jenkins        &amp;quot;/usr/bin/tini -- /u…&amp;quot;   25 minutes ago   Up 25 minutes   0.0.0.0:8080-&amp;gt;8080/tcp, 0.0.0.0:50000-&amp;gt;50000/tcp   jenkins</span>
<span class="c"># &lt;span style="color: green;"&gt;👉 Docker-out-of-Docker 이기 때문에 호스트 도커 데몬에서 운영되는 컨테이너를 볼 수 있습니다!&lt;/span&gt;</span>
<span class="nv">$ </span>which docker
<span class="c"># =&gt; /usr/bin/docker</span>

<span class="c"># Jenkins 컨테이너 내부에서 root가 아닌 jenkins 유저도 docker를 실행할 수 있도록 권한을 부여</span>
<span class="nv">$ </span>groupadd <span class="nt">-g</span> 2000 <span class="nt">-f</span> docker
<span class="nv">$ </span><span class="nb">chgrp </span>docker /var/run/docker.sock
<span class="nv">$ </span><span class="nb">chmod </span>770 /var/run/docker.sock
<span class="nv">$ </span><span class="nb">ls</span> <span class="nt">-l</span> /var/run/docker.sock
<span class="c"># =&gt; srwxr-xr-x 1 root docker 0 Dec  7 03:12 /var/run/docker.sock</span>
<span class="nv">$ </span>usermod <span class="nt">-aG</span> docker jenkins
<span class="nv">$ </span><span class="nb">cat</span> /etc/group | <span class="nb">grep </span>docker
<span class="c"># =&gt; docker:x:2000:jenkins</span>

<span class="nv">$ </span><span class="nb">exit</span>
<span class="nt">--------------------------------------------</span>

<span class="c"># jenkins item 실행 시 docker 명령 실행 권한 에러 발생 : Jenkins 컨테이너 재기동으로 위 설정 내용을 Jenkins app 에도 적용 필요</span>
<span class="nv">$ </span>docker compose restart jenkins
<span class="c"># $ sudo docker compose restart jenkins  # Windows 경우 이후부터 sudo 붙여서 실행</span>

<span class="c"># jenkins user로 docker 명령 실행 확인</span>
<span class="nv">$ </span>docker compose <span class="nb">exec </span>jenkins <span class="nb">id</span>
<span class="c"># =&gt; uid=1000(jenkins) gid=1000(jenkins) groups=1000(jenkins),2000(docker)</span>
<span class="nv">$ </span>docker compose <span class="nb">exec </span>jenkins docker info
<span class="c"># =&gt; Client: Docker Engine - Community</span>
<span class="c">#     Version:    27.3.1</span>
<span class="c">#     Context:    default</span>
<span class="c">#     ...</span>
<span class="c">#    </span>
<span class="c">#    Server:</span>
<span class="c">#     Containers: 27</span>
<span class="c">#      Running: 3</span>
<span class="c">#      Paused: 0</span>
<span class="c">#      Stopped: 24</span>
<span class="c">#     Images: 37</span>
<span class="c">#     Server Version: 20.10.12</span>
<span class="c">#    ...</span>
<span class="nv">$ </span>docker compose <span class="nb">exec </span>jenkins docker ps
<span class="c"># =&gt; CONTAINER ID   IMAGE                  COMMAND                  CREATED          STATUS         PORTS                                              NAMES</span>
<span class="c">#    06409fe2219f   jenkins/jenkins        &amp;quot;/usr/bin/tini -- /u…&amp;quot;   30 minutes ago   Up 2 minutes   0.0.0.0:8080-&amp;gt;8080/tcp, 0.0.0.0:50000-&amp;gt;50000/tcp   jenkins</span>
<span class="c">#    ...</span>
</code></pre></div></div>

<h4 id="gitlab-소개">Gitlab 소개</h4>

<ul>
  <li>Gitlab은 Github와 유사한 Git 기반의 코드 저장소 서비스로, 코드 저장소, 이슈 트래커, CI/CD 파이프라인, 코드 검토 등의 기능을 제공합니다.
<a href="https://www.gitlab.com">Gitlab.com</a></li>
  <li>이번 실습에서는 코드 저장소 기능만 사용하고 Jenkins의 CI/CD 파이프라인을 사용하겠습니다.</li>
  <li>주요 기능
    <ul>
      <li><strong>소스 코드 관리</strong> : GitLab은 Git 기반의 소스 코드 저장소를 제공하여 버전 관리를 쉽게 할 수 있습니다.</li>
      <li><strong>CI/CD 파이프라인</strong> : GitLab은 CI/CD 파이프라인을 통해 코드의 빌드, 테스트, 배포를 자동화할 수 있습니다.</li>
      <li><strong>이슈 트래킹</strong> : 프로젝트의 버그, 기능 요청 등을 관리할 수 있는 이슈 트래킹 시스템을 제공합니다.</li>
      <li><strong>코드 리뷰</strong> : 병합 요청(Merge Request)을 통해 코드 리뷰를 쉽게 진행할 수 있습니다.</li>
      <li><strong>위키</strong> : 프로젝트 관련 문서를 작성하고 관리할 수 있는 위키 기능을 제공합니다.</li>
      <li><strong>프로젝트 관리</strong> : 마일스톤, 보드, 라벨 등을 통해 프로젝트를 체계적으로 관리할 수 있습니다.</li>
      <li><strong>통합 및 확장성</strong> : 다양한 외부 도구와의 통합 및 확장을 지원하여 유연한 개발 환경을 구축할 수 있습니다.</li>
      <li><strong>셀프 호스트 가능</strong> : GitLab은 오픈소스로 제공되어 무료로 자체 서버에 설치하여 사용할 수 있습니다. (일부 기능 차이가 있음)</li>
    </ul>
  </li>
</ul>

<h4 id="gitlab-프로젝트-생성-및-설정">Gitlab 프로젝트 생성 및 설정</h4>

<ul>
  <li>
    <p>Gitlab.com에서 새로운 프로젝트를 생성하겠습니다.
<img src="/assets/2024/cicd-lite/w1/20241205_cicd_lite_w1_9.png" alt="img.png" class="image-center" />
<em class="image-caption">Gitlab 프로젝트 생성</em></p>
  </li>
  <li>
    <p>프로젝트 생성시 프로젝트 이름과 가시성을 설정하고 생성합니다.</p>
    <ul>
      <li><strong>프로젝트 이름</strong> : 2024-cicd-lite-w1</li>
      <li><strong>가시성</strong> : Private</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/2024/cicd-lite/w1/20241205_cicd_lite_w1_10.png" alt="img_1.png" class="image-center" />
<em class="image-caption">Jenkins와 연동을 위해 토큰 발급</em></p>

<ul>
  <li>프로젝트 생성 후 프로젝트 설정에서 CI/CD 파이프라인을 위한 토큰을 발급받습니다.</li>
  <li>프로필 아이콘 클릭 &gt; Preferences &gt; Access Tokens &gt; Add new token을 클릭하여 토큰을 발급받습니다.
    <ul>
      <li><strong>토큰 이름</strong> : jenkins</li>
      <li><strong>권한</strong> : read_repository, write_repository</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/2024/cicd-lite/w1/20241205_cicd_lite_w1_11.png" alt="img_2.png" class="image-center" />
<em class="image-caption">토큰 발급 완료</em></p>

<ul>
  <li>토큰이 완료되면 복사할 수 있습니다. 이후에는 다시 확인할 수 없으므로 잘 기록해두어야 합니다.</li>
</ul>

<h5 id="gitlab에서-소스-받기">Gitlab에서 소스 받기</h5>

<ul>
  <li>소스를 받기 위해 git 주소를 복사합니다.
<img src="/assets/2024/cicd-lite/w1/20241205_cicd_lite_w1_12.png" alt="img.png" class="image-center" />
<em class="image-caption">Gitlab 프로젝트 주소 복사</em></li>
  <li>프로젝트 페이지에 접속 후 Code 버튼을 클릭하고 클립보드 아이콘을 클릭하여 주소를 복사할 수 있습니다.</li>
  <li>복사한 주소로 소스를 받아서 필요한 파일들을 생성해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># &lt;span style="color: green;"&gt;👉 아이디와 비밀번호를 물으면 토큰이름과 발급받은 토큰을 입력하시면 됩니다.&lt;/span&gt;</span>
<span class="nv">$ </span>git clone https://gitlab.com/littlebird/2024-cicd-lite-w1.git
<span class="c"># =&gt; Cloning into '2024-cicd-lite-w1'...</span>
<span class="c">#    Username for 'https://gitlab.com': jenkins</span>
<span class="c">#    Password for 'https://jenkins@gitlab.com': glpat-ABCD1234</span>
<span class="c">#    remote: Enumerating objects: 3, done.</span>
<span class="c">#    remote: Counting objects: 100% (3/3), done.</span>
<span class="c">#    remote: Compressing objects: 100% (2/2), done.</span>
<span class="c">#    remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)</span>
<span class="c">#    Receiving objects: 100% (3/3), done.</span>
<span class="nv">$ </span><span class="nb">cd </span>2024-cicd-lite-w1/

<span class="c"># 소스코드 작성</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&gt;</span> app.rb <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
require 'sinatra'

get '/' do
  "Hello, World! The time is #{Time.now}"
end
</span><span class="no">EOF

</span><span class="c"># Dockerfile 작성</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&gt;</span> Dockerfile <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
FROM ruby:3.3
RUN gem install sinatra rackup puma
COPY app.rb /app/
WORKDIR /app
CMD ["ruby", "app.rb", "-o", "0.0.0.0"]
</span><span class="no">EOF

</span><span class="c"># VERSION 파일 생성</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"0.0.1"</span> <span class="o">&gt;</span> VERSION

<span class="c">#</span>
<span class="nv">$ </span>git add <span class="nb">.</span>
<span class="nv">$ </span>git commit <span class="nt">-m</span> <span class="s2">"Initial commit"</span>
<span class="c"># =&gt; [main 569ce3c] Initial commit</span>
<span class="c">#     3 files changed, 11 insertions(+)</span>
<span class="c">#     create mode 100644 Dockerfile</span>
<span class="c">#     create mode 100644 VERSION</span>
<span class="c">#     create mode 100644 app.rb</span>
<span class="nv">$ </span>git push <span class="nt">-u</span> origin main
<span class="c"># =&gt; Enumerating objects: 6, done.</span>
<span class="c">#    Counting objects: 100% (6/6), done.</span>
<span class="c">#    Delta compression using up to 8 threads</span>
<span class="c">#    Compressing objects: 100% (4/4), done.</span>
<span class="c">#    Writing objects: 100% (5/5), 536 bytes | 536.00 KiB/s, done.</span>
<span class="c">#    Total 5 (delta 0), reused 0 (delta 0), pack-reused 0</span>
<span class="c">#    To https://gitlab.com/littlebird/2024-cicd-lite-w1.git</span>
<span class="c">#       fe2fb73..569ce3c  main -&amp;gt; main</span>
<span class="c">#    branch 'main' set up to track 'origin/main'.</span>
</code></pre></div></div>

<ul>
  <li>Gitlab 리파지토리에서 확인해보겠습니다.</li>
</ul>

<p><img src="/assets/2024/cicd-lite/w1/20241205_cicd_lite_w1_13.png" alt="img.png" /></p>

<ul>
  <li>Gitlab 프로젝트에 소스가 정상적으로 업로드 되었습니다.</li>
</ul>

<h4 id="docker-hub-소개">Docker Hub 소개</h4>

<ul>
  <li><a href="https://hub.docker.com">도커허브(Docker Hub)</a>는 도커 이미지를 저장하고 공유할 수 있는 클라우드 서비스입니다.</li>
  <li>여러 사용자가 자신이 만든 도커 이미지를 서로 자유롭게 공유할 수 있습니다.</li>
  <li>유의 사항
    <ul>
      <li>Docker Hub는 무료로 누구나 업로드 할 수 있기 때문에, 공식(Official) 라벨이 없는 이미지는 보안에 취약할 수 있고, 사용법을 알 수 없거나, 제대로 작동하지 않을 수 있습니다.</li>
      <li>도커 악성 이미지를 통한 취약점 공격 기사 모음
        <ul>
          <li>도커도 이제 공격 통로! 악성 이미지 늘어나고 있다 - <a href="https://www.boannews.com/media/view.asp?idx=93080&amp;page=1&amp;kind=1">링크</a></li>
          <li>도커 환경 공격하는 해커들, 전략을 또 변경했다 - <a href="https://www.boannews.com/media/view.asp?idx=89841&amp;page=1&amp;kind=1">링크</a></li>
          <li>암호화폐 채굴 공격자들, 잘못 설정된 도커 집중 공략 - <a href="https://www.boannews.com/media/view.asp?idx=87427&amp;page=1&amp;kind=1">링크</a></li>
          <li>리눅스 노리던 봇넷 멀웨어 둘, 최근 들어 도커 서버도 노리기 시작 - <a href="https://www.boannews.com/media/view.asp?idx=89205&amp;page=1&amp;kind=1">링크</a></li>
          <li>도커 호스트 감염시켜가며 암호화폐 채굴하는 웜 발견 - <a href="https://www.boannews.com/media/view.asp?idx=83854&amp;page=1&amp;kind=1">링크</a>  *</li>
        </ul>
      </li>
      <li>사용자당 1개의 Private Repository만 무료로 사용할 수 있습니다.</li>
    </ul>
  </li>
  <li>주요 기능
    <ul>
      <li><strong>도커 이미지 저장소</strong> : 도커 이미지를 저장하고 공유할 수 있습니다.</li>
      <li><strong>자동 빌드</strong> : Github, Gitlab과 연동하여 코드가 업데이트 될 때마다 자동으로 이미지를 빌드할 수 있습니다.</li>
      <li><strong>웹훅</strong> : 타 서비스와 연동하여 이벤트가 발생할 때마다 특정 URL로 요청을 보낼 수 있습니다.</li>
      <li><strong>Docker Hub CLI 도구</strong> : 도커 이미지를 커맨드라인으로 관리할 수 있는 도구를 제공합니다.</li>
    </ul>
  </li>
</ul>

<h4 id="docker-hub에-dev-app-private-repo-생성">Docker Hub에 dev-app (private) repo 생성</h4>

<ul>
  <li>Docker Hub에 dev-app이라는 private 리포지토리를 생성하겠습니다.</li>
  <li>Docker Hub에 로그인 후 Repositories &gt; Create Repository를 클릭하여 리포지토리를 생성합니다.
    <ul>
      <li><strong>Repository Name</strong> : dev-app</li>
      <li><strong>Visibility</strong> : Private</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/2024/cicd-lite/w1/20241205_cicd_lite_w1_14.png" alt="img.png" /></p>

<h3 id="jenkins-기본-사용">Jenkins 기본 사용</h3>

<ul>
  <li>Jenkins를 사용하여 CI/CD 파이프라인을 구축해보겠습니다.</li>
  <li>작업 소개
    <ol>
      <li><strong>Trigger</strong> : 작업을 수행하는 시점을 지정합니다.
        <ul>
          <li>작업 수행 태스크 task가 언제 시작될지를 지시할 수 있습니다.</li>
        </ul>
      </li>
      <li><strong>Built step</strong> : 작업을 구성하는 단계별 태스크를 지정합니다.
        <ul>
          <li>특정 목표를 수행하기 위한 태스크를 단계별 step로 구성할 수 있습니다.</li>
          <li>이것을 젠킨스에서는 빌드 스텝 build step이라고 부릅니다.</li>
        </ul>
      </li>
      <li><strong>Post-build action</strong> : 태스크가 완료 후 수행할 명령을 지정합니다.
        <ul>
          <li>예를 들어 작업의 결과(성공 or 실패)를 사용자에게 알려주는 후속 동작이나, 자바 코드를 컴파일한 후 생성된 클래스 파일을 특정 위치로 복사 등의 작업을 수행할 수 있습니다.
     - (참고) 젠킨스의 <strong>빌드</strong> : 젠킨스 작업의 특정 실행 버전
       - 사용자는 젠킨스 작업을 여러번 실행할 수 있는데, 실행될 때마다 <strong>고유 빌드 번호</strong>가 부여됩니다.
       - 작업 실행 중에 생성된 <strong>아티팩트</strong>, <strong>콘솔 로드</strong> 등 특정 실행 버전과 관련된 모든 세부 정보가 해당 빌드 번호로 저장됩니다.</li>
        </ul>
      </li>
    </ol>
  </li>
  <li>첫번째 작업 생성
    <ul>
      <li>name : first</li>
      <li>item type : freestyle project</li>
      <li>Build Steps : Execute shell
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">echo</span> <span class="s2">"docker check"</span> | <span class="nb">tee </span>test.txt
docker ps
</code></pre></div>        </div>
        <p><img src="/assets/2024/cicd-lite/w1/20241205_cicd_lite_w1_15.png" alt="img.png" class="image-center" />
<em class="image-caption">Jenkins 첫번째 작업 생성</em></p>
      </li>
    </ul>
  </li>
  <li>
    <p>“Build Now”(지금 실행) 메뉴를 클릭하여 작업을 실행합니다.
<img src="/assets/2024/cicd-lite/w1/20241205_cicd_lite_w1_16.png" alt="img.png" class="image-center" />
<em class="image-caption">빌드 결과 (Console Output)</em></p>
  </li>
  <li>작업 공간 확인</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>docker compose <span class="nb">exec </span>jenkins <span class="nb">ls</span> /var/jenkins_home/workspace
<span class="c"># =&gt; first</span>

<span class="nv">$ </span>docker compose <span class="nb">exec </span>jenkins <span class="nb">ls</span> /var/jenkins_home/workspace/first
<span class="c"># =&gt; test.txt</span>

<span class="c"># 작업 결과 확인</span>
<span class="nv">$ </span>docker compose <span class="nb">exec </span>jenkins <span class="nb">cat</span> /var/jenkins_home/workspace/first/test.txt
<span class="c"># =&gt; docker check</span>
</code></pre></div></div>

<h4 id="jenkins-플러그인-설치">Jenkins 플러그인 설치</h4>

<ul>
  <li>Jenkins 플러그인을 설치하여 더 다양한 기능을 사용할 수 있습니다.</li>
  <li>Dashboard &gt; Manage Jenkins 메뉴를 클릭하고, 플러그인 관리를 클릭합니다.</li>
  <li>Available plugins 를 클릭하여 다양한 플러그인을 설치할 수 있습니다.</li>
</ul>

<p><img src="/assets/2024/cicd-lite/w1/20241205_cicd_lite_w1_17.png" alt="img.png" class="image-center" />
<em class="image-caption">Jenkins 플러그인 설치 화면</em></p>

<ul>
  <li>다음의 plugin 을 설치합니다.
    <ul>
      <li><strong>Pipeline Stage View</strong> : 파이프라인 스테이지를 시각적으로 보여주는 플러그인 <a href="https://plugins.jenkins.io/pipeline-stage-view/">링크</a></li>
      <li><strong>Docker Pipeline</strong> : 파이프라인에서 도커를 사용할 수 있도록 지원하는 플러그인 <a href="https://plugins.jenkins.io/docker-workflow/">링크</a></li>
      <li><strong>Gitlab</strong> : Gitlab과 Jenkins를 연동할 수 있도록 지원하는 플러그인 <a href="https://plugins.jenkins.io/gitlab-plugin/">링크</a></li>
    </ul>
  </li>
  <li>Dashboard &gt; Manage Jenkins &gt; Credentials &gt; System &gt; Global credentials (unrestricted) &gt; Add Credentials 를 클릭하여 자격증명을 추가합니다.
    <ul>
      <li>Docker hub 크레덴셜
        <ul>
          <li>Kind : Username with password</li>
          <li>Username : <code class="language-plaintext highlighter-rouge">&lt;docker hub 계정&gt;</code></li>
          <li>Password : <code class="language-plaintext highlighter-rouge">&lt;docker hub 비밀번호 혹은 토큰&gt;</code></li>
          <li>ID : <code class="language-plaintext highlighter-rouge">dockerhub-credentials</code> =&gt; 자격증명 이름으로 pipeline에서 사용됩니다.</li>
        </ul>
      </li>
      <li>Gitlab 저장소 크레덴셜
        <ul>
          <li>Kind : Username with password</li>
          <li>Username : <code class="language-plaintext highlighter-rouge">&lt;gitlab 토큰 이름&gt;</code></li>
          <li>Password : <code class="language-plaintext highlighter-rouge">&lt;gitlab 토큰&gt;</code></li>
          <li>ID : <code class="language-plaintext highlighter-rouge">gitlab-credentials</code> =&gt; 자격증명 이름으로 pipeline에서 사용됩니다.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h4 id="파이프라인">파이프라인</h4>

<ul>
  <li>pipeline은 CI/CD 파이프라인을 코드로 정의하는 플러그인 스크립트입니다. <a href="https://www.jenkins.io/doc/book/pipeline/">docs</a></li>
</ul>

<p><img src="/assets/2024/cicd-lite/w1/20241205_cicd_lite_w1_18.png" alt="img.png" /></p>

<ul>
  <li>파이프라인의 <strong>장점</strong>
    <ul>
      <li><strong>코드</strong> : 애플리케이션 CI/CD 프로세스를 코드 형식으로 작성할 수 있고, 해당 코드를 중앙 리포지터리에 저장하여 팀원과 공유 및 작업 가능합니다.</li>
      <li><strong>내구성</strong> : 젠킨스 서비스가 의도적으로 또는 우발적으로 재시작되더라도 문제없이 유지됩니다.</li>
      <li><strong>일시 중지 가능</strong> : 파이프라인을 실행하는 도중 사람의 승인이나 입력을 기다리기 위해 중단하거나 기다리는 것이 가능합니다.</li>
      <li><strong>다양성</strong> : 분기나 반복, 병렬 처리와 같은 다양한 CI/CD 요구 사항을 지원합니다.</li>
    </ul>
  </li>
  <li>파이프라인 용어
<img src="/assets/2024/cicd-lite/w1/20241205_cicd_lite_w1_19.png" alt="img.png" class="image-center" />
    <ul>
      <li><strong>Pipeline(파이프라인)</strong> : 전체 빌드 프로세스를 정의하는 코드</li>
      <li><strong>Node(노드) = Agent</strong> : 파이프라인을 실행하는 시스템</li>
      <li><strong>Stages</strong> : 순차 작업 명세인 stage 들의 묶음</li>
      <li><strong>Stage</strong> : 특정 단계에서 수행되는 작업들의 정의</li>
      <li><strong>Steps</strong> : 파이프라인의 특정 단계에서 수행되는 단일 작업을 의미.</li>
      <li><strong>Post</strong> : 빌드 후 조치, 일반적으로 stages 작업이 끝난 후 추가적인 steps/step</li>
      <li><strong>Directive</strong> - <a href="https://www.jenkins.io/doc/book/pipeline/syntax/#declarative-directives">Docs</a>
        <ul>
          <li><strong>Environment</strong> (key=value) : 파이프라인 내부에서 사용할 환경변수</li>
          <li><strong>Parameters</strong> : 입력 받아야할 변수를 정의 - Type(string, text, choice, password …)</li>
          <li><strong>Triggers</strong> : 파이프라인을 실행하는 조건 설정</li>
          <li><strong>Input</strong> : 파이프라인 실행 중 사용자 입력을 받을 수 있도록 설정</li>
          <li><strong>When</strong> : stage 를 실행 할 조건 설정</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>파이프라인 구성 형태 3가지
    <ol>
      <li><strong>Pipeline Script</strong> : 일반적인 방식으로 Jenkins 파이프라인을 생성하여 Shell Script 형태로 작성 <a href="https://www.jenkins.io/doc/book/pipeline/getting-started/#through-the-classic-ui">링크</a></li>
      <li><strong>Pipeline Script from SCM</strong> : Jenkinsfile을 git 등의 SCM(Source Code Management)에 저장하고, 빌드 시작 시 해당 파일을 읽어 파이프라인을 실행 <a href="https://www.jenkins.io/doc/book/pipeline/getting-started/#defining-a-pipeline-in-scm">링크</a></li>
      <li><strong>Blue Ocean 기반</strong> : Blue Ocean 플러그인을 설치하여 UI로 파이프라인을 구성하면 Jenkinsfile이 자동으로 생성됨 <a href="https://www.jenkins.io/doc/book/pipeline/getting-started/#through-blue-ocean">링크</a></li>
    </ol>
  </li>
  <li>파이프라인 구문 형태 2가지
<img src="/assets/2024/cicd-lite/w1/20241205_cicd_lite_w1_20.png" alt="img.png" class="image-center w-80" />
<em class="image-caption">파이프라인 구문 형태별 구조</em>
    <ol>
      <li><strong>Declarative Pipeline</strong> : 간결하고 가독성이 좋으며, 최근 문법이고, 권장하는 방법. step은 필수로 사용
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pipeline <span class="o">{</span>
   agent any     <span class="c"># Execute this Pipeline or any of its stages, on any available agent.</span>
   stages <span class="o">{</span>
     stage<span class="o">(</span><span class="s1">'Build'</span><span class="o">)</span> <span class="o">{</span>   <span class="c"># Defines the "Build" stage.</span>
         steps <span class="o">{</span>
             //         <span class="c"># Perform some steps related to the "Build" stage.</span>
         <span class="o">}</span>
     <span class="o">}</span>
     stage<span class="o">(</span><span class="s1">'Test'</span><span class="o">)</span> <span class="o">{</span> 
         steps <span class="o">{</span>
             // 
         <span class="o">}</span>
     <span class="o">}</span>
     stage<span class="o">(</span><span class="s1">'Deploy'</span><span class="o">)</span> <span class="o">{</span> 
         steps <span class="o">{</span>
             // 
         <span class="o">}</span>
     <span class="o">}</span>
   <span class="o">}</span>
<span class="o">}</span>
</code></pre></div>        </div>
      </li>
      <li><strong>Scripted Pipeline</strong> : 커스텀이 용이하나 복잡도가 높고, step은 필수가 아님
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>node <span class="o">{</span>             <span class="c"># Execute this Pipeline or any of its stages, on any available agent.</span>
  stage<span class="o">(</span><span class="s1">'Build'</span><span class="o">)</span> <span class="o">{</span> <span class="c"># Defines the "Build" stage. stage blocks are optional in Scripted Pipeline syntax. However, implementing stage blocks in a Scripted Pipeline provides clearer visualization of each stage's subset of tasks/steps in the Jenkins UI.</span>
   //              <span class="c"># Perform some steps related to the "Build" stage.</span>
  <span class="o">}</span>
  stage<span class="o">(</span><span class="s1">'Test'</span><span class="o">)</span> <span class="o">{</span> 
   // 
  <span class="o">}</span>
  stage<span class="o">(</span><span class="s1">'Deploy'</span><span class="o">)</span> <span class="o">{</span> 
   // 
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div>        </div>
      </li>
    </ol>
  </li>
</ul>

<h5 id="jenkins-pipeline-실습">Jenkins Pipeline 실습</h5>

<ul>
  <li>New Item &gt; Pipeline 으로 파이프라인을 생성합니다.
    <ul>
      <li><strong>Name</strong> : First-Pipeline</li>
      <li><strong>Definition</strong> : Pipeline script</li>
      <li><strong>Script</strong> : 아래의 파이프라인 스크립트를 입력합니다.</li>
    </ul>
  </li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pipeline <span class="o">{</span>
    agent any

    stages <span class="o">{</span>
        stage<span class="o">(</span><span class="s1">'Hello'</span><span class="o">)</span> <span class="o">{</span>
            steps <span class="o">{</span>
                <span class="nb">echo</span> <span class="s1">'Hello World'</span>
            <span class="o">}</span>
        <span class="o">}</span>
        stage<span class="o">(</span><span class="s1">'Deploy'</span><span class="o">)</span> <span class="o">{</span>
            steps <span class="o">{</span>
                <span class="nb">echo</span> <span class="s2">"Deployed successfully!"</span><span class="p">;</span>
            <span class="o">}</span>
        <span class="o">}</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div></div>

<ul>
  <li>
    <p>Save 하여 저장후 “Build Now”를 클릭하여 파이프라인을 실행합니다.
<img src="/assets/2024/cicd-lite/w1/20241205_cicd_lite_w1_21.png" alt="img.png" class="image-center" />
<em class="image-caption">파이프라인 실행 결과</em></p>
  </li>
  <li>
    <p>아래 처럼 수정 후 확인: 환경변수 사용, 문자열 보간 → Console Output 확인</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pipeline <span class="o">{</span>
    agent any
    environment <span class="o">{</span> 
        CC <span class="o">=</span> <span class="s1">'clang'</span>
    <span class="o">}</span>
      
    stages <span class="o">{</span>
        stage<span class="o">(</span><span class="s1">'Example'</span><span class="o">)</span> <span class="o">{</span>
            environment <span class="o">{</span> 
                AN_ACCESS_KEY <span class="o">=</span> <span class="s1">'abcdefg'</span>
            <span class="o">}</span>
            steps <span class="o">{</span>
                <span class="nb">echo</span> <span class="s2">"</span><span class="k">${</span><span class="nv">CC</span><span class="k">}</span><span class="s2">"</span><span class="p">;</span>
                sh <span class="s1">'echo ${AN_ACCESS_KEY}'</span>
            <span class="o">}</span>
        <span class="o">}</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div>    </div>

    <p><img src="/assets/2024/cicd-lite/w1/20241205_cicd_lite_w1_22.png" alt="img.png" class="image-center" />
<em class="image-caption">Console Output</em></p>
  </li>
  <li>
    <p>아래 처럼 수정 후 확인: 파이프라인 빌드 시작(트리거) → Console Output 확인</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pipeline <span class="o">{</span>
    agent any
    triggers <span class="o">{</span>
        cron<span class="o">(</span><span class="s1">'H */4 * * 1-5'</span><span class="o">)</span>
    <span class="o">}</span>
    stages <span class="o">{</span>
        stage<span class="o">(</span><span class="s1">'Example'</span><span class="o">)</span> <span class="o">{</span>
            steps <span class="o">{</span>
                <span class="nb">echo</span> <span class="s1">'Hello World'</span>
            <span class="o">}</span>
        <span class="o">}</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div>    </div>

    <p><img src="/assets/2024/cicd-lite/w1/20241205_cicd_lite_w1_23.png" alt="img.png" class="image-center" />
<em class="image-caption">Console Output</em></p>
  </li>
  <li>
    <p>아래 처럼 수정 후 확인: 파라미터와 함께 빌드 → Console Output 확인 ⇒ 다시 한번 더 빌드 클릭 (변수 입력 칸 확인)</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pipeline <span class="o">{</span>
    agent any
    parameters <span class="o">{</span>
        string<span class="o">(</span>name: <span class="s1">'PERSON'</span>, defaultValue: <span class="s1">'Mr Jenkins'</span>, description: <span class="s1">'Who should I say hello to?'</span><span class="o">)</span>
        text<span class="o">(</span>name: <span class="s1">'BIOGRAPHY'</span>, defaultValue: <span class="s1">''</span>, description: <span class="s1">'Enter some information about the person'</span><span class="o">)</span>
        booleanParam<span class="o">(</span>name: <span class="s1">'TOGGLE'</span>, defaultValue: <span class="nb">true</span>, description: <span class="s1">'Toggle this value'</span><span class="o">)</span>
        choice<span class="o">(</span>name: <span class="s1">'CHOICE'</span>, choices: <span class="o">[</span><span class="s1">'One'</span>, <span class="s1">'Two'</span>, <span class="s1">'Three'</span><span class="o">]</span>, description: <span class="s1">'Pick something'</span><span class="o">)</span>
        password<span class="o">(</span>name: <span class="s1">'PASSWORD'</span>, defaultValue: <span class="s1">'SECRET'</span>, description: <span class="s1">'Enter a password'</span><span class="o">)</span>
    <span class="o">}</span>
    stages <span class="o">{</span>
        stage<span class="o">(</span><span class="s1">'Example'</span><span class="o">)</span> <span class="o">{</span>
            steps <span class="o">{</span>
                <span class="nb">echo</span> <span class="s2">"Hello </span><span class="k">${</span><span class="nv">params</span><span class="p">.PERSON</span><span class="k">}</span><span class="s2">"</span>
                <span class="nb">echo</span> <span class="s2">"Biography: </span><span class="k">${</span><span class="nv">params</span><span class="p">.BIOGRAPHY</span><span class="k">}</span><span class="s2">"</span>
                <span class="nb">echo</span> <span class="s2">"Toggle: </span><span class="k">${</span><span class="nv">params</span><span class="p">.TOGGLE</span><span class="k">}</span><span class="s2">"</span>
                <span class="nb">echo</span> <span class="s2">"Choice: </span><span class="k">${</span><span class="nv">params</span><span class="p">.CHOICE</span><span class="k">}</span><span class="s2">"</span>
                <span class="nb">echo</span> <span class="s2">"Password: </span><span class="k">${</span><span class="nv">params</span><span class="p">.PASSWORD</span><span class="k">}</span><span class="s2">"</span>
            <span class="o">}</span>
        <span class="o">}</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>파이프라인 스크립트를 수정하고 저장 후 “Build with Parameters”를 클릭하여 파라미터를 입력하고 빌드를 실행하면
아래와 같이 지정된 파라미터로 빌드를 실행할 수 있습니다.</p>
  </li>
</ul>

<p><img src="/assets/2024/cicd-lite/w1/20241205_cicd_lite_w1_24.png" alt="img.png" class="image-center" />
<em class="image-caption">Build with Parameters 화면</em></p>

<ul>
  <li>
    <p>아래처럼 post (빌드 후 조치) 블록을 추가하여 빌드 후 조치를 설정할 수 있습니다.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pipeline <span class="o">{</span>
    agent any
    stages <span class="o">{</span>
        stage<span class="o">(</span><span class="s1">'Compile'</span><span class="o">)</span> <span class="o">{</span>
            steps <span class="o">{</span>
                <span class="nb">echo</span> <span class="s2">"Compiled successfully!"</span><span class="p">;</span>
            <span class="o">}</span>
        <span class="o">}</span>
  
        stage<span class="o">(</span><span class="s1">'JUnit'</span><span class="o">)</span> <span class="o">{</span>
            steps <span class="o">{</span>
                <span class="nb">echo</span> <span class="s2">"JUnit passed successfully!"</span><span class="p">;</span>
            <span class="o">}</span>
        <span class="o">}</span>
  
        stage<span class="o">(</span><span class="s1">'Code Analysis'</span><span class="o">)</span> <span class="o">{</span>
            steps <span class="o">{</span>
                <span class="nb">echo</span> <span class="s2">"Code Analysis completed successfully!"</span><span class="p">;</span>
            <span class="o">}</span>
        <span class="o">}</span>
  
        stage<span class="o">(</span><span class="s1">'Deploy'</span><span class="o">)</span> <span class="o">{</span>
            steps <span class="o">{</span>
                <span class="nb">echo</span> <span class="s2">"Deployed successfully!"</span><span class="p">;</span>
            <span class="o">}</span>
        <span class="o">}</span>
    <span class="o">}</span>
    post <span class="o">{</span> 
        always <span class="o">{</span> 
            <span class="nb">echo</span> <span class="s1">'I will always say Hello again!'</span>
        <span class="o">}</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div>    </div>

    <p><img src="/assets/2024/cicd-lite/w1/20241205_cicd_lite_w1_25.png" alt="img.png" class="image-center w-80" />
<em class="image-caption">step 별 메시지와 post 메시지</em></p>

    <ul>
      <li>post에는 always 외에도 다음과 같은 옵션을 사용할 수 있습니다.
        <ul>
          <li><strong>always</strong> : 항상 실행</li>
          <li><strong>changed</strong> : 성공 또는 실패가 변경되었을 때 실행</li>
          <li><strong>success</strong> : 성공했을 때 실행</li>
          <li><strong>failure</strong> : 실패했을 때 실행</li>
          <li><strong>unstable</strong> : 불안정한 상태일 때 실행</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Pipeline Syntax -&gt; Snippet Generator 를 사용하여 파이프라인 스크립트를 생성할 수 있습니다.
<img src="/assets/2024/cicd-lite/w1/20241205_cicd_lite_w1_26.png" alt="img.png" class="image-center w-80" /></p>
  </li>
</ul>

<h5 id="gitlab과-jenkins-pipeline-연동-실습">Gitlab과 Jenkins pipeline 연동 실습</h5>

<ul>
  <li>Gitlab에서 소스를 받아 빌드 후 Docker Hub에 이미지를 업로드하는 파이프라인을 구성해보겠습니다.</li>
  <li>Pipeline script</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pipeline <span class="o">{</span>
    agent any
    environment <span class="o">{</span>
        DOCKER_IMAGE <span class="o">=</span> <span class="s1">'sweetlittlebird/dev-app'</span> // Docker 이미지 이름
    <span class="o">}</span>
    stages <span class="o">{</span>
        stage<span class="o">(</span><span class="s1">'Checkout'</span><span class="o">)</span> <span class="o">{</span>
            steps <span class="o">{</span>
                 git branch: <span class="s1">'main'</span>,
                 url: <span class="s1">'https://gitlab.com/littlebird/2024-cicd-lite-w1.git'</span>,  // Git에서 코드 체크아웃
                 credentialsId: <span class="s1">'gitlab-credentials'</span>  // Credentials ID
            <span class="o">}</span>
        <span class="o">}</span>
        stage<span class="o">(</span><span class="s1">'Read VERSION'</span><span class="o">)</span> <span class="o">{</span>
            steps <span class="o">{</span>
                script <span class="o">{</span>
                    // VERSION 파일 읽기
                    def version <span class="o">=</span> readFile<span class="o">(</span><span class="s1">'VERSION'</span><span class="o">)</span>.trim<span class="o">()</span>
                    <span class="nb">echo</span> <span class="s2">"Version found: </span><span class="k">${</span><span class="nv">version</span><span class="k">}</span><span class="s2">"</span>
                    // 환경 변수 설정
                    env.DOCKER_TAG <span class="o">=</span> version
                <span class="o">}</span>
            <span class="o">}</span>
        <span class="o">}</span>
        stage<span class="o">(</span><span class="s1">'Docker Build and Push'</span><span class="o">)</span> <span class="o">{</span>
            steps <span class="o">{</span>
                script <span class="o">{</span>
                    docker.withRegistry<span class="o">(</span><span class="s1">'https://index.docker.io/v1/'</span>, <span class="s1">'dockerhub-credentials'</span><span class="o">)</span> <span class="o">{</span>
                        // DOCKER_TAG 사용
                        def appImage <span class="o">=</span> docker.build<span class="o">(</span><span class="s2">"</span><span class="k">${</span><span class="nv">DOCKER_IMAGE</span><span class="k">}</span><span class="s2">:</span><span class="k">${</span><span class="nv">DOCKER_TAG</span><span class="k">}</span><span class="s2">"</span><span class="o">)</span>
                        appImage.push<span class="o">()</span>
                    <span class="o">}</span>
                <span class="o">}</span>
            <span class="o">}</span>
        <span class="o">}</span>
    <span class="o">}</span>
    post <span class="o">{</span>
        success <span class="o">{</span>
            <span class="nb">echo</span> <span class="s2">"Docker image </span><span class="k">${</span><span class="nv">DOCKER_IMAGE</span><span class="k">}</span><span class="s2">:</span><span class="k">${</span><span class="nv">DOCKER_TAG</span><span class="k">}</span><span class="s2"> has been built and pushed successfully!"</span>
        <span class="o">}</span>
        failure <span class="o">{</span>
            <span class="nb">echo</span> <span class="s2">"Pipeline failed. Please check the logs."</span>
        <span class="o">}</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div></div>

<ul>
  <li>
    <p>Build Now -&gt; Console Output 확인</p>

    <p><img src="/assets/2024/cicd-lite/w1/20241205_cicd_lite_w1_27.png" alt="img.png" class="image-center w-80" /></p>
  </li>
  <li>
    <p>Docker Hub에서 이미지 확인</p>

    <p><img src="/assets/2024/cicd-lite/w1/20241205_cicd_lite_w1_28.png" alt="img.png" class="image-center" /></p>
  </li>
</ul>

<h4 id="도커-기반-어플리케이션의-cicd-구성">도커 기반 어플리케이션의 CI/CD 구성</h4>

<ul>
  <li>Jenkins와 Gitlab을 사용하여 다음 그림과 같은 형태의 도커 기반 어플리케이션의 CI/CD 파이프라인을 구성해보겠습니다.</li>
</ul>

<p><img src="/assets/2024/cicd-lite/w1/20241205_cicd_lite_w1_29.png" alt="img.png" class="image-center w-80" />
<em class="image-caption">목표 CI/CD 파이프라인</em></p>

<h5 id="gitlab에서-jenkins-연동-설정">Gitlab에서 Jenkins 연동 설정</h5>

<ul>
  <li>gitlab 프로젝트 페이지 &gt; Settings &gt; Integrations &gt; Jenkins 연결후 아래의 정보를 입력 합니다.
    <ul>
      <li><strong>Enable integration</strong> : Active 체크</li>
      <li><strong>Trigger</strong> : Push, Merge request 체크</li>
      <li><strong>URL</strong> : <code class="language-plaintext highlighter-rouge">http://&lt;Jenkins접속주소&gt;:&lt;Jenkins포트&gt;</code></li>
      <li><strong>Project name</strong> : <code class="language-plaintext highlighter-rouge">SCM-Pipeline</code> (Jenkins 프로젝트 이름)</li>
      <li><strong>Username</strong> : <code class="language-plaintext highlighter-rouge">&lt;Jenkins 아이디&gt;</code></li>
      <li><strong>Password</strong> : <code class="language-plaintext highlighter-rouge">&lt;Jenkins 비밀번호&gt;</code></li>
    </ul>

    <p><img src="/assets/2024/cicd-lite/w1/20241205_cicd_lite_w1_30.png" alt="img.png" class="image-center" /></p>
  </li>
</ul>

<h5 id="jenkins에서-gitlab-연동-설정">Jenkins에서 Gitlab 연동 설정</h5>

<ul>
  <li>Jenkins Item 생성
    <ul>
      <li>Dashboard &gt; New Item &gt; Pipeline (item name : <code class="language-plaintext highlighter-rouge">SCM-Pipeline</code>)</li>
    </ul>
  </li>
  <li>Build Triggers 설정
    <ul>
      <li>Configuration &gt; Build Triggers
        <ul>
          <li><strong>Build when a change is pushed to GitLab</strong> 체크</li>
          <li><strong>Push Events</strong> 체크</li>
          <li><strong>Accepted Merge Request Events</strong> 체크
<img src="/assets/2024/cicd-lite/w1/20241205_cicd_lite_w1_31.png" alt="img.png" /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Jenkins 파일 생성 후 push</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cat</span> <span class="o">&gt;</span> Jenkinsfile <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
pipeline {
    agent any
    environment {
        DOCKER_IMAGE = 'sweetlittlebird/dev-app' // Docker 이미지 이름
    }
    stages {
        stage('Checkout') {
            steps {
                 git branch: 'main',
                 url: 'https://gitlab.com/littlebird/2024-cicd-lite-w1.git',  // Git에서 코드 체크아웃
                 credentialsId: 'gitlab-credentials'  // Credentials ID
            }
        }
        stage('Read VERSION') {
            steps {
                script {
                    // VERSION 파일 읽기
                    def version = readFile('VERSION').trim()
                    echo "Version found: </span><span class="se">\$</span><span class="sh">{version}"
                    // 환경 변수 설정
                    env.DOCKER_TAG = version
                }
            }
        }
        stage('Docker Build and Push') {
            steps {
                script {
                    docker.withRegistry('https://index.docker.io/v1/', 'dockerhub-credentials') {
                        // DOCKER_TAG 사용
                        def appImage = docker.build("</span><span class="se">\$</span><span class="sh">{DOCKER_IMAGE}:</span><span class="se">\$</span><span class="sh">{DOCKER_TAG}")
                        appImage.push()
                    }
                }
            }
        }
    }
    post {
        success {
            echo "Docker image </span><span class="se">\$</span><span class="sh">{DOCKER_IMAGE}:</span><span class="se">\$</span><span class="sh">{DOCKER_TAG} has been built and pushed successfully!"
        }
        failure {
            echo "Pipeline failed. Please check the logs."
        }
    }
}
</span><span class="no">EOF
  
</span><span class="c"># 버전 업데이트</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> &gt; VERSION
0.0.2
</span><span class="no">EOF
  
</span><span class="nv">$ </span>git add <span class="nb">.</span>
<span class="nv">$ </span>git commit <span class="nt">-m</span> <span class="s2">"Add Jenkinsfile"</span>
<span class="c"># =&gt; [main e5671f2] Add Jenkinsfile</span>
<span class="c">#     1 file changed, 45 insertions(+)</span>
<span class="c">#     create mode 100644 Jenkinsfile</span>
<span class="nv">$ </span>git push <span class="nt">-u</span> origin main
<span class="c"># =&gt; Enumerating objects: 4, done.</span>
<span class="c">#    Counting objects: 100% (4/4), done.</span>
<span class="c">#    Delta compression using up to 8 threads</span>
<span class="c">#    Compressing objects: 100% (3/3), done.</span>
<span class="c">#    Writing objects: 100% (3/3), 859 bytes | 859.00 KiB/s, done.</span>
<span class="c">#    Total 3 (delta 1), reused 0 (delta 0), pack-reused 0</span>
<span class="c">#    To https://gitlab.com/littlebird/2024-cicd-lite-w1.git</span>
<span class="c">#       41a6efc..e5671f2  main -&amp;gt; main</span>
<span class="c">#    branch 'main' set up to track 'origin/main'.</span>
</code></pre></div>    </div>
  </li>
  <li>Jenkins 트리거 빌드 확인
<img src="/assets/2024/cicd-lite/w1/20241205_cicd_lite_w1_32.png" alt="img.png" />
    <ul>
      <li>git push에 의해 자동으로 빌드가 잘 되었습니다.</li>
    </ul>
  </li>
  <li>Docker 저장소 확인
<img src="/assets/2024/cicd-lite/w1/20241205_cicd_lite_w1_33.png" alt="img.png" />
    <ul>
      <li>Docker Hub에 수정된 0.0.2 버전의 이미지가 업로드 되었습니다.</li>
    </ul>
  </li>
  <li>
    <p>Gitlab Webhook 기록 확인
<img src="/assets/2024/cicd-lite/w1/20241205_cicd_lite_w1_34.png" alt="img.png" class="image-center" /></p>
  </li>
  <li>
    <p>app.rb 소스와 VERSION 변경 후 Jenkins 트리거 작업 한번 더 확인</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># app.rb 수정</span>
<span class="nv">$ </span><span class="nb">sed</span> <span class="nt">-i</span> <span class="nt">-e</span> <span class="s1">'s/Hello, World!/Hello, Jenkins! 😀/'</span> app.rb
  
<span class="c"># VERSION 수정</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"0.0.3"</span> <span class="o">&gt;</span> VERSION
  
<span class="nv">$ </span>git add <span class="nb">.</span> <span class="o">&amp;&amp;</span> git commit <span class="nt">-m</span> <span class="s2">"Update app.rb and VERSION </span><span class="si">$(</span><span class="nb">cat </span>VERSION<span class="si">)</span><span class="s2">"</span> <span class="o">&amp;&amp;</span> git push <span class="nt">-u</span> origin main
<span class="c"># =&gt; [main a100e97] Update app.rb and VERSION 0.0.3</span>
<span class="c">#     3 files changed, 7 insertions(+), 2 deletions(-)</span>
<span class="c">#     create mode 100644 app.rb-e</span>
<span class="c">#    Enumerating objects: 7, done.</span>
<span class="c">#    Counting objects: 100% (7/7), done.</span>
<span class="c">#    Delta compression using up to 8 threads</span>
<span class="c">#    Compressing objects: 100% (3/3), done.</span>
<span class="c">#    Writing objects: 100% (4/4), 509 bytes | 509.00 KiB/s, done.</span>
<span class="c">#    Total 4 (delta 0), reused 0 (delta 0), pack-reused 0</span>
<span class="c">#    To https://gitlab.com/littlebird/2024-cicd-lite-w1.git</span>
<span class="c">#       1bd4fcb..a100e97  main -&amp;gt; main</span>
<span class="c">#    branch 'main' set up to track 'origin/main'.</span>
</code></pre></div>    </div>

    <p><img src="/assets/2024/cicd-lite/w1/20241205_cicd_lite_w1_35.png" alt="img.png" /></p>
    <ul>
      <li>빌드가 잘 되고 Docker Hub에 이미지가 업로드 되었습니다.</li>
    </ul>
  </li>
</ul>

<h5 id="jenkins-빌드-후-컨테이너-실행">Jenkins 빌드 후 컨테이너 실행</h5>

<ul>
  <li>Jenkins pipline 빌드 후 Docker 컨테이너를 실행하는 파이프라인을 구성해보겠습니다.</li>
  <li>
    <p>Jenkinsfile 수정</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pipeline <span class="o">{</span>
    agent any
    environment <span class="o">{</span>
        DOCKER_IMAGE <span class="o">=</span> <span class="s1">'sweetlittlebird/dev-app'</span> // Docker 이미지 이름
        CONTAINER_NAME <span class="o">=</span> <span class="s1">'dev-app'</span>  // Docker 컨테이너 이름
    <span class="o">}</span>
    stages <span class="o">{</span>
        stage<span class="o">(</span><span class="s1">'Checkout'</span><span class="o">)</span> <span class="o">{</span>
            steps <span class="o">{</span>
                git branch: <span class="s1">'main'</span>,
                url: <span class="s1">'https://gitlab.com/littlebird/2024-cicd-lite-w1.git'</span>,  // Git에서 코드 체크아웃
                credentialsId: <span class="s1">'gitlab-credentials'</span>  // Credentials ID
            <span class="o">}</span>
        <span class="o">}</span>
        stage<span class="o">(</span><span class="s1">'Read VERSION'</span><span class="o">)</span> <span class="o">{</span>
            steps <span class="o">{</span>
                script <span class="o">{</span>
                    // VERSION 파일 읽기
                    def version <span class="o">=</span> readFile<span class="o">(</span><span class="s1">'VERSION'</span><span class="o">)</span>.trim<span class="o">()</span>
                    <span class="nb">echo</span> <span class="s2">"Version found: </span><span class="k">${</span><span class="nv">version</span><span class="k">}</span><span class="s2">"</span>
                    // 환경 변수 설정
                    env.DOCKER_TAG <span class="o">=</span> version
                <span class="o">}</span>
            <span class="o">}</span>
        <span class="o">}</span>
        stage<span class="o">(</span><span class="s1">'Docker Build and Push'</span><span class="o">)</span> <span class="o">{</span>
            steps <span class="o">{</span>
                script <span class="o">{</span>
                    docker.withRegistry<span class="o">(</span><span class="s1">'https://index.docker.io/v1/'</span>, <span class="s1">'dockerhub-credentials'</span><span class="o">)</span> <span class="o">{</span>
                        // DOCKER_TAG 사용
                        def appImage <span class="o">=</span> docker.build<span class="o">(</span><span class="s2">"</span><span class="k">${</span><span class="nv">DOCKER_IMAGE</span><span class="k">}</span><span class="s2">:</span><span class="k">${</span><span class="nv">DOCKER_TAG</span><span class="k">}</span><span class="s2">"</span><span class="o">)</span>
                        appImage.push<span class="o">()</span>
                    <span class="o">}</span>
                <span class="o">}</span>
            <span class="o">}</span>
        <span class="o">}</span>
        stage<span class="o">(</span><span class="s1">'Check, Stop and Run Docker Container'</span><span class="o">)</span> <span class="o">{</span>
            steps <span class="o">{</span>
                script <span class="o">{</span>
                    // 실행 중인 컨테이너 확인
                    def isRunning <span class="o">=</span> sh<span class="o">(</span>
                        script: <span class="s2">"docker ps -q -f name=</span><span class="k">${</span><span class="nv">CONTAINER_NAME</span><span class="k">}</span><span class="s2">"</span>,
                        returnStdout: <span class="nb">true</span>
                    <span class="o">)</span>.trim<span class="o">()</span>
                      
                    <span class="k">if</span> <span class="o">(</span>isRunning<span class="o">)</span> <span class="o">{</span>
                        <span class="nb">echo</span> <span class="s2">"Container '</span><span class="k">${</span><span class="nv">CONTAINER_NAME</span><span class="k">}</span><span class="s2">' is already running. Stopping it..."</span>
                        // 실행 중인 컨테이너 중지
                        sh <span class="s2">"docker stop </span><span class="k">${</span><span class="nv">CONTAINER_NAME</span><span class="k">}</span><span class="s2">"</span>
                        // 컨테이너 제거
                        sh <span class="s2">"docker rm </span><span class="k">${</span><span class="nv">CONTAINER_NAME</span><span class="k">}</span><span class="s2">"</span>
                        <span class="nb">echo</span> <span class="s2">"Container '</span><span class="k">${</span><span class="nv">CONTAINER_NAME</span><span class="k">}</span><span class="s2">' stopped and removed."</span>
                    <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
                        <span class="nb">echo</span> <span class="s2">"Container '</span><span class="k">${</span><span class="nv">CONTAINER_NAME</span><span class="k">}</span><span class="s2">' is not running."</span>
                    <span class="o">}</span>
                      
                    // 5초 대기
                    <span class="nb">echo</span> <span class="s2">"Waiting for 5 seconds before starting the new container..."</span>
                    <span class="nb">sleep</span><span class="o">(</span>5<span class="o">)</span>
                      
                    // 신규 컨테이너 실행
                    <span class="nb">echo</span> <span class="s2">"Starting a new container '</span><span class="k">${</span><span class="nv">CONTAINER_NAME</span><span class="k">}</span><span class="s2">'..."</span>
                    sh <span class="s2">"""
                    docker run -d --name </span><span class="k">${</span><span class="nv">CONTAINER_NAME</span><span class="k">}</span><span class="s2"> -p 4000:4567 </span><span class="k">${</span><span class="nv">DOCKER_IMAGE</span><span class="k">}</span><span class="s2">:</span><span class="k">${</span><span class="nv">DOCKER_TAG</span><span class="k">}</span><span class="s2">
                    """</span>
                <span class="o">}</span>
            <span class="o">}</span>
        <span class="o">}</span>        
    <span class="o">}</span>
    post <span class="o">{</span>
        success <span class="o">{</span>
            <span class="nb">echo</span> <span class="s2">"Docker image </span><span class="k">${</span><span class="nv">DOCKER_IMAGE</span><span class="k">}</span><span class="s2">:</span><span class="k">${</span><span class="nv">DOCKER_TAG</span><span class="k">}</span><span class="s2"> has been built and pushed successfully!"</span>
        <span class="o">}</span>
        failure <span class="o">{</span>
            <span class="nb">echo</span> <span class="s2">"Pipeline failed. Please check the logs."</span>
        <span class="o">}</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>git commit 후 push</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">echo </span>0.0.4 <span class="o">&gt;</span> VERSION
  
<span class="nv">$ </span>git add <span class="nb">.</span> <span class="o">&amp;&amp;</span> git commit <span class="nt">-m</span> <span class="s2">"Update Jenkinsfile </span><span class="si">$(</span><span class="nb">cat </span>VERSION<span class="si">)</span><span class="s2">"</span> <span class="o">&amp;&amp;</span> git push <span class="nt">-u</span> origin main
<span class="c"># =&gt; [main 25e5c95] Update Jenkinsfile 0.0.4</span>
<span class="c">#     2 files changed, 2 insertions(+), 1 deletion(-)</span>
<span class="c">#    Enumerating objects: 7, done.</span>
<span class="c">#    Counting objects: 100% (7/7), done.</span>
<span class="c">#    Delta compression using up to 8 threads</span>
<span class="c">#    Compressing objects: 100% (3/3), done.</span>
<span class="c">#    Writing objects: 100% (4/4), 385 bytes | 385.00 KiB/s, done.</span>
<span class="c">#    Total 4 (delta 2), reused 0 (delta 0), pack-reused 0</span>
<span class="c">#    To https://gitlab.com/littlebird/2024-cicd-lite-w1.git</span>
<span class="c">#       26c809d..25e5c95  main -&amp;gt; main</span>
<span class="c">#    branch 'main' set up to track 'origin/main'.</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>생성된 컨테이너 접속 확인</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>docker image <span class="nb">ls</span>
<span class="c"># =&gt; REPOSITORY                                                                   TAG           IMAGE ID       CREATED             SIZE</span>
<span class="c">#    sweetlittlebird/dev-app                                                      0.0.4         2f5f42fa7dd6   9 minutes ago       1.01GB</span>
<span class="c">#    ... </span>
<span class="nv">$ </span>docker ps <span class="nt">--filter</span> <span class="nv">name</span><span class="o">=</span>dev-app
<span class="c"># =&gt; CONTAINER ID   IMAGE                           COMMAND                  CREATED         STATUS         PORTS                  NAMES</span>
<span class="c">#    e5d4760ea725   sweetlittlebird/dev-app:0.0.4   &amp;quot;ruby app.rb -o 0.0.…&amp;quot;   2 minutes ago   Up 2 minutes   0.0.0.0:4000-&amp;gt;80/tcp   dev-app</span>
<span class="nv">$ </span>curl http://localhost:4000
<span class="c"># =&gt; Hello, Jenkins! 😀 The time is 2024-10-01 15:49:44 +0000!!</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>app.rb와 VERSION 수정 후 push 후 컨테이너 접속 후 반영 확인</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># app.rb 수정</span>
<span class="nv">$ </span><span class="nb">sed</span> <span class="nt">-i</span> <span class="nt">-e</span> <span class="s1">'s/Hello, Jenkins! 😀/Hello, Jenkins again!!! 😎/'</span> app.rb
  
<span class="c"># VERSION 수정</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"0.0.5"</span> <span class="o">&gt;</span> VERSION
  
<span class="nv">$ </span>git add <span class="nb">.</span> <span class="o">&amp;&amp;</span> git commit <span class="nt">-m</span> <span class="s2">"Update app.rb and VERSION </span><span class="si">$(</span><span class="nb">cat </span>VERSION<span class="si">)</span><span class="s2">"</span> <span class="o">&amp;&amp;</span> git push <span class="nt">-u</span> origin main
<span class="c"># =&gt; [main 70eaa32] Update app.rb and VERSION 0.0.5</span>
<span class="c">#     3 files changed, 3 insertions(+), 3 deletions(-)</span>
<span class="c">#    Enumerating objects: 7, done.</span>
<span class="c">#    Counting objects: 100% (7/7), done.</span>
<span class="c">#    Delta compression using up to 8 threads</span>
<span class="c">#    Compressing objects: 100% (3/3), done.</span>
<span class="c">#    Writing objects: 100% (4/4), 519 bytes | 519.00 KiB/s, done.</span>
<span class="c">#    Total 4 (delta 0), reused 0 (delta 0), pack-reused 0</span>
<span class="c">#    To https://gitlab.com/littlebird/2024-cicd-lite-w1.git</span>
<span class="c">#       cc87e97..70eaa32  main -&amp;gt; main</span>
<span class="c">#    branch 'main' set up to track 'origin/main'.</span>
  
<span class="c"># 호스트 PC에서 반복 접속 실행 : 서비스 중단 시간 체크!</span>
<span class="nv">$ </span><span class="k">while </span><span class="nb">true</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nt">--connect-timeout</span> 1 http://127.0.0.1:4000 <span class="p">;</span> <span class="nb">date</span><span class="p">;</span> <span class="nb">sleep </span>1 <span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; Hello, Jenkins again!!! 😎 The time is 2024-10-01 15:52:25 +0000!!Sun Oct  1 00:52:25 KST 2024</span>
<span class="c">#    Hello, Jenkins again!!! 😎 The time is 2024-10-01 15:52:26 +0000!!Sun Oct  1 00:52:26 KST 2024</span>
<span class="c">#    Sun Oct  1 00:52:27 KST 2024</span>
<span class="c">#    Sun Oct  1 00:52:28 KST 2024</span>
<span class="c">#    Sun Oct  1 00:52:29 KST 2024</span>
<span class="c">#    Sun Oct  1 00:52:30 KST 2024</span>
<span class="c">#    Sun Oct  1 00:52:31 KST 2024</span>
<span class="c">#    Sun Oct  1 00:52:32 KST 2024</span>
<span class="c">#    Hello, Jenkins again!!! 😎 The time is 2024-10-01 15:52:33 +0000!!Sun Oct  1 00:52:33 KST 2024</span>
<span class="c">#    Hello, Jenkins again!!! 😎 The time is 2024-10-01 15:52:34 +0000!!Sun Oct  1 00:52:34 KST 2024</span>
<span class="c">#    Hello, Jenkins again!!! 😎 The time is 2024-10-01 15:52:35 +0000!!Sun Oct  1 00:52:35 KST 2024</span>
</code></pre></div>    </div>
  </li>
  <li>수정사항이 적용은 잘 되었지만 6~7초 가량 서비스가 중단되는 것을 확인할 수 있습니다. 이는 컨테이너 중지 및 재시작 시간이 소요되기 때문입니다.</li>
  <li>이러한 문제를 해결하기 위해 docker swarm이나 kubernetes 등의 컨테이너 오케스트레이션 툴을 사용하여 서비스 중단 없이 배포할 수 있습니다.
이 부분은 다음에 다루도록 하겠습니다.</li>
</ul>

<h2 id="마치며">마치며</h2>

<p>이번 시간에는 Jenkins, Gitlab 등을 사용하여 CI/CD 파이프라인을 구성하는 방법을 알아보았습니다.
다양하게 테스트해보고 싶었는데 생각보다 정리하는데 시간이 많이 소요되어 다양한 예제를 다루지 못한 점이 아쉽습니다.</p>

<p>Jenkins는 예전에 써보고 Teamcity나 Github action을 주로 사용해왔는데, 
다시 사용해보니 Jenkins도 Jenkinsfile과 Pipeline도 지원하고 예전에 비해서 훨씬 좋아진 것 같습니다.
이번 스터디를 통해 Jenkins를 재발견한것 같습니다.
준비해주신 Gasida 님께 감사드립니다.</p>]]></content><author><name></name></author><category term="kans" /><category term="cicd," /><category term="jenkins," /><category term="git," /><category term="linux" /><summary type="html"><![CDATA[Jenkins와 Git, Docker를 이용하여 CI/CD 파이프라인을 구축해보겠습니다.]]></summary></entry><entry><title type="html">[KANS 3기] AWS EKS : VPC CNI</title><link href="https://sweetlittlebird.github.io/posts/2024-11-03-KANS-Study-Week9/" rel="alternate" type="text/html" title="[KANS 3기] AWS EKS : VPC CNI" /><published>2024-11-03T00:00:18+09:00</published><updated>2024-11-03T00:00:18+09:00</updated><id>https://sweetlittlebird.github.io/posts/KANS%20Study%20-%20Week9</id><content type="html" xml:base="https://sweetlittlebird.github.io/posts/2024-11-03-KANS-Study-Week9/"><![CDATA[<h2 id="들어가며">들어가며</h2>

<p>안녕하세요. 이번 주에는 AWS EKS의 VPC CNI에 대해 알아보겠습니다.
KANS 3기 9주차 스터디를 시작하겠습니다.</p>

<hr />

<h2 id="aws-eks--vpc-cni">AWS EKS : VPC CNI</h2>

<h3 id="aws-vpc-cni-소개">AWS VPC CNI 소개</h3>

<ul>
  <li>AWS VPC CNI는 AWS에서 제공하는 CNI(Container Network Interface) 플러그인으로, AWS VPC(Virtual Private Cloud)의 네트워크를 사용하여 파드 간 통신을 지원하는 CNI 플러그인입니다.</li>
  <li>AWS VPC CNI의 특징
    <ul>
      <li>가장 큰 특징은 파드의 IP 네트워크 대역과 노드(인스턴스)의 IP 네트워크 대역이 같아서 직접 통신이 가능하다는 것입니다. - 
<a href="https://github.com/aws/amazon-vpc-cni-k8s">Github</a>, <a href="https://github.com/aws/amazon-vpc-cni-k8s/blob/master/docs/cni-proposal.md">Proposal</a></li>
      <li>또한 VPC와 통합되어 VPC Flow logs나 VPC 라우팅 정책, 보안 그룹(Security Group) 등을 활용할 수 있습니다.</li>
      <li>아래에서 설명할 Warm Pool을 통해 노드의 IP 주소를 재사용하여 파드의 생성과 삭제를 빠르게 할 수 있습니다.</li>
    </ul>
  </li>
  <li>Amazon VPC CNI는 크게 두가지 구성요소로  이루어집니다.
    <ul>
      <li>CNI 바이너리 : 파드간 통신을 활성화 하도록 파드 네트워크를 설정합니다. CNI 바이너리는 노드의 루트 파일 시스템에서 실행되며,
새로운 파드가 추가되거나, 기존 파드가 제거 될때 kubelet에 의해 호출됩니다.</li>
      <li>ipamd : IP 주소를 할당하고 관리하는 데몬으로 다음을 담당 합니다.
        <ul>
          <li>노드에서 ENI(<code class="language-plaintext highlighter-rouge">Elastic Network Interface</code> - EC2에서 사용하는 일종의 가상 랜카드) 관리</li>
          <li>사용가능한 IP 주소들의 웜풀 유지</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>인스턴스가 생성되면 EC2는 기본 서브넷과 연결된 기본 ENI를 생성하고 연결합니다.
호스트 네트워크 모드에서 실행되는 포드는 노드 기본 ENI에 할당된 기본 IP 주소를 
사용하며 호스트와 동일한 네트워크 네임스페이스를 공유합니다.</li>
</ul>

<blockquote>
  <p><strong>웜풀(warm pool)</strong>은
  노드가 프로비저닝 될 때 미리 할당된 IP 주소들의 집합입니다. 
  이것은 노드가 프로비저닝 될 때마다 새로운 IP 주소를 할당하는 것을 방지하고, 노드가 삭제되어도 IP 주소를 재사용할 수 있도록 하여
  조금 더 빠른 파드의 생성과 삭제를 가능하게 합니다.</p>
</blockquote>

<ul>
  <li>EC2 인스턴스별 최대 사용가능한 ENI 갯수는 인스턴스 타입에 따라 다르며, 
인스턴스 타입별 ENI 갯수는 <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html#network-cards">여기</a>에서 확인할 수 있습니다. 
이 ENI 갯수에 따라 할당 가능한 IP 주소의 갯수가 결정되며, 이에 따라 파드의 갯수가 제한될 수 있습니다.</li>
  <li>
    <p>각 ENI는 할당할 수 있는 IP수가 제한되어 있으며, 이 수는 EC2 인스턴스 타입에 따라 다릅니다.
인스턴스 타입별 ENI 당 IP 할당 수는 <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AvailableIpPerENI.html">여기</a>에서 확인할 수 있습니다.
ENI 당 할당할 수 있는 IP 를 slot 이라고 하며, 이 slot은 VPC CNI를 통해 파드에 할당됩니다.</p>

    <p><img src="/assets/2024/kans-3th/w9/20241102_kans_w9_1.png" alt="img.png" class="image-center" />
<em class="image-caption">파드에 IP를 할당하기 위한 절차</em></p>
  </li>
</ul>

<h4 id="calico-cni와의-차이점">Calico CNI와의 차이점</h4>

<ul>
  <li>노드와 파드의 네트워크 대역을 동일하게 설정함으로써 NAT(Network Address Translation)을 사용하지 않아도 되기 때문에
성능이 향상되고 지연이 최소화 됩니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w9/20241102_kans_w9_2.png" alt="img.png" class="image-center w-80" /></p>

<ul>
  <li>파드간 통신시 Calico CNI 등의 일반적인 CNI는 오버레이(VXLAN, IP-in-IP)를 사용하여 통신하지만,
AWS VPC CNI는 VPC의 네트워크를 사용하여 직접 통신합니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w9/20241102_kans_w9_3.png" alt="img_1.png" class="image-center w-90" /></p>

<h4 id="ipv4-prefix-위임-delegation">IPv4 Prefix 위임 (Delegation)</h4>

<ul>
  <li>앞서 알아 본것 처럼 각 파드는 노드의 ENI에 할당된 IP 주소를 사용하고, ENI당 IP (slot) 수와 ENI 갯수는
인스턴스별로 제한이 있고, 이 값이 큰 편이 아닙니다. 이를 해결하기위해 IPv4 Prefix 위임이 도입되었습니다.</li>
  <li>IPv4 Prefix는 ENI 별로 IP를 할당하지 않고, /28의 접두사 길이 CIDR 블록을 노드에 할당하여 파드에 할당할 IP 주소를 관리합니다.
이렇게 하면 각 슬랏당 16개의 IP주소를 할당할 수 있어서 다음과 같은 갯수의 최대 IP 또는 파드를 사용할 수 있습니다.</li>
  <li>최대 사용가능한 IP 수 계산 : 
<code class="language-plaintext highlighter-rouge">사용가능한 파드 IP 수 = (최대 네트워크 인터페이스(ENI) 갯수 * (네트워크 인터페이스 당 slot 수 - 1) * 16)</code>
    <ul>
      <li>단, 실제로는 인스턴스 유형별로 권장되는 최대 갯수로 선정됩니다.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/2024/kans-3th/w9/20241102_kans_w9_4.png" alt="img.png" class="image-center" /></p>

<ul>
  <li>참고 링크 : <a href="https://trans.yonghochoi.com/translations/aws_vpc_cni_increase_pods_per_node_limits.ko">Amazon VPC CNI 플러그인으로 노드당 파드수 제한 늘리기</a></li>
</ul>

<hr />

<h3 id="실습-준비">실습 준비</h3>

<h4 id="구성-환경">구성 환경</h4>

<ul>
  <li>사전 준비물 : AWS 계정, SSH 키 페어, IAM 계정 생성 후 키</li>
  <li>전체 구성도 : VPC 1개(퍼블릭 서브넷 3개, 프라이빗 서브넷 3개), EKS 클러스터(Control Plane), 관리형 노드 그룹(EC2 3대), Add-on
<img src="/assets/2024/kans-3th/w9/20241102_kans_w9_28.png" alt="img.png" class="image-center" />
    <ul>
      <li>CloudFormation 스택 실행 시 <strong>파라미터</strong>를 기입하면, 해당 정보가 반영되어 배포됩니다.</li>
      <li>실습 환경을 위한 <strong>VPC</strong> 1개가 생성되고, <strong>퍼블릭</strong> 서브넷 3개와 <strong>프라이빗</strong> 서브넷 3개가 생성됩니다.</li>
      <li>CloudFormation 에 EC2의 <strong>UserData</strong> 부분(<strong>Script</strong> 실행)으로 Amazon EKS <strong>설치(with OIDC, Endpoint Public)</strong>를 진행합니다.</li>
      <li><strong>관리형 노드 그룹</strong>(워커 노드)는 AZ1~AZ3를 사용하여, 기본 <strong>3</strong>대로 구성됩니다</li>
      <li><strong>Add-on</strong> 같이 설치 됨 : 최신 버전 - kube-proxy, coredns, aws vpc cni - <a href="https://docs.aws.amazon.com/eks/latest/userguide/eks-add-ons.html">링크</a></li>
      <li><strong>노드</strong>에 <strong>EC2 IAM Profile</strong> 권한 추가 : external-dns-access, full-ecr-access, alb-ingress-access, awsLoadBalancerController</li>
    </ul>
  </li>
</ul>

<h4 id="배포-및-테스트">배포 및 테스트</h4>

<ul>
  <li>배포
    <ul>
      <li>aws cli를 설치하고, aws configure로 자격증명을 설정 후 아래의 명령을 실행합니다.</li>
      <li>다음의 파라미터가 있고, :point_right: 표시가 있는 부분은 필수로 설정해주어야 합니다.
        <ul>
          <li><strong>Deploy EC2</strong>
            <ol>
              <li>:point_right: <strong>KeyName</strong> : 작업용 bastion ec2에 SSH 접속을 위한 <strong>SSH 키페어</strong> 선택 <em>← 미리 SSH 키 생성 해두자!</em></li>
              <li>:point_right: <strong>MyIamUserAccessKeyID</strong> : <strong>관리자</strong> 수준의 권한을 가진 IAM User의 액세스 키ID 입력</li>
              <li>:point_right: <strong>MyIamUserSecretAccessKey</strong> : <strong>관리자</strong> 수준의 권한을 가진 IAM User의 <strong>시크릿 키ID</strong> 입력 <strong>← 노출되지 않게 보안 주의</strong></li>
              <li>:point_right: <strong>SgIngressSshCidr</strong> : 작업용 bastion ec2에 <strong>SSH 접속 가능한 IP</strong> 입력 (<strong>집 공인IP</strong>/32 입력), 보안그룹 인바운드 규칙에 반영됨</li>
              <li>MyInstanceType: 작업용 bastion EC2 인스턴스의 타입 (기본 <strong>t3.medium</strong>) ⇒ 변경 가능</li>
              <li>LatestAmiId : 작업용 bastion EC2에 사용할 AMI는 아마존리눅스2 최신 버전 사용</li>
            </ol>
          </li>
          <li><strong>EKS Config</strong>
            <ol>
              <li><strong>ClusterBaseName</strong> : EKS <strong>클러스터 이름</strong>이며, <strong>myeks</strong> 기본값 사용을 권장 → 이유: 실습 리소스 태그명과 실습 커멘드에서 사용</li>
              <li><strong>KubernetesVersion</strong> : EKS 호환, 쿠버네티스 버전 (기본 v1.30, 실습은 <strong>1.30</strong> 버전 사용)</li>
              <li><strong>WorkerNodeInstanceType</strong>: 워커 노드 EC2 인스턴스의 타입 (기본 <strong>t3.medium</strong>) ⇒ 변경 가능</li>
              <li><strong>WorkerNodeCount</strong> : 워커노드의 갯수를 입력 (기본 3대) ⇒ 변경 가능</li>
              <li><strong>WorkerNodeVolumesize</strong> : 워커노드의 EBS 볼륨 크기 (기본 80GiB) ⇒ 변경 가능</li>
            </ol>
          </li>
          <li><strong>Region AZ</strong> : 리전과 가용영역을 지정, 기본값 그대로 사용</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># YAML 파일 다운로드</span>
<span class="nv">$ </span>curl <span class="nt">-O</span> https://s3.ap-northeast-2.amazonaws.com/cloudformation.cloudneta.net/kans/eks-oneclick.yaml

<span class="c"># CloudFormation 스택 배포</span>
<span class="c"># aws cloudformation deploy --template-file eks-oneclick.yaml --stack-name myeks --parameter-overrides KeyName=&lt;My SSH Keyname&gt; SgIngressSshCidr=&lt;My Home Public IP Address&gt;/32 MyIamUserAccessKeyID=&lt;IAM User의 액세스키&gt; MyIamUserSecretAccessKey=&lt;IAM User의 시크릿 키&gt; ClusterBaseName='&lt;eks 이름&gt;' --region ap-northeast-2</span>
<span class="c"># 예시) aws cloudformation deploy --template-file eks-oneclick.yaml --stack-name myeks --parameter-overrides KeyName=aws-ec2 SgIngressSshCidr=$(curl -s ipinfo.io/ip)/32  MyIamUserAccessKeyID=AKIA5... MyIamUserSecretAccessKey='CVNa2...' ClusterBaseName=myeks --region ap-northeast-2</span>

<span class="c">## Tip. 워커노드 인스턴스 타입 변경 : WorkerNodeInstanceType=t3.xlarge</span>
<span class="c"># 예시) aws cloudformation deploy --template-file eks-oneclick.yaml --stack-name myeks --parameter-overrides KeyName=aws-ec2 SgIngressSshCidr=$(curl -s ipinfo.io/ip)/32  MyIamUserAccessKeyID=AKIA5... MyIamUserSecretAccessKey='CVNa2...' ClusterBaseName=myeks --region ap-northeast-2 WorkerNodeInstanceType=t3.xlarge </span>

<span class="nv">$ </span>aws cloudformation deploy <span class="nt">--template-file</span> eks-oneclick.yaml <span class="nt">--stack-name</span> myeks <span class="se">\</span>
  <span class="nt">--parameter-overrides</span> <span class="nv">KeyName</span><span class="o">=</span>aws-ec2 <span class="se">\</span>
  <span class="nv">SgIngressSshCidr</span><span class="o">=</span><span class="si">$(</span>curl <span class="nt">-s</span> ipinfo.io/ip<span class="si">)</span>/32 <span class="se">\</span>
  <span class="nv">MyIamUserAccessKeyID</span><span class="o">=</span>AKIA5... <span class="se">\</span>
  <span class="nv">MyIamUserSecretAccessKey</span><span class="o">=</span><span class="s1">'CVNa2...'</span> <span class="se">\ </span> 
  <span class="nv">ClusterBaseName</span><span class="o">=</span>myeks <span class="se">\ </span>
  <span class="nt">--region</span> ap-northeast-2
<span class="c"># =&gt; Waiting for changeset to be created..</span>
<span class="c">#    Waiting for stack create/update to complete</span>
<span class="c">#    Successfully created/updated stack - myeks</span>

<span class="c"># CloudFormation 스택 배포 완료 후 작업용 EC2 IP 출력</span>
<span class="nv">$ </span>aws cloudformation describe-stacks <span class="nt">--stack-name</span> myeks <span class="nt">--query</span> <span class="s1">'Stacks[*].Outputs[0].OutputValue'</span> <span class="nt">--output</span> text
<span class="c"># =&gt; 3.35.140.75</span>

<span class="c"># 작업용 EC2 SSH 접속</span>
<span class="nv">$ </span>ssh <span class="nt">-i</span> ~/.ssh/aws-ec2-key.cer ec2-user@<span class="si">$(</span>aws cloudformation describe-stacks <span class="nt">--stack-name</span> myeks <span class="nt">--query</span> <span class="s1">'Stacks[*].Outputs[0].OutputValue'</span> <span class="nt">--output</span> text<span class="si">)</span>
<span class="c"># =&gt;    ,     #_</span>
<span class="c">#       ~\_  ####_        Amazon Linux 2</span>
<span class="c">#      ~~  \_#####\</span>
<span class="c">#      ~~     \###|       AL2 End of Life is 2025-06-30.</span>
<span class="c">#      ~~       \#/ ___</span>
<span class="c">#       ~~       V~' '-&amp;gt;</span>
<span class="c">#        ~~~         /    A newer version of Amazon Linux is available!</span>
<span class="c">#          ~~._.   _/</span>
<span class="c">#             _/ _/       Amazon Linux 2023, GA and supported until 2028-03-15.</span>
<span class="c">#           _/m/'           https://aws.amazon.com/linux/amazon-linux-2023/</span>
<span class="c">#    </span>
<span class="c">#    10 package(s) needed for security, out of 13 available</span>
<span class="c">#    Run &amp;quot;sudo yum update&amp;quot; to apply all updates.</span>
<span class="c">#    [root@myeks-bastion ~]#</span>
</code></pre></div></div>

<ul>
  <li>작업용 EC2에 SSH 키 파일 사용하여 SSH 접속 후 확인해보겠습니다.</li>
  <li>쿠버네티스 정상 설치 확인은 스택 생성 시작 후 20분 후 접속하는 것이 좋습니다.</li>
  <li>접속 후 기본 확인</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># SSH 접속</span>
<span class="nv">$ </span>ssh <span class="nt">-i</span> ~/.ssh/aws-ec2-key.cer ec2-user@<span class="si">$(</span>aws cloudformation describe-stacks <span class="nt">--stack-name</span> myeks <span class="nt">--query</span> <span class="s1">'Stacks[*].Outputs[0].OutputValue'</span> <span class="nt">--output</span> text<span class="si">)</span>

<span class="c"># cloud-init 실행 과정 로그 확인</span>
<span class="nv">$ </span><span class="nb">tail</span> <span class="nt">-f</span> /var/log/cloud-init-output.log
<span class="c"># =&gt;  ...</span>
<span class="c">#     69  dnsmasq                  available    [ =stable ]</span>
<span class="c">#     70  unbound1.17              available    [ =stable ]</span>
<span class="c">#     72  collectd-python3         available    [ =stable ]</span>
<span class="c">#    † Note on end-of-support. Use 'info' subcommand.</span>
<span class="c">#    Created symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.</span>
<span class="c">#    cloudinit End!</span>
<span class="c">#    Cloud-init v. 19.3-46.amzn2.0.2 finished at Sat, 01 Nov 2024 07:27:24 +0000. Datasource DataSourceEc2.  Up 84.79 seconds</span>

<span class="c"># cloud-init 정상 완료 후 eksctl 실행 과정 로그 확인</span>
<span class="nv">$ </span><span class="nb">tail</span> <span class="nt">-f</span> /root/create-eks.log
<span class="c"># =&gt; ...</span>
<span class="c">#    2024-10-01 16:26:46 [ℹ]  deploying stack &amp;quot;eksctl-myeks-cluster&amp;quot;</span>
<span class="c">#    2024-10-01 16:27:16 [ℹ]  waiting for CloudFormation stack &amp;quot;eksctl-myeks-cluster&amp;quot;</span>
<span class="c">#    ...</span>
<span class="c">#    2024-10-01 16:40:31 [✔]  EKS cluster &amp;quot;myeks&amp;quot; in &amp;quot;ap-northeast-2&amp;quot; region is ready</span>

<span class="c"># default 네임스페이스 적용</span>
<span class="nv">$ </span>kubectl ns default
<span class="c"># =&gt; Context &amp;quot;anonym@myeks.ap-northeast-2.eksctl.io&amp;quot; modified.</span>
<span class="c">#    Active namespace is &amp;quot;default&amp;quot;.</span>

<span class="c"># 설치 확인</span>
<span class="nv">$ </span>kubectl cluster-info
<span class="c"># =&gt; Kubernetes control plane is running at https://CF0FD5E1EEB937DD6FF881B7EBADDFD4.yl4.ap-northeast-2.eks.amazonaws.com</span>
<span class="c">#    CoreDNS is running at https://CF0FD5E1EEB937DD6FF881B7EBADDFD4.yl4.ap-northeast-2.eks.amazonaws.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy</span>
<span class="nv">$ </span>eksctl get cluster
<span class="c"># =&gt; NAME REGION    EKSCTL CREATED</span>
<span class="c">#    myeks  ap-northeast-2  True</span>
<span class="nv">$ </span>eksctl get nodegroup <span class="nt">--cluster</span> <span class="nv">$CLUSTER_NAME</span>
<span class="c"># =&gt; CLUSTER  NODEGROUP STATUS  CREATED               MIN SIZE  MAX SIZE  DESIRED CAPACITY  INSTANCE TYPE  IMAGE ID    ASG NAME                                      TYPE</span>
<span class="c">#    myeks    ng1       ACTIVE  2024-10-01T07:37:58Z  3         3         3                 t3.medium      AL2_x86_64  eks-ng1-4ec975e7-9584-1403-37c4-fc55cc2ec860  managed</span>

<span class="c"># 환경변수 정보 확인</span>
<span class="nv">$ </span><span class="nb">export</span> | egrep <span class="s1">'ACCOUNT|AWS_|CLUSTER|KUBERNETES|VPC|Subnet'</span>
<span class="nv">$ </span><span class="nb">export</span> | egrep <span class="s1">'ACCOUNT|AWS_|CLUSTER|KUBERNETES|VPC|Subnet'</span> | egrep <span class="nt">-v</span> <span class="s1">'SECRET|KEY'</span>
<span class="c"># =&gt; declare -x ACCOUNT_ID=&amp;quot;123456789012&amp;quot;</span>
<span class="c">#    declare -x AWS_DEFAULT_REGION=&amp;quot;ap-northeast-2&amp;quot;</span>
<span class="c">#    declare -x AWS_PAGER=&amp;quot;&amp;quot;</span>
<span class="c">#    declare -x AWS_REGION=&amp;quot;ap-northeast-2&amp;quot;</span>
<span class="c">#    declare -x CLUSTER_NAME=&amp;quot;myeks&amp;quot;</span>
<span class="c">#    declare -x KUBERNETES_VERSION=&amp;quot;1.30&amp;quot;</span>
<span class="c">#    declare -x PrivateSubnet1=&amp;quot;subnet-02550e25cd3a9d814&amp;quot;</span>
<span class="c">#    declare -x PrivateSubnet2=&amp;quot;subnet-0c2adfcc3d586e58d&amp;quot;</span>
<span class="c">#    declare -x PrivateSubnet3=&amp;quot;subnet-04dd8e21b159de4cb&amp;quot;</span>
<span class="c">#    declare -x PubSubnet1=&amp;quot;subnet-0a06ed52d587bd707&amp;quot;</span>
<span class="c">#    declare -x PubSubnet2=&amp;quot;subnet-00b4dacf7eef35d33&amp;quot;</span>
<span class="c">#    declare -x PubSubnet3=&amp;quot;subnet-03c911c48452b41b2&amp;quot;</span>
<span class="c">#    declare -x VPCID=&amp;quot;vpc-0837921f515624150&amp;quot;</span>

<span class="c"># 인증 정보 확인</span>
<span class="nv">$ </span><span class="nb">cat</span> /root/.kube/config
<span class="nv">$ </span>kubectl config view
<span class="nv">$ </span>kubectl ctx
<span class="c"># =&gt; anonym@myeks.ap-northeast-2.eksctl.io</span>

<span class="c"># 노드 정보 확인</span>
<span class="nv">$ </span>kubectl get node <span class="nt">--label-columns</span><span class="o">=</span>node.kubernetes.io/instance-type,eks.amazonaws.com/capacityType,topology.kubernetes.io/zone
<span class="c"># =&gt; NAME                                               STATUS   ROLES    AGE     VERSION               INSTANCE-TYPE   CAPACITYTYPE   ZONE</span>
<span class="c">#    ip-192-168-1-186.ap-northeast-2.compute.internal   Ready    &amp;lt;none&amp;gt;   7m26s   v1.30.4-eks-a737599   t3.medium       ON_DEMAND      ap-northeast-2a</span>
<span class="c">#    ip-192-168-2-92.ap-northeast-2.compute.internal    Ready    &amp;lt;none&amp;gt;   7m22s   v1.30.4-eks-a737599   t3.medium       ON_DEMAND      ap-northeast-2b</span>
<span class="c">#    ip-192-168-3-235.ap-northeast-2.compute.internal   Ready    &amp;lt;none&amp;gt;   7m22s   v1.30.4-eks-a737599   t3.medium       ON_DEMAND      ap-northeast-2c</span>
<span class="nv">$ </span>eksctl get iamidentitymapping <span class="nt">--cluster</span> myeks
<span class="c"># =&gt; ARN                      USERNAME        GROUPS          ACCOUNT</span>
<span class="c">#    arn:aws:iam::123456789012:role/eksctl-myeks-nodegroup-ng1-NodeInstanceRole-kPdCWLD1sAsU  system:node: system:bootstrappers,system:nodes</span>

<span class="c"># krew 플러그인 확인</span>
<span class="nv">$ </span>kubectl krew list
<span class="c"># =&gt; PLUGIN   VERSION</span>
<span class="c">#    ctx      v0.9.5</span>
<span class="c">#    get-all  v1.3.8</span>
<span class="c">#    krew     v0.4.4</span>
<span class="c">#    neat     v2.0.4</span>
<span class="c">#    ns       v0.9.5</span>
<span class="c">#    stern    v1.31.0</span>

<span class="c"># 모든 네임스페이스에서 모든 리소스 확인</span>
<span class="nv">$ </span>kubectl get-all
</code></pre></div></div>

<ul>
  <li>노드 접속 확인 및 SSH 접속</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 노드 IP 확인 및 PrivateIP 변수 지정</span>
<span class="nv">$ </span>aws ec2 describe-instances <span class="nt">--query</span> <span class="s2">"Reservations[*].Instances[*].{PublicIPAdd:PublicIpAddress,PrivateIPAdd:PrivateIpAddress,InstanceName:Tags[?Key=='Name']|[0].Value,Status:State.Name}"</span> <span class="nt">--filters</span> <span class="nv">Name</span><span class="o">=</span>instance-state-name,Values<span class="o">=</span>running <span class="nt">--output</span> table
<span class="c"># =&gt; ------------------------------------------------------------------</span>
<span class="c">#    |                        DescribeInstances                       |</span>
<span class="c">#    +----------------+-----------------+------------------+----------+</span>
<span class="c">#    |  InstanceName  |  PrivateIPAdd   |   PublicIPAdd    | Status   |</span>
<span class="c">#    +----------------+-----------------+------------------+----------+</span>
<span class="c">#    |  myeks-ng1-Node|  192.168.2.92   |  43.203.143.233  |  running |</span>
<span class="c">#    |  myeks-ng1-Node|  192.168.3.235  |  15.164.95.12    |  running |</span>
<span class="c">#    |  myeks-bastion |  192.168.1.100  |  3.35.140.75     |  running |</span>
<span class="c">#    |  myeks-ng1-Node|  192.168.1.186  |  3.38.183.82     |  running |</span>
<span class="c">#    +----------------+-----------------+------------------+----------+</span>
<span class="nv">$ N1</span><span class="o">=</span><span class="si">$(</span>kubectl get node <span class="nt">--label-columns</span><span class="o">=</span>topology.kubernetes.io/zone <span class="nt">--selector</span><span class="o">=</span>topology.kubernetes.io/zone<span class="o">=</span>ap-northeast-2a <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.items[0].status.addresses[0].address<span class="o">}</span><span class="si">)</span>
<span class="nv">$ N2</span><span class="o">=</span><span class="si">$(</span>kubectl get node <span class="nt">--label-columns</span><span class="o">=</span>topology.kubernetes.io/zone <span class="nt">--selector</span><span class="o">=</span>topology.kubernetes.io/zone<span class="o">=</span>ap-northeast-2b <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.items[0].status.addresses[0].address<span class="o">}</span><span class="si">)</span>
<span class="nv">$ N3</span><span class="o">=</span><span class="si">$(</span>kubectl get node <span class="nt">--label-columns</span><span class="o">=</span>topology.kubernetes.io/zone <span class="nt">--selector</span><span class="o">=</span>topology.kubernetes.io/zone<span class="o">=</span>ap-northeast-2c <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.items[0].status.addresses[0].address<span class="o">}</span><span class="si">)</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"export N1=</span><span class="nv">$N1</span><span class="s2">"</span> <span class="o">&gt;&gt;</span> /etc/profile
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"export N2=</span><span class="nv">$N2</span><span class="s2">"</span> <span class="o">&gt;&gt;</span> /etc/profile
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"export N3=</span><span class="nv">$N3</span><span class="s2">"</span> <span class="o">&gt;&gt;</span> /etc/profile
<span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$N1</span>, <span class="nv">$N2</span>, <span class="nv">$N3</span>
<span class="c"># =&gt; 192.168.1.186, 192.168.2.92, 192.168.3.235</span>

<span class="c"># 보안그룹 ID와 보안그룹 이름(Name아님을 주의!) 확인</span>
<span class="nv">$ </span>aws ec2 describe-security-groups <span class="nt">--query</span> <span class="s1">'SecurityGroups[*].[GroupId, GroupName]'</span> <span class="nt">--output</span> text
<span class="c"># =&gt; sg-07b8900660cb85dff default</span>
<span class="c">#    sg-0ece66fedd1fb4abe myeks-EKSEC2SG-1GNaTdbIbBNG</span>
<span class="c">#    sg-07663468a536ba88b eksctl-myeks-cluster-ControlPlaneSecurityGroup-S7dWa32uw1S7</span>
<span class="c">#    sg-0edc706b941f0aef7 eksctl-myeks-cluster-ClusterSharedNodeSecurityGroup-sO3DEJP4xewT</span>
<span class="c">#    sg-02c614a038ec1f7e7 eks-cluster-sg-myeks-104368993</span>
<span class="c">#    sg-036e220bf3d8aaf20 eksctl-myeks-nodegroup-ng1-remoteAccess</span>
<span class="c">#    sg-035e2b98b5ac89231 default</span>

<span class="c"># 노드 보안그룹 ID 확인</span>
<span class="nv">$ </span>aws ec2 describe-security-groups <span class="nt">--filters</span> <span class="nv">Name</span><span class="o">=</span>group-name,Values<span class="o">=</span><span class="k">*</span>ng1<span class="k">*</span> <span class="nt">--query</span> <span class="s2">"SecurityGroups[*].[GroupId]"</span> <span class="nt">--output</span> text
<span class="c"># =&gt; sg-036e220bf3d8aaf20</span>
<span class="nv">$ NGSGID</span><span class="o">=</span><span class="si">$(</span>aws ec2 describe-security-groups <span class="nt">--filters</span> <span class="nv">Name</span><span class="o">=</span>group-name,Values<span class="o">=</span><span class="k">*</span>ng1<span class="k">*</span> <span class="nt">--query</span> <span class="s2">"SecurityGroups[*].[GroupId]"</span> <span class="nt">--output</span> text<span class="si">)</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$NGSGID</span>
<span class="c"># =&gt; sg-036e220bf3d8aaf20</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"export NGSGID=</span><span class="nv">$NGSGID</span><span class="s2">"</span> <span class="o">&gt;&gt;</span> /etc/profile

<span class="c"># 노드 보안그룹에 eksctl-host 에서 노드(파드)에 접속 가능하게 룰(Rule) 추가 설정</span>
<span class="nv">$ </span>aws ec2 authorize-security-group-ingress <span class="nt">--group-id</span> <span class="nv">$NGSGID</span> <span class="nt">--protocol</span> <span class="s1">'-1'</span> <span class="nt">--cidr</span> 192.168.1.100/32

<span class="c"># eksctl-host 에서 노드의IP나 coredns 파드IP로 ping 테스트</span>
<span class="nv">$ </span>ping <span class="nt">-c</span> 1 <span class="nv">$N1</span>
<span class="c"># =&gt; PING 192.168.1.186 (192.168.1.186) 56(84) bytes of data.</span>
<span class="c">#    64 bytes from 192.168.1.186: icmp_seq=1 ttl=255 time=0.492 ms</span>
<span class="c">#    </span>
<span class="c">#    --- 192.168.1.186 ping statistics ---</span>
<span class="c">#    1 packets transmitted, 1 received, 0% packet loss, time 0ms</span>
<span class="c">#    rtt min/avg/max/mdev = 0.492/0.492/0.492/0.000 ms</span>
<span class="nv">$ </span>ping <span class="nt">-c</span> 1 <span class="nv">$N2</span>
<span class="nv">$ </span>ping <span class="nt">-c</span> 1 <span class="nv">$N3</span>

<span class="c"># 워커 노드 SSH 접속 : '-i ~/.ssh/id_rsa' 생략 가능</span>
<span class="nv">$ </span><span class="k">for </span>node <span class="k">in</span> <span class="nv">$N1</span> <span class="nv">$N2</span> <span class="nv">$N3</span><span class="p">;</span> <span class="k">do </span>ssh <span class="nt">-o</span> <span class="nv">StrictHostKeyChecking</span><span class="o">=</span>no ec2-user@<span class="nv">$node</span> <span class="nb">hostname</span><span class="p">;</span> <span class="k">done</span>
<span class="nv">$ </span>ssh ec2-user@<span class="nv">$N1</span>
<span class="nv">$ </span><span class="nb">exit</span>
<span class="nv">$ </span>ssh ec2-user@<span class="nv">$N2</span>
<span class="nv">$ </span><span class="nb">exit</span>
<span class="nv">$ </span>ssh ec2-user@<span class="nv">$N3</span>
<span class="nv">$ </span><span class="nb">exit</span>
</code></pre></div></div>

<ul>
  <li>Add-on 정보확인 : 최신 버전 - kube-proxy, coredns, aws vpc cni - 링크 mgmt</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 모든 파드의 컨테이너 이미지 정보 확인</span>
<span class="nv">$ </span>kubectl get pods <span class="nt">--all-namespaces</span> <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.items[*].spec.containers[*].image}"</span> | <span class="nb">tr</span> <span class="nt">-s</span> <span class="s1">'[[:space:]]'</span> <span class="s1">'\n'</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span>
<span class="c"># =&gt;       3 602401143452.dkr.ecr.ap-northeast-2.amazonaws.com/amazon/aws-network-policy-agent:v1.1.4-eksbuild.1</span>
<span class="c">#          3 602401143452.dkr.ecr.ap-northeast-2.amazonaws.com/amazon-k8s-cni:v1.18.6-eksbuild.1</span>
<span class="c">#          2 602401143452.dkr.ecr.ap-northeast-2.amazonaws.com/eks/coredns:v1.11.3-eksbuild.2</span>
<span class="c">#          3 602401143452.dkr.ecr.ap-northeast-2.amazonaws.com/eks/kube-proxy:v1.30.5-minimal-eksbuild.2</span>
<span class="c"># 위 버전은 Add-on 으로 최신 버전 설치</span>
<span class="nv">$ </span>kubectl get pods <span class="nt">-A</span>
<span class="nv">$ </span>kubectl get pods <span class="nt">--all-namespaces</span> <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.items[*].spec.containers[*].image}"</span> | <span class="nb">tr</span> <span class="nt">-s</span> <span class="s1">'[[:space:]]'</span> <span class="s1">'\n'</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span>
<span class="c"># =&gt;       3 602401143452.dkr.ecr.ap-northeast-2.amazonaws.com/amazon/aws-network-policy-agent:v1.1.4-eksbuild.1</span>
<span class="c">#          3 602401143452.dkr.ecr.ap-northeast-2.amazonaws.com/amazon-k8s-cni:v1.18.6-eksbuild.1</span>
<span class="c">#          2 602401143452.dkr.ecr.ap-northeast-2.amazonaws.com/eks/coredns:v1.11.3-eksbuild.2</span>
<span class="c">#          3 602401143452.dkr.ecr.ap-northeast-2.amazonaws.com/eks/kube-proxy:v1.30.5-minimal-eksbuild.2</span>

<span class="c"># eksctl 설치/업데이트 addon 확인</span>
<span class="nv">$ </span>eksctl get addon <span class="nt">--cluster</span> <span class="nv">$CLUSTER_NAME</span>
<span class="c"># =&gt; NAME        VERSION             STATUS  ISSUES      IAMROLE                                                                       UPDATE                AVAILABLE  CONFIGURATION  VALUES  POD      IDENTITY  ASSOCIATION  ROLES</span>
<span class="c">#    coredns     v1.11.3-eksbuild.2  ACTIVE  0</span>
<span class="c">#    kube-proxy  v1.30.5-eksbuild.2  ACTIVE  0</span>
<span class="c">#    vpc-cni     v1.18.6-eksbuild.1  ACTIVE  0           arn:aws:iam::123456789012:role/eksctl-myeks-addon-vpc-cni-Role1-QUy8qUBqFjcx  enableNetworkPolicy:  &amp;quot;true&amp;quot;</span>

<span class="c"># (참고) eks 설치 yaml 중 addon 내용</span>
<span class="nv">$ </span><span class="nb">tail</span> <span class="nt">-n11</span> myeks.yaml
<span class="c"># =&gt; addons:</span>
<span class="c">#      - name: vpc-cni # no version is specified so it deploys the default version</span>
<span class="c">#        version: latest # auto discovers the latest available</span>
<span class="c">#        attachPolicyARNs:</span>
<span class="c">#          - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy</span>
<span class="c">#        configurationValues: |-</span>
<span class="c">#          enableNetworkPolicy: &amp;quot;true&amp;quot;</span>
<span class="c">#      - name: kube-proxy</span>
<span class="c">#        version: latest</span>
<span class="c">#      - name: coredns</span>
<span class="c">#        version: latest</span>
</code></pre></div></div>

<hr />

<h3 id="노드에서-기본-네트워크-정보-확인">노드에서 기본 네트워크 정보 확인</h3>

<h4 id="워커-노드-기본-네트워크-구성">워커 노드 기본 네트워크 구성</h4>

<p><img src="/assets/2024/kans-3th/w9/20241102_kans_w9_5.png" alt="img.png" class="image-center w-60" /></p>

<ul>
  <li>네트워크 네임스페이스는 호스트(Root)와 파드별(Per Pod)로 구분됩니다.</li>
  <li>특정한 파드 (kube-proxy, aws-node)는 호스트의 IP를 그대로 사용합니다. =&gt; 파드의 Host Network 옵션 - <a href="https://xn--vj5b11biyw.kr/306">참고</a></li>
  <li>t3.medium 의 경우 ENI 마다 최대 6개의 IP를 가질 수 있습니다. (ENI 당 5개의 보조 IP)</li>
  <li>ENI0, ENI1 으로 2개의 ENI는 자신의 IP 이외에 추가적으로 5개의 보조 프라이빗 IP를 가질 수 있습니다.</li>
  <li>coredns 파드는 veth으로 호스트에는 eniY@ifN 인터페이스와 파드에 eth0과 연결되어 있습니다.</li>
</ul>

<h4 id="실습-보조-ipv4-주소를-파드가-사용하는지-확인">[실습] 보조 IPv4 주소를 파드가 사용하는지 확인</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># coredns 파드 IP 정보 확인</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-n</span> kube-system <span class="nt">-l</span> k8s-app<span class="o">=</span>kube-dns <span class="nt">-owide</span>
<span class="c"># =&gt; NAME                       READY   STATUS    RESTARTS   AGE   IP             NODE                                               NOMINATED NODE   READINESS GATES</span>
<span class="c">#    coredns-699d8c5988-bvxtz   1/1     Running   0          18m   192.168.1.30   ip-192-168-1-186.ap-northeast-2.compute.internal   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    coredns-699d8c5988-vt68k   1/1     Running   0          18m   192.168.3.82   ip-192-168-3-235.ap-northeast-2.compute.internal   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>

<span class="c"># 노드의 라우팅 정보 확인 &gt;&gt; EC2 네트워크 정보의 '보조 프라이빗 IPv4 주소'와 비교해보자</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="nv">$N1</span> <span class="nv">$N2</span> <span class="nv">$N3</span><span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> ssh ec2-user@<span class="nv">$i</span> <span class="nb">sudo </span>ip <span class="nt">-c</span> route<span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; &amp;gt;&amp;gt; node 192.168.1.186 &amp;lt;&amp;lt;</span>
<span class="c">#    default via 192.168.1.1 dev eth0</span>
<span class="c">#    169.254.169.254 dev eth0</span>
<span class="c">#    192.168.1.0/24 dev eth0 proto kernel scope link src 192.168.1.186</span>
<span class="c">#    192.168.1.30 dev eni319ad74733c scope link</span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; node 192.168.2.92 &amp;lt;&amp;lt;</span>
<span class="c">#    default via 192.168.2.1 dev eth0</span>
<span class="c">#    169.254.169.254 dev eth0</span>
<span class="c">#    192.168.2.0/24 dev eth0 proto kernel scope link src 192.168.2.92</span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; node 192.168.3.235 &amp;lt;&amp;lt;</span>
<span class="c">#    default via 192.168.3.1 dev eth0</span>
<span class="c">#    169.254.169.254 dev eth0</span>
<span class="c">#    192.168.3.0/24 dev eth0 proto kernel scope link src 192.168.3.235</span>
<span class="c">#    192.168.3.82 dev enid438418b082 scope link</span>

<span class="c"># &lt;span style="color: green;"&gt;👉 노드의 IP와 파드의 IP가 같은 대역임을 확인 할 수 있습니다.&lt;/span&gt;</span>
<span class="c"># &lt;span style="color: green;"&gt;    예를 들어서 노드 IP가 192.168.1.186인 경우 파드는 192.168.1.30으로&lt;/span&gt;</span>
<span class="c"># &lt;span style="color: green;"&gt;    동일한 IP 대역으로, 보조 IPv4가 사용됨을 확인 할 수 있습니다.&lt;/span&gt;</span>
</code></pre></div></div>

<h4 id="실습-테스트용-파드-생성---nicolakanetshoot">[실습] 테스트용 파드 생성 - <a href="https://github.com/nicolaka/netshoot">nicolaka/netshoot</a></h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># [터미널1~3] 노드 모니터링</span>
<span class="nv">$ </span>ssh ec2-user@<span class="nv">$N1</span>
<span class="c"># =&gt; [ec2-user@ip-192-168-1-186 ~]$</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s2">"ip link | egrep 'eth|eni' ;echo;echo "</span><span class="o">[</span>ROUTE TABLE]<span class="s2">"; route -n | grep eni"</span>
<span class="c"># =&gt; 2: eth0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 9001 qdisc</span>
<span class="c">#     mq state UP mode DEFAULT group default qlen 1000</span>
<span class="c">#        link/ether 02:35:26:fe:f6:2f brd ff:ff:ff:ff:ff:ff</span>
<span class="c">#    3: eni319ad74733c@if3: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt;</span>
<span class="c">#    mtu 9001 qdisc noqueue state UP mode DEFAULT group defaul</span>
<span class="c">#    t</span>
<span class="c">#        link/ether 16:69:60:63:cf:f7 brd ff:ff:ff:ff:ff:ff li</span>
<span class="c">#    nk-netns cni-a953612e-ef39-78ca-660c-2a1ecde16b39</span>
<span class="c">#    4: eth1: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 9001 qdisc</span>
<span class="c">#     mq state UP mode DEFAULT group default qlen 1000</span>
<span class="c">#        link/ether 02:86:42:43:71:d1 brd ff:ff:ff:ff:ff:ff</span>
<span class="c">#    </span>
<span class="c">#    [ROUTE TABLE]</span>
<span class="c">#    192.168.1.30    0.0.0.0         255.255.255.255 UH    0</span>
<span class="c">#        0        0 eni319ad74733c</span>

<span class="nv">$ </span>ssh ec2-user@<span class="nv">$N2</span>
<span class="c"># =&gt; [ec2-user@ip-192-168-2-92 ~]$</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s2">"ip link | egrep 'eth|eni' ;echo;echo "</span><span class="o">[</span>ROUTE TABLE]<span class="s2">"; route -n | grep eni"</span>
<span class="c"># =&gt; 2: eth0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 9001 qdisc</span>
<span class="c">#     mq state UP mode DEFAULT group default qlen 1000</span>
<span class="c">#        link/ether 06:bc:36:ed:7c:d3 brd ff:ff:ff:ff:ff:ff</span>
<span class="c">#    </span>
<span class="c">#    [ROUTE TABLE]</span>

<span class="nv">$ </span>ssh ec2-user@<span class="nv">$N3</span>
<span class="c"># =&gt; [ec2-user@ip-192-168-3-235 ~]$</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s2">"ip link | egrep 'eth|eni' ;echo;echo "</span><span class="o">[</span>ROUTE TABLE]<span class="s2">"; route -n | grep eni"</span>
<span class="c"># =&gt; 2: eth0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 9001 qdisc</span>
<span class="c">#     mq state UP mode DEFAULT group default qlen 1000</span>
<span class="c">#        link/ether 0a:3e:a1:00:00:05 brd ff:ff:ff:ff:ff:ff</span>
<span class="c">#    3: enid438418b082@if3: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt;</span>
<span class="c">#    mtu 9001 qdisc noqueue state UP mode DEFAULT group defaul</span>
<span class="c">#    t</span>
<span class="c">#        link/ether 02:50:3a:92:0e:1b brd ff:ff:ff:ff:ff:ff li</span>
<span class="c">#    nk-netns cni-23569158-d846-4981-4105-ec08005e9b95</span>
<span class="c">#    4: eth1: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 9001 qdisc</span>
<span class="c">#     mq state UP mode DEFAULT group default qlen 1000</span>
<span class="c">#        link/ether 0a:e8:ac:41:8c:ab brd ff:ff:ff:ff:ff:ff</span>
<span class="c">#    </span>
<span class="c">#    [ROUTE TABLE]</span>
<span class="c">#    192.168.3.82    0.0.0.0         255.255.255.255 UH    0</span>
<span class="c">#        0        0 enid438418b082</span>

<span class="c"># 테스트용 파드 netshoot-pod 생성</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: netshoot-pod
spec:
  replicas: 3
  selector:
    matchLabels:
      app: netshoot-pod
  template:
    metadata:
      labels:
        app: netshoot-pod
    spec:
      containers:
      - name: netshoot-pod
        image: nicolaka/netshoot
        command: ["tail"]
        args: ["-f", "/dev/null"]
      terminationGracePeriodSeconds: 0
</span><span class="no">EOF
</span><span class="c"># =&gt; deployment.apps/netshoot-pod created</span>

<span class="c"># 파드 이름 변수 지정</span>
<span class="nv">$ PODNAME1</span><span class="o">=</span><span class="si">$(</span>kubectl get pod <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>netshoot-pod <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.items[0].metadata.name<span class="o">}</span><span class="si">)</span>
<span class="nv">$ PODNAME2</span><span class="o">=</span><span class="si">$(</span>kubectl get pod <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>netshoot-pod <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.items[1].metadata.name<span class="o">}</span><span class="si">)</span>
<span class="nv">$ PODNAME3</span><span class="o">=</span><span class="si">$(</span>kubectl get pod <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>netshoot-pod <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.items[2].metadata.name<span class="o">}</span><span class="si">)</span>

<span class="c"># 파드 확인</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-o</span> wide
<span class="c"># =&gt; NAME                            READY   STATUS    RESTARTS   AGE   IP              NODE                                               NOMINATED NODE   READINESS GATES</span>
<span class="c">#    netshoot-pod-74b7555dc7-6qsvf   1/1     Running   0          29s   192.168.1.112   ip-192-168-1-186.ap-northeast-2.compute.internal   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    netshoot-pod-74b7555dc7-x6lxb   1/1     Running   0          29s   192.168.2.172   ip-192-168-2-92.ap-northeast-2.compute.internal    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    netshoot-pod-74b7555dc7-x7r96   1/1     Running   0          29s   192.168.3.246   ip-192-168-3-235.ap-northeast-2.compute.internal   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-o</span><span class="o">=</span>custom-columns<span class="o">=</span>NAME:.metadata.name,IP:.status.podIP
<span class="c"># =&gt; NAME                            IP</span>
<span class="c">#    netshoot-pod-74b7555dc7-6qsvf   192.168.1.112</span>
<span class="c">#    netshoot-pod-74b7555dc7-x6lxb   192.168.2.172</span>
<span class="c">#    netshoot-pod-74b7555dc7-x7r96   192.168.3.246</span>

<span class="c"># 노드에 라우팅 정보 확인</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="nv">$N1</span> <span class="nv">$N2</span> <span class="nv">$N3</span><span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> ssh ec2-user@<span class="nv">$i</span> <span class="nb">sudo </span>ip <span class="nt">-c</span> route<span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; &amp;gt; node 192.168.1.186 &amp;lt;&amp;lt;</span>
<span class="c">#    default via 192.168.1.1 dev eth0</span>
<span class="c">#    169.254.169.254 dev eth0</span>
<span class="c">#    192.168.1.0/24 dev eth0 proto kernel scope link src 192.168.1.186</span>
<span class="c">#    192.168.1.30 dev eni319ad74733c scope link</span>
<span class="c">#    192.168.1.112 dev eni7ecb7efa346 scope link  # &lt;span style="color: green;"&gt;👉 추가된 신규 파드의 IPv4 보조 IP&lt;/span&gt;</span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; node 192.168.2.92 &amp;lt;&amp;lt;</span>
<span class="c">#    default via 192.168.2.1 dev eth0</span>
<span class="c">#    169.254.169.254 dev eth0</span>
<span class="c">#    192.168.2.0/24 dev eth0 proto kernel scope link src 192.168.2.92</span>
<span class="c">#    192.168.2.172 dev enif06a5cbfaaa scope link  # &lt;span style="color: green;"&gt;👉 추가된 신규 파드의 IPv4 보조 IP&lt;/span&gt;</span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; node 192.168.3.235 &amp;lt;&amp;lt;</span>
<span class="c">#    default via 192.168.3.1 dev eth0</span>
<span class="c">#    169.254.169.254 dev eth0</span>
<span class="c">#    192.168.3.0/24 dev eth0 proto kernel scope link src 192.168.3.235</span>
<span class="c">#    192.168.3.82 dev enid438418b082 scope link</span>
<span class="c">#    192.168.3.246 dev eniea7f0ec96dd scope link  # &lt;span style="color: green;"&gt;👉 추가된 신규 파드의 IPv4 보조 IP&lt;/span&gt;</span>
</code></pre></div></div>

<ul>
  <li>파드가 생성되면, 워커 노드에 eniY@ifN 추가되고 라우팅 테이블에도 정보가 추가된것을 확인 할 수 있습니다.</li>
  <li>테스트용 파드 eniY 정보 확인 - 워커 노드 EC2</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 노드3에서 네트워크 인터페이스 정보 확인</span>
<span class="nv">$ </span>ssh ec2-user@<span class="nv">$N3</span>
<span class="nt">----------------</span>
<span class="nv">$ </span>ip <span class="nt">-br</span> <span class="nt">-c</span> addr show
<span class="c"># =&gt; lo               UNKNOWN        127.0.0.1/8 ::1/128</span>
<span class="c">#    eth0             UP             192.168.3.235/24 fe80::83e:a1ff:fe00:5/64</span>
<span class="c">#    enid438418b082@if3 UP             fe80::50:3aff:fe92:e1b/64</span>
<span class="c">#    eth1             UP             192.168.3.236/24 fe80::8e8:acff:fe41:8cab/64</span>
<span class="c">#    eniea7f0ec96dd@if3 UP             fe80::8cd8:57ff:fee7:29c/64</span>
<span class="nv">$ </span>ip <span class="nt">-c</span> <span class="nb">link</span>
<span class="c"># =&gt; 1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000</span>
<span class="c">#        link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span>
<span class="c">#    2: eth0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 9001 qdisc mq state UP mode DEFAULT group default qlen 1000</span>
<span class="c">#        link/ether 0a:3e:a1:00:00:05 brd ff:ff:ff:ff:ff:ff</span>
<span class="c">#    3: enid438418b082@if3: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 9001 qdisc noqueue state UP mode DEFAULT group default</span>
<span class="c">#        link/ether 02:50:3a:92:0e:1b brd ff:ff:ff:ff:ff:ff link-netns cni-23569158-d846-4981-4105-ec08005e9b95</span>
<span class="c">#    4: eth1: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 9001 qdisc mq state UP mode DEFAULT group default qlen 1000</span>
<span class="c">#        link/ether 0a:e8:ac:41:8c:ab brd ff:ff:ff:ff:ff:ff</span>
<span class="c">#    5: eniea7f0ec96dd@if3: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 9001 qdisc noqueue state UP mode DEFAULT group default</span>
<span class="c">#        link/ether 8e:d8:57:e7:02:9c brd ff:ff:ff:ff:ff:ff link-netns cni-508d7216-7484-4497-a023-c6236435d535</span>
<span class="nv">$ </span>ip <span class="nt">-c</span> addr
<span class="c"># =&gt; ...</span>
<span class="c">#    2: eth0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 9001 qdisc mq state UP group default qlen 1000</span>
<span class="c">#        link/ether 0a:3e:a1:00:00:05 brd ff:ff:ff:ff:ff:ff</span>
<span class="c">#        inet 192.168.3.235/24 brd 192.168.3.255 scope global dynamic eth0</span>
<span class="c">#           valid_lft 2166sec preferred_lft 2166sec</span>
<span class="c">#        inet6 fe80::83e:a1ff:fe00:5/64 scope link</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    3: enid438418b082@if3: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 9001 qdisc noqueue state UP group default</span>
<span class="c">#        link/ether 02:50:3a:92:0e:1b brd ff:ff:ff:ff:ff:ff link-netns cni-23569158-d846-4981-4105-ec08005e9b95</span>
<span class="c">#        inet6 fe80::50:3aff:fe92:e1b/64 scope link</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    4: eth1: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 9001 qdisc mq state UP group default qlen 1000</span>
<span class="c">#        link/ether 0a:e8:ac:41:8c:ab brd ff:ff:ff:ff:ff:ff</span>
<span class="c">#        inet 192.168.3.236/24 brd 192.168.3.255 scope global eth1</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#        inet6 fe80::8e8:acff:fe41:8cab/64 scope link</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    5: eniea7f0ec96dd@if3: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 9001 qdisc noqueue state UP group default</span>
<span class="c">#        link/ether 8e:d8:57:e7:02:9c brd ff:ff:ff:ff:ff:ff link-netns cni-508d7216-7484-4497-a023-c6236435d535</span>
<span class="c">#        inet6 fe80::8cd8:57ff:fee7:29c/64 scope link</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="nv">$ </span>ip route <span class="c"># 혹은 route -n</span>
<span class="c"># =&gt; default via 192.168.3.1 dev eth0</span>
<span class="c">#    169.254.169.254 dev eth0</span>
<span class="c">#    192.168.3.0/24 dev eth0 proto kernel scope link src 192.168.3.235</span>
<span class="c">#    192.168.3.82 dev enid438418b082 scope link</span>
<span class="c">#    192.168.3.246 dev eniea7f0ec96dd scope link</span>
  
<span class="c"># 마지막 생성된 네임스페이스 정보 출력 -t net(네트워크 타입)</span>
<span class="nv">$ </span><span class="nb">sudo </span>lsns <span class="nt">-o</span> PID,COMMAND <span class="nt">-t</span> net | <span class="nb">awk</span> <span class="s1">'NR&gt;2 {print $1}'</span> | <span class="nb">tail</span> <span class="nt">-n</span> 1
<span class="c"># =&gt; 9774</span>
  
<span class="c"># 마지막 생성된 네임스페이스 net PID 정보 출력 -t net(네트워크 타입)를 변수 지정</span>
<span class="nv">$ MyPID</span><span class="o">=</span><span class="si">$(</span><span class="nb">sudo </span>lsns <span class="nt">-o</span> PID,COMMAND <span class="nt">-t</span> net | <span class="nb">awk</span> <span class="s1">'NR&gt;2 {print $1}'</span> | <span class="nb">tail</span> <span class="nt">-n</span> 1<span class="si">)</span>
  
<span class="c"># PID 정보로 파드 정보 확인</span>
<span class="nv">$ </span><span class="nb">sudo </span>nsenter <span class="nt">-t</span> <span class="nv">$MyPID</span> <span class="nt">-n</span> ip <span class="nt">-c</span> addr
<span class="c"># =&gt; ...</span>
<span class="c">#    3: eth0@if5: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 9001 qdisc noqueue state UP group default</span>
<span class="c">#        link/ether 6a:d5:4d:a7:45:7e brd ff:ff:ff:ff:ff:ff link-netnsid 0</span>
<span class="c">#        inet 192.168.3.246/32 scope global eth0</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#        inet6 fe80::68d5:4dff:fea7:457e/64 scope link</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="nv">$ </span><span class="nb">sudo </span>nsenter <span class="nt">-t</span> <span class="nv">$MyPID</span> <span class="nt">-n</span> ip <span class="nt">-c</span> route
<span class="c"># =&gt; default via 169.254.1.1 dev eth0</span>
<span class="c">#    169.254.1.1 dev eth0 scope link</span>
  
<span class="nv">$ </span><span class="nb">exit</span>
<span class="nt">----------------</span>
</code></pre></div></div>

<ul>
  <li>테스트용 파드 접속(exec) 후 확인</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 테스트용 파드 접속(exec) 후 Shell 실행</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> <span class="nv">$PODNAME1</span> <span class="nt">--</span> zsh
<span class="c"># =&gt;  netshoot-pod-74b7555dc7-6qsvf  ~ </span>
  
<span class="c"># 아래부터는 pod-1 Shell 에서 실행 : 네트워크 정보 확인</span>
<span class="nt">----------------------------</span>
<span class="nv">$ </span>ip <span class="nt">-c</span> addr
<span class="c"># =&gt; ...</span>
<span class="c">#    3: eth0@if5: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 9001 qdisc noqueue state UP group default</span>
<span class="c">#        link/ether c6:cb:41:a0:85:57 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span>
<span class="c">#        inet 192.168.1.112/32 scope global eth0</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#        inet6 fe80::c4cb:41ff:fea0:8557/64 scope link</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="nv">$ </span>ip <span class="nt">-c</span> route
<span class="c"># =&gt; default via 169.254.1.1 dev eth0</span>
<span class="c">#    169.254.1.1 dev eth0 scope link</span>
<span class="nv">$ </span>route <span class="nt">-n</span>
<span class="c"># =&gt; Kernel IP routing table</span>
<span class="c">#    Destination     Gateway         Genmask         Flags Metric Ref    Use Iface</span>
<span class="c">#    0.0.0.0         169.254.1.1     0.0.0.0         UG    0      0        0 eth0</span>
<span class="c">#    169.254.1.1     0.0.0.0         255.255.255.255 UH    0      0        0 eth0</span>
<span class="c">#$ ping -c 1 &lt;pod-2 IP&gt;</span>
<span class="nv">$ </span>ping <span class="nt">-c</span> 1 192.168.2.172
<span class="c"># =&gt; PING 192.168.2.172 (192.168.2.172) 56(84) bytes of data.</span>
<span class="c">#    64 bytes from 192.168.2.172: icmp_seq=1 ttl=125 time=1.25 ms</span>
<span class="c">#    </span>
<span class="c">#    --- 192.168.2.172 ping statistics ---</span>
<span class="c">#    1 packets transmitted, 1 received, 0% packet loss, time 0ms</span>
<span class="c">#    rtt min/avg/max/mdev = 1.249/1.249/1.249/0.000 ms</span>
<span class="nv">$ </span>ps
<span class="c"># =&gt; PID   USER     TIME  COMMAND</span>
<span class="c">#        1 root      0:00 tail -f /dev/null</span>
<span class="c">#      109 root      0:00 zsh</span>
<span class="c">#      186 root      0:00 ps</span>
<span class="nv">$ </span><span class="nb">cat</span> /etc/resolv.conf
<span class="c"># =&gt; search default.svc.cluster.local svc.cluster.local cluster.local ap-northeast-2.compute.internal</span>
<span class="c">#    nameserver 10.100.0.10</span>
<span class="c">#    options ndots:5</span>
<span class="nv">$ </span><span class="nb">exit</span>
<span class="nt">----------------------------</span>
  
<span class="c"># 파드2 Shell 실행</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> <span class="nv">$PODNAME2</span> <span class="nt">--</span> ip <span class="nt">-c</span> addr
<span class="c"># =&gt; ...</span>
<span class="c">#    3: eth0@if3: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 9001 qdisc noqueue state UP group default</span>
<span class="c">#        link/ether 02:ea:11:63:f6:9f brd ff:ff:ff:ff:ff:ff link-netnsid 0</span>
<span class="c">#        inet 192.168.2.172/32 scope global eth0</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#        inet6 fe80::ea:11ff:fe63:f69f/64 scope link</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
  
<span class="c"># 파드3 Shell 실행</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> <span class="nv">$PODNAME3</span> <span class="nt">--</span> ip <span class="nt">-br</span> <span class="nt">-c</span> addr
<span class="c"># =&gt; lo               UNKNOWN        127.0.0.1/8 ::1/128</span>
<span class="c">#    eth0@if5         UP             192.168.3.246/32 fe80::68d5:4dff:fea7:457e/64</span>
</code></pre></div></div>

<hr />

<h3 id="노드간-파드-통신">노드간 파드 통신</h3>

<ul>
  <li><strong>파드간 통신 흐름</strong> : AWS VPC CNI의 경우 별도의 오버레이(Overlay) 통신 기술없이, VPC에서 Native하게 파드간 직접 통신이 가능합니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w9/20241102_kans_w9_6.png" alt="img.png" class="image-center" /></p>

<ul>
  <li>파드간 통신 과정 참고</li>
</ul>

<p><img src="/assets/2024/kans-3th/w9/20241102_kans_w9_7.png" alt="img.png" class="image-center" />
<em class="image-caption"><a href="https://github.com/aws/amazon-vpc-cni-k8s/blob/master/docs/cni-proposal.md">https://github.com/aws/amazon-vpc-cni-k8s/blob/master/docs/cni-proposal.md</a></em></p>

<h4 id="실습-파드간-통신-테스트-및-확인--별도의-nat-동작-없이-통신-가능">[실습] 파드간 통신 테스트 및 확인 : 별도의 NAT 동작 없이 통신 가능!</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 파드 IP 변수 지정</span>
<span class="nv">$ PODIP1</span><span class="o">=</span><span class="si">$(</span>kubectl get pod <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>netshoot-pod <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.items[0].status.podIP<span class="o">}</span><span class="si">)</span>
<span class="nv">$ PODIP2</span><span class="o">=</span><span class="si">$(</span>kubectl get pod <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>netshoot-pod <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.items[1].status.podIP<span class="o">}</span><span class="si">)</span>
<span class="nv">$ PODIP3</span><span class="o">=</span><span class="si">$(</span>kubectl get pod <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>netshoot-pod <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.items[2].status.podIP<span class="o">}</span><span class="si">)</span>

<span class="c"># 파드1 Shell 에서 파드2로 ping 테스트</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> <span class="nv">$PODNAME1</span> <span class="nt">--</span> ping <span class="nt">-c</span> 2 <span class="nv">$PODIP2</span>
<span class="c"># =&gt; PING 192.168.2.172 (192.168.2.172) 56(84) bytes of data.</span>
<span class="c">#    64 bytes from 192.168.2.172: icmp_seq=1 ttl=125 time=0.915 ms</span>
<span class="c">#    64 bytes from 192.168.2.172: icmp_seq=2 ttl=125 time=0.870 ms</span>
<span class="c">#    </span>
<span class="c">#    --- 192.168.2.172 ping statistics ---</span>
<span class="c">#    2 packets transmitted, 2 received, 0% packet loss, time 1001ms</span>
<span class="c">#    rtt min/avg/max/mdev = 0.870/0.892/0.915/0.022 ms</span>

<span class="c"># 파드2 Shell 에서 파드3로 ping 테스트</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> <span class="nv">$PODNAME2</span> <span class="nt">--</span> ping <span class="nt">-c</span> 2 <span class="nv">$PODIP3</span>
<span class="c"># =&gt; PING 192.168.3.246 (192.168.3.246) 56(84) bytes of data.</span>
<span class="c">#    64 bytes from 192.168.3.246: icmp_seq=1 ttl=125 time=1.79 ms</span>
<span class="c">#    64 bytes from 192.168.3.246: icmp_seq=2 ttl=125 time=1.25 ms</span>
<span class="c">#    </span>
<span class="c">#    --- 192.168.3.246 ping statistics ---</span>
<span class="c">#    2 packets transmitted, 2 received, 0% packet loss, time 1002ms</span>
<span class="c">#    rtt min/avg/max/mdev = 1.245/1.516/1.788/0.271 ms</span>

<span class="c"># 파드3 Shell 에서 파드1로 ping 테스트</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> <span class="nv">$PODNAME3</span> <span class="nt">--</span> ping <span class="nt">-c</span> 2 <span class="nv">$PODIP1</span>
<span class="c"># =&gt; PING 192.168.1.112 (192.168.1.112) 56(84) bytes of data.</span>
<span class="c">#    64 bytes from 192.168.1.112: icmp_seq=1 ttl=125 time=1.08 ms</span>
<span class="c">#    64 bytes from 192.168.1.112: icmp_seq=2 ttl=125 time=1.23 ms</span>
<span class="c">#    </span>
<span class="c">#    --- 192.168.1.112 ping statistics ---</span>
<span class="c">#    2 packets transmitted, 2 received, 0% packet loss, time 1001ms</span>
<span class="c">#    rtt min/avg/max/mdev = 1.084/1.155/1.227/0.071 ms</span>

<span class="c"># 워커 노드 EC2 : TCPDUMP 확인</span>
<span class="c">## For Pod to external (outside VPC) traffic, we will program iptables to SNAT using Primary IP address on the Primary ENI.</span>
<span class="nv">$ </span><span class="nb">sudo </span>tcpdump <span class="nt">-i</span> any <span class="nt">-nn</span> icmp
<span class="c"># =&gt; 08:11:33.865634 IP 192.168.2.172 &amp;gt; 192.168.3.246: ICMP echo request, id 19, seq 1, length 64</span>
<span class="c">#    08:11:33.865684 IP 192.168.2.172 &amp;gt; 192.168.3.246: ICMP echo request, id 19, seq 1, length 64</span>
<span class="c">#    08:11:33.867117 IP 192.168.3.246 &amp;gt; 192.168.2.172: ICMP echo reply, id 19, seq 1, length 64</span>
<span class="c">#    08:11:33.867195 IP 192.168.3.246 &amp;gt; 192.168.2.172: ICMP echo reply, id 19, seq 1, length 64</span>
<span class="c">#    08:11:34.866610 IP 192.168.2.172 &amp;gt; 192.168.3.246: ICMP echo request, id 19, seq 2, length 64</span>
<span class="c">#    08:11:34.866654 IP 192.168.2.172 &amp;gt; 192.168.3.246: ICMP echo request, id 19, seq 2, length 64</span>
<span class="c">#    08:11:34.867865 IP 192.168.3.246 &amp;gt; 192.168.2.172: ICMP echo reply, id 19, seq 2, length 64</span>
<span class="c">#    08:11:34.867891 IP 192.168.3.246 &amp;gt; 192.168.2.172: ICMP echo reply, id 19, seq 2, length 64</span>
<span class="nv">$ </span><span class="nb">sudo </span>tcpdump <span class="nt">-i</span> eth1 <span class="nt">-nn</span> icmp
<span class="c"># &lt;span style="color: green;"&gt;👉 캡쳐된 패킷이 없습니다.&lt;/span&gt;</span>
<span class="nv">$ </span><span class="nb">sudo </span>tcpdump <span class="nt">-i</span> eth0 <span class="nt">-nn</span> icmp
<span class="c"># =&gt; 08:11:33.865689 IP 192.168.2.172 &amp;gt; 192.168.3.246: ICMP echo request, id 19, seq 1, length 64</span>
<span class="c">#    08:11:33.867117 IP 192.168.3.246 &amp;gt; 192.168.2.172: ICMP echo reply, id 19, seq 1, length 64</span>
<span class="c">#    08:11:34.866654 IP 192.168.2.172 &amp;gt; 192.168.3.246: ICMP echo request, id 19, seq 2, length 64</span>
<span class="c">#    08:11:34.867865 IP 192.168.3.246 &amp;gt; 192.168.2.172: ICMP echo reply, id 19, seq 2, length 64</span>
<span class="nv">$ </span><span class="nb">sudo </span>tcpdump <span class="nt">-i</span> eniYYYYYYYY <span class="nt">-nn</span> icmp
<span class="c"># =&gt; 08:11:33.865643 IP 192.168.2.172 &amp;gt; 192.168.3.246: ICMP echo request, id 19, seq 1, length 64</span>
<span class="c">#    08:11:33.867195 IP 192.168.3.246 &amp;gt; 192.168.2.172: ICMP echo reply, id 19, seq 1, length 64</span>
<span class="c">#    08:11:34.866610 IP 192.168.2.172 &amp;gt; 192.168.3.246: ICMP echo request, id 19, seq 2, length 64</span>
<span class="c">#    08:11:34.867891 IP 192.168.3.246 &amp;gt; 192.168.2.172: ICMP echo reply, id 19, seq 2, length 64</span>

<span class="c"># [워커 노드1]</span>
<span class="c"># routing policy database management 확인</span>
<span class="nv">$ </span>ip rule
<span class="c"># =&gt; 0: from all lookup local</span>
<span class="c">#    512: from all to 192.168.1.30 lookup main</span>
<span class="c">#    512: from all to 192.168.1.112 lookup main</span>
<span class="c">#    1024:  from all fwmark 0x80/0x80 lookup main</span>
<span class="c">#    32766: from all lookup main</span>
<span class="c">#    32767: from all lookup default</span>

<span class="c"># routing table management 확인</span>
<span class="nv">$ </span>ip route show table <span class="nb">local</span>
<span class="c"># =&gt; broadcast 127.0.0.0 dev lo proto kernel scope link src 127.0.0.1</span>
<span class="c">#    local 127.0.0.0/8 dev lo proto kernel scope host src 127.0.0.1</span>
<span class="c">#    local 127.0.0.1 dev lo proto kernel scope host src 127.0.0.1</span>
<span class="c">#    broadcast 127.255.255.255 dev lo proto kernel scope link src 127.0.0.1</span>
<span class="c">#    broadcast 192.168.1.0 dev eth0 proto kernel scope link src 192.168.1.186</span>
<span class="c">#    broadcast 192.168.1.0 dev eth1 proto kernel scope link src 192.168.1.9</span>
<span class="c">#    local 192.168.1.9 dev eth1 proto kernel scope host src 192.168.1.9</span>
<span class="c">#    local 192.168.1.186 dev eth0 proto kernel scope host src 192.168.1.186</span>
<span class="c">#    broadcast 192.168.1.255 dev eth0 proto kernel scope link src 192.168.1.186</span>
<span class="c">#    broadcast 192.168.1.255 dev eth1 proto kernel scope link src 192.168.1.9</span>

<span class="c"># 디폴트 네트워크 정보를 eth0 을 통해서 빠져나간다</span>
<span class="nv">$ </span>ip route show table main
<span class="c"># =&gt; default via 192.168.1.1 dev eth0</span>
<span class="c">#    ...</span>
</code></pre></div></div>

<hr />

<h3 id="파드에서-외부-통신">파드에서 외부 통신</h3>

<ul>
  <li>파드에서 외부 통신 흐름 : iptable 에 SNAT 을 통하여 노드의 eth0 IP로 변경되어서 외부와 통신합니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w9/20241102_kans_w9_8.png" alt="img.png" class="image-center" />
<em class="image-caption"><a href="https://github.com/aws/amazon-vpc-cni-k8s/blob/master/docs/cni-proposal.md">https://github.com/aws/amazon-vpc-cni-k8s/blob/master/docs/cni-proposal.md</a></em></p>

<ul>
  <li>VPC CNI 의 External source network address translation (<code class="language-plaintext highlighter-rouge">SNAT</code>) 설정에 따라, 외부(인터넷) 통신 시 <strong>SNAT</strong> 하거나 혹은 <strong>SNAT 없이</strong> 통신을 할 수 있다 - <a href="https://docs.aws.amazon.com/eks/latest/userguide/external-snat.html">링크</a></li>
</ul>

<h4 id="실습-파드에서-외부-통신-테스트-및-확인"><strong>[실습] 파드에서 외부 통신</strong> 테스트 및 확인</h4>

<ul>
  <li>파드 shell 실행 후 외부로 ping 테스트 &amp; 워커 노드에서 tcpdump 및 iptables 정보 확인</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 작업용 EC2 : pod-1 Shell 에서 외부로 ping</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> <span class="nv">$PODNAME1</span> <span class="nt">--</span> ping <span class="nt">-c</span> 1 www.google.com
<span class="c"># =&gt; PING www.google.com (172.217.25.164) 56(84) bytes of data.</span>
<span class="c">#    64 bytes from kix06s19-in-f4.1e100.net (172.217.25.164): icmp_seq=1 ttl=104 time=39.0 ms</span>
<span class="c">#    </span>
<span class="c">#    --- www.google.com ping statistics ---</span>
<span class="c">#    1 packets transmitted, 1 received, 0% packet loss, time 0ms</span>
<span class="c">#    rtt min/avg/max/mdev = 38.967/38.967/38.967/0.000 ms</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> <span class="nv">$PODNAME1</span> <span class="nt">--</span> ping <span class="nt">-i</span> 0.1 www.google.com
  
<span class="c"># 워커 노드 EC2 : TCPDUMP 확인</span>
<span class="nv">$ </span><span class="nb">sudo </span>tcpdump <span class="nt">-i</span> any <span class="nt">-nn</span> icmp
<span class="nv">$ </span><span class="nb">sudo </span>tcpdump <span class="nt">-i</span> eth0 <span class="nt">-nn</span> icmp
  
<span class="c"># 작업용 EC2 : 퍼블릭IP 확인</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="nv">$N1</span> <span class="nv">$N2</span> <span class="nv">$N3</span><span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> ssh ec2-user@<span class="nv">$i</span> curl <span class="nt">-s</span> ipinfo.io/ip<span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
  
<span class="c"># 작업용 EC2 : pod-1 Shell 에서 외부 접속 확인 - 공인IP는 어떤 주소인가?</span>
<span class="c">## The right way to check the weather - [링크](https://github.com/chubin/wttr.in)</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="nv">$PODNAME1</span> <span class="nv">$PODNAME2</span> <span class="nv">$PODNAME3</span><span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; Pod : </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> kubectl <span class="nb">exec</span> <span class="nt">-it</span> <span class="nv">$i</span> <span class="nt">--</span> curl <span class="nt">-s</span> ipinfo.io/ip<span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> <span class="nv">$PODNAME1</span> <span class="nt">--</span> curl <span class="nt">-s</span> wttr.in/seoul
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> <span class="nv">$PODNAME1</span> <span class="nt">--</span> curl <span class="nt">-s</span> wttr.in/seoul?format<span class="o">=</span>3
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> <span class="nv">$PODNAME1</span> <span class="nt">--</span> curl <span class="nt">-s</span> wttr.in/Moon
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> <span class="nv">$PODNAME1</span> <span class="nt">--</span> curl <span class="nt">-s</span> wttr.in/:help
  
<span class="c"># 워커 노드 EC2</span>
<span class="nv">$ </span>ip rule
<span class="nv">$ </span>ip route show table main
<span class="nv">$ </span><span class="nb">sudo </span>iptables <span class="nt">-L</span> <span class="nt">-n</span> <span class="nt">-v</span> <span class="nt">-t</span> nat
<span class="nv">$ </span><span class="nb">sudo </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span>
  
<span class="c"># 파드가 외부와 통신시에는 아래 처럼 'AWS-SNAT-CHAIN-0' 룰(rule)에 의해서 SNAT 되어서 외부와 통신!</span>
<span class="c"># 참고로 뒤 IP는 eth0(ENI 첫번째)의 IP 주소입니다.</span>
<span class="c"># --random-fully 동작 - [링크1](https://ssup2.github.io/issue/Linux_TCP_SYN_Packet_Drop_SNAT_Port_Race_Condition/)  [링크2](https://ssup2.github.io/issue/Kubernetes_TCP_Connection_Delay_VXLAN_CNI_Plugin/)</span>
<span class="nv">$ </span><span class="nb">sudo </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span> | <span class="nb">grep</span> <span class="s1">'A AWS-SNAT-CHAIN'</span>
<span class="c"># =&gt; -A AWS-SNAT-CHAIN-0 -d 192.168.0.0/16 -m comment --comment &amp;quot;AWS SNAT CHAIN&amp;quot; -j RETURN</span>
<span class="c">#    -A AWS-SNAT-CHAIN-0 ! -o vlan+ -m comment --comment &amp;quot;AWS, SNAT&amp;quot; -m addrtype ! --dst-type LOCAL -j SNAT --to-source 192.168.2.92 --random-fully</span>
  
<span class="c">## 아래 'mark 0x4000/0x4000' 매칭되지 않아서 RETURN 됨!</span>
<span class="c"># =&gt; -A KUBE-POSTROUTING -m mark ! --mark 0x4000/0x4000 -j RETURN</span>
<span class="c">#    -A KUBE-POSTROUTING -j MARK --set-xmark 0x4000/0x0</span>
<span class="c">#    -A KUBE-POSTROUTING -m comment --comment &amp;quot;kubernetes service traffic requiring SNAT&amp;quot; -j MASQUERADE --random-fully</span>
<span class="c">#    ...</span>
  
<span class="c"># 카운트 확인 시 AWS-SNAT-CHAIN-0에 매칭되어, 목적지가 192.168.0.0/16 아니고 외부 빠져나갈때 SNAT 192.168.1.251(EC2 노드1 IP) 변경되어 나갑니다.</span>
<span class="nv">$ </span><span class="nb">sudo </span>iptables <span class="nt">-t</span> filter <span class="nt">--zero</span><span class="p">;</span> <span class="nb">sudo </span>iptables <span class="nt">-t</span> nat <span class="nt">--zero</span><span class="p">;</span> <span class="nb">sudo </span>iptables <span class="nt">-t</span> mangle <span class="nt">--zero</span><span class="p">;</span> <span class="nb">sudo </span>iptables <span class="nt">-t</span> raw <span class="nt">--zero</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s1">'sudo iptables -v --numeric --table nat --list AWS-SNAT-CHAIN-0; echo ; sudo iptables -v --numeric --table nat --list KUBE-POSTROUTING; echo ; sudo iptables -v --numeric --table nat --list POSTROUTING'</span>
<span class="c"># =&gt; Chain AWS-SNAT-CHAIN-0 (1 references)</span>
<span class="c">#     pkts bytes target     prot opt in     out     source               destination</span>
<span class="c">#       12  1106 RETURN     all  --  *      *       0.0.0.0/0            192.168.0.0/16  /* AWS SNAT CHAIN */</span>
<span class="c">#       31  1924 SNAT       all  --  *      !vlan+  0.0.0.0/0            0.0.0.0/0            /* AWS, SNAT */ ADDRTYPE match dst-type !LOCAL to:192.168.2.92 random-fully</span>
<span class="c">#    </span>
<span class="c">#    Chain KUBE-POSTROUTING (1 references)</span>
<span class="c">#     pkts bytes target     prot opt in     out     source               destination</span>
<span class="c">#       53  3630 RETURN     all  --  *      *       0.0.0.0/0            0.0.0.0/0            mark match ! 0x4000/0x4000</span>
<span class="c">#        0     0 MARK       all  --  *      *       0.0.0.0/0            0.0.0.0/0            MARK xor 0x4000</span>
<span class="c">#        0     0 MASQUERADE  all  --  * * 0.0.0.0/0            0.0.0.0/0            /* kubernetes service traffic requiring SNAT */ random-fully</span>
<span class="c">#    </span>
<span class="c">#    Chain POSTROUTING (policy ACCEPT 22 packets, 1706 bytes)</span>
<span class="c">#     pkts bytes target     prot opt in     out     source               destination</span>
<span class="c">#       53  3630 KUBE-POSTROUTING  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes postrouting rules */</span>
<span class="c">#       53  3630 AWS-SNAT-CHAIN-0  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* AWS SNAT CHAIN */</span>
  
<span class="c"># conntrack 확인</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="nv">$N1</span> <span class="nv">$N2</span> <span class="nv">$N3</span><span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> ssh ec2-user@<span class="nv">$i</span> <span class="nb">sudo </span>conntrack <span class="nt">-L</span> <span class="nt">-n</span> |grep <span class="nt">-v</span> <span class="s1">'169.254.169'</span><span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; &amp;gt;&amp;gt; node 192.168.1.186 &amp;lt;&amp;lt;</span>
<span class="c">#    udp      17 29 src=192.168.1.186 dst=146.56.40.151 sport=50772 dport=123 src=146.56.40.151 dst=192.168.1.186 sport=123 dport=47629 mark=128 use=1</span>
<span class="c">#    tcp      6 86395 ESTABLISHED src=192.168.1.186 dst=52.95.197.175 sport=45832 dport=443 src=52.95.197.175 dst=192.168.1.186 sport=443 dport=30346 [ASSURED] mark=128 use=1</span>
<span class="c">#    tcp      6 86395 ESTABLISHED src=192.168.1.186 dst=52.95.195.121 sport=48862 dport=443 src=52.95.195.121 dst=192.168.1.186 sport=443 dport=41664 [ASSURED] mark=128 use=1</span>
<span class="c">#    conntrack v1.4.4 (conntrack-tools): 59 flow entries have been shown.</span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; node 192.168.2.92 &amp;lt;&amp;lt;</span>
<span class="c">#    conntrack v1.4.4 (conntrack-tools): 50 flow entries have been shown.</span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; node 192.168.3.235 &amp;lt;&amp;lt;</span>
<span class="c">#    conntrack v1.4.4 (conntrack-tools): 51 flow entries have been shown.</span>
<span class="c">#    udp      17 23 src=192.168.3.235 dst=146.56.40.151 sport=51849 dport=123 src=146.56.40.151 dst=192.168.3.235 sport=123 dport=29777 mark=128 use=1</span>
</code></pre></div></div>

<ul>
  <li>다음 실습을 위해서 파드 삭제</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl delete deploy netshoot-pod
<span class="c"># =&gt; deployment.apps &amp;quot;netshoot-pod&amp;quot; deleted</span>
</code></pre></div></div>

<hr />

<h3 id="노드의-파드-생성-갯수-제한">노드의 파드 생성 갯수 제한</h3>

<h4 id="사전-준비--kube-ops-view-설치">사전 준비 : kube-ops-view 설치</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># kube-ops-view</span>
<span class="nv">$ </span>helm repo add geek-cookbook https://geek-cookbook.github.io/charts/
<span class="c"># =&gt; &amp;quot;geek-cookbook&amp;quot; has been added to your repositories</span>
<span class="nv">$ </span>helm <span class="nb">install </span>kube-ops-view geek-cookbook/kube-ops-view <span class="nt">--version</span> 1.2.2 <span class="nt">--set</span> service.main.type<span class="o">=</span>LoadBalancer <span class="nt">--set</span> env.TZ<span class="o">=</span><span class="s2">"Asia/Seoul"</span> <span class="nt">--namespace</span> kube-system
<span class="c"># =&gt; NAME: kube-ops-view</span>
<span class="c">#    LAST DEPLOYED: Sat Oct  1 17:30:33 2024</span>
<span class="c">#    NAMESPACE: kube-system</span>
<span class="c">#    STATUS: deployed</span>
<span class="c">#    REVISION: 1</span>
<span class="c">#    TEST SUITE: None</span>
<span class="c">#    NOTES:</span>
<span class="c">#    1. Get the application URL by running these commands:</span>
<span class="c">#         NOTE: It may take a few minutes for the LoadBalancer IP to be available.</span>
<span class="c">#               You can watch the status of by running 'kubectl get svc -w kube-ops-view'</span>
<span class="c">#      export SERVICE_IP=$(kubectl get svc --namespace kube-system kube-ops-view -o jsonpath='{.status.loadBalancer.ingress[0].ip}')</span>
<span class="c">#      echo http://$SERVICE_IP:8080</span>

<span class="c"># kube-ops-view 접속 URL 확인 (1.5 배율)</span>
<span class="nv">$ </span>kubectl get svc <span class="nt">-n</span> kube-system kube-ops-view <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.status.loadBalancer.ingress[0].hostname<span class="o">}</span> | <span class="nb">awk</span> <span class="s1">'{ print "KUBE-OPS-VIEW URL = http://"$1":8080/#scale=1.5"}'</span>
<span class="c"># =&gt; KUBE-OPS-VIEW URL = http://ae19b387d6fee48239f44d3f9f121378-262130081.ap-northeast-2.elb.amazonaws.com:8080/#scale=1.5</span>
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w9/20241102_kans_w9_29.png" alt="img.png" class="image-center" /></p>

<ul>
  <li><strong>Secondary IPv4 addresses</strong> (기본값) : 인스턴스 유형에 최대 ENI 갯수와 할당 가능 IP 수를 조합하여 선정</li>
</ul>

<h4 id="워커-노드의-인스턴스-타입-별-파드-생성-갯수-제한">워커 노드의 인스턴스 타입 별 파드 생성 갯수 제한</h4>

<ul>
  <li><strong>인스턴스 타입</strong> 별 ENI 최대 갯수와 할당 가능한 최대 IP 갯수에 따라서 파드 배치 갯수가 결정됨</li>
  <li>단, aws-node 와 kube-proxy 파드는 호스트의 IP를 사용함으로 최대 갯수에서 제외함</li>
</ul>

<p><img src="/assets/2024/kans-3th/w9/20241102_kans_w9_9.png" alt="img.png" class="image-center w-90" /></p>

<blockquote>
  <p>👉 최대 파드 생성 갯수 : <code class="language-plaintext highlighter-rouge">(Number of network interfaces for the instance type × (the number of IP addressess per network interface - 1)) + 2</code></p>
</blockquote>

<h4 id="워커-노드의-인스턴스-정보-확인--t3medium-사용-시">워커 노드의 인스턴스 정보 확인 : t3.medium 사용 시</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># t3 타입의 정보(필터) 확인</span>
<span class="nv">$ </span>aws ec2 describe-instance-types <span class="nt">--filters</span> <span class="nv">Name</span><span class="o">=</span>instance-type,Values<span class="o">=</span>t3.<span class="k">*</span> <span class="se">\</span>
  <span class="nt">--query</span> <span class="s2">"InstanceTypes[].{Type: InstanceType, MaxENI: NetworkInfo.MaximumNetworkInterfaces, IPv4addr: NetworkInfo.Ipv4AddressesPerInterface}"</span> <span class="se">\</span>
  <span class="nt">--output</span> table
<span class="c"># =&gt; --------------------------------------</span>
<span class="c">#    |        DescribeInstanceTypes       |</span>
<span class="c">#    +----------+----------+--------------+</span>
<span class="c">#    | IPv4addr | MaxENI   |    Type      |</span>
<span class="c">#    +----------+----------+--------------+</span>
<span class="c">#    |  12      |  3       |  t3.large    |</span>
<span class="c">#    |  6       |  3       |  t3.medium   |</span>
<span class="c">#    |  15      |  4       |  t3.2xlarge  |</span>
<span class="c">#    |  15      |  4       |  t3.xlarge   |</span>
<span class="c">#    |  2       |  2       |  t3.micro    |</span>
<span class="c">#    |  2       |  2       |  t3.nano     |</span>
<span class="c">#    |  4       |  3       |  t3.small    |</span>
<span class="c">#    +----------+----------+--------------+</span>

<span class="c"># c5 타입의 정보(필터) 확인</span>
<span class="nv">$ </span>aws ec2 describe-instance-types <span class="nt">--filters</span> <span class="nv">Name</span><span class="o">=</span>instance-type,Values<span class="o">=</span>c5<span class="k">*</span>.<span class="k">*</span> <span class="se">\</span>
  <span class="nt">--query</span> <span class="s2">"InstanceTypes[].{Type: InstanceType, MaxENI: NetworkInfo.MaximumNetworkInterfaces, IPv4addr: NetworkInfo.Ipv4AddressesPerInterface}"</span> <span class="se">\</span>
  <span class="nt">--output</span> table
<span class="c"># =&gt; ----------------------------------------</span>
<span class="c">#    |         DescribeInstanceTypes        |</span>
<span class="c">#    +----------+----------+----------------+</span>
<span class="c">#    | IPv4addr | MaxENI   |     Type       |</span>
<span class="c">#    +----------+----------+----------------+</span>
<span class="c">#    |  30      |  8       |  c5d.12xlarge  |</span>
<span class="c">#    |  10      |  3       |  c5d.large     |</span>
<span class="c">#    |  10      |  3       |  c5n.large     |</span>
<span class="c">#    |  15      |  4       |  c5n.2xlarge   |</span>
<span class="c">#    |  30      |  8       |  c5.4xlarge    |</span>
<span class="c">#    |  15      |  4       |  c5a.xlarge    |</span>
<span class="c">#    |  30      |  8       |  c5a.4xlarge   |</span>
<span class="c">#    |  15      |  4       |  c5d.2xlarge   |</span>
<span class="c">#    |  50      |  15      |  c5d.24xlarge  |</span>
<span class="c">#    |  30      |  8       |  c5.12xlarge   |</span>
<span class="c">#    |  30      |  8       |  c5n.9xlarge   |</span>
<span class="c">#    |  30      |  8       |  c5a.12xlarge  |</span>
<span class="c">#    |  15      |  4       |  c5.xlarge     |</span>
<span class="c">#    |  30      |  8       |  c5n.4xlarge   |</span>
<span class="c">#    |  50      |  15      |  c5n.18xlarge  |</span>
<span class="c">#    |  50      |  15      |  c5.24xlarge   |</span>
<span class="c">#    |  30      |  8       |  c5d.4xlarge   |</span>
<span class="c">#    |  30      |  8       |  c5.9xlarge    |</span>
<span class="c">#    |  15      |  4       |  c5a.2xlarge   |</span>
<span class="c">#    |  50      |  15      |  c5.metal      |</span>
<span class="c">#    |  50      |  15      |  c5a.24xlarge  |</span>
<span class="c">#    |  50      |  15      |  c5d.18xlarge  |</span>
<span class="c">#    |  50      |  15      |  c5a.16xlarge  |</span>
<span class="c">#    |  30      |  8       |  c5a.8xlarge   |</span>
<span class="c">#    |  50      |  15      |  c5n.metal     |</span>
<span class="c">#    |  50      |  15      |  c5d.metal     |</span>
<span class="c">#    |  10      |  3       |  c5.large      |</span>
<span class="c">#    |  15      |  4       |  c5.2xlarge    |</span>
<span class="c">#    |  15      |  4       |  c5d.xlarge    |</span>
<span class="c">#    |  10      |  3       |  c5a.large     |</span>
<span class="c">#    |  50      |  15      |  c5.18xlarge   |</span>
<span class="c">#    |  30      |  8       |  c5d.9xlarge   |</span>
<span class="c">#    |  15      |  4       |  c5n.xlarge    |</span>
<span class="c">#    +----------+----------+----------------+  </span>

<span class="c"># 파드 사용 가능 계산 예시 : aws-node 와 kube-proxy 파드는 host-networking 사용으로 IP 2개 남음</span>
<span class="c"># ((MaxENI * (IPv4addr-1)) + 2)</span>
<span class="c"># t3.medium 경우 : ((3 * (6 - 1) + 2 ) = 17개 &gt;&gt; aws-node 와 kube-proxy 2개 제외하면 15개</span>

<span class="c"># 워커노드 상세 정보 확인 : 노드 상세 정보의 Allocatable 에 pods 에 17개 정보 확인</span>
<span class="nv">$ </span>kubectl describe node | <span class="nb">grep </span>Allocatable: <span class="nt">-A6</span>
<span class="c"># =&gt; Allocatable:</span>
<span class="c">#      cpu:                1930m</span>
<span class="c">#      ephemeral-storage:  27905944324</span>
<span class="c">#      hugepages-1Gi:      0</span>
<span class="c">#      hugepages-2Mi:      0</span>
<span class="c">#      memory:             3388304Ki</span>
<span class="c">#      pods:               17</span>
<span class="c">#    ...</span>
</code></pre></div></div>

<h4 id="최대-파드-생성-및-확인">최대 파드 생성 및 확인</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 워커 노드 EC2 - 모니터링</span>
<span class="nv">$ </span><span class="k">while </span><span class="nb">true</span><span class="p">;</span> <span class="k">do </span>ip <span class="nt">-br</span> <span class="nt">-c</span> addr show <span class="o">&amp;&amp;</span> <span class="nb">echo</span> <span class="s2">"--------------"</span> <span class="p">;</span> <span class="nb">date</span> <span class="s2">"+%Y-%m-%d %H:%M:%S"</span> <span class="p">;</span> <span class="nb">sleep </span>1<span class="p">;</span> <span class="k">done</span>

<span class="c"># 작업용 EC2 - 터미널1</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s1">'kubectl get pods -o wide'</span>

<span class="c"># 작업용 EC2 - 터미널2</span>
<span class="c"># 디플로이먼트 생성</span>
<span class="nv">$ </span>curl <span class="nt">-s</span> <span class="nt">-O</span> https://raw.githubusercontent.com/gasida/PKOS/main/2/nginx-dp.yaml
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> nginx-dp.yaml
<span class="c"># =&gt; deployment.apps/nginx-deployment created</span>

<span class="c"># 파드 확인</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-o</span> wide
<span class="nv">$ </span>kubectl get pod <span class="nt">-o</span><span class="o">=</span>custom-columns<span class="o">=</span>NAME:.metadata.name,IP:.status.podIP
<span class="c"># =&gt; NAME                                IP</span>
<span class="c">#    nginx-deployment-6f999cfffb-44rw7   192.168.1.230</span>
<span class="c">#    nginx-deployment-6f999cfffb-xk4nh   192.168.3.246</span>

<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="nv">$N1</span> <span class="nv">$N2</span> <span class="nv">$N3</span><span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> ssh ec2-user@<span class="nv">$i</span> <span class="nb">sudo </span>ip <span class="nt">-brief</span> addr | <span class="nb">wc</span> <span class="nt">-l</span> <span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; &amp;gt;&amp;gt; node 192.168.1.186 &amp;lt;&amp;lt;</span>
<span class="c">#    5</span>
<span class="c">#    &amp;gt;&amp;gt; node 192.168.2.92 &amp;lt;&amp;lt;</span>
<span class="c">#    4</span>
<span class="c">#    &amp;gt;&amp;gt; node 192.168.3.235 &amp;lt;&amp;lt;</span>
<span class="c">#    5</span>

<span class="c"># 파드 증가 테스트 &gt;&gt; 파드 정상 생성 확인, 워커 노드에서 eth, eni 갯수 확인</span>
<span class="nv">$ </span>kubectl scale deployment nginx-deployment <span class="nt">--replicas</span><span class="o">=</span>8
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="nv">$N1</span> <span class="nv">$N2</span> <span class="nv">$N3</span><span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> ssh ec2-user@<span class="nv">$i</span> <span class="nb">sudo </span>ip <span class="nt">-brief</span> addr | <span class="nb">wc</span> <span class="nt">-l</span> <span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; &amp;gt;&amp;gt; node 192.168.1.186 &amp;lt;&amp;lt;</span>
<span class="c">#    7</span>
<span class="c">#    &amp;gt;&amp;gt; node 192.168.2.92 &amp;lt;&amp;lt;</span>
<span class="c">#    6</span>
<span class="c">#    &amp;gt;&amp;gt; node 192.168.3.235 &amp;lt;&amp;lt;</span>
<span class="c">#    7</span>

<span class="c"># 파드 증가 테스트 &gt;&gt; 파드 정상 생성 확인, 워커 노드에서 eth, eni 갯수 확인 &gt;&gt; 어떤일이 벌어졌는가?</span>
<span class="nv">$ </span>kubectl scale deployment nginx-deployment <span class="nt">--replicas</span><span class="o">=</span>15
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="nv">$N1</span> <span class="nv">$N2</span> <span class="nv">$N3</span><span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> ssh ec2-user@<span class="nv">$i</span> <span class="nb">sudo </span>ip <span class="nt">-brief</span> addr | <span class="nb">wc</span> <span class="nt">-l</span> <span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; &amp;gt;&amp;gt; node 192.168.1.186 &amp;lt;&amp;lt;</span>
<span class="c">#    9</span>
<span class="c">#    &amp;gt;&amp;gt; node 192.168.2.92 &amp;lt;&amp;lt;</span>
<span class="c">#    9</span>
<span class="c">#    &amp;gt;&amp;gt; node 192.168.3.235 &amp;lt;&amp;lt;</span>
<span class="c">#    10</span>

<span class="c"># 파드 증가 테스트 &gt;&gt; 파드 정상 생성 확인, 워커 노드에서 eth, eni 갯수 확인 &gt;&gt; 어떤일이 벌어졌는가?</span>
<span class="nv">$ </span>kubectl scale deployment nginx-deployment <span class="nt">--replicas</span><span class="o">=</span>30
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="nv">$N1</span> <span class="nv">$N2</span> <span class="nv">$N3</span><span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> ssh ec2-user@<span class="nv">$i</span> <span class="nb">sudo </span>ip <span class="nt">-brief</span> addr | <span class="nb">wc</span> <span class="nt">-l</span> <span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; &amp;gt;&amp;gt; node 192.168.1.186 &amp;lt;&amp;lt;</span>
<span class="c">#    15</span>
<span class="c">#    &amp;gt;&amp;gt; node 192.168.2.92 &amp;lt;&amp;lt;</span>
<span class="c">#    15</span>
<span class="c">#    &amp;gt;&amp;gt; node 192.168.3.235 &amp;lt;&amp;lt;</span>
<span class="c">#    15</span>

<span class="c"># 파드 증가 테스트 &gt;&gt; 파드 정상 생성 확인, 워커 노드에서 eth, eni 갯수 확인 &gt;&gt; 어떤일이 벌어졌는가?</span>
<span class="nv">$ </span>kubectl scale deployment nginx-deployment <span class="nt">--replicas</span><span class="o">=</span>50
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="nv">$N1</span> <span class="nv">$N2</span> <span class="nv">$N3</span><span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> ssh ec2-user@<span class="nv">$i</span> <span class="nb">sudo </span>ip <span class="nt">-brief</span> addr | <span class="nb">wc</span> <span class="nt">-l</span> <span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; &amp;gt;&amp;gt; node 192.168.1.186 &amp;lt;&amp;lt;</span>
<span class="c">#    19</span>
<span class="c">#    &amp;gt;&amp;gt; node 192.168.2.92 &amp;lt;&amp;lt;</span>
<span class="c">#    19</span>
<span class="c">#    &amp;gt;&amp;gt; node 192.168.3.235 &amp;lt;&amp;lt;</span>
<span class="c">#    19</span>

<span class="c"># 파드 생성 실패!</span>
<span class="nv">$ </span>kubectl get pods | <span class="nb">grep </span>Pending
<span class="c"># =&gt; nginx-deployment-6f999cfffb-59cj6   0/1     Pending   0          69s</span>
<span class="c">#    nginx-deployment-6f999cfffb-5z5qg   0/1     Pending   0          69s</span>
<span class="c">#    nginx-deployment-6f999cfffb-6gk4m   0/1     Pending   0          69s</span>
<span class="c">#    nginx-deployment-6f999cfffb-bhzc8   0/1     Pending   0          69s</span>
<span class="c">#    nginx-deployment-6f999cfffb-hlns8   0/1     Pending   0          69s</span>
<span class="c">#    nginx-deployment-6f999cfffb-n64nc   0/1     Pending   0          69s</span>
<span class="c">#    nginx-deployment-6f999cfffb-w8x7g   0/1     Pending   0          69s</span>
<span class="c">#    nginx-deployment-6f999cfffb-zb2pk   0/1     Pending   0          69s</span>

<span class="c">#$ kubectl describe pod &lt;Pending 파드&gt; | grep Events: -A5</span>
<span class="nv">$ </span>kubectl describe pod nginx-deployment-6f999cfffb-59cj6 | <span class="nb">grep </span>Events: <span class="nt">-A5</span>
<span class="c"># =&gt; Events:</span>
<span class="c">#      Type     Reason            Age   From               Message</span>
<span class="c">#      ----     ------            ----  ----               -------</span>
<span class="c">#      Warning  FailedScheduling  101s  default-scheduler  0/3 nodes are available: 3 Too many pods. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod.</span>
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w9/20241102_kans_w9_30.png" alt="img.png" class="image-center" /></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 디플로이먼트 삭제</span>
<span class="nv">$ </span>kubectl delete deploy nginx-deployment
<span class="c"># =&gt; deployment.apps &amp;quot;nginx-deployment&amp;quot; deleted</span>
</code></pre></div></div>

<ul>
  <li>👉 해결 방안 : IPv4 Prefix Delegation, WARM &amp; MIN IP/Prefix Targets, Custom Network</li>
</ul>

<p><img src="/assets/2024/kans-3th/w9/20241102_kans_w9_10.png" alt="img.png" class="image-center w-80" />
<em class="image-caption">IPv4 Prefix Delegation을 통한 IP 갯수 제한 해소</em></p>

<hr />

<h3 id="service--aws-loadbalancer-controller">Service &amp; AWS LoadBalancer Controller</h3>

<h4 id="서비스-종류">서비스 종류</h4>

<ul>
  <li>ClusterIP 타입</li>
</ul>

<p><img src="/assets/2024/kans-3th/w9/20241102_kans_w9_16.png" alt="img.png" class="image-center w-90" /></p>

<ul>
  <li>NodePort 타입</li>
</ul>

<p><img src="/assets/2024/kans-3th/w9/20241102_kans_w9_17.png" alt="img_1.png" class="image-center w-90" /></p>

<ul>
  <li>LoadBalancer 타입 (기본 모드) : NLB 인스턴스 유형</li>
</ul>

<p><img src="/assets/2024/kans-3th/w9/20241102_kans_w9_18.png" alt="img_2.png" class="image-center w-90" /></p>

<ul>
  <li>Service (LoadBalancer Controller) : AWS Load Balancer Controller + NLB IP 모드 동작 with AWS VPC CNI</li>
</ul>

<p><img src="/assets/2024/kans-3th/w9/20241102_kans_w9_19.png" alt="img_3.png" class="image-center w-90" /></p>

<h4 id="nlb-모드-전체-정리">NLB 모드 전체 정리</h4>

<h5 id="instance-mode">Instance mode</h5>

<p><img src="/assets/2024/kans-3th/w9/20241102_kans_w9_20.png" alt="img_4.png" class="image-center w-90" />
<em class="image-caption"><a href="https://aws.amazon.com/blogs/networking-and-content-delivery/deploying-aws-load-balancer-controller-on-amazon-eks/">https://aws.amazon.com/blogs/networking-and-content-delivery/deploying-aws-load-balancer-controller-on-amazon-eks/</a></em></p>

<ul>
  <li><strong>externalTrafficPolicy</strong>에 따른 동작은 다음과 같습니다.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">externalTrafficPolicy: ClusterIP</code> : 2번 분산 및 SNAT으로 Client IP 확인 불가능합니다. &lt;- LoadBalancer 타입 (기본 모드) 동작</li>
      <li><code class="language-plaintext highlighter-rouge">externalTrafficPolicy: Local</code> : 1번 분산 및 ClientIP가 유지되고, 워커 노드의 iptables을 사용합니다.
        <ul>
          <li>
            <p><strong>통신 흐름</strong> : 외부 클라이언트가 ‘로드밸런서’ 접속 시 부하분산 되어 노드 도달 후 iptables 룰로 목적지 파드와 통신됩니다.</p>

            <p><img src="/assets/2024/kans-3th/w9/20241102_kans_w9_21.png" alt="img_5.png" class="image-center w-90" />
<em class="image-caption"><code class="language-plaintext highlighter-rouge">externalTrafficPolicy: Local</code>일때의 통신흐름의 예</em></p>

            <p><img src="/assets/2024/kans-3th/w9/20241102_kans_w9_22.png" alt="img_6.png" class="image-center w-90" /></p>

            <ul>
              <li>노드는 외부에 공개되지 않고 로드밸런서만 외부에 공개되며, 외부 클라이언트는 로드밸랜서에 접속을 할 뿐 내부 노드의 정보를 알 수 없습니다.</li>
              <li>로드밸런서가 부하분산하여 파드가 존재하는 노드들에게 전달합니다. iptables 룰에서는 자신의 노드에 있는 파드만 연결합니다. (<code class="language-plaintext highlighter-rouge">externalTrafficPolicy: local</code>)</li>
              <li>DNAT가 2번 동작합니다. (1) 로드밸런서 접속 후 빠져 나갈때, (2) 노드의 iptables 룰에서 파드IP 전달 시</li>
              <li>외부 클라이언트의 IP가 보존됩니다. AWS NLB 는 <strong>타켓</strong>이 <strong>인스턴스</strong>일 경우 클라이언트 IP를 유지, iptables 룰 경우도 <code class="language-plaintext highlighter-rouge">externalTrafficPolicy: local</code> 로 클라이언트 IP를 보존합니다.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>부하분산 최적화</strong> : 노드에 파드가 없을 경우 ‘로드밸런서’에서 노드에 헬스 체크(상태 검사)가 실패하여 해당 노드로는 외부 요청 트래픽을 전달하지 않습니다.</p>

    <p><img src="/assets/2024/kans-3th/w9/20241102_kans_w9_23.png" alt="img_7.png" class="image-center w-90" />
<img src="/assets/2024/kans-3th/w9/20241102_kans_w9_26.png" alt="img.png" /></p>

    <p>위의 이미지 처럼 3번째 인스턴스(Node3)은 상태 확인이 실패한 경우, 해당 노드로는 외부 요청 트래픽 전달하지 않습니다.</p>
  </li>
</ul>

<h5 id="ip-mode">IP mode</h5>

<ul>
  <li>
    <p>IP 모드는 반드시 AWS LoadBalancer 컨트롤러 파드 및 정책 설정이 필요합니다.</p>

    <p><img src="/assets/2024/kans-3th/w9/20241102_kans_w9_24.png" alt="img_8.png" class="image-center w-90" />
<em class="image-caption"><a href="https://aws.amazon.com/blogs/networking-and-content-delivery/deploying-aws-load-balancer-controller-on-amazon-eks/">https://aws.amazon.com/blogs/networking-and-content-delivery/deploying-aws-load-balancer-controller-on-amazon-eks/</a></em></p>

    <ul>
      <li><code class="language-plaintext highlighter-rouge">Proxy Protocol v2 비활성화</code> ⇒ NLB에서 바로 파드로 인입되며, 단 ClientIP가 NLB로 SNAT 되어 Client IP 확인 불가능합니다.</li>
      <li><code class="language-plaintext highlighter-rouge">Proxy Protocol v2 활성화</code> ⇒ NLB에서 바로 파드로 인입 및 ClientIP 확인 가능합니다. 단, PPv2를 애플리케이션이 인지할 수 있게 설정이 필요합니다.</li>
    </ul>
  </li>
  <li>
    <p><strong>AWS LoadBalancer Controller 배포</strong> - <a href="https://kubernetes-sigs.github.io/aws-load-balancer-controller/latest/deploy/installation/"><strong>Link</strong></a></p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Helm Chart 설치</span>
<span class="nv">$ </span>helm repo add eks https://aws.github.io/eks-charts
<span class="c"># =&gt; &amp;quot;eks&amp;quot; has been added to your repositories</span>
<span class="nv">$ </span>helm repo update
<span class="c"># =&gt; Hang tight while we grab the latest from your chart repositories...</span>
<span class="c">#    ...Successfully got an update from the &amp;quot;eks&amp;quot; chart repository</span>
<span class="c">#    ...Successfully got an update from the &amp;quot;geek-cookbook&amp;quot; chart repository</span>
<span class="c">#    Update Complete. ⎈Happy Helming!⎈</span>
<span class="nv">$ </span>helm <span class="nb">install </span>aws-load-balancer-controller eks/aws-load-balancer-controller <span class="nt">-n</span> kube-system <span class="nt">--set</span> <span class="nv">clusterName</span><span class="o">=</span><span class="nv">$CLUSTER_NAME</span>
<span class="c"># =&gt; NAME: aws-load-balancer-controller</span>
<span class="c">#    LAST DEPLOYED: Sat Oct  1 17:45:09 2024</span>
<span class="c">#    NAMESPACE: kube-system</span>
<span class="c">#    STATUS: deployed</span>
<span class="c">#    REVISION: 1</span>
<span class="c">#    TEST SUITE: None</span>
<span class="c">#    NOTES:</span>
<span class="c">#    AWS Load Balancer controller installed!</span>
  
<span class="c">## 설치 확인</span>
<span class="nv">$ </span>kubectl get crd
<span class="c"># =&gt; NAME                                         CREATED AT</span>
<span class="c">#    ...</span>
<span class="c">#    ingressclassparams.elbv2.k8s.aws             2024-10-01T08:45:08Z</span>
<span class="c">#    ...</span>
<span class="c">#    targetgroupbindings.elbv2.k8s.aws            2024-10-01T08:45:08Z</span>
<span class="nv">$ </span>kubectl get deployment <span class="nt">-n</span> kube-system aws-load-balancer-controller
<span class="c"># =&gt; NAME                           READY   UP-TO-DATE   AVAILABLE   AGE</span>
<span class="c">#    aws-load-balancer-controller   2/2     2            2           58s</span>
<span class="nv">$ </span>kubectl describe deploy <span class="nt">-n</span> kube-system aws-load-balancer-controller
<span class="nv">$ </span>kubectl describe deploy <span class="nt">-n</span> kube-system aws-load-balancer-controller | <span class="nb">grep</span> <span class="s1">'Service Account'</span>
<span class="c"># =&gt;   Service Account:  aws-load-balancer-controller</span>
  
<span class="c"># 클러스터롤, 롤 확인</span>
<span class="nv">$ </span>kubectl describe clusterrolebindings.rbac.authorization.k8s.io aws-load-balancer-controller-rolebinding
<span class="nv">$ </span>kubectl describe clusterroles.rbac.authorization.k8s.io aws-load-balancer-controller-role
<span class="c"># =&gt; ...</span>
<span class="c">#    PolicyRule:</span>
<span class="c">#      Resources                                     Non-Resource URLs  Resource Names  Verbs</span>
<span class="c">#      ---------                                     -----------------  --------------  -----</span>
<span class="c">#      targetgroupbindings.elbv2.k8s.aws             []                 []              [create delete get list patch update watch]</span>
<span class="c">#      events                                        []                 []              [create patch]</span>
<span class="c">#      configmaps                                    []                 []              [get delete create update]</span>
<span class="c">#      ingresses                                     []                 []              [get list patch update watch]</span>
<span class="c">#      services                                      []                 []              [get list patch update watch]</span>
<span class="c">#      ingresses.extensions                          []                 []              [get list patch update watch]</span>
<span class="c">#      services.extensions                           []                 []              [get list patch update watch]</span>
<span class="c">#      ingresses.networking.k8s.io                   []                 []              [get list patch update watch]</span>
<span class="c">#      services.networking.k8s.io                    []                 []              [get list patch update watch]</span>
<span class="c">#      endpoints                                     []                 []              [get list watch]</span>
<span class="c">#      namespaces                                    []                 []              [get list watch]</span>
<span class="c">#      nodes                                         []                 []              [get list watch]</span>
<span class="c">#      pods                                          []                 []              [get list watch]</span>
<span class="c">#      endpointslices.discovery.k8s.io               []                 []              [get list watch]</span>
<span class="c">#      ingressclassparams.elbv2.k8s.aws              []                 []              [get list watch]</span>
<span class="c">#      ingressclasses.networking.k8s.io              []                 []              [get list watch]</span>
<span class="c">#      ingresses/status                              []                 []              [update patch]</span>
<span class="c">#      pods/status                                   []                 []              [update patch]</span>
<span class="c">#      services/status                               []                 []              [update patch]</span>
<span class="c">#      targetgroupbindings/status                    []                 []              [update patch]</span>
<span class="c">#      ingresses.elbv2.k8s.aws/status                []                 []              [update patch]</span>
<span class="c">#      pods.elbv2.k8s.aws/status                     []                 []              [update patch]</span>
<span class="c">#      services.elbv2.k8s.aws/status                 []                 []              [update patch]</span>
<span class="c">#      targetgroupbindings.elbv2.k8s.aws/status      []                 []              [update patch]</span>
<span class="c">#      ingresses.extensions/status                   []                 []              [update patch]</span>
<span class="c">#      pods.extensions/status                        []                 []              [update patch]</span>
<span class="c">#      services.extensions/status                    []                 []              [update patch]</span>
<span class="c">#      targetgroupbindings.extensions/status         []                 []              [update patch]</span>
<span class="c">#      ingresses.networking.k8s.io/status            []                 []              [update patch]</span>
<span class="c">#      pods.networking.k8s.io/status                 []                 []              [update patch]</span>
<span class="c">#      services.networking.k8s.io/status             []                 []              [update patch]</span>
<span class="c">#      targetgroupbindings.networking.k8s.io/status  []                 []              [update patch]</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>서비스/파드 배포 테스트 with NLB - <a href="https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.4/guide/service/nlb/">링크</a> <a href="https://docs.aws.amazon.com/eks/latest/userguide/network-load-balancing.html">NLB</a></p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 모니터링</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> kubectl get pod,svc,ep
  
<span class="c"># 작업용 EC2 - 디플로이먼트 &amp; 서비스 생성</span>
<span class="nv">$ </span>curl <span class="nt">-s</span> <span class="nt">-O</span> https://raw.githubusercontent.com/gasida/PKOS/main/2/echo-service-nlb.yaml
<span class="nv">$ </span><span class="nb">cat </span>echo-service-nlb.yaml
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> echo-service-nlb.yaml
<span class="c"># =&gt; deployment.apps/deploy-echo created</span>
<span class="c">#    service/svc-nlb-ip-type created</span>
  
<span class="c"># 확인</span>
<span class="nv">$ </span>kubectl get deploy,pod
<span class="c"># =&gt; NAME                          READY   UP-TO-DATE   AVAILABLE   AGE</span>
<span class="c">#    deployment.apps/deploy-echo   2/2     2            2           10s</span>
<span class="c">#    </span>
<span class="c">#    NAME                               READY   STATUS    RESTARTS   AGE</span>
<span class="c">#    pod/deploy-echo-857b6cfb88-452cd   1/1     Running   0          10s</span>
<span class="c">#    pod/deploy-echo-857b6cfb88-5xz5j   1/1     Running   0          10s</span>
<span class="nv">$ </span>kubectl get svc,ep,ingressclassparams,targetgroupbindings
<span class="c"># =&gt; NAME                      TYPE           CLUSTER-IP      EXTERNAL-IP                                                                         PORT(S)        AGE</span>
<span class="c">#    service/kubernetes        ClusterIP      10.100.0.1      &amp;lt;none&amp;gt;                                                                              443/TCP        76m</span>
<span class="c">#    service/svc-nlb-ip-type   LoadBalancer   10.100.83.184   k8s-default-svcnlbip-50f5d03f41-f28c6aa861a2e815.elb.ap-northeast-2.amazonaws.com   80:30771/TCP   21s</span>
<span class="c">#    </span>
<span class="c">#    NAME                        ENDPOINTS                              AGE</span>
<span class="c">#    endpoints/kubernetes        192.168.1.65:443,192.168.3.189:443     76m</span>
<span class="c">#    endpoints/svc-nlb-ip-type   192.168.1.112:8080,192.168.2.24:8080   21s</span>
<span class="c">#    </span>
<span class="c">#    NAME                                   GROUP-NAME   SCHEME   IP-ADDRESS-TYPE   AGE</span>
<span class="c">#    ingressclassparams.elbv2.k8s.aws/alb                                           3m9s</span>
<span class="c">#    </span>
<span class="c">#    NAME                                                               SERVICE-NAME      SERVICE-PORT   TARGET-TYPE   AGE</span>
<span class="c">#    targetgroupbinding.elbv2.k8s.aws/k8s-default-svcnlbip-f58255c318   svc-nlb-ip-type   80             ip            16s</span>
<span class="nv">$ </span>kubectl get targetgroupbindings <span class="nt">-o</span> json | jq
<span class="c"># =&gt; {</span>
<span class="c">#      &amp;quot;apiVersion&amp;quot;: &amp;quot;v1&amp;quot;,</span>
<span class="c">#      &amp;quot;items&amp;quot;: [</span>
<span class="c">#        {</span>
<span class="c">#          &amp;quot;apiVersion&amp;quot;: &amp;quot;elbv2.k8s.aws/v1beta1&amp;quot;,</span>
<span class="c">#          &amp;quot;kind&amp;quot;: &amp;quot;TargetGroupBinding&amp;quot;,</span>
<span class="c">#          &amp;quot;metadata&amp;quot;: {</span>
<span class="c">#            &amp;quot;annotations&amp;quot;: {</span>
<span class="c">#              &amp;quot;elbv2.k8s.aws/checkpoint&amp;quot;: &amp;quot;gs98wrXMlQMdFGryEbrFbVngcODAXK0Yk4czpOdn9bg/biShKK1OQPD05qA040YQHH29qU6aPNq6J-fRu4M-dKY&amp;quot;,</span>
<span class="c">#              &amp;quot;elbv2.k8s.aws/checkpoint-timestamp&amp;quot;: &amp;quot;1730537287&amp;quot;</span>
<span class="c">#            },</span>
<span class="c">#            &amp;quot;creationTimestamp&amp;quot;: &amp;quot;2024-10-01T08:48:04Z&amp;quot;,</span>
<span class="c">#            &amp;quot;finalizers&amp;quot;: [</span>
<span class="c">#              &amp;quot;elbv2.k8s.aws/resources&amp;quot;</span>
<span class="c">#            ],</span>
<span class="c">#            &amp;quot;generation&amp;quot;: 1,</span>
<span class="c">#            &amp;quot;labels&amp;quot;: {</span>
<span class="c">#              &amp;quot;service.k8s.aws/stack-name&amp;quot;: &amp;quot;svc-nlb-ip-type&amp;quot;,</span>
<span class="c">#              &amp;quot;service.k8s.aws/stack-namespace&amp;quot;: &amp;quot;default&amp;quot;</span>
<span class="c">#            },</span>
<span class="c">#            &amp;quot;name&amp;quot;: &amp;quot;k8s-default-svcnlbip-f58255c318&amp;quot;,</span>
<span class="c">#            &amp;quot;namespace&amp;quot;: &amp;quot;default&amp;quot;,</span>
<span class="c">#            &amp;quot;resourceVersion&amp;quot;: &amp;quot;17369&amp;quot;,</span>
<span class="c">#            &amp;quot;uid&amp;quot;: &amp;quot;bdf37bcf-7fab-47ad-8b73-fb97c0239c9a&amp;quot;</span>
<span class="c">#          },</span>
<span class="c">#          &amp;quot;spec&amp;quot;: {</span>
<span class="c">#            &amp;quot;ipAddressType&amp;quot;: &amp;quot;ipv4&amp;quot;,</span>
<span class="c">#            &amp;quot;networking&amp;quot;: {</span>
<span class="c">#              &amp;quot;ingress&amp;quot;: [</span>
<span class="c">#                {</span>
<span class="c">#                  &amp;quot;from&amp;quot;: [</span>
<span class="c">#                    {</span>
<span class="c">#                      &amp;quot;securityGroup&amp;quot;: {</span>
<span class="c">#                        &amp;quot;groupID&amp;quot;: &amp;quot;sg-0a7a0dcdcda95c9fc&amp;quot;</span>
<span class="c">#                      }</span>
<span class="c">#                    }</span>
<span class="c">#                  ],</span>
<span class="c">#                  &amp;quot;ports&amp;quot;: [</span>
<span class="c">#                    {</span>
<span class="c">#                      &amp;quot;port&amp;quot;: 8080,</span>
<span class="c">#                      &amp;quot;protocol&amp;quot;: &amp;quot;TCP&amp;quot;</span>
<span class="c">#                    }</span>
<span class="c">#                  ]</span>
<span class="c">#                }</span>
<span class="c">#              ]</span>
<span class="c">#            },</span>
<span class="c">#            &amp;quot;serviceRef&amp;quot;: {</span>
<span class="c">#              &amp;quot;name&amp;quot;: &amp;quot;svc-nlb-ip-type&amp;quot;,</span>
<span class="c">#              &amp;quot;port&amp;quot;: 80</span>
<span class="c">#            },</span>
<span class="c">#            &amp;quot;targetGroupARN&amp;quot;: &amp;quot;arn:aws:elasticloadbalancing:ap-northeast-2:123456789012:targetgroup/k8s-default-svcnlbip-f58255c318/0cfa12d6c579f11b&amp;quot;,</span>
<span class="c">#            &amp;quot;targetType&amp;quot;: &amp;quot;ip&amp;quot;,</span>
<span class="c">#            &amp;quot;vpcID&amp;quot;: &amp;quot;vpc-0837921f515624150&amp;quot;</span>
<span class="c">#          },</span>
<span class="c">#          &amp;quot;status&amp;quot;: {</span>
<span class="c">#            &amp;quot;observedGeneration&amp;quot;: 1</span>
<span class="c">#          }</span>
<span class="c">#        }</span>
<span class="c">#      ],</span>
<span class="c">#      &amp;quot;kind&amp;quot;: &amp;quot;List&amp;quot;,</span>
<span class="c">#      &amp;quot;metadata&amp;quot;: {</span>
<span class="c">#        &amp;quot;resourceVersion&amp;quot;: &amp;quot;&amp;quot;</span>
<span class="c">#      }</span>
<span class="c">#    }</span>
  
<span class="c"># (옵션) 빠른 실습을 위해서 등록 취소 지연(드레이닝 간격) 수정 : 기본값 300초</span>
<span class="nv">$ </span>vi echo-service-nlb.yaml
<span class="nt">---</span>
..
apiVersion: v1
kind: Service
metadata:
  name: svc-nlb-ip-type
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip
    service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing
    service.beta.kubernetes.io/aws-load-balancer-healthcheck-port: <span class="s2">"8080"</span>
    service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: <span class="s2">"true"</span>
    service.beta.kubernetes.io/aws-load-balancer-target-group-attributes: deregistration_delay.timeout_seconds<span class="o">=</span>60
...
:wq!
<span class="nt">---</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> echo-service-nlb.yaml
<span class="c"># =&gt; deployment.apps/deploy-echo unchanged</span>
<span class="c">#    service/svc-nlb-ip-type configured</span>
  
<span class="c"># AWS ELB(NLB) 정보 확인</span>
<span class="nv">$ </span>aws elbv2 describe-load-balancers | jq
<span class="c"># =&gt; {</span>
<span class="c">#      &amp;quot;LoadBalancers&amp;quot;: [</span>
<span class="c">#        {</span>
<span class="c">#          &amp;quot;LoadBalancerArn&amp;quot;: &amp;quot;arn:aws:elasticloadbalancing:ap-northeast-2:123456789012:loadbalancer/net/k8s-default-svcnlbip-50f5d03f41/f28c6aa861a2e815&amp;quot;,</span>
<span class="c">#          &amp;quot;DNSName&amp;quot;: &amp;quot;k8s-default-svcnlbip-50f5d03f41-f28c6aa861a2e815.elb.ap-northeast-2.amazonaws.com&amp;quot;,</span>
<span class="c">#          &amp;quot;CanonicalHostedZoneId&amp;quot;: &amp;quot;ZIBE1TIR4HY56&amp;quot;,</span>
<span class="c">#          &amp;quot;CreatedTime&amp;quot;: &amp;quot;2024-10-01T08:48:03.532000+00:00&amp;quot;,</span>
<span class="c">#          &amp;quot;LoadBalancerName&amp;quot;: &amp;quot;k8s-default-svcnlbip-50f5d03f41&amp;quot;,</span>
<span class="c">#          &amp;quot;Scheme&amp;quot;: &amp;quot;internet-facing&amp;quot;,</span>
<span class="c">#          &amp;quot;VpcId&amp;quot;: &amp;quot;vpc-0837921f515624150&amp;quot;,</span>
<span class="c">#          &amp;quot;State&amp;quot;: {</span>
<span class="c">#            &amp;quot;Code&amp;quot;: &amp;quot;provisioning&amp;quot;</span>
<span class="c">#          },</span>
<span class="c">#          &amp;quot;Type&amp;quot;: &amp;quot;network&amp;quot;,</span>
<span class="c">#          &amp;quot;AvailabilityZones&amp;quot;: [</span>
<span class="c">#            {</span>
<span class="c">#              &amp;quot;ZoneName&amp;quot;: &amp;quot;ap-northeast-2a&amp;quot;,</span>
<span class="c">#              &amp;quot;SubnetId&amp;quot;: &amp;quot;subnet-0a06ed52d587bd707&amp;quot;,</span>
<span class="c">#              &amp;quot;LoadBalancerAddresses&amp;quot;: []</span>
<span class="c">#            },</span>
<span class="c">#            {</span>
<span class="c">#              &amp;quot;ZoneName&amp;quot;: &amp;quot;ap-northeast-2c&amp;quot;,</span>
<span class="c">#              &amp;quot;SubnetId&amp;quot;: &amp;quot;subnet-03c911c48452b41b2&amp;quot;,</span>
<span class="c">#              &amp;quot;LoadBalancerAddresses&amp;quot;: []</span>
<span class="c">#            },</span>
<span class="c">#            {</span>
<span class="c">#              &amp;quot;ZoneName&amp;quot;: &amp;quot;ap-northeast-2b&amp;quot;,</span>
<span class="c">#              &amp;quot;SubnetId&amp;quot;: &amp;quot;subnet-00b4dacf7eef35d33&amp;quot;,</span>
<span class="c">#              &amp;quot;LoadBalancerAddresses&amp;quot;: []</span>
<span class="c">#            }</span>
<span class="c">#          ],</span>
<span class="c">#          &amp;quot;SecurityGroups&amp;quot;: [</span>
<span class="c">#            &amp;quot;sg-0a7a0dcdcda95c9fc&amp;quot;,</span>
<span class="c">#            &amp;quot;sg-0e325a379a10ced56&amp;quot;</span>
<span class="c">#          ],</span>
<span class="c">#          &amp;quot;IpAddressType&amp;quot;: &amp;quot;ipv4&amp;quot;,</span>
<span class="c">#          &amp;quot;EnablePrefixForIpv6SourceNat&amp;quot;: &amp;quot;off&amp;quot;</span>
<span class="c">#        }</span>
<span class="c">#      ]</span>
<span class="c">#    }</span>
<span class="nv">$ </span>aws elbv2 describe-load-balancers <span class="nt">--query</span> <span class="s1">'LoadBalancers[*].State.Code'</span> <span class="nt">--output</span> text
<span class="c"># =&gt; provisioning</span>
<span class="nv">$ ALB_ARN</span><span class="o">=</span><span class="si">$(</span>aws elbv2 describe-load-balancers <span class="nt">--query</span> <span class="s1">'LoadBalancers[?contains(LoadBalancerName, `k8s-default-svcnlbip`) == `true`].LoadBalancerArn'</span> | jq <span class="nt">-r</span> <span class="s1">'.[0]'</span><span class="si">)</span>
<span class="nv">$ </span>aws elbv2 describe-target-groups <span class="nt">--load-balancer-arn</span> <span class="nv">$ALB_ARN</span> | jq
<span class="c"># =&gt; {</span>
<span class="c">#      &amp;quot;TargetGroups&amp;quot;: [</span>
<span class="c">#        {</span>
<span class="c">#          &amp;quot;TargetGroupArn&amp;quot;: &amp;quot;arn:aws:elasticloadbalancing:ap-northeast-2:123456789012:targetgroup/k8s-default-svcnlbip-f58255c318/0cfa12d6c579f11b&amp;quot;,</span>
<span class="c">#          &amp;quot;TargetGroupName&amp;quot;: &amp;quot;k8s-default-svcnlbip-f58255c318&amp;quot;,</span>
<span class="c">#          &amp;quot;Protocol&amp;quot;: &amp;quot;TCP&amp;quot;,</span>
<span class="c">#          &amp;quot;Port&amp;quot;: 8080,</span>
<span class="c">#          &amp;quot;VpcId&amp;quot;: &amp;quot;vpc-0837921f515624150&amp;quot;,</span>
<span class="c">#          &amp;quot;HealthCheckProtocol&amp;quot;: &amp;quot;TCP&amp;quot;,</span>
<span class="c">#          &amp;quot;HealthCheckPort&amp;quot;: &amp;quot;8080&amp;quot;,</span>
<span class="c">#          &amp;quot;HealthCheckEnabled&amp;quot;: true,</span>
<span class="c">#          &amp;quot;HealthCheckIntervalSeconds&amp;quot;: 10,</span>
<span class="c">#          &amp;quot;HealthCheckTimeoutSeconds&amp;quot;: 10,</span>
<span class="c">#          &amp;quot;HealthyThresholdCount&amp;quot;: 3,</span>
<span class="c">#          &amp;quot;UnhealthyThresholdCount&amp;quot;: 3,</span>
<span class="c">#          &amp;quot;LoadBalancerArns&amp;quot;: [</span>
<span class="c">#            &amp;quot;arn:aws:elasticloadbalancing:ap-northeast-2:123456789012:loadbalancer/net/k8s-default-svcnlbip-50f5d03f41/f28c6aa861a2e815&amp;quot;</span>
<span class="c">#          ],</span>
<span class="c">#          &amp;quot;TargetType&amp;quot;: &amp;quot;ip&amp;quot;,</span>
<span class="c">#          &amp;quot;IpAddressType&amp;quot;: &amp;quot;ipv4&amp;quot;</span>
<span class="c">#        }</span>
<span class="c">#      ]</span>
<span class="c">#    }</span>
<span class="nv">$ TARGET_GROUP_ARN</span><span class="o">=</span><span class="si">$(</span>aws elbv2 describe-target-groups <span class="nt">--load-balancer-arn</span> <span class="nv">$ALB_ARN</span> | jq <span class="nt">-r</span> <span class="s1">'.TargetGroups[0].TargetGroupArn'</span><span class="si">)</span>
<span class="nv">$ </span>aws elbv2 describe-target-health <span class="nt">--target-group-arn</span> <span class="nv">$TARGET_GROUP_ARN</span> | jq
<span class="c"># =&gt; {</span>
<span class="c">#      &amp;quot;TargetHealthDescriptions&amp;quot;: [</span>
<span class="c">#        {</span>
<span class="c">#          &amp;quot;Target&amp;quot;: {</span>
<span class="c">#            &amp;quot;Id&amp;quot;: &amp;quot;192.168.2.24&amp;quot;,</span>
<span class="c">#            &amp;quot;Port&amp;quot;: 8080,</span>
<span class="c">#            &amp;quot;AvailabilityZone&amp;quot;: &amp;quot;ap-northeast-2b&amp;quot;</span>
<span class="c">#          },</span>
<span class="c">#          &amp;quot;HealthCheckPort&amp;quot;: &amp;quot;8080&amp;quot;,</span>
<span class="c">#          &amp;quot;TargetHealth&amp;quot;: {</span>
<span class="c">#            &amp;quot;State&amp;quot;: &amp;quot;healthy&amp;quot;</span>
<span class="c">#          },</span>
<span class="c">#          &amp;quot;AdministrativeOverride&amp;quot;: {</span>
<span class="c">#            &amp;quot;State&amp;quot;: &amp;quot;no_override&amp;quot;,</span>
<span class="c">#            &amp;quot;Reason&amp;quot;: &amp;quot;AdministrativeOverride.NoOverride&amp;quot;,</span>
<span class="c">#            &amp;quot;Description&amp;quot;: &amp;quot;No override is currently active on target&amp;quot;</span>
<span class="c">#          }</span>
<span class="c">#        },</span>
<span class="c">#        {</span>
<span class="c">#          &amp;quot;Target&amp;quot;: {</span>
<span class="c">#            &amp;quot;Id&amp;quot;: &amp;quot;192.168.1.112&amp;quot;,</span>
<span class="c">#            &amp;quot;Port&amp;quot;: 8080,</span>
<span class="c">#            &amp;quot;AvailabilityZone&amp;quot;: &amp;quot;ap-northeast-2a&amp;quot;</span>
<span class="c">#          },</span>
<span class="c">#          &amp;quot;HealthCheckPort&amp;quot;: &amp;quot;8080&amp;quot;,</span>
<span class="c">#          &amp;quot;TargetHealth&amp;quot;: {</span>
<span class="c">#            &amp;quot;State&amp;quot;: &amp;quot;initial&amp;quot;,</span>
<span class="c">#            &amp;quot;Reason&amp;quot;: &amp;quot;Elb.InitialHealthChecking&amp;quot;,</span>
<span class="c">#            &amp;quot;Description&amp;quot;: &amp;quot;Initial health checks in progress&amp;quot;</span>
<span class="c">#          },</span>
<span class="c">#          &amp;quot;AdministrativeOverride&amp;quot;: {</span>
<span class="c">#            &amp;quot;State&amp;quot;: &amp;quot;no_override&amp;quot;,</span>
<span class="c">#            &amp;quot;Reason&amp;quot;: &amp;quot;AdministrativeOverride.NoOverride&amp;quot;,</span>
<span class="c">#            &amp;quot;Description&amp;quot;: &amp;quot;No override is currently active on target&amp;quot;</span>
<span class="c">#          }</span>
<span class="c">#        }</span>
<span class="c">#      ]</span>
<span class="c">#    }</span>
  
<span class="c"># 웹 접속 주소 확인</span>
<span class="nv">$ </span>kubectl get svc svc-nlb-ip-type <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.status.loadBalancer.ingress[0].hostname<span class="o">}</span> | <span class="nb">awk</span> <span class="s1">'{ print "Pod Web URL = http://"$1 }'</span>
<span class="c"># =&gt; Pod Web URL = http://k8s-default-svcnlbip-50f5d03f41-f28c6aa861a2e815.elb.ap-northeast-2.amazonaws.com</span>
  
<span class="c"># 파드 로깅 모니터링</span>
<span class="nv">$ </span>kubectl logs <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>deploy-websrv <span class="nt">-f</span>
  
<span class="c"># 분산 접속 확인</span>
<span class="nv">$ NLB</span><span class="o">=</span><span class="si">$(</span>kubectl get svc svc-nlb-ip-type <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.status.loadBalancer.ingress[0].hostname<span class="o">}</span><span class="si">)</span>
<span class="nv">$ </span>curl <span class="nt">-s</span> <span class="nv">$NLB</span>
<span class="c"># =&gt; Hostname: deploy-echo-857b6cfb88-5xz5j</span>
<span class="c">#    </span>
<span class="c">#    Pod Information:</span>
<span class="c">#     -no pod information available-</span>
<span class="c">#    </span>
<span class="c">#    Server values:</span>
<span class="c">#     server_version=nginx: 1.13.0 - lua: 10008</span>
<span class="c">#    </span>
<span class="c">#    Request Information:</span>
<span class="c">#     client_address=192.168.3.171</span>
<span class="c">#     method=GET</span>
<span class="c">#     real path=/</span>
<span class="c">#     query=</span>
<span class="c">#     request_version=1.1</span>
<span class="c">#     request_uri=http://k8s-default-svcnlbip-50f5d03f41-f28c6aa861a2e815.elb.ap-northeast-2.amazonaws.com:8080/</span>
<span class="c">#    </span>
<span class="c">#    Request Headers:</span>
<span class="c">#     accept=*/*</span>
<span class="c">#     host=k8s-default-svcnlbip-50f5d03f41-f28c6aa861a2e815.elb.ap-northeast-2.amazonaws.com</span>
<span class="c">#     user-agent=curl/8.3.0</span>
<span class="c">#    </span>
<span class="c">#    Request Body:</span>
<span class="c">#     -no body in request-</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$NLB</span> | <span class="nb">grep </span>Hostname <span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
<span class="c"># =&gt; 52 Hostname: deploy-echo-857b6cfb88-452cd</span>
<span class="c">#    48 Hostname: deploy-echo-857b6cfb88-5xz5j</span>
  
<span class="c"># 지속적인 접속 시도 : 아래 상세 동작 확인 시 유용(패킷 덤프 등)</span>
<span class="nv">$ </span><span class="k">while </span><span class="nb">true</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nt">--connect-timeout</span> 1 <span class="nv">$NLB</span> | egrep <span class="s1">'Hostname|client_address'</span><span class="p">;</span> <span class="nb">echo</span> <span class="s2">"----------"</span> <span class="p">;</span> <span class="nb">date</span> <span class="s2">"+%Y-%m-%d %H:%M:%S"</span> <span class="p">;</span> <span class="nb">sleep </span>1<span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; Hostname: deploy-echo-857b6cfb88-5xz5j</span>
<span class="c">#     client_address=192.168.1.39</span>
<span class="c">#    ----------</span>
<span class="c">#    2024-10-01 17:52:08</span>
<span class="c">#    Hostname: deploy-echo-857b6cfb88-452cd</span>
<span class="c">#     client_address=192.168.3.171</span>
<span class="c">#    ----------</span>
<span class="c">#    2024-10-01 17:52:09</span>
<span class="c">#    Hostname: deploy-echo-857b6cfb88-5xz5j</span>
<span class="c">#     client_address=192.168.1.39</span>
<span class="c">#    ----------</span>
<span class="c">#    2024-10-01 17:52:10</span>
<span class="c">#    Hostname: deploy-echo-857b6cfb88-452cd</span>
<span class="c">#     client_address=192.168.3.171</span>
<span class="c">#    ...</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>파드 2개 → 1개 → 3개 설정 시 동작을 확인해보겠습니다. 파드의 IP가 auto discovery되는데 이것은 service에 엔드포인트 정보를 사용한 것입니다.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># (신규 터미널) 모니터링</span>
<span class="nv">$ </span><span class="k">while </span><span class="nb">true</span><span class="p">;</span> <span class="k">do </span>aws elbv2 describe-target-health <span class="nt">--target-group-arn</span> <span class="nv">$TARGET_GROUP_ARN</span> <span class="nt">--output</span> text<span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; TARGETHEALTHDESCRIPTIONS 8080</span>
<span class="c">#    ADMINISTRATIVEOVERRIDE No override is currently active on target AdministrativeOverride.NoOverride no_override</span>
<span class="c">#    TARGET ap-northeast-2b 192.168.2.24  8080</span>
<span class="c">#    TARGETHEALTH healthy</span>
<span class="c">#    TARGETHEALTHDESCRIPTIONS 8080</span>
<span class="c">#    ADMINISTRATIVEOVERRIDE No override is currently active on target AdministrativeOverride.NoOverride no_override</span>
<span class="c">#    TARGET ap-northeast-2a 192.168.1.112 8080</span>
<span class="c">#    TARGETHEALTH healthy</span>
<span class="c">#    ...</span>
    
<span class="c"># 작업용 EC2 - 파드 1개 설정 </span>
<span class="nv">$ </span>kubectl scale deployment deploy-echo <span class="nt">--replicas</span><span class="o">=</span>1
    
<span class="c"># 확인</span>
<span class="nv">$ </span>kubectl get deploy,pod,svc,ep
<span class="c"># =&gt; NAME                          READY   UP-TO-DATE   AVAILABLE   AGE</span>
<span class="c">#    deployment.apps/deploy-echo   1/1     1            1           6m34s</span>
<span class="c">#    </span>
<span class="c">#    NAME                               READY   STATUS    RESTARTS   AGE</span>
<span class="c">#    pod/deploy-echo-857b6cfb88-452cd   1/1     Running   0          6m34s</span>
<span class="c">#    </span>
<span class="c">#    NAME                      TYPE           CLUSTER-IP      EXTERNAL-IP                                                                         PORT(S)        AGE</span>
<span class="c">#    service/kubernetes        ClusterIP      10.100.0.1      &amp;lt;none&amp;gt;                                                                              443/TCP        82m</span>
<span class="c">#    service/svc-nlb-ip-type   LoadBalancer   10.100.83.184   k8s-default-svcnlbip-50f5d03f41-f28c6aa861a2e815.elb.ap-northeast-2.amazonaws.com   80:30771/TCP   6m34s</span>
<span class="c">#    </span>
<span class="c">#    NAME                        ENDPOINTS                            AGE</span>
<span class="c">#    endpoints/kubernetes        192.168.1.65:443,192.168.3.189:443   82m</span>
<span class="c">#    endpoints/svc-nlb-ip-type   192.168.2.24:8080                    6m34s</span>
<span class="nv">$ </span>curl <span class="nt">-s</span> <span class="nv">$NLB</span>
<span class="c"># =&gt; Hostname: deploy-echo-857b6cfb88-452cd</span>
<span class="c">#    </span>
<span class="c">#    Pod Information:</span>
<span class="c">#     -no pod information available-</span>
<span class="c">#    </span>
<span class="c">#    Server values:</span>
<span class="c">#     server_version=nginx: 1.13.0 - lua: 10008</span>
<span class="c">#    </span>
<span class="c">#    Request Information:</span>
<span class="c">#     client_address=192.168.3.171</span>
<span class="c">#     method=GET</span>
<span class="c">#     real path=/</span>
<span class="c">#     query=</span>
<span class="c">#     request_version=1.1</span>
<span class="c">#     request_uri=http://k8s-default-svcnlbip-50f5d03f41-f28c6aa861a2e815.elb.ap-northeast-2.amazonaws.com:8080/</span>
<span class="c">#    </span>
<span class="c">#    Request Headers:</span>
<span class="c">#     accept=*/*</span>
<span class="c">#     host=k8s-default-svcnlbip-50f5d03f41-f28c6aa861a2e815.elb.ap-northeast-2.amazonaws.com</span>
<span class="c">#     user-agent=curl/8.3.0</span>
<span class="c">#    </span>
<span class="c">#    Request Body:</span>
<span class="c">#     -no body in request-</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nt">--connect-timeout</span> 1 <span class="nv">$NLB</span> | <span class="nb">grep </span>Hostname <span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
<span class="c"># =&gt;     100 Hostname: deploy-echo-857b6cfb88-452cd</span>
    
<span class="c"># 작업용 EC2 - 파드 3개 설정 </span>
<span class="nv">$ </span>kubectl scale deployment deploy-echo <span class="nt">--replicas</span><span class="o">=</span>3
<span class="c"># =&gt; deployment.apps/deploy-echo scaled</span>
    
<span class="c"># 확인 : NLB 대상 타켓이 아직 initial 일 때 100번 반복 접속 시 어떻게 되는지 확인해보자!</span>
<span class="nv">$ </span>kubectl get deploy,pod,svc,ep
<span class="c"># =&gt; NAME                          READY   UP-TO-DATE   AVAILABLE   AGE</span>
<span class="c">#    deployment.apps/deploy-echo   3/3     3            3           7m41s</span>
<span class="c">#    </span>
<span class="c">#    NAME                               READY   STATUS    RESTARTS   AGE</span>
<span class="c">#    ...</span>
<span class="c">#    pod/deploy-echo-857b6cfb88-8wn7b   1/1     Running   0          10s</span>
<span class="c">#    pod/deploy-echo-857b6cfb88-qhqbt   1/1     Running   0          10s</span>
<span class="c">#    </span>
<span class="c">#    ...</span>
<span class="c">#    </span>
<span class="c">#    NAME                        ENDPOINTS                                                 AGE</span>
<span class="c">#    ...</span>
<span class="c">#    endpoints/svc-nlb-ip-type   192.168.1.161:8080,192.168.2.24:8080,192.168.3.241:8080   7m41s</span>
<span class="nv">$ </span>curl <span class="nt">-s</span> <span class="nv">$NLB</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nt">--connect-timeout</span> 1 <span class="nv">$NLB</span> | <span class="nb">grep </span>Hostname <span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
<span class="c"># =&gt;      37 Hostname: deploy-echo-857b6cfb88-452cd</span>
<span class="c">#         33 Hostname: deploy-echo-857b6cfb88-8wn7b</span>
<span class="c">#         30 Hostname: deploy-echo-857b6cfb88-qhqbt</span>
    
<span class="c"># </span>
<span class="nv">$ </span>kubectl describe deploy <span class="nt">-n</span> kube-system aws-load-balancer-controller | <span class="nb">grep</span> <span class="nt">-i</span> <span class="s1">'Service Account'</span>
<span class="c"># =&gt;   Service Account:  aws-load-balancer-controller</span>
    
<span class="c"># [AWS LB Ctrl] 클러스터 롤 바인딩 정보 확인</span>
<span class="nv">$ </span>kubectl describe clusterrolebindings.rbac.authorization.k8s.io aws-load-balancer-controller-rolebinding
    
<span class="c"># [AWS LB Ctrl] 클러스터롤 확인 </span>
<span class="nv">$ </span>kubectl describe clusterroles.rbac.authorization.k8s.io aws-load-balancer-controller-role
</code></pre></div>    </div>

    <ul>
      <li>실습 리소스 삭제:  <code class="language-plaintext highlighter-rouge">kubectl delete deploy deploy-echo; kubectl delete svc svc-nlb-ip-type</code></li>
    </ul>
  </li>
  <li><strong>(심화) Pod readiness gate</strong> : ALB/NLB 대상(ip mode)이 ALB/NLB의 헬스체크에 의해 정상일 경우 해당 파드로 전달할 수 있는 기능입니다. - <a href="https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.7/deploy/pod_readiness_gate/">Link</a> <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-readiness-gate">K8S</a>
    <ul>
      <li>
        <p>사전 준비</p>

        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 바로 위에서 실습 리소스 삭제했다면, 다시 생성 : deregistration_delay.timeout_seconds=60 확인</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> echo-service-nlb.yaml
<span class="c"># =&gt; deployment.apps/deploy-echo created</span>
<span class="c">#    service/svc-nlb-ip-type created</span>
<span class="nv">$ </span>kubectl scale deployment deploy-echo <span class="nt">--replicas</span><span class="o">=</span>1
<span class="c"># =&gt; deployment.apps/deploy-echo scaled</span>
    
<span class="c">#</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-owide</span>
<span class="c"># =&gt; NAME                           READY   STATUS    RESTARTS   AGE   IP              NODE                                               NOMINATED NODE   READINESS GATES</span>
<span class="c">#    deploy-echo-857b6cfb88-sx8lg   1/1     Running   0          14m   192.168.3.124   ip-192-168-3-235.ap-northeast-2.compute.internal   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
    
<span class="c"># mutatingwebhookconfigurations 확인 : mutating 대상(네임스페이스에 아래 매칭 시)</span>
<span class="nv">$ </span>kubectl get mutatingwebhookconfigurations
<span class="c"># =&gt; NAME                            WEBHOOKS   AGE</span>
<span class="c">#    aws-load-balancer-webhook       3          28m</span>
<span class="c">#    pod-identity-webhook            1          102m</span>
<span class="c">#    vpc-resource-mutating-webhook   1          102m</span>
<span class="nv">$ </span>kubectl get mutatingwebhookconfigurations aws-load-balancer-webhook <span class="nt">-o</span> yaml | kubectl neat
<span class="c"># =&gt; ...</span>
<span class="c">#      name: mpod.elbv2.k8s.aws</span>
<span class="c">#      namespaceSelector:</span>
<span class="c">#        matchExpressions:</span>
<span class="c">#        - key: elbv2.k8s.aws/pod-readiness-gate-inject</span>
<span class="c">#          operator: In</span>
<span class="c">#          values:</span>
<span class="c">#          - enabled</span>
<span class="c">#      objectSelector:</span>
<span class="c">#        matchExpressions:</span>
<span class="c">#        - key: app.kubernetes.io/name</span>
<span class="c">#          operator: NotIn</span>
<span class="c">#          values:</span>
<span class="c">#          - aws-load-balancer-controller</span>
<span class="c">#    ...</span>
    
<span class="c"># 현재 확인</span>
<span class="nv">$ </span>kubectl get ns <span class="nt">--show-labels</span>
<span class="c"># =&gt; NAME              STATUS   AGE    LABELS</span>
<span class="c">#    default           Active   103m   kubernetes.io/metadata.name=default</span>
<span class="c">#    kube-node-lease   Active   103m   kubernetes.io/metadata.name=kube-node-lease</span>
<span class="c">#    kube-public       Active   103m   kubernetes.io/metadata.name=kube-public</span>
<span class="c">#    kube-system       Active   103m   kubernetes.io/metadata.name=kube-system</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>설정 및 확인</p>

        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># (터미널 각각 2개) 모니터링</span>
<span class="nv">$ ALB_ARN</span><span class="o">=</span><span class="si">$(</span>aws elbv2 describe-load-balancers <span class="nt">--query</span> <span class="s1">'LoadBalancers[?contains(LoadBalancerName, `k8s-default-svcnlbip`) == `true`].LoadBalancerArn'</span> | jq <span class="nt">-r</span> <span class="s1">'.[0]'</span><span class="si">)</span>
<span class="nv">$ TARGET_GROUP_ARN</span><span class="o">=</span><span class="si">$(</span>aws elbv2 describe-target-groups <span class="nt">--load-balancer-arn</span> <span class="nv">$ALB_ARN</span> | jq <span class="nt">-r</span> <span class="s1">'.TargetGroups[0].TargetGroupArn'</span><span class="si">)</span>
    
<span class="nv">$ </span>watch <span class="nt">-d</span> kubectl get pod,svc,ep <span class="nt">-owide</span>
<span class="nv">$ </span><span class="k">while </span><span class="nb">true</span><span class="p">;</span> <span class="k">do </span>aws elbv2 describe-target-health <span class="nt">--target-group-arn</span> <span class="nv">$TARGET_GROUP_ARN</span> <span class="nt">--output</span> text<span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
    
<span class="c">#</span>
<span class="nv">$ </span>kubectl label namespace default elbv2.k8s.aws/pod-readiness-gate-inject<span class="o">=</span>enabled
<span class="c"># =&gt; namespace/default labeled</span>
<span class="nv">$ </span>kubectl get ns <span class="nt">--show-labels</span>
<span class="c"># =&gt; NAME              STATUS   AGE    LABELS</span>
<span class="c">#    default           Active   108m   elbv2.k8s.aws/pod-readiness-gate-inject=enabled,kubernetes.io/metadata.name=default</span>
<span class="c">#    kube-node-lease   Active   108m   kubernetes.io/metadata.name=kube-node-lease</span>
<span class="c">#    kube-public       Active   108m   kubernetes.io/metadata.name=kube-public</span>
<span class="c">#    kube-system       Active   108m   kubernetes.io/metadata.name=kube-system</span>
    
<span class="c"># READINESS GATES 항목 추가 확인</span>
<span class="nv">$ </span>kubectl describe pod
<span class="nv">$ </span>kubectl get pod <span class="nt">-owide</span>
<span class="c"># =&gt; NAME                           READY   STATUS    RESTARTS   AGE   IP              NODE                                               NOMINATED NODE   READINESS GATES</span>
<span class="c">#    deploy-echo-857b6cfb88-sx8lg   1/1     Running   0          20m   192.168.3.124   ip-192-168-3-235.ap-northeast-2.compute.internal   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
    
<span class="c">#</span>
<span class="nv">$ </span>kubectl delete pod <span class="nt">--all</span>
<span class="c"># =&gt; pod &amp;quot;deploy-echo-857b6cfb88-sx8lg&amp;quot; deleted</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-owide</span>
<span class="c"># =&gt; NAME                           READY   STATUS    RESTARTS   AGE   IP             NODE                                              NOMINATED NODE   READINESS GATES</span>
<span class="c">#    deploy-echo-857b6cfb88-njwdj   1/1     Running   0          54s   192.168.2.18   ip-192-168-2-92.ap-northeast-2.compute.internal   &amp;lt;none&amp;gt;           1/1</span>
    
<span class="nv">$ </span>kubectl describe pod
<span class="c"># =&gt; ...</span>
<span class="c">#    Readiness Gates:</span>
<span class="c">#      Type                                                          Status</span>
<span class="c">#      target-health.elbv2.k8s.aws/k8s-default-svcnlbip-2e5acb9719   True</span>
<span class="c">#    Conditions:</span>
<span class="c">#      Type                                                          Status</span>
<span class="c">#      target-health.elbv2.k8s.aws/k8s-default-svcnlbip-2e5acb9719   True</span>
<span class="c">#      PodReadyToStartContainers                                     True</span>
<span class="c">#      Initialized                                                   True</span>
<span class="c">#      Ready                                                         True</span>
<span class="c">#      ContainersReady                                               True</span>
<span class="c">#      PodScheduled                                                  True</span>
<span class="c">#      ...</span>
    
<span class="nv">$ </span>kubectl get pod <span class="nt">-o</span> yaml | yh
<span class="c"># =&gt; ...</span>
<span class="c">#        readinessGates:</span>
<span class="c">#        - conditionType: target-health.elbv2.k8s.aws/k8s-default-svcnlbip-2e5acb9719</span>
<span class="c">#    ...</span>
<span class="c">#      status:</span>
<span class="c">#        conditions:</span>
<span class="c">#        - lastProbeTime: null</span>
<span class="c">#          lastTransitionTime: &amp;quot;2024-10-01T09:21:28Z&amp;quot;</span>
<span class="c">#          status: &amp;quot;True&amp;quot;</span>
<span class="c">#          type: target-health.elbv2.k8s.aws/k8s-default-svcnlbip-2e5acb9719</span>
<span class="c">#    ...</span>
    
<span class="c"># 분산 접속 확인</span>
<span class="nv">$ NLB</span><span class="o">=</span><span class="si">$(</span>kubectl get svc svc-nlb-ip-type <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.status.loadBalancer.ingress[0].hostname<span class="o">}</span><span class="si">)</span>
<span class="nv">$ </span>curl <span class="nt">-s</span> <span class="nv">$NLB</span>
<span class="c"># =&gt; Hostname: deploy-echo-857b6cfb88-njwdj</span>
<span class="c">#    </span>
<span class="c">#    Pod Information:</span>
<span class="c">#     -no pod information available-</span>
<span class="c">#    </span>
<span class="c">#    Server values:</span>
<span class="c">#     server_version=nginx: 1.13.0 - lua: 10008</span>
<span class="c">#    </span>
<span class="c">#    Request Information:</span>
<span class="c">#     client_address=192.168.3.71</span>
<span class="c">#     method=GET</span>
<span class="c">#     real path=/</span>
<span class="c">#     query=</span>
<span class="c">#     request_version=1.1</span>
<span class="c">#     request_uri=http://k8s-default-svcnlbip-f4c21d9697-0d3512de2bf74357.elb.ap-northeast-2.amazonaws.com:8080/</span>
<span class="c">#    </span>
<span class="c">#    Request Headers:</span>
<span class="c">#     accept=*/*</span>
<span class="c">#     host=k8s-default-svcnlbip-f4c21d9697-0d3512de2bf74357.elb.ap-northeast-2.amazonaws.com</span>
<span class="c">#     user-agent=curl/8.3.0</span>
<span class="c">#    </span>
<span class="c">#    Request Body:</span>
<span class="c">#     -no body in request-</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$NLB</span> | <span class="nb">grep </span>Hostname <span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
<span class="c"># =&gt;     100 Hostname: deploy-echo-857b6cfb88-njwdj</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>실습 리소스 삭제:  <code class="language-plaintext highlighter-rouge">kubectl delete deploy deploy-echo; kubectl delete svc svc-nlb-ip-type</code></p>
      </li>
    </ul>
  </li>
  <li>NLB 대상 타켓을 <strong>Instance mode</strong> 로 설정해보기
    <ul>
      <li>다음 링크에서 확인해볼 수 있습니다.
<a href="https://www.notion.so/AWS-NLB-Client-IP-Proxy-protocol-57827e2c83fc474992b37e65db81f669?pvs=21">AWS NLB - Client IP 확인 &amp; Proxy protocol</a></li>
    </ul>
  </li>
  <li>
    <p>NLB IP Target &amp; <strong>Proxy Protocol v2</strong> 활성화 : NLB에서 바로 파드로 인입 및 ClientIP 확인 설정 - <a href="https://www.notion.so/AWS-NLB-Client-IP-Proxy-protocol-57827e2c83fc474992b37e65db81f669?pvs=21">링크</a> <a href="https://hub.docker.com/r/gasida/httpd/tags">image</a> <a href="https://canaryrelease.tistory.com/42">참고</a></p>

    <p><img src="/assets/2024/kans-3th/w9/20241102_kans_w9_25.png" alt="img_9.png" class="image-center w-90" /></p>
  </li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 생성</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gasida-web
spec:
  replicas: 1
  selector:
    matchLabels:
      app: gasida-web
  template:
    metadata:
      labels:
        app: gasida-web
    spec:
      terminationGracePeriodSeconds: 0
      containers:
      - name: gasida-web
        image: sweetlittlebird/httpd:pp
        ports:
        - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: svc-nlb-ip-type-pp
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip
    service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing
    service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true"
    service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: "*"
spec:
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
  type: LoadBalancer
  loadBalancerClass: service.k8s.aws/nlb
  selector:
    app: gasida-web
</span><span class="no">EOF
</span><span class="c"># =&gt; deployment.apps/gasida-web created</span>
<span class="c">#    service/svc-nlb-ip-type-pp created</span>

<span class="c"># apache에 proxy protocol 활성화 확인</span>
<span class="nv">$ </span>kubectl <span class="nb">exec </span>deploy/gasida-web <span class="nt">--</span> apachectl <span class="nt">-t</span> <span class="nt">-D</span> DUMP_MODULES
<span class="c"># =&gt; Loaded Modules:</span>
<span class="c">#     ...</span>
<span class="c">#     remoteip_module (shared)</span>
<span class="c">#     ...</span>
<span class="nv">$ </span>kubectl <span class="nb">exec </span>deploy/gasida-web <span class="nt">--</span> <span class="nb">cat</span> /usr/local/apache2/conf/httpd.conf
<span class="c"># =&gt; ...</span>
<span class="c">#    RemoteIPProxyProtocol On</span>
<span class="c">#    ...</span>

<span class="c"># 확인</span>
<span class="nv">$ </span>kubectl get svc,ep
<span class="c"># =&gt; NAME                         TYPE           CLUSTER-IP       EXTERNAL-IP                                                                         PORT(S)        AGE</span>
<span class="c">#    service/kubernetes           ClusterIP      10.100.0.1       &amp;lt;none&amp;gt;                                                                              443/TCP        5h52m</span>
<span class="c">#    service/svc-nlb-ip-type-pp   LoadBalancer   10.100.160.172   k8s-default-svcnlbip-c11e4bd02e-ec82d8f688f176d3.elb.ap-northeast-2.amazonaws.com   80:31348/TCP   36m</span>
<span class="c">#    </span>
<span class="c">#    NAME                           ENDPOINTS                            AGE</span>
<span class="c">#    endpoints/kubernetes           192.168.1.65:443,192.168.3.189:443   5h52m</span>
<span class="c">#    endpoints/svc-nlb-ip-type-pp   192.168.2.51:80                      36m</span>
<span class="nv">$ </span>kubectl describe svc svc-nlb-ip-type-pp
<span class="nv">$ </span>kubectl describe svc svc-nlb-ip-type-pp | <span class="nb">grep </span>Annotations: <span class="nt">-A5</span>
<span class="c"># =&gt; Annotations: service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: true</span>
<span class="c">#                 service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip   # &lt;span style="color: green;"&gt;👉 NLB Target Type이 IP입니다.&lt;/span&gt;</span>
<span class="c">#                 service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: *</span>
<span class="c">#                 service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing</span>
<span class="c">#    Selector:    app=gasida-web</span>
<span class="c">#    Type:        LoadBalancer</span>

<span class="c"># 접속 확인</span>
<span class="nv">$ NLB</span><span class="o">=</span><span class="si">$(</span>kubectl get svc svc-nlb-ip-type-pp <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.status.loadBalancer.ingress[0].hostname<span class="o">}</span><span class="si">)</span>
<span class="nv">$ </span>curl <span class="nt">-s</span> <span class="nv">$NLB</span>
<span class="c"># =&gt; &amp;lt;html&amp;gt;&amp;lt;body&amp;gt;&amp;lt;h1&amp;gt;It works!&amp;lt;/h1&amp;gt;&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;</span>

<span class="c"># 지속적인 접속 시도 : 아래 상세 동작 확인 시 유용(패킷 덤프 등)</span>
<span class="nv">$ </span><span class="k">while </span><span class="nb">true</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nt">--connect-timeout</span> 1 <span class="nv">$NLB</span><span class="p">;</span> <span class="nb">echo</span> <span class="s2">"----------"</span> <span class="p">;</span> <span class="nb">date</span> <span class="s2">"+%Y-%m-%d %H:%M:%S"</span> <span class="p">;</span> <span class="nb">sleep </span>1<span class="p">;</span> <span class="k">done</span>

<span class="c"># 베스쳔 호스트 IP 확인</span>
<span class="nv">$ </span>curl ipinfo.io/ip
<span class="c"># =&gt; 3.35.140.75</span>

<span class="c"># 로그 확인</span>
<span class="nv">$ </span>kubectl logs <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>gasida-web <span class="nt">-f</span>
<span class="c"># =&gt; ...</span>
<span class="c">#    3.35.140.75 - - [01/Oct/2024:13:46:35 +0000] &amp;quot;GET / HTTP/1.1&amp;quot; 200 45</span>
<span class="c">#    3.35.140.75 - - [01/Oct/2024:13:46:36 +0000] &amp;quot;GET / HTTP/1.1&amp;quot; 200 45</span>
<span class="c">#    3.35.140.75 - - [01/Oct/2024:13:46:37 +0000] &amp;quot;GET / HTTP/1.1&amp;quot; 200 45</span>
<span class="c">#    3.35.140.75 - - [01/Oct/2024:13:46:38 +0000] &amp;quot;GET / HTTP/1.1&amp;quot; 200 45</span>

<span class="c"># &lt;span style="color: green;"&gt;👉 proxy protocol을 통해 외부 클라이언트의 IP가 전달되어 로그에 남는것을 확인할 수 있었습니다.&lt;/span&gt;</span>

<span class="c"># 삭제</span>
<span class="nv">$ </span>kubectl delete deploy gasida-web<span class="p">;</span> kubectl delete svc svc-nlb-ip-type-pp
</code></pre></div></div>

<hr />

<h3 id="ingress">Ingress</h3>

<ul>
  <li>Ingress는 클러스터 내부의 서비스(ClusterIP, NodePort, Loadbalancer)를 외부로 노출(<strong>HTTP/HTTPS</strong>)시키는 일종의 Web Proxy 역할을 수행합니다.</li>
</ul>

<h4 id="aws-load-balancer-controller--ingress-alb-ip-모드-동작-with-aws-vpc-cni"><strong>AWS Load Balancer Controller</strong> + <strong>Ingress (ALB) IP 모드</strong> 동작 with <strong>AWS VPC CNI</strong></h4>

<p><img src="/assets/2024/kans-3th/w9/20241102_kans_w9_11.png" alt="img.png" class="image-center w-90" /></p>

<ul>
  <li>
    <p>서비스/파드 배포 테스트 with Ingress(ALB) - <a href="https://docs.aws.amazon.com/eks/latest/userguide/alb-ingress.html">ALB</a></p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 게임 파드와 Service, Ingress 배포</span>
<span class="nv">$ </span>curl <span class="nt">-s</span> <span class="nt">-O</span> https://raw.githubusercontent.com/gasida/PKOS/main/3/ingress1.yaml
<span class="nv">$ </span><span class="nb">cat </span>ingress1.yaml
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> ingress1.yaml
<span class="c"># =&gt; namespace/game-2048 created</span>
<span class="c">#    deployment.apps/deployment-2048 created</span>
<span class="c">#    service/service-2048 created</span>
<span class="c">#    ingress.networking.k8s.io/ingress-2048 created</span>
  
<span class="c"># 모니터링</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> kubectl get pod,ingress,svc,ep <span class="nt">-n</span> game-2048
  
<span class="c"># 생성 확인</span>
<span class="nv">$ </span>kubectl get-all <span class="nt">-n</span> game-2048
<span class="c"># =&gt; NAME                                                               NAMESPACE  AGE</span>
<span class="c">#    configmap/kube-root-ca.crt                                         game-2048  38s</span>
<span class="c">#    endpoints/service-2048                                             game-2048  38s</span>
<span class="c">#    pod/deployment-2048-85f8c7d69-gz295                                game-2048  38s</span>
<span class="c">#    pod/deployment-2048-85f8c7d69-t2blb                                game-2048  38s</span>
<span class="c">#    serviceaccount/default                                             game-2048  38s</span>
<span class="c">#    service/service-2048                                               game-2048  38s</span>
<span class="c">#    deployment.apps/deployment-2048                                    game-2048  38s</span>
<span class="c">#    replicaset.apps/deployment-2048-85f8c7d69                          game-2048  38s</span>
<span class="c">#    endpointslice.discovery.k8s.io/service-2048-s58q4                  game-2048  38s</span>
<span class="c">#    targetgroupbinding.elbv2.k8s.aws/k8s-game2048-service2-00c3b27023  game-2048  34s</span>
<span class="c">#    ingress.networking.k8s.io/ingress-2048                             game-2048  38s</span>
<span class="nv">$ </span>kubectl get ingress,svc,ep,pod <span class="nt">-n</span> game-2048
<span class="c"># =&gt; NAME                                     CLASS   HOSTS   ADDRESS                                                                       PORTS   AGE</span>
<span class="c">#    ingress.networking.k8s.io/ingress-2048   alb     *       k8s-game2048-ingress2-70d50ce3fd-333540411.ap-northeast-2.elb.amazonaws.com   80      49s</span>
<span class="c">#    </span>
<span class="c">#    NAME                   TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE</span>
<span class="c">#    service/service-2048   NodePort   10.100.41.151   &amp;lt;none&amp;gt;        80:32522/TCP   49s</span>
<span class="c">#    </span>
<span class="c">#    NAME                     ENDPOINTS                          AGE</span>
<span class="c">#    endpoints/service-2048   192.168.1.112:80,192.168.2.18:80   49s</span>
<span class="c">#    </span>
<span class="c">#    NAME                                  READY   STATUS    RESTARTS   AGE</span>
<span class="c">#    pod/deployment-2048-85f8c7d69-gz295   1/1     Running   0          49s</span>
<span class="c">#    pod/deployment-2048-85f8c7d69-t2blb   1/1     Running   0          49s</span>
<span class="nv">$ </span>kubectl get targetgroupbindings <span class="nt">-n</span> game-2048
<span class="c"># =&gt; NAME                               SERVICE-NAME   SERVICE-PORT   TARGET-TYPE   AGE</span>
<span class="c">#    k8s-game2048-service2-00c3b27023   service-2048   80             ip            56s</span>
  
<span class="c"># ALB 생성 확인</span>
<span class="nv">$ </span>aws elbv2 describe-load-balancers <span class="nt">--query</span> <span class="s1">'LoadBalancers[?contains(LoadBalancerName, `k8s-game2048`) == `true`]'</span> | jq
<span class="nv">$ ALB_ARN</span><span class="o">=</span><span class="si">$(</span>aws elbv2 describe-load-balancers <span class="nt">--query</span> <span class="s1">'LoadBalancers[?contains(LoadBalancerName, `k8s-game2048`) == `true`].LoadBalancerArn'</span> | jq <span class="nt">-r</span> <span class="s1">'.[0]'</span><span class="si">)</span>
<span class="nv">$ </span>aws elbv2 describe-target-groups <span class="nt">--load-balancer-arn</span> <span class="nv">$ALB_ARN</span>
<span class="nv">$ TARGET_GROUP_ARN</span><span class="o">=</span><span class="si">$(</span>aws elbv2 describe-target-groups <span class="nt">--load-balancer-arn</span> <span class="nv">$ALB_ARN</span> | jq <span class="nt">-r</span> <span class="s1">'.TargetGroups[0].TargetGroupArn'</span><span class="si">)</span>
<span class="nv">$ </span>aws elbv2 describe-target-health <span class="nt">--target-group-arn</span> <span class="nv">$TARGET_GROUP_ARN</span> | jq
<span class="c"># =&gt; {</span>
<span class="c">#      &amp;quot;TargetHealthDescriptions&amp;quot;: [</span>
<span class="c">#        {</span>
<span class="c">#          &amp;quot;Target&amp;quot;: {</span>
<span class="c">#            &amp;quot;Id&amp;quot;: &amp;quot;192.168.1.112&amp;quot;,</span>
<span class="c">#            &amp;quot;Port&amp;quot;: 80,</span>
<span class="c">#            &amp;quot;AvailabilityZone&amp;quot;: &amp;quot;ap-northeast-2a&amp;quot;</span>
<span class="c">#          },</span>
<span class="c">#          &amp;quot;HealthCheckPort&amp;quot;: &amp;quot;80&amp;quot;,</span>
<span class="c">#          &amp;quot;TargetHealth&amp;quot;: {</span>
<span class="c">#            &amp;quot;State&amp;quot;: &amp;quot;healthy&amp;quot;</span>
<span class="c">#          }</span>
<span class="c">#        },</span>
<span class="c">#        {</span>
<span class="c">#          &amp;quot;Target&amp;quot;: {</span>
<span class="c">#            &amp;quot;Id&amp;quot;: &amp;quot;192.168.2.18&amp;quot;,</span>
<span class="c">#            &amp;quot;Port&amp;quot;: 80,</span>
<span class="c">#            &amp;quot;AvailabilityZone&amp;quot;: &amp;quot;ap-northeast-2b&amp;quot;</span>
<span class="c">#          },</span>
<span class="c">#          &amp;quot;HealthCheckPort&amp;quot;: &amp;quot;80&amp;quot;,</span>
<span class="c">#          &amp;quot;TargetHealth&amp;quot;: {</span>
<span class="c">#            &amp;quot;State&amp;quot;: &amp;quot;healthy&amp;quot;</span>
<span class="c">#          }</span>
<span class="c">#        }</span>
<span class="c">#      ]</span>
<span class="c">#    }</span>
  
<span class="c"># Ingress 확인</span>
<span class="nv">$ </span>kubectl describe ingress <span class="nt">-n</span> game-2048 ingress-2048
<span class="nv">$ </span>kubectl get ingress <span class="nt">-n</span> game-2048 ingress-2048 <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.status.loadBalancer.ingress[*].hostname}{'</span><span class="se">\n</span><span class="s2">'}"</span>
<span class="c"># =&gt; k8s-game2048-ingress2-70d50ce3fd-333540411.ap-northeast-2.elb.amazonaws.com</span>
  
<span class="c"># 게임 접속 : ALB 주소로 웹 접속</span>
<span class="nv">$ </span>kubectl get ingress <span class="nt">-n</span> game-2048 ingress-2048 <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.status.loadBalancer.ingress[0].hostname<span class="o">}</span> | <span class="nb">awk</span> <span class="s1">'{ print "Game URL = http://"$1 }'</span>
<span class="c"># =&gt; Game URL = http://k8s-game2048-ingress2-70d50ce3fd-333540411.ap-northeast-2.elb.amazonaws.com</span>
  
<span class="c"># 파드 IP 확인</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-n</span> game-2048 <span class="nt">-owide</span>
<span class="c"># =&gt; NAME                              READY   STATUS    RESTARTS   AGE     IP              NODE                                               NOMINATED NODE   READINESS GATES</span>
<span class="c">#    deployment-2048-85f8c7d69-gz295   1/1     Running   0          3m42s   192.168.2.18    ip-192-168-2-92.ap-northeast-2.compute.internal    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    deployment-2048-85f8c7d69-t2blb   1/1     Running   0          3m42s   192.168.1.112   ip-192-168-1-186.ap-northeast-2.compute.internal   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
</code></pre></div>    </div>
    <p><img src="/assets/2024/kans-3th/w9/20241102_kans_w9_31.png" alt="img.png" /></p>

    <ul>
      <li>
        <p><strong>ALB 대상 그룹</strong>에 등록된 대상 확인 : ALB에서 파드 IP로 직접 전달</p>

        <p><img src="/assets/2024/kans-3th/w9/20241102_kans_w9_32.png" alt="img.png" class="image-center" />
<em class="image-caption">파드 IP로 바로 직접 연결된 ALB 대상 그룹</em></p>
      </li>
      <li>
        <p>파드 3개로 증가</p>

        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 터미널1</span>
<span class="nv">$ </span>watch kubectl get pod <span class="nt">-n</span> game-2048
<span class="nv">$ </span><span class="k">while </span><span class="nb">true</span><span class="p">;</span> <span class="k">do </span>aws elbv2 describe-target-health <span class="nt">--target-group-arn</span> <span class="nv">$TARGET_GROUP_ARN</span> <span class="nt">--output</span> text<span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
    
<span class="c"># 터미널2 : 파드 3개로 증가</span>
<span class="nv">$ </span>kubectl scale deployment <span class="nt">-n</span> game-2048 deployment-2048 <span class="nt">--replicas</span> 3
<span class="c"># =&gt; deployment.apps/deployment-2048 scaled</span>
</code></pre></div>        </div>
        <p><img src="/assets/2024/kans-3th/w9/20241102_kans_w9_33.png" alt="img.png" class="image-center" />
<em class="image-caption">추가된 파드 IP가 연결됨</em></p>
      </li>
      <li>
        <p>파드 1개로 감소</p>

        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 터미널2 : 파드 1개로 감소</span>
<span class="nv">$ </span>kubectl scale deployment <span class="nt">-n</span> game-2048 deployment-2048 <span class="nt">--replicas</span> 1
<span class="c"># =&gt; deployment.apps/deployment-2048 scaled</span>
</code></pre></div>        </div>

        <p><img src="/assets/2024/kans-3th/w9/20241102_kans_w9_34.png" alt="img.png" class="image-center" />
<em class="image-caption">삭제되는 2개의 파드가 삭제 중임을 확인</em></p>
      </li>
      <li>
        <p>실습 리소스  삭제</p>

        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl delete ingress ingress-2048 <span class="nt">-n</span> game-2048
<span class="nv">$ </span>kubectl delete svc service-2048 <span class="nt">-n</span> game-2048 <span class="o">&amp;&amp;</span> kubectl delete deploy deployment-2048 <span class="nt">-n</span> game-2048 <span class="o">&amp;&amp;</span> kubectl delete ns game-2048
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
</ul>

<h4 id="kubernetes-응용프로그램을-외부로-노출-시키는-방법들-비교">Kubernetes 응용프로그램을 외부로 노출 시키는 방법들 비교</h4>

<ul>
  <li>간략하게 Kubernetes 응용프로그램을 외부로 노출 시기는 방법들을 비교해보겠습니다.</li>
  <li>자세한 내용은 다음 블로그에서 살펴 볼 수 있습니다. <a href="https://aws.amazon.com/blogs/containers/exposing-kubernetes-applications-part-1-service-and-ingress-resources/">Exposing Kubernetes Applications, Part 1: Service and Ingress Resources </a>
    <ol>
      <li>Exposing a <strong>Service</strong> : In-tree Service Controller
  <img src="/assets/2024/kans-3th/w9/20241102_kans_w9_12.png" alt="img.png" class="image-center" />
        <ul>
          <li>AWS CLB(Classic Load Balancer)나 AWS NLB(Network Load Balancer)를 사용하여 서비스를 직접 외부로 노출합니다.
 하지만 서비스 수가 많아지면 Load Balancer의 수도 많아지게 되어 관리가 어려워집니다.</li>
        </ul>
      </li>
      <li><strong>Ingress Implementations : External Load Balancer</strong>
  <img src="/assets/2024/kans-3th/w9/20241102_kans_w9_13.png" alt="img.png" class="image-center" />
        <ul>
          <li>외부에 ALB(Application Load Balancer)를 생성하고 라우팅합니다. 이때 외부의 ALB가 Ingress rule을 ALB rule로 변환하여 
 외부 ALB가 직접 파드와 통신합니다. 앞에서 본 서비스를 직접 외부로 노출하는것 보다, CSP(Cloud Service Provider)에서 제공하는
 확장성이나, DDOS 방어기능 등을 활용 할 수 있습니다.</li>
        </ul>
      </li>
      <li><strong>Ingress Implementations : Internal Reverse Proxy</strong>
  <img src="/assets/2024/kans-3th/w9/20241102_kans_w9_14.png" alt="img.png" class="image-center" />
        <ul>
          <li>외부의 ALB에 더해 nginx 등의 Layer 7 리버스 프록시를 두는 방식입니다. 이 방식은 외부 ALB가 직접 파드와 통신하는 것이 아니라,
 리버스 프록시를 통해 통신하게 됩니다. 성능상으로는 불이익이 있지만 L7 리버스 프록시에서 제공하는 추가 기능들을 활용 할 수 있습니다.</li>
          <li>하지만 관리요소가 추가되는 것이기 때문에 이를 고려하여 사용해야 합니다.</li>
        </ul>
      </li>
      <li><strong>Kubernetes Gateway API</strong>
  <img src="/assets/2024/kans-3th/w9/20241102_kans_w9_15.png" alt="img.png" class="image-center" />
        <ul>
          <li>Kubernetes Gateway API는 Ingress Controller를 대체하는 새로운 API로 현시점에서 Beta 상태로, 정식 지원하지는 않는듯하며,
 외부 Loadbalancer나 Internal reverse proxy 방식처럼 사용할 수 있습니다.</li>
        </ul>
      </li>
    </ol>
  </li>
</ul>

<h3 id="externaldns">ExternalDNS</h3>

<ul>
  <li>
    <p>K8S 서비스/인그레스 생성 시 도메인을 설정하면, AWS(Route 53), Azure(DNS), GCP(Cloud DNS)에 A 레코드(TXT 레코드)가 자동으로 생성/삭제됩니다.
<img src="/assets/2024/kans-3th/w9/20241102_kans_w9_27.png" alt="img.png" class="image-center" />
<em class="image-caption"><a href="https://edgehog.blog/a-self-hosted-external-dns-resolver-for-kubernetes-111a27d6fc2c">https://edgehog.blog/a-self-hosted-external-dns-resolver-for-kubernetes-111a27d6fc2c</a></em></p>
  </li>
  <li>
    <p>AWS Route 53 정보 확인 &amp; 변수 지정 : Public 도메인 소유를 하고 있어야 합니다</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 자신의 도메인 변수 지정 : 소유하고 있는 자신의 도메인을 입력하시면 됩니다</span>
<span class="c">#$ MyDomain=&lt;자신의 도메인&gt;</span>
<span class="nv">$ MyDomain</span><span class="o">=</span>kans.loremipsum.sweetlittlebird.io
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"export MyDomain=kans.loremipsum.sweetlittlebird.io"</span> <span class="o">&gt;&gt;</span> /etc/profile
  
<span class="c"># 자신의 Route 53 도메인 ID 조회 및 변수 지정</span>
<span class="nv">$ </span>aws route53 list-hosted-zones-by-name <span class="nt">--dns-name</span> <span class="s2">"</span><span class="k">${</span><span class="nv">MyDomain</span><span class="k">}</span><span class="s2">."</span> | jq
<span class="c"># =&gt; {</span>
<span class="c">#      &amp;quot;HostedZones&amp;quot;: [</span>
<span class="c">#        {</span>
<span class="c">#          &amp;quot;Id&amp;quot;: &amp;quot;/hostedzone/Z0416620XQJAGAPWXO31&amp;quot;,</span>
<span class="c">#          &amp;quot;Name&amp;quot;: &amp;quot;kans.loremipsum.sweetlittlebird.io.&amp;quot;,</span>
<span class="c">#          &amp;quot;CallerReference&amp;quot;: &amp;quot;3aba3324-d6a0-44a4-82b5-004044e06dd6&amp;quot;,</span>
<span class="c">#          &amp;quot;Config&amp;quot;: {</span>
<span class="c">#            &amp;quot;Comment&amp;quot;: &amp;quot;&amp;quot;,</span>
<span class="c">#            &amp;quot;PrivateZone&amp;quot;: false</span>
<span class="c">#          },</span>
<span class="c">#          &amp;quot;ResourceRecordSetCount&amp;quot;: 3</span>
<span class="c">#        }</span>
<span class="c">#      ],</span>
<span class="c">#      &amp;quot;DNSName&amp;quot;: &amp;quot;kans.loremipsum.sweetlittlebird.io.&amp;quot;,</span>
<span class="c">#      &amp;quot;IsTruncated&amp;quot;: false,</span>
<span class="c">#      &amp;quot;MaxItems&amp;quot;: &amp;quot;100&amp;quot;</span>
<span class="c">#    }</span>
<span class="nv">$ </span>aws route53 list-hosted-zones-by-name <span class="nt">--dns-name</span> <span class="s2">"</span><span class="k">${</span><span class="nv">MyDomain</span><span class="k">}</span><span class="s2">."</span> <span class="nt">--query</span> <span class="s2">"HostedZones[0].Name"</span>
<span class="c"># =&gt; &amp;quot;kans.loremipsum.sweetlittlebird.io.&amp;quot;</span>
<span class="nv">$ </span>aws route53 list-hosted-zones-by-name <span class="nt">--dns-name</span> <span class="s2">"</span><span class="k">${</span><span class="nv">MyDomain</span><span class="k">}</span><span class="s2">."</span> <span class="nt">--query</span> <span class="s2">"HostedZones[0].Id"</span> <span class="nt">--output</span> text
<span class="c"># =&gt; /hostedzone/Z0416620XQJAGAPWXO31</span>
<span class="nv">$ MyDnzHostedZoneId</span><span class="o">=</span><span class="sb">`</span>aws route53 list-hosted-zones-by-name <span class="nt">--dns-name</span> <span class="s2">"</span><span class="k">${</span><span class="nv">MyDomain</span><span class="k">}</span><span class="s2">."</span> <span class="nt">--query</span> <span class="s2">"HostedZones[0].Id"</span> <span class="nt">--output</span> text<span class="sb">`</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$MyDnzHostedZoneId</span>
<span class="c"># =&gt; /hostedzone/Z0416620XQJAGAPWXO31</span>
  
<span class="c"># (옵션) NS 레코드 타입 첫번째 조회</span>
<span class="nv">$ </span>aws route53 list-resource-record-sets <span class="nt">--output</span> json <span class="nt">--hosted-zone-id</span> <span class="s2">"</span><span class="k">${</span><span class="nv">MyDnzHostedZoneId</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--query</span> <span class="s2">"ResourceRecordSets[?Type == 'NS']"</span> | jq <span class="nt">-r</span> <span class="s1">'.[0].ResourceRecords[].Value'</span>
<span class="c"># =&gt; ns-979.awsdns-58.net.</span>
<span class="c">#    ns-1489.awsdns-58.org.</span>
<span class="c">#    ns-355.awsdns-44.com.</span>
<span class="c">#    ns-1760.awsdns-28.co.uk.</span>
<span class="c"># (옵션) A 레코드 타입 모두 조회</span>
<span class="nv">$ </span>aws route53 list-resource-record-sets <span class="nt">--output</span> json <span class="nt">--hosted-zone-id</span> <span class="s2">"</span><span class="k">${</span><span class="nv">MyDnzHostedZoneId</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--query</span> <span class="s2">"ResourceRecordSets[?Type == 'A']"</span>
<span class="c"># =&gt; []</span>
  
<span class="c"># A 레코드 타입 조회</span>
<span class="nv">$ </span>aws route53 list-resource-record-sets <span class="nt">--hosted-zone-id</span> <span class="s2">"</span><span class="k">${</span><span class="nv">MyDnzHostedZoneId</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--query</span> <span class="s2">"ResourceRecordSets[?Type == 'A']"</span> | jq
<span class="nv">$ </span>aws route53 list-resource-record-sets <span class="nt">--hosted-zone-id</span> <span class="s2">"</span><span class="k">${</span><span class="nv">MyDnzHostedZoneId</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--query</span> <span class="s2">"ResourceRecordSets[?Type == 'A'].Name"</span> | jq
<span class="nv">$ </span>aws route53 list-resource-record-sets <span class="nt">--hosted-zone-id</span> <span class="s2">"</span><span class="k">${</span><span class="nv">MyDnzHostedZoneId</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--query</span> <span class="s2">"ResourceRecordSets[?Type == 'A'].Name"</span> <span class="nt">--output</span> text
  
<span class="c"># A 레코드 값 반복 조회</span>
<span class="nv">$ </span><span class="k">while </span><span class="nb">true</span><span class="p">;</span> <span class="k">do </span>aws route53 list-resource-record-sets <span class="nt">--hosted-zone-id</span> <span class="s2">"</span><span class="k">${</span><span class="nv">MyDnzHostedZoneId</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--query</span> <span class="s2">"ResourceRecordSets[?Type == 'A']"</span> | jq <span class="p">;</span> <span class="nb">date</span> <span class="p">;</span> <span class="nb">echo</span> <span class="p">;</span> <span class="nb">sleep </span>1<span class="p">;</span> <span class="k">done</span>
</code></pre></div>    </div>
  </li>
  <li>ExternalDNS 설치 - <a href="https://github.com/kubernetes-sigs/external-dns/blob/master/docs/tutorials/aws.md">링크</a>
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># EKS 배포 시 Node IAM Role 설정되어 있음</span>
<span class="c"># eksctl create cluster ... --external-dns-access ...</span>
  
<span class="c"># </span>
<span class="c">#$ MyDomain=&lt;자신의 도메인&gt;</span>
<span class="nv">$ MyDomain</span><span class="o">=</span>kans.loremipsum.sweetlittlebird.io
  
<span class="c"># 자신의 Route 53 도메인 ID 조회 및 변수 지정</span>
<span class="nv">$ MyDnzHostedZoneId</span><span class="o">=</span><span class="si">$(</span>aws route53 list-hosted-zones-by-name <span class="nt">--dns-name</span> <span class="s2">"</span><span class="k">${</span><span class="nv">MyDomain</span><span class="k">}</span><span class="s2">."</span> <span class="nt">--query</span> <span class="s2">"HostedZones[0].Id"</span> <span class="nt">--output</span> text<span class="si">)</span>
  
<span class="c"># 변수 확인</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$MyDomain</span>, <span class="nv">$MyDnzHostedZoneId</span>
<span class="c"># =&gt; kans.loremipsum.sweetlittlebird.io, /hostedzone/Z0416620XQJAGAPWXO31</span>
  
<span class="c"># ExternalDNS 배포</span>
<span class="nv">$ </span>curl <span class="nt">-s</span> <span class="nt">-O</span> https://raw.githubusercontent.com/gasida/PKOS/main/aews/externaldns.yaml
<span class="nv">$ </span><span class="nb">cat </span>externaldns.yaml
<span class="nv">$ MyDomain</span><span class="o">=</span><span class="nv">$MyDomain</span> <span class="nv">MyDnzHostedZoneId</span><span class="o">=</span><span class="nv">$MyDnzHostedZoneId</span> envsubst &lt; externaldns.yaml | kubectl apply <span class="nt">-f</span> -
<span class="c"># =&gt; serviceaccount/external-dns created</span>
<span class="c">#    clusterrole.rbac.authorization.k8s.io/external-dns created</span>
<span class="c">#    clusterrolebinding.rbac.authorization.k8s.io/external-dns-viewer created</span>
<span class="c">#    deployment.apps/external-dns created</span>
  
<span class="c"># 확인 및 로그 모니터링</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-l</span> app.kubernetes.io/name<span class="o">=</span>external-dns <span class="nt">-n</span> kube-system
<span class="c"># =&gt; NAME                            READY   STATUS    RESTARTS   AGE</span>
<span class="c">#    external-dns-648996678b-7r4mz   1/1     Running   0          12s</span>
<span class="nv">$ </span>kubectl logs deploy/external-dns <span class="nt">-n</span> kube-system <span class="nt">-f</span>
</code></pre></div>    </div>

    <ul>
      <li>
        <p>(참고) 기존에 ExternalDNS를 통해 사용한 A/TXT 레코드가 있는 존의 경우에 policy 정책을 upsert-only 로 설정 후 사용 하자 - <a href="https://github.com/kubernetes-sigs/external-dns/blob/master/docs/tutorials/aws.md#deploy-externaldns">Link</a></p>

        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- <span class="c">#--policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>Service(<strong>NLB</strong>) + 도메인 연동(<strong>ExternalDNS</strong>) - <a href="https://www.whatsmydns.net/">도메인체크</a>
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 터미널1 (모니터링)</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s1">'kubectl get pod,svc'</span>
<span class="nv">$ </span>kubectl logs deploy/external-dns <span class="nt">-n</span> kube-system <span class="nt">-f</span>
<span class="c"># =&gt; ...</span>
<span class="c">#    time=&amp;quot;2024-10-01T14:18:10Z&amp;quot; level=info msg=&amp;quot;Instantiating new Kubernetes client&amp;quot;</span>
<span class="c">#    time=&amp;quot;2024-10-01T14:18:10Z&amp;quot; level=info msg=&amp;quot;Using inCluster-config based on serviceaccount-token&amp;quot;</span>
<span class="c">#    time=&amp;quot;2024-10-01T14:18:10Z&amp;quot; level=info msg=&amp;quot;Created Kubernetes client https://10.100.0.1:443&amp;quot;</span>
<span class="c">#    time=&amp;quot;2024-10-01T14:18:11Z&amp;quot; level=info msg=&amp;quot;Applying provider record filter for domains: [kans.loremipsum.sweetlittlebird.io. .kans.loremipsum.sweetlittlebird.io.]&amp;quot;</span>
<span class="c">#    time=&amp;quot;2024-10-01T14:18:11Z&amp;quot; level=info msg=&amp;quot;All records are already up to date&amp;quot;</span>
<span class="c">#    ... (deployment 배포 후) ...</span>
<span class="c">#    time=&amp;quot;2024-10-01T14:20:11Z&amp;quot; level=info msg=&amp;quot;Applying provider record filter for domains: [kans.loremipsum.sweetlittlebird.io. .kans.loremipsum.sweetlittlebird.io.]&amp;quot;</span>
<span class="c">#    time=&amp;quot;2024-10-01T14:20:12Z&amp;quot; level=info msg=&amp;quot;Desired change: CREATE cname-tetris.kans.loremipsum.sweetlittlebird.io TXT&amp;quot; profile=default zoneID=/hostedzone/Z0416620XQJAGAPWXO31 zoneName=kans.loremipsum.sweetlittlebird.io.</span>
<span class="c">#    time=&amp;quot;2024-10-01T14:20:12Z&amp;quot; level=info msg=&amp;quot;Desired change: CREATE tetris.kans.loremipsum.sweetlittlebird.io A&amp;quot; profile=default zoneID=/hostedzone/Z0416620XQJAGAPWXO31 zoneName=kans.loremipsum.sweetlittlebird.io.</span>
<span class="c">#    time=&amp;quot;2024-10-01T14:20:12Z&amp;quot; level=info msg=&amp;quot;Desired change: CREATE tetris.kans.loremipsum.sweetlittlebird.io TXT&amp;quot; profile=default zoneID=/hostedzone/Z0416620XQJAGAPWXO31 zoneName=kans.loremipsum.sweetlittlebird.io.</span>
<span class="c">#    time=&amp;quot;2024-10-01T14:20:12Z&amp;quot; level=info msg=&amp;quot;3 record(s) were successfully updated&amp;quot; profile=default zoneID=/hostedzone/Z0416620XQJAGAPWXO31 zoneName=kans.loremipsum.sweetlittlebird.io.</span>
  
<span class="c"># 테트리스 디플로이먼트 배포</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tetris
  labels:
    app: tetris
spec:
  replicas: 1
  selector:
    matchLabels:
      app: tetris
  template:
    metadata:
      labels:
        app: tetris
    spec:
      containers:
      - name: tetris
        image: bsord/tetris
---
apiVersion: v1
kind: Service
metadata:
  name: tetris
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip
    service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing
    service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true"
    service.beta.kubernetes.io/aws-load-balancer-backend-protocol: "http"
    #service.beta.kubernetes.io/aws-load-balancer-healthcheck-port: "80"
spec:
  selector:
    app: tetris
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
  type: LoadBalancer
  loadBalancerClass: service.k8s.aws/nlb
</span><span class="no">EOF
  
</span><span class="c"># 배포 확인</span>
<span class="nv">$ </span>kubectl get deploy,svc,ep tetris
<span class="c"># =&gt; NAME                     READY   UP-TO-DATE   AVAILABLE   AGE</span>
<span class="c">#    deployment.apps/tetris   0/1     1            0           5s</span>
<span class="c">#    </span>
<span class="c">#    NAME             TYPE           CLUSTER-IP      EXTERNAL-IP                                                                       PORT(S)        AGE</span>
<span class="c">#    service/tetris   LoadBalancer   10.100.39.204   k8s-default-tetris-4d2a39458c-e91cd840ce214644.elb.ap-northeast-2.amazonaws.com   80:32310/TCP   5s</span>
<span class="c">#    </span>
<span class="c">#    NAME               ENDPOINTS   AGE</span>
<span class="c">#    endpoints/tetris   &amp;lt;none&amp;gt;      5s</span>
  
<span class="c"># NLB에 ExternanDNS 로 도메인 연결</span>
<span class="nv">$ </span>kubectl annotate service tetris <span class="s2">"external-dns.alpha.kubernetes.io/hostname=tetris.</span><span class="nv">$MyDomain</span><span class="s2">"</span>
<span class="c"># =&gt; service/tetris annotated</span>
<span class="nv">$ </span><span class="k">while </span><span class="nb">true</span><span class="p">;</span> <span class="k">do </span>aws route53 list-resource-record-sets <span class="nt">--hosted-zone-id</span> <span class="s2">"</span><span class="k">${</span><span class="nv">MyDnzHostedZoneId</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--query</span> <span class="s2">"ResourceRecordSets[?Type == 'A']"</span> | jq <span class="p">;</span> <span class="nb">date</span> <span class="p">;</span> <span class="nb">echo</span> <span class="p">;</span> <span class="nb">sleep </span>1<span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; [</span>
<span class="c">#      {</span>
<span class="c">#        &amp;quot;Name&amp;quot;: &amp;quot;tetris.kans.loremipsum.sweetlittlebird.io.&amp;quot;,</span>
<span class="c">#        &amp;quot;Type&amp;quot;: &amp;quot;A&amp;quot;,</span>
<span class="c">#        &amp;quot;AliasTarget&amp;quot;: {</span>
<span class="c">#          &amp;quot;HostedZoneId&amp;quot;: &amp;quot;ZIBE1TIR4HY56&amp;quot;,</span>
<span class="c">#          &amp;quot;DNSName&amp;quot;: &amp;quot;k8s-default-tetris-4d2a39458c-e91cd840ce214644.elb.ap-northeast-2.amazonaws.com.&amp;quot;,</span>
<span class="c">#          &amp;quot;EvaluateTargetHealth&amp;quot;: true</span>
<span class="c">#        }</span>
<span class="c">#      }</span>
<span class="c">#    ]</span>
<span class="c">#    Sat Oct  1 23:21:27 KST 2024</span>
<span class="c">#    ...</span>
  
<span class="c"># Route53에 A레코드 확인</span>
<span class="nv">$ </span>aws route53 list-resource-record-sets <span class="nt">--hosted-zone-id</span> <span class="s2">"</span><span class="k">${</span><span class="nv">MyDnzHostedZoneId</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--query</span> <span class="s2">"ResourceRecordSets[?Type == 'A']"</span> | jq
<span class="c"># =&gt; [</span>
<span class="c">#      {</span>
<span class="c">#        &amp;quot;Name&amp;quot;: &amp;quot;tetris.kans.loremipsum.sweetlittlebird.io.&amp;quot;,</span>
<span class="c">#        &amp;quot;Type&amp;quot;: &amp;quot;A&amp;quot;,</span>
<span class="c">#        &amp;quot;AliasTarget&amp;quot;: {</span>
<span class="c">#          &amp;quot;HostedZoneId&amp;quot;: &amp;quot;ZIBE1TIR4HY56&amp;quot;,</span>
<span class="c">#          &amp;quot;DNSName&amp;quot;: &amp;quot;k8s-default-tetris-4d2a39458c-e91cd840ce214644.elb.ap-northeast-2.amazonaws.com.&amp;quot;,</span>
<span class="c">#          &amp;quot;EvaluateTargetHealth&amp;quot;: true</span>
<span class="c">#        }</span>
<span class="c">#      }</span>
<span class="c">#    ]</span>
<span class="nv">$ </span>aws route53 list-resource-record-sets <span class="nt">--hosted-zone-id</span> <span class="s2">"</span><span class="k">${</span><span class="nv">MyDnzHostedZoneId</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--query</span> <span class="s2">"ResourceRecordSets[?Type == 'A'].Name"</span> | jq .[]
<span class="c"># =&gt; &amp;quot;tetris.kans.loremipsum.sweetlittlebird.io.&amp;quot;</span>
  
<span class="c"># 확인</span>
<span class="nv">$ </span>dig +short tetris.<span class="nv">$MyDomain</span> @8.8.8.8
<span class="c"># =&gt; 3.38.82.70</span>
<span class="c">#    3.36.187.152</span>
<span class="c">#    3.35.184.161</span>
<span class="nv">$ </span>dig +short tetris.<span class="nv">$MyDomain</span>
<span class="c"># =&gt; 3.38.82.70</span>
  
<span class="c"># 도메인 체크</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"My Domain Checker = https://www.whatsmydns.net/#A/tetris.</span><span class="nv">$MyDomain</span><span class="s2">"</span>
<span class="c"># =&gt; My Domain Checker = https://www.whatsmydns.net/#A/tetris.kans.loremipsum.sweetlittlebird.io</span>
  
<span class="c"># 웹 접속 주소 확인 및 접속</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"Tetris Game URL = http://tetris.</span><span class="nv">$MyDomain</span><span class="s2">"</span>
<span class="c"># =&gt; Tetris Game URL = http://tetris.kans.loremipsum.sweetlittlebird.io</span>
</code></pre></div>    </div>

    <ul>
      <li>
        <p>웹 접속(http) → 화살표키, 일시중지(space bar)</p>

        <p><img src="/assets/2024/kans-3th/w9/20241102_kans_w9_35.png" alt="img.png" class="image-center" /></p>
      </li>
    </ul>
  </li>
  <li>
    <p>Ingress(ALB + HTTPS) + 도메인 연동(ExternalDNS)</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 테트리스 디플로이먼트 배포</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tetris
  labels:
    app: tetris
spec:
  replicas: 1
  selector:
    matchLabels:
      app: tetris
  template:
    metadata:
      labels:
        app: tetris
    spec:
      containers:
      - name: tetris
        image: bsord/tetris
---
apiVersion: v1
kind: Service
metadata:
  name: tetris
  labels:
    app: tetris
spec:
  selector:
    app: tetris
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
  type: LoadBalancer
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: tetris-ingress
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:ap-northeast-2:123456789012:certificate/256538f4-6c5d-4109-9d4d-9aa6af41adaa
    alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 80}, {"HTTPS":443}]'
    alb.ingress.kubernetes.io/actions.ssl-redirect: '{"Type": "redirect", "RedirectConfig": { "Protocol": "HTTPS", "Port": "443", "StatusCode": "HTTP_301"}}'
spec:
  ingressClassName: alb
  rules:
    - http:
        paths:
        - path: /
          pathType: Prefix
          backend:
            service:
              name: tetris
              port:
                number: 80  
</span><span class="no">EOF
</span><span class="c"># =&gt; deployment.apps/tetris created</span>
<span class="c">#    service/tetris created</span>
<span class="c">#    ingress.networking.k8s.io/tetris-ingress created</span>
  
<span class="c"># ingress에 도메인 부여</span>
<span class="nv">$ </span>kubectl annotate ingress tetris-ingress <span class="s2">"external-dns.alpha.kubernetes.io/hostname=tetris2.</span><span class="nv">$MyDomain</span><span class="s2">"</span>
<span class="c"># =&gt; ingress.networking.k8s.io/tetris-ingress annotated</span>
  
<span class="c"># &lt;span style="color: green;"&gt;👉 기존에 사용했던 도메인(tetris.$MyDomain)을 사용하면 DNS 레코드가 전파되는데 시간이 더 걸리기 때문에&lt;/span&gt;</span>
<span class="c"># &lt;span style="color: green;"&gt;   다른 도메인 (tetris2.$MyDomain)을 사용하였습니다.&lt;/span&gt;</span>
</code></pre></div>    </div>

    <p><img src="/assets/2024/kans-3th/w9/20241102_kans_w9_36.png" alt="img.png" class="image-center" />
<em class="image-caption">dns 적용 확인 (<a href="https://www.whatsmydns.net/">확인 사이트 링크</a>)</em></p>

    <ul>
      <li><code class="language-plaintext highlighter-rouge">https://tetris2.kans.loremipsum.sweetlittlebird.io</code> 로 접속하여 https 통신 확인
<img src="/assets/2024/kans-3th/w9/20241102_kans_w9_37.png" alt="img.png" class="image-center" />
<em class="image-caption">https 적용 확인</em></li>
    </ul>
  </li>
  <li><strong>리소스 삭제</strong> : <code class="language-plaintext highlighter-rouge">kubectl delete deploy,svc tetris</code> ← 삭제 시 externaldns 에 의해서 A레코드도 같이 삭제됩니다.</li>
</ul>

<ul>
  <li>
    <p>(참고) ACM 퍼블릭 인증서 요청 및 해당 인증서에 대한 Route53 도메인 검증 설정 with AWS CLI</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 각자 자신의 도메인 변수 지정</span>
<span class="c">#$ MyDomain=&lt;각자 자신의 도메인&gt;</span>
<span class="nv">$ MyDomain</span><span class="o">=</span>kans.loremipsum.sweetlittlebird.io
  
<span class="c"># ACM 퍼블릭 인증서 요청</span>
<span class="nv">$ CERT_ARN</span><span class="o">=</span><span class="si">$(</span>aws acm request-certificate <span class="se">\</span>
  <span class="nt">--domain-name</span> <span class="nv">$MyDomain</span> <span class="se">\</span>
  <span class="nt">--validation-method</span> <span class="s1">'DNS'</span> <span class="se">\</span>
  <span class="nt">--key-algorithm</span> <span class="s1">'RSA_2048'</span> <span class="se">\</span>
  | jq <span class="nt">--raw-output</span> <span class="s1">'.CertificateArn'</span><span class="si">)</span>
  
<span class="c"># 생성한 인증서 CNAME 이름 가져오기</span>
<span class="nv">$ CnameName</span><span class="o">=</span><span class="si">$(</span>aws acm describe-certificate <span class="se">\</span>
  <span class="nt">--certificate-arn</span> <span class="nv">$CERT_ARN</span> <span class="se">\</span>
  <span class="nt">--query</span> <span class="s1">'Certificate.DomainValidationOptions[*].ResourceRecord.Name'</span> <span class="se">\</span>
  <span class="nt">--output</span> text<span class="si">)</span>
  
<span class="c"># 생성한 인증서 CNAME 값 가져오기</span>
<span class="nv">$ CnameValue</span><span class="o">=</span><span class="si">$(</span>aws acm describe-certificate <span class="se">\</span>
  <span class="nt">--certificate-arn</span> <span class="nv">$CERT_ARN</span> <span class="se">\</span>
  <span class="nt">--query</span> <span class="s1">'Certificate.DomainValidationOptions[*].ResourceRecord.Value'</span> <span class="se">\</span>
  <span class="nt">--output</span> text<span class="si">)</span>
  
<span class="c"># 정상 출력 확인하기</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$CERT_ARN</span>, <span class="nv">$CnameName</span>, <span class="nv">$CnameValue</span>
<span class="c"># =&gt; arn:aws:acm:ap-northeast-2:123456789012:certificate/256538f4-6c5d-4109-9d4d-9aa6af41adaa, _e784949706a5e6eee7f350436cc9d501.kans.loremipsum.sweetlittlebird.io., _e62ee6e59a48ea2f3d1ab952d289bcea.djqtsrsxkq.acm-validations.aws.</span>
  
<span class="c"># 레코드 파일</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOT</span><span class="sh"> &gt; cname.json
{
  "Comment": "create a acm's CNAME record",
  "Changes": [
    {
      "Action": "CREATE",
      "ResourceRecordSet": {
        "Name": "CnameName",
        "Type": "CNAME",
        "TTL": 300,
        "ResourceRecords": [
          {
            "Value": "CnameValue"
          }
        ]
      }
    }
  ]
}
</span><span class="no">EOT
  
</span><span class="c"># CNAME 이름, 값 치환하기</span>
<span class="nv">$ </span><span class="nb">sed</span> <span class="nt">-i</span> <span class="s2">"s/CnameName/</span><span class="nv">$CnameName</span><span class="s2">/g"</span> cname.json
<span class="nv">$ </span><span class="nb">sed</span> <span class="nt">-i</span> <span class="s2">"s/CnameValue/</span><span class="nv">$CnameValue</span><span class="s2">/g"</span> cname.json
<span class="nv">$ </span><span class="nb">cat </span>cname.json
<span class="c"># =&gt; {</span>
<span class="c">#      &amp;quot;Comment&amp;quot;: &amp;quot;create a acm's CNAME record&amp;quot;,</span>
<span class="c">#      &amp;quot;Changes&amp;quot;: [</span>
<span class="c">#        {</span>
<span class="c">#          &amp;quot;Action&amp;quot;: &amp;quot;CREATE&amp;quot;,</span>
<span class="c">#          &amp;quot;ResourceRecordSet&amp;quot;: {</span>
<span class="c">#            &amp;quot;Name&amp;quot;: &amp;quot;_e784949706a5e6eee7f350436cc9d501.kans.loremipsum.sweetlittlebird.io.&amp;quot;,</span>
<span class="c">#            &amp;quot;Type&amp;quot;: &amp;quot;CNAME&amp;quot;,</span>
<span class="c">#            &amp;quot;TTL&amp;quot;: 300,</span>
<span class="c">#            &amp;quot;ResourceRecords&amp;quot;: [</span>
<span class="c">#              {</span>
<span class="c">#                &amp;quot;Value&amp;quot;: &amp;quot;_e62ee6e59a48ea2f3d1ab952d289bcea.djqtsrsxkq.acm-validations.aws.&amp;quot;</span>
<span class="c">#              }</span>
<span class="c">#            ]</span>
<span class="c">#          }</span>
<span class="c">#        }</span>
<span class="c">#      ]</span>
<span class="c">#    }</span>
  
<span class="c"># 해당 인증서에 대한 Route53 도메인 검증 설정을 위한 Route53 레코드 생성</span>
<span class="nv">$ </span>aws route53 change-resource-record-sets <span class="nt">--hosted-zone-id</span> <span class="nv">$MyDnzHostedZoneId</span> <span class="nt">--change-batch</span> file://cname.json
</code></pre></div>    </div>
  </li>
</ul>

<h3 id="실습-완료-후-자원-삭제">실습 완료 후 자원 삭제</h3>

<ul>
  <li>삭제 : 장점(1줄 명령어로 완전 삭제), 단점(삭제 실행과 완료까지 SSH 세션 유지 필요)
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>eksctl delete cluster <span class="nt">--name</span> <span class="nv">$CLUSTER_NAME</span> <span class="o">&amp;&amp;</span> aws cloudformation delete-stack <span class="nt">--stack-name</span> <span class="nv">$CLUSTER_NAME</span>
</code></pre></div>    </div>
  </li>
</ul>

<hr />

<h2 id="마치며">마치며</h2>

<p>와.. 드디어 마지막 주차인 9주차 과제를 완료하였습니다.
마지막 주차인 만큼 과제를 빨리 마치고 싶었지만, 생각보다 시간이 많이 걸려서 결국 오늘도 자정을 넘긴 일요일입니다. :cry:
매과제마다 저도 시간을 많이 쏟지만, 가시다 님을 비롯하여 스터디 조력자분들이 이 스터디를 위해 지금까지 얼마나 많은 시간을 투자하셨을지
생각하면 스스로가 머쓱하면서도, 감사함과 존경심을 느낍니다.</p>

<p>이번 스터디를 통해 많은 것을 배웠고, 블로그를 작성하는데 있어서도 많은 도움이 되었습니다.
회사에서 필요한 글만 쓰다가, 정말 오랜만에 개인을 위한 글도 이렇게 꾸준히 쓴것도 참 오랜만입니다. 
지난 테라폼 스터디부터 하면 5개월 정도는 거의 매주 글을 썼던 것 같습니다. 
—물론 실습 위주여서 “글”을 쓴게 맞냐는 문제가 있긴합니다.— 
예전에는 정말 시덥잖은 블로그 글도 많이 썼었는데, 나이도 들고, 사회적 지위도 (아직은 낮지만 예전보다) 높아지다보니
글을 쓰는데 있어서도 조금은 부담이 되었었는데, 이번 스터디를 통해 다시 글을 쓰는 재미와 글을 쓸 수 있다는 자신감을 느끼게 되었습니다.
앞으로도 스터디가 아니더라도 꾸준히 글을 쓰고 싶습니다.</p>

<p>약 3개월 간 스터디를 잘 이끌어주신 가시다 님과 조력자 분들께 큰 감사를 드립니다. :bow:</p>

<p>스터디에 참여하신 모든 분들도 고생 많으셨습니다. 다음에 또 뵐 수 있기를 기대합니다. :smile:</p>]]></content><author><name></name></author><category term="kans" /><category term="kubernetes," /><category term="network," /><category term="linux" /><summary type="html"><![CDATA[이번 주에는 AWS EKS의 VPC CNI에 대해 알아보겠습니다.]]></summary></entry><entry><title type="html">[KANS 3기] Cilium CNI</title><link href="https://sweetlittlebird.github.io/posts/2024-10-26-KANS-Study-Week8/" rel="alternate" type="text/html" title="[KANS 3기] Cilium CNI" /><published>2024-10-26T01:00:18+09:00</published><updated>2024-10-26T01:00:18+09:00</updated><id>https://sweetlittlebird.github.io/posts/KANS%20Study%20-%20Week8</id><content type="html" xml:base="https://sweetlittlebird.github.io/posts/2024-10-26-KANS-Study-Week8/"><![CDATA[<h2 id="들어가며">들어가며</h2>

<p>이번주에는 드디어 CNI의 끝판 왕(제가 아는 한에서)인 Cilium에 대해 알아보겠습니다.
어떤 것들이 가능할지 궁금합니다. 
KANS 3기 8주차 스터디를 시작하겠습니다.</p>

<hr />

<h2 id="cilium">Cilium</h2>

<p>Cilium은 eBPF(Berkeley Packet Filter)를 사용하여 네트워크 보안 및 라우팅을 제공하는 CNI(Container Network Interface) 플러그인입니다.
먼저 Cilium의 근간이 되는 eBPF에 대해 간단히 소개하고, Cilium에 대한 소개와 실습을 진행하겠습니다.</p>

<h3 id="bpfebpf-소개">BPF/eBPF 소개</h3>

<ul>
  <li>소개글 : <a href="https://hyeyoo.com/133">링크</a></li>
</ul>

<p><img src="/assets/2024/kans-3th/w8/20241026_kans_w8_1.png" alt="img.png" class="image-center" />
<em class="image-caption">기존 리눅스 Netfilter 기반 네트워크 스택</em></p>

<p>기존의 네트워크 스택은 Netfilter 기반으로 동작하며, 복잡한 네트워크 레이어를 거쳐야하고 이 레이어를 건너뛰기 어렵습니다.
또한 kube-proxy와 같은 userland 프로세스를 통해 네트워크 패킷을 처리합니다. 
그러다보니 오버헤드가 커져서 성능이 떨어지고, 룰이 복잡해질 경우 수 많은 룰을 관리해야하는 문제가 있습니다.</p>

<p><img src="/assets/2024/kans-3th/w8/20241026_kans_w8_2.png" alt="img_1.png" class="image-center" />
<em class="image-caption">eBPF 기반 네트워크 스택</em></p>

<p>eBPF는 커널 내부에서 동작하는 프로그램을 실행할 수 있으며, 이를 통해 네트워크 스택을 확장할 수 있습니다.
특히 샌드박스 방식을 통해 eBPF 프로그램이 커널에 영향을 미치지 않도록 보호할 수 있습니다. 
즉, eBPF 프로그램이 잘못된 동작을 하더라도 커널 패닉등의 발생이 거의 없습니다.</p>

<p>또한 XDP(eXpress Data Path)를 통해 네트워크 패킷을 처리할 수 있으며, 
이 XDP는 네트워크 카드(Offloaded mode), 네트워크 드라이버(Native mode), 커널 스페이스(Generic Mode)에서
동작하여 훨씬 빠르게 패킷을 처리할 수 있습니다.</p>

<p><img src="/assets/2024/kans-3th/w8/20241026_kans_w8_3.png" alt="img.png" class="image-center w-80" />
<em class="image-caption">iptables와 eBPF 성능 비교</em></p>

<ul>
  <li>eBPF 활용처
<img src="/assets/2024/kans-3th/w8/20241026_kans_w8_4.png" alt="img.png" class="image-center w-80" />
    <ul>
      <li><strong>Security</strong> (보안) : eBPF는 하드웨어 레벨에서 부터 모든 시스템 콜을 이해하고 처리할 수 있기 때문에 보다 더 세밀하고 강력한 보안 정책을 적용할 수 있습니다.</li>
      <li><strong>Tracing &amp; Profiling</strong> (추적 및 프로파일링) : eBPF는 커널 내부의 모든 이벤트를 추적하고 프로파일링 할 수 있습니다. 기존에 해결하기 어려웠던 성능 문제들도 eBPF를 통해 해결할 수 있습니다.</li>
      <li><strong>Networking</strong> (네트워킹) : eBPF는 커널 스페이스를 떠나지 않고 새로운 프로토콜을 만든다던지, 라우팅을 구현하는 등의 다양한 네트워크 기능을 만들 수 있습니다.</li>
      <li><strong>Observability</strong> (가시성) : 커널내부에서 다양한 소스에서 메트릭을 수집하고, 처리할 수 있고, 일부 데이터만 샘플링하는 것이 아닌 모든 데이터를 수집할 수 있습니다.</li>
    </ul>
  </li>
</ul>

<h3 id="cilium-소개">Cilium 소개</h3>

<ul>
  <li><strong>Cilium</strong>은 기존의 복잡한 네트워크 스택을 <strong>eBPF</strong>를 통해 <strong>간소화</strong>하고, <strong>빠르게 처리</strong>할 수 있도록 하는 CNI 플러그인입니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w8/20241026_kans_w8_5.png" alt="img.png" class="image-center w-80" />
<em class="image-caption"><a href="https://isovalent.com/blog/post/migrating-from-metallb-to-cilium/">https://isovalent.com/blog/post/migrating-from-metallb-to-cilium/</a></em></p>

<ul>
  <li>Cilium eBPF는 추가적인 코드 수정이나 설정 변경없이, 리눅스 커널에서 동작하는 Bytecode를 자유롭게 프로그래밍하여 커널에 로딩시켜 동작이 가능합니다. <a href="https://www.youtube.com/watch?v=qsnR-s4XuGo&amp;t=1059s">링크</a></li>
  <li>또한 eBPF는 모든 패킷을 가로채기 위해서 수신 NIC의 ingress TC(<a href="https://netbeez.net/blog/how-to-use-the-linux-traffic-control/">Traffic Control</a>) hooks를 사용할 수 있습니다.
<img src="/assets/2024/kans-3th/w8/20241026_kans_w8_6.png" alt="img.png" class="image-center w-60" />
<em class="image-caption">NIC의 TC Hooks에 eBPF 프로그램이 attach 된 예</em></li>
  <li>Cilium은 터널모드(VXLAN, GENEVE), 네이티브 라우팅 모드의 2가지 네트워크 모드를 제공합니다.
    <ul>
      <li><strong>터널모드</strong> : Cilium이 VXLAN(UDP 8472), GENEVE(UDP 6081) 인터페이스를 만들어서 이들을 통해 트래픽을 전달합니다. Encapsulation 모드라고도 합니다.</li>
      <li><strong>네이티브 라우팅 모드</strong> : Cilium가 패킷 전달을 위해 구성을 변경하지 않고, 외부에서 제공되는 패킷 전달 방법(클라우드 또는 BGP 라우팅등)을 사용합니다. Direct Routing 모드라고도 합니다.
<img src="/assets/2024/kans-3th/w8/20241026_kans_w8_7.png" alt="img.png" class="image-center w-80" />
<em class="image-caption">Cilium 네트워크 모드 - <a href="https://docs.cilium.io/en/stable/network/concepts/routing/">링크</a></em></li>
    </ul>
  </li>
  <li>2021년 10월 Cilium은 CNCF에 채택되었습니다. <a href="https://cilium.io/blog/2021/10/13/cilium-joins-cncf/">링크</a></li>
  <li>Googke GKE dataplane과 AWS EKS Anywhere에서 기본 CNI로 Cilium을 사용하고 있습니다. <a href="https://isovalent.com/blog/post/2021-09-aws-eks-anywhere-chooses-cilium/">링크</a></li>
  <li>Cilium은 Kube-Proxy를 100% 대체 가능합니다.
    <ul>
      <li>iptables를 거의 사용하지 않고도 동작하며, 이를 통해 성능을 향상시킬 수 있습니다.</li>
      <li>하지만 istio, FHRP, VRRP 등과 같이 iptables 기능을 활용하는 동작들은 이슈가 발생할 수 있으며, 차츰 해결해 나가고 있습니다.</li>
    </ul>
  </li>
</ul>

<h4 id="cilium-아키텍쳐">Cilium 아키텍쳐</h4>

<ul>
  <li>구성요소 - <a href="https://ssup2.github.io/blog-software/docs/theory-analysis/kubernetes-cilium-plugin/">링크</a>
<img src="/assets/2024/kans-3th/w8/20241026_kans_w8_8.png" alt="img.png" class="image-center w-80" />
<em class="image-caption">Cilium 아키텍쳐 - <a href="https://github.com/cilium/cilium">출처</a></em>
    <ul>
      <li>Cilium <strong>Agent</strong> : 데몬셋으로 실행, K8S API 설정으로 부터 ‘네트워크 설정, 네트워크 정책, 서비스 부하분산, 모니터링’ 등을 수행하며, eBPF 프로그램을 관리합니다.</li>
      <li>Cilium <strong>Client</strong> (CLI) : Cilium 커멘드툴로 eBPF maps 에 직접 접속하여 상태를 확인할 수 있습니다.</li>
      <li>Cilium <strong>Operator</strong> : K8S 클러스터에 대한 한 번씩 처리해야 하는 작업을 관리합니다.</li>
      <li><strong>Hubble</strong> : 네트워크와 보안 모니터링 플랫폼 역할을 하여, Server, Relay, Client, Graphical UI 로 구성되어 있습니다.</li>
      <li><strong>Data Store</strong> : Cilium Agent 간의 상태를 저장하고 전파하는 데이터 저장소, 2가지 종류 중 선택(K8S CRDs, Key-Value Store)할 수 있습니다.</li>
    </ul>
  </li>
</ul>

<h3 id="cilium-실습">Cilium 실습</h3>

<p>실습을 통해 Cilium CNI에 대해서 알아보겠습니다. 먼저 Cilium을 설치하고, 기본적인 설정을 확인하고, 네트워크 정책을 설정해보겠습니다.</p>

<h4 id="cilium-설치">Cilium 설치</h4>

<h5 id="helm을-통한-설치-및-확인">Helm을 통한 설치 및 확인</h5>

<ul>
  <li>helm 옵션 : <a href="https://docs.cilium.io/en/stable/helm-reference/">https://docs.cilium.io/en/stable/helm-reference/</a></li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 모니터링</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> kubectl get node,pod <span class="nt">-A</span> <span class="nt">-owide</span>

<span class="c">#</span>
<span class="nv">$ </span>helm repo add cilium https://helm.cilium.io/
<span class="c"># =&gt; &amp;quot;cilium&amp;quot; has been added to your repositories</span>
<span class="nv">$ </span>helm repo update
<span class="c"># =&gt; ...Successfully got an update from the &amp;quot;cilium&amp;quot; chart repository</span>
<span class="c">#    Update Complete. ⎈Happy Helming!⎈</span>

<span class="c">#</span>
<span class="nv">$ </span>helm <span class="nb">install </span>cilium cilium/cilium <span class="nt">--version</span> 1.16.3 <span class="nt">--namespace</span> kube-system <span class="se">\</span>
  <span class="nt">--set</span> <span class="nv">k8sServiceHost</span><span class="o">=</span>192.168.10.10 <span class="nt">--set</span> <span class="nv">k8sServicePort</span><span class="o">=</span>6443 <span class="nt">--set</span> debug.enabled<span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
  <span class="nt">--set</span> <span class="nv">rollOutCiliumPods</span><span class="o">=</span><span class="nb">true</span> <span class="nt">--set</span> <span class="nv">routingMode</span><span class="o">=</span>native <span class="nt">--set</span> <span class="nv">autoDirectNodeRoutes</span><span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
  <span class="nt">--set</span> bpf.masquerade<span class="o">=</span><span class="nb">true</span> <span class="nt">--set</span> bpf.hostRouting<span class="o">=</span><span class="nb">true</span> <span class="nt">--set</span> endpointRoutes.enabled<span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
  <span class="nt">--set</span> ipam.mode<span class="o">=</span>kubernetes <span class="nt">--set</span> k8s.requireIPv4PodCIDR<span class="o">=</span><span class="nb">true</span> <span class="nt">--set</span> <span class="nv">kubeProxyReplacement</span><span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
  <span class="nt">--set</span> <span class="nv">ipv4NativeRoutingCIDR</span><span class="o">=</span>192.168.0.0/16 <span class="nt">--set</span> <span class="nv">installNoConntrackIptablesRules</span><span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
  <span class="nt">--set</span> hubble.ui.enabled<span class="o">=</span><span class="nb">true</span> <span class="nt">--set</span> hubble.relay.enabled<span class="o">=</span><span class="nb">true</span> <span class="nt">--set</span> prometheus.enabled<span class="o">=</span><span class="nb">true</span> <span class="nt">--set</span> operator.prometheus.enabled<span class="o">=</span><span class="nb">true</span> <span class="nt">--set</span> hubble.metrics.enableOpenMetrics<span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
  <span class="nt">--set</span> hubble.metrics.enabled<span class="o">=</span><span class="s2">"{dns:query;ignoreAAAA,drop,tcp,flow,port-distribution,icmp,httpV2:exemplars=true;labelsContext=source_ip</span><span class="se">\,</span><span class="s2">source_namespace</span><span class="se">\,</span><span class="s2">source_workload</span><span class="se">\,</span><span class="s2">destination_ip</span><span class="se">\,</span><span class="s2">destination_namespace</span><span class="se">\,</span><span class="s2">destination_workload</span><span class="se">\,</span><span class="s2">traffic_direction}"</span> <span class="se">\</span>
  <span class="nt">--set</span> operator.replicas<span class="o">=</span>1
<span class="c"># =&gt; NAME: cilium</span>
<span class="c">#    LAST DEPLOYED: Sat Oct 01 07:23:34 2024</span>
<span class="c">#    NAMESPACE: kube-system</span>
<span class="c">#    STATUS: deployed</span>
<span class="c">#    REVISION: 1</span>
<span class="c">#    TEST SUITE: None</span>
<span class="c">#    NOTES:</span>
<span class="c">#    You have successfully installed Cilium with Hubble Relay and Hubble UI.</span>
<span class="c">#    </span>
<span class="c">#    Your release version is 1.16.3.</span>
<span class="c">#    </span>
<span class="c">#    For any further help, visit https://docs.cilium.io/en/v1.16/gettinghelp</span>

<span class="c">## 주요 파라미터 설명</span>
<span class="c"># --set debug.enabled=true # cilium 파드에 로그 레벨을 debug 설정</span>
<span class="c"># --set autoDirectNodeRoutes=true # 동일 대역 내의 노드들 끼리는 상대 노드의 podCIDR 대역의 라우팅이 자동으로 설정</span>
<span class="c"># --set endpointRoutes.enabled=true # 호스트에 endpoint(파드)별 개별 라우팅 설정</span>
<span class="c"># --set hubble.relay.enabled=true --set hubble.ui.enabled=true # hubble 활성화</span>
<span class="c"># --set ipam.mode=kubernetes --set k8s.requireIPv4PodCIDR=true # k8s IPAM 활용</span>
<span class="c"># --set kubeProxyReplacement=true # kube-proxy 없이 (최대한) 대처할수 있수 있게</span>
<span class="c"># --set ipv4NativeRoutingCIDR=192.168.0.0/16 # 해당 대역과 통신 시 IP Masq 하지 않음, 보통 사내망 대역을 지정</span>
<span class="c"># --set operator.replicas=1 # cilium-operator 파드 기본 1개</span>
<span class="c"># --set enableIPv4Masquerade=true --set bpf.masquerade=true # 파드를 위한 Masquerade , 추가로 Masquerade 을 BPF 로 처리 &gt;&gt; enableIPv4Masquerade=true 인 상태에서 추가로 bpf.masquerade=true 적용이 가능</span>

<span class="c"># 설정 및 확인</span>
<span class="nv">$ </span>ip <span class="nt">-c</span> addr
<span class="c"># =&gt; ...</span>
<span class="c">#    4: cilium_net@cilium_host: &amp;lt;BROADCAST,MULTICAST,NOARP,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UP group default qlen 1000</span>
<span class="c">#        link/ether 3a:03:2f:7e:cc:72 brd ff:ff:ff:ff:ff:ff</span>
<span class="c">#        inet6 fe80::3803:2fff:fe7e:cc72/64 scope link</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    5: cilium_host@cilium_net: &amp;lt;BROADCAST,MULTICAST,NOARP,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UP group default qlen 1000</span>
<span class="c">#        link/ether ca:73:d2:a6:00:b4 brd ff:ff:ff:ff:ff:ff</span>
<span class="c">#        inet 172.16.0.227/32 scope global cilium_host</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#        inet6 fe80::c873:d2ff:fea6:b4/64 scope link</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    ...</span>
<span class="nv">$ </span>kubectl get node,pod,svc <span class="nt">-A</span> <span class="nt">-owide</span>
<span class="c"># =&gt; NAME          STATUS   ROLES                  AGE     VERSION        INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION       CONTAINER-RUNTIME</span>
<span class="c">#    node/k3s-m    Ready    control-plane,master   3h58m   v1.30.5+k3s1   192.168.10.10    &amp;lt;none&amp;gt;        Ubuntu 22.04.5 LTS   5.15.0-119-generic   containerd://1.7.21-k3s2</span>
<span class="c">#    node/k3s-w1   Ready    &amp;lt;none&amp;gt;                 3h57m   v1.30.5+k3s1   192.168.10.101   &amp;lt;none&amp;gt;        Ubuntu 22.04.5 LTS   5.15.0-119-generic   containerd://1.7.21-k3s2</span>
<span class="c">#    node/k3s-w2   Ready    &amp;lt;none&amp;gt;                 3h55m   v1.30.5+k3s1   192.168.10.102   &amp;lt;none&amp;gt;        Ubuntu 22.04.5 LTS   5.15.0-119-generic   containerd://1.7.21-k3s2</span>
<span class="c">#    </span>
<span class="c">#    NAMESPACE     NAME                                          READY   STATUS    RESTARTS         AGE     IP               NODE     NOMINATED NODE   READINESS GATES</span>
<span class="c">#    kube-system   pod/cilium-envoy-9q7c6                        1/1     Running   0                5m19s   192.168.10.102   k3s-w2   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    kube-system   pod/cilium-ljv9t                              1/1     Running   0                5m19s   192.168.10.101   k3s-w1   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    kube-system   pod/cilium-operator-76bb588dbc-gxrqx          1/1     Running   0                5m19s   192.168.10.101   k3s-w1   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    kube-system   pod/cilium-q96l4                              1/1     Running   0                5m19s   192.168.10.102   k3s-w2   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    kube-system   pod/coredns-7b98449c4-x5756                   1/1     Running   0                2m59s   172.16.1.21      k3s-w1   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    kube-system   pod/hubble-relay-88f7f89d4-fcq2s              1/1     Running   0                5m19s   172.16.1.99      k3s-w1   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    kube-system   pod/hubble-ui-59bb4cb67b-r48tz                2/2     Running   0                5m19s   172.16.0.238     k3s-m    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    kube-system   pod/local-path-provisioner-6795b5f9d8-84m96   1/1     Running   11 (5m28s ago)   3h58m   172.16.2.184     k3s-w2   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    kube-system   pod/metrics-server-cdcc87586-g5m2d            1/1     Running   11 (5m10s ago)   3h58m   172.16.2.223     k3s-w2   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    ...</span>
<span class="c">#    </span>
<span class="c">#    NAMESPACE     NAME                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                  AGE     SELECTOR</span>
<span class="c">#    default       service/kubernetes       ClusterIP   10.10.200.1     &amp;lt;none&amp;gt;        443/TCP                  3h58m   &amp;lt;none&amp;gt;</span>
<span class="c">#    kube-system   service/cilium-envoy     ClusterIP   None            &amp;lt;none&amp;gt;        9964/TCP                 5m19s   k8s-app=cilium-envoy</span>
<span class="c">#    kube-system   service/hubble-metrics   ClusterIP   None            &amp;lt;none&amp;gt;        9965/TCP                 5m19s   k8s-app=cilium</span>
<span class="c">#    kube-system   service/hubble-peer      ClusterIP   10.10.200.203   &amp;lt;none&amp;gt;        443/TCP                  5m19s   k8s-app=cilium</span>
<span class="c">#    kube-system   service/hubble-relay     ClusterIP   10.10.200.175   &amp;lt;none&amp;gt;        80/TCP                   5m19s   k8s-app=hubble-relay</span>
<span class="c">#    kube-system   service/hubble-ui        ClusterIP   10.10.200.125   &amp;lt;none&amp;gt;        80/TCP                   5m19s   k8s-app=hubble-ui</span>
<span class="c">#    kube-system   service/kube-dns         ClusterIP   10.10.200.10    &amp;lt;none&amp;gt;        53/UDP,53/TCP,9153/TCP   3h58m   k8s-app=kube-dns</span>
<span class="c">#    kube-system   service/metrics-server   ClusterIP   10.10.200.180   &amp;lt;none&amp;gt;        443/TCP                  3h58m   k8s-app=metrics-server</span>

<span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> filter <span class="nt">-S</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> raw <span class="nt">-S</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> mangle <span class="nt">-S</span>
<span class="nv">$ </span>conntrack <span class="nt">-L</span>

<span class="nv">$ </span>kubectl get crd | <span class="nb">grep </span>cilium
<span class="c"># =&gt; ciliumcidrgroups.cilium.io                   2024-10-01T11:10:57Z</span>
<span class="c">#    ciliumclusterwidenetworkpolicies.cilium.io   2024-10-01T11:10:58Z</span>
<span class="c">#    ciliumendpoints.cilium.io                    2024-10-01T11:10:57Z</span>
<span class="c">#    ciliumexternalworkloads.cilium.io            2024-10-01T11:10:57Z</span>
<span class="c">#    ciliumidentities.cilium.io                   2024-10-01T11:10:58Z</span>
<span class="c">#    ciliuml2announcementpolicies.cilium.io       2024-10-01T11:10:57Z</span>
<span class="c">#    ciliumloadbalancerippools.cilium.io          2024-10-01T11:10:57Z</span>
<span class="c">#    ciliumnetworkpolicies.cilium.io              2024-10-01T11:10:58Z</span>
<span class="c">#    ciliumnodeconfigs.cilium.io                  2024-10-01T11:10:57Z</span>
<span class="c">#    ciliumnodes.cilium.io                        2024-10-01T11:10:57Z</span>
<span class="c">#    ciliumpodippools.cilium.io                   2024-10-01T11:10:57Z</span>
<span class="nv">$ </span>kubectl get ciliumnodes <span class="c"># cilium_host 인터페이스의 IP 확인 : CILIUMINTERNALIP</span>
<span class="c"># =&gt; NAME     CILIUMINTERNALIP   INTERNALIP       AGE</span>
<span class="c">#    k3s-m    172.16.0.227       192.168.10.10    21m</span>
<span class="c">#    k3s-w1   172.16.1.82        192.168.10.101   21m</span>
<span class="c">#    k3s-w2   172.16.2.25        192.168.10.102   21m</span>
<span class="nv">$ </span>kubectl get ciliumendpoints <span class="nt">-A</span>
<span class="c"># =&gt; NAMESPACE     NAME                                      SECURITY IDENTITY   ENDPOINT STATE   IPV4           IPV6</span>
<span class="c">#    kube-system   coredns-7b98449c4-x5756                   11970               ready            172.16.1.21</span>
<span class="c">#    kube-system   hubble-relay-88f7f89d4-fcq2s              4124                ready            172.16.1.99</span>
<span class="c">#    kube-system   hubble-ui-59bb4cb67b-r48tz                37523               ready            172.16.0.238</span>
<span class="c">#    kube-system   local-path-provisioner-6795b5f9d8-84m96   11088               ready            172.16.2.184</span>
<span class="c">#    kube-system   metrics-server-cdcc87586-g5m2d            27624               ready            172.16.2.223</span>

<span class="nv">$ </span>kubectl get cm <span class="nt">-n</span> kube-system cilium-config <span class="nt">-o</span> json | jq

<span class="nv">$ </span>kubetail <span class="nt">-n</span> kube-system <span class="nt">-l</span> k8s-app<span class="o">=</span>cilium <span class="nt">--since</span> 1h
<span class="nv">$ </span>kubetail <span class="nt">-n</span> kube-system <span class="nt">-l</span> k8s-app<span class="o">=</span>cilium-envoy <span class="nt">--since</span> 1h

<span class="c"># Native XDP 지원 NIC 확인 : https://docs.cilium.io/en/stable/bpf/progtypes/#xdp-drivers</span>
<span class="nv">$ </span>ethtool <span class="nt">-i</span> enp0s8
<span class="c"># =&gt; driver: virtio_net   # &gt;= XDP 4.10 부터 지원되는듯 합니다.</span>
<span class="c">#    version: 1.0.0</span>
<span class="c">#    ...</span>

<span class="c"># https://docs.cilium.io/en/stable/operations/performance/tuning/#bypass-iptables-connection-tracking</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> kubectl get pod <span class="nt">-A</span> <span class="c"># 모니터링</span>
<span class="nv">$ </span>helm upgrade cilium cilium/cilium <span class="nt">--namespace</span> kube-system <span class="nt">--reuse-values</span> <span class="nt">--set</span> <span class="nv">installNoConntrackIptablesRules</span><span class="o">=</span><span class="nb">true</span>
<span class="c"># =&gt; Release &amp;quot;cilium&amp;quot; has been upgraded. Happy Helming!</span>
<span class="c">#    NAME: cilium</span>
<span class="c">#    LAST DEPLOYED: Sat Oct 01 11:42:10 2024</span>
<span class="c">#    NAMESPACE: kube-system</span>
<span class="c">#    STATUS: deployed</span>
<span class="c">#    REVISION: 2</span>
<span class="c">#    TEST SUITE: None</span>
<span class="c">#    NOTES:</span>
<span class="c">#    You have successfully installed Cilium with Hubble Relay and Hubble UI.</span>
<span class="c">#    </span>
<span class="c">#    Your release version is 1.16.3.</span>
<span class="c">#    </span>
<span class="c">#    For any further help, visit https://docs.cilium.io/en/v1.16/gettinghelp</span>

<span class="c"># 확인: 기존 raw 에 아래 rule 추가 확인</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> raw <span class="nt">-S</span> | <span class="nb">grep </span>notrack
<span class="c"># =&gt; -A CILIUM_OUTPUT_raw -d 192.168.0.0/16 -m comment --comment "cilium: NOTRACK for pod traffic" -j CT --notrack</span>
<span class="c">#    -A CILIUM_OUTPUT_raw -s 192.168.0.0/16 -m comment --comment "cilium: NOTRACK for pod traffic" -j CT --notrack</span>
<span class="c">#    ...</span>

<span class="nv">$ </span>conntrack <span class="nt">-F</span>
<span class="nv">$ </span>conntrack <span class="nt">-L</span>
<span class="nv">$ </span>conntrack <span class="nt">-L</span> |grep <span class="nt">-v</span> 2379
</code></pre></div></div>

<h5 id="cilium-cli를-통한-확인">Cilium CLI를 통한 확인</h5>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># cilium CLI 설치</span>
<span class="nv">$ CILIUM_CLI_VERSION</span><span class="o">=</span><span class="si">$(</span>curl <span class="nt">-s</span> https://raw.githubusercontent.com/cilium/cilium-cli/main/stable.txt<span class="si">)</span>
<span class="nv">$ CLI_ARCH</span><span class="o">=</span>amd64
<span class="nv">$ </span><span class="k">if</span> <span class="o">[</span> <span class="s2">"</span><span class="si">$(</span><span class="nb">uname</span> <span class="nt">-m</span><span class="si">)</span><span class="s2">"</span> <span class="o">=</span> <span class="s2">"aarch64"</span> <span class="o">]</span><span class="p">;</span> <span class="k">then </span><span class="nv">CLI_ARCH</span><span class="o">=</span>arm64<span class="p">;</span> <span class="k">fi</span>
<span class="nv">$ </span>curl <span class="nt">-L</span> <span class="nt">--fail</span> <span class="nt">--remote-name-all</span> https://github.com/cilium/cilium-cli/releases/download/<span class="k">${</span><span class="nv">CILIUM_CLI_VERSION</span><span class="k">}</span>/cilium-linux-<span class="k">${</span><span class="nv">CLI_ARCH</span><span class="k">}</span>.tar.gz<span class="o">{</span>,.sha256sum<span class="o">}</span>
<span class="nv">$ </span><span class="nb">sha256sum</span> <span class="nt">--check</span> cilium-linux-<span class="k">${</span><span class="nv">CLI_ARCH</span><span class="k">}</span>.tar.gz.sha256sum
<span class="nv">$ </span><span class="nb">sudo tar </span>xzvfC cilium-linux-<span class="k">${</span><span class="nv">CLI_ARCH</span><span class="k">}</span>.tar.gz /usr/local/bin
<span class="nv">$ </span><span class="nb">rm </span>cilium-linux-<span class="k">${</span><span class="nv">CLI_ARCH</span><span class="k">}</span>.tar.gz<span class="o">{</span>,.sha256sum<span class="o">}</span>

<span class="c"># 테스트</span>
<span class="nv">$ </span>cilium status <span class="nt">--wait</span>
<span class="c"># =&gt;     /¯¯\</span>
<span class="c">#     /¯¯\__/¯¯\    Cilium:             OK</span>
<span class="c">#     \__/¯¯\__/    Operator:           OK</span>
<span class="c">#     /¯¯\__/¯¯\    Envoy DaemonSet:    OK</span>
<span class="c">#     \__/¯¯\__/    Hubble Relay:       OK</span>
<span class="c">#        \__/       ClusterMesh:        disabled</span>
<span class="c">#    </span>
<span class="c">#    DaemonSet              cilium             Desired: 3, Ready: 3/3, Available: 3/3</span>
<span class="c">#    DaemonSet              cilium-envoy       Desired: 3, Ready: 3/3, Available: 3/3</span>
<span class="c">#    Deployment             cilium-operator    Desired: 1, Ready: 1/1, Available: 1/1</span>
<span class="c">#    Deployment             hubble-relay       Desired: 1, Ready: 1/1, Available: 1/1</span>
<span class="c">#    Deployment             hubble-ui          Desired: 1, Ready: 1/1, Available: 1/1</span>
<span class="c">#    Containers:            cilium             Running: 3</span>
<span class="c">#                           cilium-envoy       Running: 3</span>
<span class="c">#                           cilium-operator    Running: 1</span>
<span class="c">#                           hubble-relay       Running: 1</span>
<span class="c">#                           hubble-ui          Running: 1</span>
<span class="c">#    Cluster Pods:          5/5 managed by Cilium</span>
<span class="c">#    Helm chart version:    1.16.3</span>
<span class="c">#    Image versions         cilium             quay.io/cilium/cilium:v1.16.3@sha256:62d2a09bbef840a46099ac4c69421c90f84f28d018d479749049011329aa7f28: 3</span>
<span class="c">#                           cilium-envoy       quay.io/cilium/cilium-envoy:v1.29.9-1728346947-0d05e48bfbb8c4737ec40d5781d970a550ed2bbd@sha256:42614a44e508f70d03a04470df5f61e3cffd22462471a0be0544cf116f2c50ba: 3</span>
<span class="c">#                           cilium-operator    quay.io/cilium/operator-generic:v1.16.3@sha256:6e2925ef47a1c76e183c48f95d4ce0d34a1e5e848252f910476c3e11ce1ec94b: 1</span>
<span class="c">#                           hubble-relay       quay.io/cilium/hubble-relay:v1.16.3@sha256:feb60efd767e0e7863a94689f4a8db56a0acc7c1d2b307dee66422e3dc25a089: 1</span>
<span class="c">#                           hubble-ui          quay.io/cilium/hubble-ui-backend:v0.13.1@sha256:0e0eed917653441fded4e7cdb096b7be6a3bddded5a2dd10812a27b1fc6ed95b: 1</span>
<span class="c">#                           hubble-ui          quay.io/cilium/hubble-ui:v0.13.1@sha256:e2e9313eb7caf64b0061d9da0efbdad59c6c461f6ca1752768942bfeda0796c6: 1</span>

<span class="nv">$ </span>cilium connectivity <span class="nb">test</span>

<span class="nv">$ </span>cilium config view
<span class="c"># =&gt; agent-not-ready-taint-key                         node.cilium.io/agent-not-ready</span>
<span class="c">#    arping-refresh-period                             30s</span>
<span class="c">#    auto-direct-node-routes                           true</span>
<span class="c">#    bpf-events-drop-enabled                           true</span>
<span class="c">#    bpf-events-policy-verdict-enabled                 true</span>
<span class="c">#    bpf-events-trace-enabled                          true</span>
<span class="c">#    bpf-lb-acceleration                               disabled</span>
<span class="c">#    bpf-lb-external-clusterip                         false</span>
<span class="c">#    bpf-lb-map-max                                    65536</span>
<span class="c">#    bpf-lb-sock                                       false</span>
<span class="c">#    bpf-lb-sock-terminate-pod-connections             false</span>
<span class="c">#    bpf-map-dynamic-size-ratio                        0.0025</span>
<span class="c">#    bpf-policy-map-max                                16384</span>
<span class="c">#    bpf-root                                          /sys/fs/bpf</span>
<span class="c">#    ...</span>
<span class="c">#    ipv4-native-routing-cidr                          192.168.0.0/16</span>
<span class="c">#    ...</span>
<span class="c">#    kube-proxy-replacement                            true</span>
<span class="c">#    kube-proxy-replacement-healthz-bind-address</span>
<span class="c">#    max-connected-clusters                            255</span>
<span class="c">#    mesh-auth-enabled                                 true</span>
<span class="c">#    ...</span>

<span class="c"># cilium 데몬셋 파드 내에서 cilium 명령어로 상태 확인</span>
<span class="nv">$ </span><span class="nb">export </span><span class="nv">CILIUMPOD0</span><span class="o">=</span><span class="si">$(</span>kubectl get <span class="nt">-l</span> k8s-app<span class="o">=</span>cilium pods <span class="nt">-n</span> kube-system <span class="nt">--field-selector</span> spec.nodeName<span class="o">=</span>k3s-m  <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.items[0].metadata.name}'</span><span class="si">)</span>
<span class="nv">$ </span><span class="nb">alias </span><span class="nv">c0</span><span class="o">=</span><span class="s2">"kubectl exec -it </span><span class="nv">$CILIUMPOD0</span><span class="s2"> -n kube-system -c cilium-agent -- cilium"</span>
<span class="nv">$ </span>c0 status <span class="nt">--verbose</span>
<span class="c"># =&gt; ...</span>
<span class="c">#    KubeProxyReplacement:   True   [enp0s3   10.0.2.15 fe80::6d:daff:feb3:d4d3, enp0s8   192.168.10.10 fe80::27ff:fe8a:de00 (Direct Routing)]</span>
<span class="c">#    ...</span>
<span class="c">#    IPAM:                   IPv4: 4/254 allocated from 172.16.0.0/24, </span>
<span class="c">#    Allocated addresses:</span>
<span class="c">#      172.16.0.223 (kube-system/hubble-ui-59bb4cb67b-r48tz)</span>
<span class="c">#      172.16.0.227 (router)</span>
<span class="c">#      172.16.0.26 (health)</span>
<span class="c">#      172.16.0.44 (cilium-test-1/client3-67f959dd9b-ptl65)</span>
<span class="c">#    ...</span>
<span class="c">#    Routing:                Network: Native   Host: BPF</span>
<span class="c">#    ...</span>
<span class="c">#    Device Mode:            veth</span>
<span class="c">#    Masquerading:           BPF   [ens5]   192.168.0.0/16 [IPv4: Enabled, IPv6: Disabled]</span>
<span class="c">#    ...  </span>
<span class="c">#    Proxy Status:            OK, ip 172.16.0.227, 0 redirects active on ports 10000-20000, Envoy: external</span>
<span class="c">#    ...</span>
<span class="c">#    KubeProxyReplacement Details:</span>
<span class="c">#      Status:                 True</span>
<span class="c">#      Socket LB:              Enabled</span>
<span class="c">#      Socket LB Tracing:      Enabled</span>
<span class="c">#      Socket LB Coverage:     Full</span>
<span class="c">#      Devices:                enp0s3   10.0.2.15 fe80::6d:daff:feb3:d4d3, enp0s8   192.168.10.10 fe80::27ff:fe8a:de00 (Direct Routing)</span>
<span class="c">#      Mode:                   SNAT</span>
<span class="c">#      Backend Selection:      Random</span>
<span class="c">#      Session Affinity:       Enabled</span>
<span class="c">#      Graceful Termination:   Enabled</span>
<span class="c">#      NAT46/64 Support:       Disabled</span>
<span class="c">#      XDP Acceleration:       Disabled</span>
<span class="c">#      Services:</span>
<span class="c">#      - ClusterIP:      Enabled</span>
<span class="c">#      - NodePort:       Enabled (Range: 30000-32767) </span>
<span class="c">#      - LoadBalancer:   Enabled </span>
<span class="c">#      - externalIPs:    Enabled </span>
<span class="c">#      - HostPort:       Enabled</span>
<span class="c">#    BPF Maps:   dynamic sizing: on (ratio: 0.002500)</span>
<span class="c">#    ...</span>

<span class="c"># Native Routing 확인 : # 192.168.0.0/16 대역은 IP Masq 없이 라우팅</span>
<span class="nv">$ </span>c0 status | <span class="nb">grep </span>KubeProxyReplacement
<span class="c"># =&gt; KubeProxyReplacement:    True   [enp0s3   10.0.2.15 fe80::6d:daff:feb3:d4d3, enp0s8   192.168.10.10 fe80::27ff:fe8a:de00 (Direct Routing)]</span>

<span class="c"># enableIPv4Masquerade=true(기본값) , bpf.masquerade=true 확인</span>
<span class="nv">$ </span>cilium config view | egrep <span class="s1">'enable-ipv4-masquerade|enable-bpf-masquerade'</span>
<span class="c"># =&gt; enable-bpf-masquerade                             true</span>
<span class="c">#    enable-ipv4-masquerade                            true</span>

<span class="nv">$ </span>c0 status <span class="nt">--verbose</span> | <span class="nb">grep </span>Masquerading
<span class="c"># =&gt; Masquerading:           BPF   [enp0s3, enp0s8]   192.168.0.0/16 [IPv4: Enabled, IPv6: Disabled]</span>

<span class="c"># Configure the eBPF-based ip-masq-agent</span>
<span class="c"># https://docs.cilium.io/en/stable/network/concepts/masquerading/</span>
<span class="nv">$ </span>helm upgrade cilium cilium/cilium <span class="nt">--namespace</span> kube-system <span class="nt">--reuse-values</span> <span class="nt">--set</span> ipMasqAgent.enabled<span class="o">=</span><span class="nb">true</span>
<span class="c"># =&gt; Release &amp;quot;cilium&amp;quot; has been upgraded. Happy Helming!</span>
<span class="c">#    ...</span>

<span class="c">#</span>
<span class="nv">$ </span>cilium config view | <span class="nb">grep</span> <span class="nt">-i</span> masq
<span class="c"># =&gt; enable-bpf-masquerade                             true</span>
<span class="c">#    enable-ip-masq-agent                              true</span>
<span class="c">#    ...</span>

<span class="nv">$ </span><span class="nb">export </span><span class="nv">CILIUMPOD0</span><span class="o">=</span><span class="si">$(</span>kubectl get <span class="nt">-l</span> k8s-app<span class="o">=</span>cilium pods <span class="nt">-n</span> kube-system <span class="nt">--field-selector</span> spec.nodeName<span class="o">=</span>k3s-m  <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.items[0].metadata.name}'</span><span class="si">)</span>
<span class="nv">$ </span><span class="nb">alias </span><span class="nv">c0</span><span class="o">=</span><span class="s2">"kubectl exec -it </span><span class="nv">$CILIUMPOD0</span><span class="s2"> -n kube-system -c cilium-agent -- cilium"</span>
<span class="nv">$ </span>c0 status <span class="nt">--verbose</span> | <span class="nb">grep </span>Masquerading
<span class="c"># =&gt; Masquerading:           BPF (ip-masq-agent)   [enp0s3, enp0s8]   192.168.0.0/16 [IPv4: Enabled, IPv6: Disabled]</span>
<span class="c"># &lt;span style="color: green;"&gt;👉 BPF가 BPF (ip-masq-agent)로 변경되었습니다.&lt;/span&gt;</span>
<span class="c"># &lt;span style="color: green;"&gt;   ip-masq-agent는 k8s 클러스터 내부에서 IP 마스커레이딩을 관리하는 컴포넌트로,&lt;/span&gt;</span>
<span class="c"># &lt;span style="color: green;"&gt;   마스커레이딩 여부를 결정하여 네트워크 자원 사용을 최적화 해줍니다.&lt;/span&gt;</span>

<span class="nv">$ </span>kubectl get cm <span class="nt">-n</span> kube-system cilium-config <span class="nt">-o</span> yaml  | <span class="nb">grep </span>ip-masq
<span class="c"># =&gt;   enable-ip-masq-agent: &amp;quot;true&amp;quot;</span>
</code></pre></div></div>

<h4 id="cilium-기본정보-확인">Cilium 기본정보 확인</h4>

<h5 id="변수--단축키">변수 &amp; 단축키</h5>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># cilium 파드 이름</span>
<span class="nv">$ </span><span class="nb">export </span><span class="nv">CILIUMPOD0</span><span class="o">=</span><span class="si">$(</span>kubectl get <span class="nt">-l</span> k8s-app<span class="o">=</span>cilium pods <span class="nt">-n</span> kube-system <span class="nt">--field-selector</span> spec.nodeName<span class="o">=</span>k3s-m  <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.items[0].metadata.name}'</span><span class="si">)</span>
<span class="nv">$ </span><span class="nb">export </span><span class="nv">CILIUMPOD1</span><span class="o">=</span><span class="si">$(</span>kubectl get <span class="nt">-l</span> k8s-app<span class="o">=</span>cilium pods <span class="nt">-n</span> kube-system <span class="nt">--field-selector</span> spec.nodeName<span class="o">=</span>k3s-w1 <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.items[0].metadata.name}'</span><span class="si">)</span>
<span class="nv">$ </span><span class="nb">export </span><span class="nv">CILIUMPOD2</span><span class="o">=</span><span class="si">$(</span>kubectl get <span class="nt">-l</span> k8s-app<span class="o">=</span>cilium pods <span class="nt">-n</span> kube-system <span class="nt">--field-selector</span> spec.nodeName<span class="o">=</span>k3s-w2 <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.items[0].metadata.name}'</span><span class="si">)</span>

<span class="c"># 단축키(alias) 지정</span>
<span class="nv">$ </span><span class="nb">alias </span><span class="nv">c0</span><span class="o">=</span><span class="s2">"kubectl exec -it </span><span class="nv">$CILIUMPOD0</span><span class="s2"> -n kube-system -c cilium-agent -- cilium"</span>
<span class="nv">$ </span><span class="nb">alias </span><span class="nv">c1</span><span class="o">=</span><span class="s2">"kubectl exec -it </span><span class="nv">$CILIUMPOD1</span><span class="s2"> -n kube-system -c cilium-agent -- cilium"</span>
<span class="nv">$ </span><span class="nb">alias </span><span class="nv">c2</span><span class="o">=</span><span class="s2">"kubectl exec -it </span><span class="nv">$CILIUMPOD2</span><span class="s2"> -n kube-system -c cilium-agent -- cilium"</span>

<span class="nv">$ </span><span class="nb">alias </span><span class="nv">c0bpf</span><span class="o">=</span><span class="s2">"kubectl exec -it </span><span class="nv">$CILIUMPOD0</span><span class="s2"> -n kube-system -c cilium-agent -- bpftool"</span>
<span class="nv">$ </span><span class="nb">alias </span><span class="nv">c1bpf</span><span class="o">=</span><span class="s2">"kubectl exec -it </span><span class="nv">$CILIUMPOD1</span><span class="s2"> -n kube-system -c cilium-agent -- bpftool"</span>
<span class="nv">$ </span><span class="nb">alias </span><span class="nv">c2bpf</span><span class="o">=</span><span class="s2">"kubectl exec -it </span><span class="nv">$CILIUMPOD2</span><span class="s2"> -n kube-system -c cilium-agent -- bpftool"</span>

<span class="c"># Hubble UI 웹 접속</span>
<span class="nv">$ </span>kubectl patch <span class="nt">-n</span> kube-system svc hubble-ui <span class="nt">-p</span> <span class="s1">'{"spec": {"type": "NodePort"}}'</span>
<span class="c"># =&gt; service/hubble-ui patched</span>
<span class="nv">$ HubbleUiNodePort</span><span class="o">=</span><span class="si">$(</span>kubectl get svc <span class="nt">-n</span> kube-system hubble-ui <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.spec.ports[0].nodePort<span class="o">}</span><span class="si">)</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"Hubble UI URL = http://</span><span class="si">$(</span>curl <span class="nt">-s</span> ipinfo.io/ip<span class="si">)</span><span class="s2">:</span><span class="nv">$HubbleUiNodePort</span><span class="s2">"</span>
<span class="c"># =&gt; Hubble UI URL = http://54.180.146.116:30732</span>
</code></pre></div></div>

<h5 id="자주-쓰는-cilium-cli-명령어">자주 쓰는 Cilium CLI 명령어</h5>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># cilium 파드 확인</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-n</span> kube-system <span class="nt">-l</span> k8s-app<span class="o">=</span>cilium <span class="nt">-owide</span>
<span class="c"># =&gt; NAME           READY   STATUS    RESTARTS   AGE     IP               NODE     NOMINATED NODE   READINESS GATES</span>
<span class="c">#    cilium-5gx8w   1/1     Running   0          9m12s   192.168.10.102   k3s-w2   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    cilium-92k6s   1/1     Running   0          8m53s   192.168.10.101   k3s-w1   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    cilium-zv22g   1/1     Running   0          9m12s   192.168.10.10    k3s-m    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>

<span class="c"># cilium 파드 재시작</span>
<span class="nv">$ </span>kubectl <span class="nt">-n</span> kube-system rollout restart ds/cilium
<span class="c"># =&gt; daemonset.apps/cilium restarted</span>
<span class="c"># 혹은</span>
<span class="nv">$ </span>kubectl delete pod <span class="nt">-n</span> kube-system <span class="nt">-l</span> k8s-app<span class="o">=</span>cilium
<span class="c"># =&gt; pod &amp;quot;cilium-5p4q4&amp;quot; deleted</span>
<span class="c">#    pod &amp;quot;cilium-bc7jz&amp;quot; deleted</span>
<span class="c">#    pod &amp;quot;cilium-zqs9l&amp;quot; deleted</span>

<span class="c"># cilium 설정 정보 확인</span>
<span class="nv">$ </span>cilium config view

<span class="c"># cilium 파드의 cilium 상태 확인</span>
<span class="nv">$ </span>c0 status <span class="nt">--verbose</span>

<span class="c"># cilium 엔드포인트 확인</span>
<span class="nv">$ </span>kubectl get ciliumendpoints <span class="nt">-A</span>
<span class="c"># =&gt; NAMESPACE       NAME                                      SECURITY IDENTITY   ENDPOINT STATE   IPV4           IPV6</span>
<span class="c">#    kube-system     coredns-7b98449c4-x5756                   11970               ready            172.16.1.21</span>
<span class="c">#    kube-system     hubble-relay-88f7f89d4-fcq2s              4124                ready            172.16.1.99</span>
<span class="c">#    kube-system     hubble-ui-59bb4cb67b-r48tz                37523               ready            172.16.0.223</span>
<span class="c">#    kube-system     local-path-provisioner-6795b5f9d8-84m96   11088               ready            172.16.2.184</span>
<span class="c">#    kube-system     metrics-server-cdcc87586-g5m2d            27624               ready            172.16.2.223</span>
<span class="nv">$ </span>c0 endpoint list
<span class="c"># =&gt; ENDPOINT   POLICY (ingress)   POLICY (egress)   IDENTITY   LABELS (source:key[=value])                                                  IPv6   IPv4           STATUS</span>
<span class="c">#               ENFORCEMENT        ENFORCEMENT</span>
<span class="c">#    640        Disabled           Disabled          37523      k8s:app.kubernetes.io/name=hubble-ui                                                172.16.0.223   ready</span>
<span class="c">#                                                               k8s:app.kubernetes.io/part-of=cilium</span>
<span class="c">#                                                               k8s:io.cilium.k8s.namespace.labels.kubernetes.io/metadata.name=kube-system</span>
<span class="c">#                                                               k8s:io.cilium.k8s.policy.cluster=default</span>
<span class="c">#                                                               k8s:io.cilium.k8s.policy.serviceaccount=hubble-ui</span>
<span class="c">#                                                               k8s:io.kubernetes.pod.namespace=kube-system</span>
<span class="c">#                                                               k8s:k8s-app=hubble-ui</span>
<span class="c">#    1196       Disabled           Disabled          1          k8s:node-role.kubernetes.io/control-plane=true                                                     ready</span>
<span class="c">#                                                               k8s:node-role.kubernetes.io/master=true</span>
<span class="c">#                                                               k8s:node.kubernetes.io/instance-type=k3s</span>
<span class="c">#                                                               reserved:host</span>
<span class="c">#    1598       Disabled           Disabled          4          reserved:health                                                                     172.16.0.26    ready</span>
<span class="nv">$ </span>c0 bpf endpoint list
<span class="c"># =&gt; IP ADDRESS        LOCAL ENDPOINT INFO</span>
<span class="c">#    172.16.0.26:0     id=1598  sec_id=4     flags=0x0000 ifindex=17  mac=CE:24:50:27:35:B9 nodemac=FE:57:C1:AD:A6:28</span>
<span class="c">#    172.16.0.223:0    id=640   sec_id=37523 flags=0x0000 ifindex=9   mac=AE:B8:81:A6:B1:28 nodemac=C2:21:B8:92:DA:FD</span>
<span class="c">#    172.16.0.227:0    (localhost)</span>
<span class="c">#    192.168.10.10:0   (localhost)</span>
<span class="c">#    10.0.2.15:0       (localhost)</span>
<span class="nv">$ </span>c0 map get cilium_lxc
<span class="c"># =&gt; Key              Value                                                                                            State   Error</span>
<span class="c">#    172.16.0.223:0   id=640   sec_id=37523 flags=0x0000 ifindex=9   mac=AE:B8:81:A6:B1:28 nodemac=C2:21:B8:92:DA:FD   sync</span>
<span class="c">#    172.16.0.26:0    id=1598  sec_id=4     flags=0x0000 ifindex=17  mac=CE:24:50:27:35:B9 nodemac=FE:57:C1:AD:A6:28   sync</span>
<span class="nv">$ </span>c0 ip list

<span class="c"># Manage the IPCache mappings for IP/CIDR &lt;-&gt; Identity</span>
<span class="nv">$ </span>c0 bpf ipcache list

<span class="c"># Service/NAT List 확인</span>
<span class="nv">$ </span>c0 service list
<span class="c"># =&gt; ID   Frontend              Service Type   Backend</span>
<span class="c">#    1    10.10.200.1:443       ClusterIP      1 =&amp;gt; 192.168.10.10:6443 (active)</span>
<span class="c">#    ...</span>
<span class="c">#    6    10.10.200.10:9153     ClusterIP      1 =&amp;gt; 172.16.1.21:9153 (active)</span>
<span class="c">#    7    10.10.200.180:443     ClusterIP      1 =&amp;gt; 172.16.2.223:10250 (active)</span>
<span class="c">#    16   0.0.0.0:30732         NodePort       1 =&amp;gt; 172.16.0.223:8081 (active)</span>
<span class="c">#    17   10.0.2.15:30732       NodePort       1 =&amp;gt; 172.16.0.223:8081 (active)</span>
<span class="c">#    18   192.168.10.10:30732   NodePort       1 =&amp;gt; 172.16.0.223:8081 (active)</span>
<span class="nv">$ </span>c0 bpf lb list
<span class="c"># =&gt; SERVICE ADDRESS           BACKEND ADDRESS (REVNAT_ID) (SLOT)</span>
<span class="c">#    10.10.200.1:443 (0)       0.0.0.0:0 (1) (0) [ClusterIP, non-routable]</span>
<span class="c">#    ...</span>
<span class="c">#    10.10.200.10:9153 (0)     0.0.0.0:0 (6) (0) [ClusterIP, non-routable]</span>
<span class="c">#    10.10.200.180:443 (1)     172.16.2.223:10250 (7) (1)</span>
<span class="c">#    0.0.0.0:30732 (0)         0.0.0.0:0 (16) (0) [NodePort, non-routable]</span>
<span class="c">#    192.168.10.10:30732 (0)   0.0.0.0:0 (18) (0) [NodePort]</span>
<span class="c">#    10.0.2.15:30732 (0)       0.0.0.0:0 (17) (0) [NodePort]</span>
<span class="nv">$ </span>c0 bpf lb list <span class="nt">--revnat</span>
<span class="c"># =&gt; ID   BACKEND ADDRESS (REVNAT_ID) (SLOT)</span>
<span class="c">#    7    10.10.200.180:443</span>
<span class="c">#    6    10.10.200.10:9153</span>
<span class="c">#    ...</span>
<span class="c">#    1    10.10.200.1:443</span>
<span class="c">#    17   10.0.2.15:30732</span>
<span class="c">#    16   0.0.0.0:30732</span>
<span class="c">#    18   192.168.10.10:30732</span>
<span class="nv">$ </span>c0 bpf nat list
<span class="c"># =&gt; TCP OUT 192.168.10.10:34576 -&amp;gt; 192.168.10.101:4240 XLATE_SRC 192.168.10.10:34576 Created=409sec ago NeedsCT=1</span>
<span class="c">#    TCP IN 192.168.10.101:4240 -&amp;gt; 192.168.10.10:34576 XLATE_DST 192.168.10.10:34576 Created=409sec ago NeedsCT=1</span>
<span class="c">#    ICMP OUT 192.168.10.10:35656 -&amp;gt; 172.16.1.224:0 XLATE_SRC 192.168.10.10:35656 Created=169sec ago NeedsCT=1</span>
<span class="c">#    ICMP OUT 192.168.10.10:35656 -&amp;gt; 192.168.10.102:0 XLATE_SRC 192.168.10.10:35656 Created=169sec ago NeedsCT=1</span>
<span class="c">#    ...</span>

<span class="c"># List all open BPF maps</span>
<span class="nv">$ </span>c0 map list
<span class="c"># =&gt; Name                       Num entries   Num errors   Cache enabled</span>
<span class="c">#    cilium_lb4_backends_v3     2             0            true</span>
<span class="c">#    cilium_lb4_source_range    0             0            true</span>
<span class="c">#    cilium_policy_01196        2             0            true</span>
<span class="c">#    cilium_policy_01598        3             0            true</span>
<span class="c">#    ...</span>
<span class="nv">$ </span>c0 map list <span class="nt">--verbose</span>

<span class="c"># List contents of a policy BPF map : Dump all policy maps</span>
<span class="nv">$ </span>c0 bpf policy get <span class="nt">--all</span>
<span class="nv">$ </span>c0 bpf policy get <span class="nt">--all</span> <span class="nt">-n</span>

<span class="c"># cilium monitor</span>
<span class="nv">$ </span>c0 monitor <span class="nt">-v</span>
<span class="c"># =&gt; Listening for events on 4 CPUs with 64x4096 of shared memory</span>
<span class="c">#    Press Ctrl-C to quit</span>
<span class="c">#    time=&amp;quot;2024-10-01T12:11:28Z&amp;quot; level=info msg=&amp;quot;Initializing dissection cache...&amp;quot; subsys=monitor</span>
<span class="c">#    -&amp;gt; network flow 0x206747a1 , identity health-&amp;gt;remote-node state reply ifindex enp0s8 orig-ip 0.0.0.0: 172.16.0.26:4240 -&amp;gt; 192.168.10.102:39142 tcp ACK</span>
<span class="c">#    -&amp;gt; endpoint 1598 flow 0x0 , identity remote-node-&amp;gt;health state established ifindex lxc_health orig-ip 192.168.10.102: 192.168.10.102:39142 -&amp;gt; 172.16.0.26:4240 tcp ACK</span>
<span class="c">#    -&amp;gt; endpoint 640 flow 0x665dd157 , identity host-&amp;gt;37523 state new ifindex lxcdd00fea91485 orig-ip 10.0.2.15: 10.0.2.15:37040 -&amp;gt; 172.16.0.223:8081 tcp SYN</span>
<span class="c">#    -&amp;gt; stack flow 0xf0f2648f , identity 37523-&amp;gt;host state reply ifindex 0 orig-ip 0.0.0.0: 172.16.0.223:8081 -&amp;gt; 10.0.2.15:37040 tcp SYN, ACK</span>
<span class="c">#    -&amp;gt; endpoint 640 flow 0x665dd157 , identity host-&amp;gt;37523 state established ifindex lxcdd00fea91485 orig-ip 10.0.2.15: 10.0.2.15:37040 -&amp;gt; 172.16.0.223:8081 tcp ACK</span>
<span class="c"># &lt;span style="color: green;"&gt;👉 cilium monitor는 자체적으로 마치 tcpdump처럼 패킷의 이동을 모니터링 할 수 있습니다.&lt;/span&gt;</span>
<span class="nv">$ </span>c0 monitor <span class="nt">-v</span> <span class="nt">--type</span> l7   <span class="c"># Layer 7 (Application layer) 만을 모니터링할 수도 있습니다.</span>
<span class="c"># =&gt; CPU 01: [pre-xlate-rev] cgroup_id: 5161 sock_cookie: 14187, dst [10.0.2.15]:43070 tcp</span>
<span class="c">#    CPU 01: [pre-xlate-rev] cgroup_id: 2800 sock_cookie: 14188, dst [172.16.0.223]:8081 tcp</span>
<span class="c">#    CPU 01: [pre-xlate-rev] cgroup_id: 5161 sock_cookie: 14189, dst [10.0.2.15]:43080 tcp</span>
<span class="c">#    CPU 01: [pre-xlate-rev] cgroup_id: 2800 sock_cookie: 10103, dst [172.16.0.223]:8081 tcp</span>
<span class="c">#    ...</span>
</code></pre></div></div>

<h5 id="네트워크-기본-정보-확인--k3s-w1w2-에-ssh-접속-후-ip--c-linkroute-정보-확인">네트워크 기본 정보 확인 : k3s-w1/w2 에 SSH 접속 후 ip -c link/route 정보 확인</h5>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 네트워크 인터페이스 정보 확인</span>
<span class="nv">$ </span>ip <span class="nt">-br</span> <span class="nt">-c</span> <span class="nb">link</span>
<span class="c"># =&gt; lo               UNKNOWN        00:00:00:00:00:00 &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt;</span>
<span class="c">#    enp0s3           UP             02:6d:da:b3:d4:d3 &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt;</span>
<span class="c">#    enp0s8           UP             08:00:27:35:1b:07 &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt;</span>
<span class="c">#    cilium_net@cilium_host UP             16:9d:bb:67:fc:a0 &amp;lt;BROADCAST,MULTICAST,NOARP,UP,LOWER_UP&amp;gt;</span>
<span class="c">#    cilium_host@cilium_net UP             7e:ea:43:69:1e:4b &amp;lt;BROADCAST,MULTICAST,NOARP,UP,LOWER_UP&amp;gt;</span>
<span class="c">#    lxcb0b60514c056@if10 UP             be:61:78:59:9a:b9 &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt;</span>
<span class="c">#    lxcb6d4cfe53c6a@if12 UP             b6:62:74:02:2f:32 &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt;</span>
<span class="c">#    lxc_health@if18  UP             aa:7e:99:a8:fd:c3 &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt;</span>
<span class="nv">$ </span>ip <span class="nt">-br</span> <span class="nt">-c</span> addr
<span class="c"># =&gt; lo               UNKNOWN        127.0.0.1/8 ::1/128</span>
<span class="c">#    enp0s3           UP             10.0.2.15/24 metric 100 fe80::6d:daff:feb3:d4d3/64</span>
<span class="c">#    enp0s8           UP             192.168.10.101/24 fe80::a00:27ff:fe35:1b07/64</span>
<span class="c">#    cilium_net@cilium_host UP             fe80::149d:bbff:fe67:fca0/64</span>
<span class="c">#    cilium_host@cilium_net UP             172.16.1.82/32 fe80::7cea:43ff:fe69:1e4b/64</span>
<span class="c">#    lxcb0b60514c056@if10 UP             fe80::bc61:78ff:fe59:9ab9/64</span>
<span class="c">#    lxcb6d4cfe53c6a@if12 UP             fe80::b462:74ff:fe02:2f32/64</span>
<span class="c">#    lxc_health@if18  UP             fe80::a87e:99ff:fea8:fdc3/64</span>

<span class="nt">--------------------------------------------</span>
<span class="c"># cilium_net 과 cilium_host 는 veth peer 관계이며, cilium_host 는 파드의 GW IP 주소로 지정되며 32bit 이다</span>
<span class="c"># =&gt; 4: cilium_net@cilium_host: &amp;lt;BROADCAST,MULTICAST,NOARP,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UP group default qlen 1000</span>
<span class="c">#        link/ether 16:9d:bb:67:fc:a0 brd ff:ff:ff:ff:ff:ff</span>
<span class="c">#        inet6 fe80::149d:bbff:fe67:fca0/64 scope link</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    5: cilium_host@cilium_net: &amp;lt;BROADCAST,MULTICAST,NOARP,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UP group default qlen 1000</span>
<span class="c">#        link/ether 7e:ea:43:69:1e:4b brd ff:ff:ff:ff:ff:ff</span>
<span class="c">#        inet 172.16.1.82/32 scope global cilium_host</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#        inet6 fe80::7cea:43ff:fe69:1e4b/64 scope link</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>

<span class="c"># proxy arp 는 disable(0) 상태이며, 파드와 연결된 lxc 도 모두 0 이다</span>
<span class="c"># 파드의 32bit ip의 gw 가 각각 연결된 veth 인터페이스의 mac 으로 cilium_host 의 IP/MAC 응답을 처리한다, 어떻게 동작이 되는걸까요? &gt;&gt; eBPF program!!!</span>
<span class="nv">$ </span><span class="nb">cat</span> /proc/sys/net/ipv4/conf/cilium_net/proxy_arp
<span class="c"># =&gt; 0</span>
<span class="nv">$ </span><span class="nb">cat</span> /proc/sys/net/ipv4/conf/cilium_host/proxy_arp
<span class="c"># =&gt; 0</span>

<span class="c"># lxc_health 인터페이스는 veth 로 cilium(NET NS 0, 호스트와 다름)과 veth pair 이다 - 링크</span>
<span class="c"># cilium 인터페이스에 파드 IP가 할당되어 있으며, cilium-health-responder 로 동작한다</span>
<span class="nv">$ </span>lsns <span class="nt">-t</span> net
<span class="c"># =&gt;         NS TYPE NPROCS   PID USER     NETNSID NSFS                                                COMMAND</span>
<span class="c">#    4026531840 net     133     1 root  unassigned                                                     /sbin/init</span>
<span class="c">#    4026532195 net       2  8433 65535          1 /run/netns/cni-0943b57a-e695-904e-87f6-7edb4fb0cd92 /pause</span>
<span class="c">#    4026532350 net       1 18060 root           0                                                     cilium-health-responder --listen 4240 --pidfile /var/run/cilium/state/health-endp</span>
<span class="c">#    4026532359 net       2 10652 65535          2 /run/netns/cni-1b359731-9a53-2db0-eee1-63b0c5643c27 /pause</span>
</code></pre></div></div>

<h4 id="hubble">Hubble</h4>

<p>Cillium은 Hubble을 통해 통신 및 서비스와 네트워킹 인프라의 동작에 대한 심층적인 가시성을 완전히 투명한 방식으로 관찰성을 제공합니다. - <a href="https://cilium.io/blog/2019/11/19/announcing-hubble/">참고</a></p>
<ul>
  <li>Hubble은 <strong>완전히 분산된 네트워킹 및 보안 모니터링</strong> 플랫폼입니다</li>
  <li>Cilium 과 eBPF 기반으로 동작하며, <strong>어플리케이션 수정없이도</strong> 보다 심도있는 가시성을 제공합니다.</li>
  <li>Hubble은 컨테이너 기반 워크로드 뿐만 아니라 전통적인 <strong>표준 리눅스 프로세스나 VM 기반 워크로드에 대해서도 가시성을 제공</strong>합니다.</li>
  <li>eBPF를 사용함으로써 전통적인 IP 기반이 아닌 <strong>서비스/파드/컨테이너 수준의 네트워크 트래픽</strong>에 대해서 보안 가시성 및 통제를 제공할 수 있습니다.
또한 <strong>어플리케이션 레이어(L7)에서 필터링 할 수 도</strong> 있습니다.</li>
  <li>기본적으로 Hubble API는 Cilium 에이전트가 실행되는 개별 노드의 범위 내에서 작동합니다. 
이는 네트워크 통찰력을 로컬 Cilium 에이전트가 관찰한 트래픽으로 제한합니다.<br />
Hubble <strong>CLI</strong>(<code class="language-plaintext highlighter-rouge">hubble</code>)를 사용하여 <strong>로컬 Unix Domain Socket</strong>을 통해 제공된 <strong>Hubble API를 쿼리</strong>할 수 있습니다. Hubble CLI 바이너리는 기본적으로 Cilium 에이전트 포드에 설치됩니다.</li>
  <li><strong>Hubble Relay</strong>를 배포하면 전체 클러스터 또는 ClusterMesh 시나리오의 여러 <strong>클러스터에 대한 네트워크 가시성</strong>이 제공됩니다. 이 모드에서 Hubble 데이터는 Hubble CLI(<code class="language-plaintext highlighter-rouge">hubble</code>)를 Hubble Relay 서비스로 지정하거나 Hubble UI를 통해 액세스할 수 있습니다. Hubble UI는 L3/L4 및 L7 계층에서 서비스 종속성 그래프를 자동으로 검색할 수 있는 웹 인터페이스로, 사용자 친화적인 시각화 및 서비스 맵으로서의 데이터 흐름 필터링을 허용합니다.
<img src="/assets/2024/kans-3th/w8/20241026_kans_w8_10.png" alt="img.png" class="image-center w-80" />
<em class="image-caption">Hubble Relay를 통한 전체 클러스터 모니터링 <a href="https://cilium.io/blog/2020/06/22/cilium-18/">링크</a></em></li>
  <li>메트릭은 <strong>Prometheus</strong>로 수집되며, <strong>Grafana</strong>를 통해 시각화할 수도 있어서 기존의 대시보드와 통합도 가능합니다.
<img src="/assets/2024/kans-3th/w8/20241026_kans_w8_9.png" alt="img.png" class="image-center w-80" />
<em class="image-caption"><a href="https://cilium.io/blog/2019/11/19/announcing-hubble/">https://cilium.io/blog/2019/11/19/announcing-hubble/</a></em></li>
  <li>Hubble UI 화면
<img src="/assets/2024/kans-3th/w8/20241026_kans_w8_11.png" alt="img.png" class="image-center w-100" />
<em class="image-caption">서비스 종속성 그래프</em></li>
  <li>통제 예시
    <ul>
      <li><code class="language-plaintext highlighter-rouge">/public/.*</code> 경로에 대한 <code class="language-plaintext highlighter-rouge">GET</code> 요청만 허용하고, 다른 모든 요청은 거부</li>
      <li><code class="language-plaintext highlighter-rouge">service1</code>이 <code class="language-plaintext highlighter-rouge">topic1</code>이라는 토픽을 생산하고, <code class="language-plaintext highlighter-rouge">service2</code>가 <code class="language-plaintext highlighter-rouge">topic1</code>을 소비하도록 허용하고, 그 외의 모든 카프카 메시지는 거부</li>
      <li>HTTP 헤더에 <code class="language-plaintext highlighter-rouge">X-Token: [0-9]+</code>가 포함된 모든 요청을 허용하고, 그렇지 않은 요청은 거부</li>
    </ul>
  </li>
</ul>

<h5 id="hubble-uicli-접근-및-확인">Hubble UI/CLI 접근 및 확인</h5>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 확인</span>
<span class="nv">$ </span>cilium status
<span class="c"># =&gt;     /¯¯\</span>
<span class="c">#     /¯¯\__/¯¯\    Cilium:             OK</span>
<span class="c">#     \__/¯¯\__/    Operator:           OK</span>
<span class="c">#     /¯¯\__/¯¯\    Envoy DaemonSet:    OK</span>
<span class="c">#     \__/¯¯\__/    Hubble Relay:       OK</span>
<span class="c">#        \__/       ClusterMesh:        disabled</span>
<span class="c">#    </span>
<span class="c">#    DaemonSet              cilium             Desired: 3, Ready: 3/3, Available: 3/3</span>
<span class="c">#    ...</span>

<span class="c"># UI 파드 정보 확인</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-n</span> kube-system <span class="nt">-l</span> k8s-app<span class="o">=</span>hubble-ui <span class="nt">-o</span> wide
<span class="c"># =&gt; NAME                         READY   STATUS    RESTARTS      AGE    IP             NODE    NOMINATED NODE   READINESS GATES</span>
<span class="c">#    hubble-ui-59bb4cb67b-r48tz   2/2     Running   2 (87m ago)   101m   172.16.0.223   k3s-m   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>

<span class="c"># Hubble UI 웹 접속</span>
<span class="nv">$ </span>kubectl patch <span class="nt">-n</span> kube-system svc hubble-ui <span class="nt">-p</span> <span class="s1">'{"spec": {"type": "NodePort"}}'</span>
<span class="c"># =&gt; service/hubble-ui patched</span>
<span class="nv">$ HubbleUiNodePort</span><span class="o">=</span><span class="si">$(</span>kubectl get svc <span class="nt">-n</span> kube-system hubble-ui <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.spec.ports[0].nodePort<span class="o">}</span><span class="si">)</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"Hubble UI URL = http://</span><span class="si">$(</span>curl <span class="nt">-s</span> ipinfo.io/ip<span class="si">)</span><span class="s2">:</span><span class="nv">$HubbleUiNodePort</span><span class="s2">"</span>
<span class="c"># =&gt; Hubble UI URL = http://54.180.146.116:30732</span>

<span class="c">## Service NodePort 생성 후 아래 정보 확인!</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span>
<span class="c"># =&gt; -P PREROUTING ACCEPT</span>
<span class="c">#    -P INPUT ACCEPT</span>
<span class="c">#    -P OUTPUT ACCEPT</span>
<span class="c">#    -P POSTROUTING ACCEPT</span>
<span class="c">#    ...</span>
<span class="c">#    -N KUBE-EXT-ZGWW2L4XLRSDZ3EF</span>
<span class="c">#    -N KUBE-MARK-MASQ</span>
<span class="c">#    -N KUBE-NODEPORTS</span>
<span class="c">#    ...</span>
<span class="c">#    -N KUBE-SEP-UOFUVE4S3JB7NP6T</span>
<span class="c">#    -N KUBE-SERVICES</span>
<span class="c">#    ...</span>
<span class="c">#    # &lt;span style="color: green;"&gt;👉 (1)&lt;/span&gt;-A PREROUTING -m comment --comment &amp;quot;kubernetes service portals&amp;quot; -j KUBE-SERVICES</span>
<span class="c">#    ...</span>
<span class="c">#    # &lt;span style="color: green;"&gt;👉 (4)&lt;/span&gt;-A KUBE-EXT-ZGWW2L4XLRSDZ3EF -m comment --comment &amp;quot;masquerade traffic for kube-system/hubble-ui:http external destinations&amp;quot; -j KUBE-MARK-MASQ</span>
<span class="c">#    # &lt;span style="color: green;"&gt;👉 (5)&lt;/span&gt;-A KUBE-EXT-ZGWW2L4XLRSDZ3EF -j KUBE-SVC-ZGWW2L4XLRSDZ3EF</span>
<span class="c">#    -A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000</span>
<span class="c">#    # &lt;span style="color: green;"&gt;👉 (3)&lt;/span&gt;-A KUBE-NODEPORTS -p tcp -m comment --comment &amp;quot;kube-system/hubble-ui:http&amp;quot; -m tcp --dport 30732 -j KUBE-EXT-ZGWW2L4XLRSDZ3EF</span>
<span class="c">#    ...</span>
<span class="c">#    # &lt;span style="color: green;"&gt;👉 (8)&lt;/span&gt;-A KUBE-SEP-UOFUVE4S3JB7NP6T -s 172.16.0.223/32 -m comment --comment &amp;quot;kube-system/hubble-ui:http&amp;quot; -j KUBE-MARK-MASQ</span>
<span class="c">#    # &lt;span style="color: green;"&gt;👉 (9)&lt;/span&gt;-A KUBE-SEP-UOFUVE4S3JB7NP6T -p tcp -m comment --comment &amp;quot;kube-system/hubble-ui:http&amp;quot; -m tcp -j DNAT --to-destination 172.16.0.223:8081</span>
<span class="c">#    ...</span>
<span class="c">#    # &lt;span style="color: green;"&gt;👉 (2)&lt;/span&gt;-A KUBE-SERVICES -m comment --comment &amp;quot;kubernetes service nodeports; NOTE: this must be the last rule in this chain&amp;quot; -m addrtype --dst-type LOCAL -j KUBE-NODEPORTS</span>
<span class="c">#    ...</span>
<span class="c">#    # &lt;span style="color: green;"&gt;👉 (6)&lt;/span&gt;-A KUBE-SVC-ZGWW2L4XLRSDZ3EF ! -s 172.16.0.0/16 -d 10.10.200.125/32 -p tcp -m comment --comment &amp;quot;kube-system/hubble-ui:http cluster IP&amp;quot; -m tcp --dport 80 -j KUBE-MARK-MASQ</span>
<span class="c">#    # &lt;span style="color: green;"&gt;👉 (7)&lt;/span&gt;-A KUBE-SVC-ZGWW2L4XLRSDZ3EF -m comment --comment &amp;quot;kube-system/hubble-ui:http -&amp;gt; 172.16.0.223:8081&amp;quot; -j KUBE-SEP-UOFUVE4S3JB7NP6T</span>
<span class="c">#    ...</span>
<span class="c"># &lt;span style="color: green;"&gt;👉 (1) PREROUTING =&gt; (2) KUBE-SERVICES =&gt; (3) KUBE-NODEPORTS &lt;/span&gt;</span>
<span class="c"># &lt;span style="color: green;"&gt;    if 노드 포트인 30732로 접속:&lt;/span&gt;</span>
<span class="c"># &lt;span style="color: green;"&gt;        =&gt; (4,5) KUBE-EXT-ZGWW2L4XLRSDZ3EF&lt;/span&gt; </span>
<span class="c"># &lt;span style="color: green;"&gt;        =&gt; (6,7) KUBE-SVC-ZGWW2L4XLRSDZ3EF =&gt; (8,9) KUBE-SEP-UOFUVE4S3JB7NP6T =&gt; 172.16.0.223:8081&lt;/span&gt;</span>

<span class="nv">$ </span>conntrack <span class="nt">-L</span>
<span class="nv">$ </span>conntrack <span class="nt">-L</span> |grep <span class="nt">-v</span> 2379

<span class="c"># Install Hubble Client</span>
<span class="nv">$ HUBBLE_VERSION</span><span class="o">=</span><span class="si">$(</span>curl <span class="nt">-s</span> https://raw.githubusercontent.com/cilium/hubble/master/stable.txt<span class="si">)</span>
<span class="nv">$ HUBBLE_ARCH</span><span class="o">=</span>amd64
<span class="nv">$ </span><span class="k">if</span> <span class="o">[</span> <span class="s2">"</span><span class="si">$(</span><span class="nb">uname</span> <span class="nt">-m</span><span class="si">)</span><span class="s2">"</span> <span class="o">=</span> <span class="s2">"aarch64"</span> <span class="o">]</span><span class="p">;</span> <span class="k">then </span><span class="nv">HUBBLE_ARCH</span><span class="o">=</span>arm64<span class="p">;</span> <span class="k">fi</span>
<span class="nv">$ </span>curl <span class="nt">-L</span> <span class="nt">--fail</span> <span class="nt">--remote-name-all</span> https://github.com/cilium/hubble/releases/download/<span class="nv">$HUBBLE_VERSION</span>/hubble-linux-<span class="k">${</span><span class="nv">HUBBLE_ARCH</span><span class="k">}</span>.tar.gz<span class="o">{</span>,.sha256sum<span class="o">}</span>
<span class="nv">$ </span><span class="nb">sha256sum</span> <span class="nt">--check</span> hubble-linux-<span class="k">${</span><span class="nv">HUBBLE_ARCH</span><span class="k">}</span>.tar.gz.sha256sum
<span class="nv">$ </span><span class="nb">sudo tar </span>xzvfC hubble-linux-<span class="k">${</span><span class="nv">HUBBLE_ARCH</span><span class="k">}</span>.tar.gz /usr/local/bin
<span class="c"># =&gt; hubble</span>
<span class="nv">$ </span><span class="nb">rm </span>hubble-linux-<span class="k">${</span><span class="nv">HUBBLE_ARCH</span><span class="k">}</span>.tar.gz<span class="o">{</span>,.sha256sum<span class="o">}</span>

<span class="c"># Hubble API Access : localhost TCP 4245 Relay 를 통해 접근, observe 를 통해서 flow 쿼리 확인 가능!</span>
<span class="nv">$ </span>cilium hubble port-forward &amp;
<span class="c"># =&gt; [1] 16534</span>

<span class="c"># CLI 로 Hubble API 상태 확인</span>
<span class="nv">$ </span>hubble status
<span class="c"># =&gt; Healthcheck (via localhost:4245): Ok</span>
<span class="c">#    Current/Max Flows: 12,285/12,285 (100.00%)</span>
<span class="c">#    Flows/s: 24.50</span>
<span class="c">#    Connected Nodes: 3/3</span>

<span class="c"># query the flow API and look for flows</span>
<span class="nv">$ </span>hubble observe
<span class="c"># =&gt; Oct 01 13:26:17.501: kube-system/local-path-provisioner-6795b5f9d8-84m96:60994 (ID:11088) -&amp;gt; 192.168.10.10:6443 (kube-apiserver) to-network FORWARDED (TCP Flags: ACK)</span>
<span class="c">#    Oct 01 13:26:17.660: kube-system/hubble-ui-59bb4cb67b-r48tz:47372 (ID:37523) -&amp;gt; kube-system/hubble-relay-88f7f89d4-fcq2s:4245 (ID:4124) to-network FORWARDED (TCP Flags: ACK)</span>
<span class="c">#    Oct 01 13:26:17.797: 127.0.0.1:36150 (world) &amp;lt;&amp;gt; kube-system/hubble-ui-59bb4cb67b-r48tz (ID:37523) pre-xlate-rev TRACED (TCP)</span>
<span class="c">#    Oct 01 13:26:17.914: 192.168.10.102:38716 (host) -&amp;gt; 192.168.10.10:6443 (kube-apiserver) to-network FORWARDED (TCP Flags: ACK)</span>
<span class="c">#    Oct 01 13:26:18.011: 127.0.0.1:36160 (world) &amp;lt;&amp;gt; kube-system/hubble-ui-59bb4cb67b-r48tz (ID:37523) pre-xlate-rev TRACED (TCP)</span>
<span class="c">#    Oct 01 13:26:18.067: 10.0.2.15:59486 (host) -&amp;gt; kube-system/metrics-server-cdcc87586-g5m2d:10250 (ID:27624) to-endpoint FORWARDED (TCP Flags: SYN)</span>
<span class="c">#    Oct 01 13:26:18.067: 10.0.2.15:59486 (host) &amp;lt;- kube-system/metrics-server-cdcc87586-g5m2d:10250 (ID:27624) to-stack FORWARDED (TCP Flags: SYN, ACK)</span>
<span class="c">#    ...</span>
<span class="c"># hubble observe --pod netpod</span>
<span class="c"># hubble observe --namespace galaxy --http-method POST --http-path /v1/request-landing</span>
<span class="c"># hubble observe --pod deathstar --protocol http</span>
<span class="c"># hubble observe --pod deathstar --verdict DROPPED</span>
</code></pre></div></div>

<h4 id="노드간-파드-통신">노드간 파드 통신</h4>

<ul>
  <li>
    <p>Endpoint to Endpoint 통신 패킷 흐름도
<img src="/assets/2024/kans-3th/w8/20241026_kans_w8_12.png" alt="img.png" class="image-center w-100" /></p>
  </li>
  <li>
    <p>Egress to Endpoint 통신 패킷 흐름도
<img src="/assets/2024/kans-3th/w8/20241026_kans_w8_13.png" alt="img_1.png" class="image-center w-100" /></p>
  </li>
  <li>
    <p>Ingress to Endpoint 통신 패킷 흐름도
<img src="/assets/2024/kans-3th/w8/20241026_kans_w8_14.png" alt="img_2.png" class="image-center w-100" /></p>
  </li>
  <li>
    <p>파드 생성 및 확인</p>
  </li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl create -f -
apiVersion: v1
kind: Pod
metadata:
  name: netpod
  labels:
    app: netpod
spec:
  nodeName: k3s-m
  containers:
  - name: netshoot-pod
    image: nicolaka/netshoot
    command: ["tail"]
    args: ["-f", "/dev/null"]
  terminationGracePeriodSeconds: 0
---
apiVersion: v1
kind: Pod
metadata:
  name: webpod1
  labels:
    app: webpod
spec:
  nodeName: k3s-w1
  containers:
  - name: container
    image: traefik/whoami
  terminationGracePeriodSeconds: 0
---
apiVersion: v1
kind: Pod
metadata:
  name: webpod2
  labels:
    app: webpod
spec:
  nodeName: k3s-w2
  containers:
  - name: container
    image: traefik/whoami
  terminationGracePeriodSeconds: 0
</span><span class="no">EOF
</span><span class="c"># =&gt; pod/netpod created</span>
<span class="c">#    pod/webpod1 created</span>
<span class="c">#    pod/webpod2 created</span>

<span class="c"># 확인</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-o</span> wide
<span class="c"># =&gt; NAME      READY   STATUS    RESTARTS   AGE   IP             NODE     NOMINATED NODE   READINESS GATES</span>
<span class="c">#    netpod    1/1     Running   0          36s   172.16.0.147   k3s-m    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    webpod1   1/1     Running   0          36s   172.16.1.247   k3s-w1   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    webpod2   1/1     Running   0          36s   172.16.2.84    k3s-w2   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="nv">$ </span>c0 status <span class="nt">--verbose</span> | <span class="nb">grep </span>Allocated <span class="nt">-A5</span>
<span class="c"># =&gt; Allocated addresses:</span>
<span class="c">#      172.16.0.147 (default/netpod)</span>
<span class="c">#      172.16.0.223 (kube-system/hubble-ui-59bb4cb67b-r48tz [restored])</span>
<span class="c">#      172.16.0.227 (router)</span>
<span class="c">#      172.16.0.26 (health)</span>
<span class="c">#    IPv4 BIG TCP:           Disabled</span>
<span class="nv">$ </span>c1 status <span class="nt">--verbose</span> | <span class="nb">grep </span>Allocated <span class="nt">-A5</span>
<span class="c"># =&gt; Allocated addresses:</span>
<span class="c">#      172.16.1.21 (kube-system/coredns-7b98449c4-x5756 [restored])</span>
<span class="c">#      172.16.1.224 (health)</span>
<span class="c">#      172.16.1.247 (default/webpod1)</span>
<span class="c">#      172.16.1.82 (router)</span>
<span class="c">#      172.16.1.99 (kube-system/hubble-relay-88f7f89d4-fcq2s [restored])</span>
<span class="nv">$ </span>c2 status <span class="nt">--verbose</span> | <span class="nb">grep </span>Allocated <span class="nt">-A5</span>
<span class="c"># =&gt; Allocated addresses:</span>
<span class="c">#      172.16.2.184 (kube-system/local-path-provisioner-6795b5f9d8-84m96 [restored])</span>
<span class="c">#      172.16.2.223 (kube-system/metrics-server-cdcc87586-g5m2d [restored])</span>
<span class="c">#      172.16.2.25 (router)</span>
<span class="c">#      172.16.2.8 (health)</span>
<span class="c">#      172.16.2.84 (default/webpod2)</span>

<span class="nv">$ </span>kubectl get ciliumendpoints
<span class="c"># =&gt; NAME      SECURITY IDENTITY   ENDPOINT STATE   IPV4           IPV6</span>
<span class="c">#    netpod    27629               ready            172.16.0.147</span>
<span class="c">#    webpod1   64309               ready            172.16.1.247</span>
<span class="c">#    webpod2   64309               ready            172.16.2.84</span>
<span class="nv">$ </span>kubectl get ciliumendpoints <span class="nt">-A</span>
<span class="c"># =&gt; root@k3s-m:~# kubectl get ciliumendpoints -A</span>
<span class="c">#    NAMESPACE     NAME                                      SECURITY IDENTITY   ENDPOINT STATE   IPV4           IPV6</span>
<span class="c">#    default       netpod                                    27629               ready            172.16.0.147</span>
<span class="c">#    default       webpod1                                   64309               ready            172.16.1.247</span>
<span class="c">#    default       webpod2                                   64309               ready            172.16.2.84</span>
<span class="c">#    kube-system   coredns-7b98449c4-x5756                   11970               ready            172.16.1.21</span>
<span class="c">#    kube-system   hubble-relay-88f7f89d4-fcq2s              4124                ready            172.16.1.99</span>
<span class="c">#    kube-system   hubble-ui-59bb4cb67b-r48tz                37523               ready            172.16.0.223</span>
<span class="c">#    kube-system   local-path-provisioner-6795b5f9d8-84m96   11088               ready            172.16.2.184</span>
<span class="c">#    kube-system   metrics-server-cdcc87586-g5m2d            27624               ready            172.16.2.223</span>
<span class="nv">$ </span>c0 endpoint list
<span class="c"># =&gt; ENDPOINT   POLICY (ingress)   POLICY (egress)   IDENTITY   LABELS (source:key[=value])                                                  IPv6   IPv4           STATUS</span>
<span class="c">#               ENFORCEMENT        ENFORCEMENT</span>
<span class="c">#    640        Disabled           Disabled          37523      k8s:app.kubernetes.io/name=hubble-ui                                                172.16.0.223   ready</span>
<span class="c">#                                                               k8s:app.kubernetes.io/part-of=cilium</span>
<span class="c">#                                                               k8s:io.cilium.k8s.namespace.labels.kubernetes.io/metadata.name=kube-system</span>
<span class="c">#                                                               k8s:io.cilium.k8s.policy.cluster=default</span>
<span class="c">#                                                               k8s:io.cilium.k8s.policy.serviceaccount=hubble-ui</span>
<span class="c">#                                                               k8s:io.kubernetes.pod.namespace=kube-system</span>
<span class="c">#                                                               k8s:k8s-app=hubble-ui</span>
<span class="c">#    ...</span>
<span class="nv">$ </span>c0 bpf endpoint list
<span class="c"># =&gt; IP ADDRESS        LOCAL ENDPOINT INFO</span>
<span class="c">#    192.168.10.10:0   (localhost)</span>
<span class="c">#    10.0.2.15:0       (localhost)</span>
<span class="c">#    172.16.0.147:0    id=2724  sec_id=27629 flags=0x0000 ifindex=19  mac=52:59:78:2F:C0:BE nodemac=7E:77:FA:0A:D3:CC</span>
<span class="c">#    172.16.0.26:0     id=1598  sec_id=4     flags=0x0000 ifindex=17  mac=CE:24:50:27:35:B9 nodemac=FE:57:C1:AD:A6:28</span>
<span class="c">#    172.16.0.223:0    id=640   sec_id=37523 flags=0x0000 ifindex=9   mac=AE:B8:81:A6:B1:28 nodemac=C2:21:B8:92:DA:FD</span>
<span class="c">#    172.16.0.227:0    (localhost)</span>
<span class="nv">$ </span>c0 map get cilium_lxc
<span class="c"># =&gt; Key              Value                                                                                            State   Error</span>
<span class="c">#    172.16.0.147:0   id=2724  sec_id=27629 flags=0x0000 ifindex=19  mac=52:59:78:2F:C0:BE nodemac=7E:77:FA:0A:D3:CC   sync</span>
<span class="c">#    172.16.0.223:0   id=640   sec_id=37523 flags=0x0000 ifindex=9   mac=AE:B8:81:A6:B1:28 nodemac=C2:21:B8:92:DA:FD   sync</span>
<span class="c">#    172.16.0.26:0    id=1598  sec_id=4     flags=0x0000 ifindex=17  mac=CE:24:50:27:35:B9 nodemac=FE:57:C1:AD:A6:28   sync</span>
<span class="nv">$ </span>c0 ip list
<span class="c"># =&gt; IP                  IDENTITY                                                                         SOURCE</span>
<span class="c">#    0.0.0.0/0           reserved:world</span>
<span class="c">#    10.0.2.15/32        reserved:host</span>
<span class="c">#                        reserved:kube-apiserver</span>
<span class="c">#    172.16.0.26/32      reserved:health</span>
<span class="c">#    172.16.0.147/32     k8s:app=netpod                                                                   custom-resource</span>
<span class="c">#                        k8s:io.cilium.k8s.namespace.labels.kubernetes.io/metadata.name=default</span>
<span class="c">#    ...</span>
</code></pre></div></div>

<ul>
  <li>파드 변수 지정</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 테스트 파드들 IP</span>
<span class="nv">$ NETPODIP</span><span class="o">=</span><span class="si">$(</span>kubectl get pods netpod <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.status.podIP}'</span><span class="si">)</span>
<span class="nv">$ WEBPOD1IP</span><span class="o">=</span><span class="si">$(</span>kubectl get pods webpod1 <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.status.podIP}'</span><span class="si">)</span>
<span class="nv">$ WEBPOD2IP</span><span class="o">=</span><span class="si">$(</span>kubectl get pods webpod2 <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.status.podIP}'</span><span class="si">)</span>

<span class="c"># 단축키(alias) 지정</span>
<span class="nv">$ </span><span class="nb">alias </span><span class="nv">p0</span><span class="o">=</span><span class="s2">"kubectl exec -it netpod  -- "</span>
<span class="nv">$ </span><span class="nb">alias </span><span class="nv">p1</span><span class="o">=</span><span class="s2">"kubectl exec -it webpod1 -- "</span>
<span class="nv">$ </span><span class="nb">alias </span><span class="nv">p2</span><span class="o">=</span><span class="s2">"kubectl exec -it webpod2 -- "</span>
</code></pre></div></div>

<ul>
  <li>파드의 ARP 동작 확인</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># netpod 네트워크 정보 확인</span>
<span class="nv">$ </span>p0 ip <span class="nt">-c</span> <span class="nt">-4</span> addr
<span class="c"># =&gt; ...</span>
<span class="c">#    18: eth0@if19: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link-netnsid 0</span>
<span class="c">#        inet 172.16.0.147/32 scope global eth0</span>
<span class="nv">$ </span>p0 route <span class="nt">-n</span>
<span class="c"># =&gt; Kernel IP routing table</span>
<span class="c">#    Destination     Gateway         Genmask         Flags Metric Ref    Use Iface</span>
<span class="c">#    0.0.0.0         172.16.0.227    0.0.0.0         UG    0      0        0 eth0</span>
<span class="c">#    172.16.0.227    0.0.0.0         255.255.255.255 UH    0      0        0 eth0</span>
<span class="nv">$ </span>p0 ping <span class="nt">-c</span> 1 <span class="nv">$WEBPOD1IP</span> <span class="o">&amp;&amp;</span> p0 ping <span class="nt">-c</span> 1 <span class="nv">$WEBPOD2IP</span>
<span class="c"># =&gt; PING 172.16.1.247 (172.16.1.247) 56(84) bytes of data.</span>
<span class="c">#    64 bytes from 172.16.1.247: icmp_seq=1 ttl=62 time=0.642 ms</span>
<span class="c">#    </span>
<span class="c">#    --- 172.16.1.247 ping statistics ---</span>
<span class="c">#    1 packets transmitted, 1 received, 0% packet loss, time 0ms</span>
<span class="c">#    rtt min/avg/max/mdev = 0.642/0.642/0.642/0.000 ms</span>
<span class="c">#    PING 172.16.2.84 (172.16.2.84) 56(84) bytes of data.</span>
<span class="c">#    64 bytes from 172.16.2.84: icmp_seq=1 ttl=62 time=0.716 ms</span>
<span class="c">#    </span>
<span class="c">#    --- 172.16.2.84 ping statistics ---</span>
<span class="c">#    1 packets transmitted, 1 received, 0% packet loss, time 0ms</span>
<span class="c">#    rtt min/avg/max/mdev = 0.716/0.716/0.716/0.000 ms</span>
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w8/20241026_kans_w8_15.png" alt="img.png" /></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>p0 curl <span class="nt">-s</span> <span class="nv">$WEBPOD1IP</span> <span class="o">&amp;&amp;</span> p0 curl <span class="nt">-s</span> <span class="nv">$WEBPOD2IP</span>
<span class="c"># =&gt; Hostname: webpod1</span>
<span class="c">#    IP: 127.0.0.1</span>
<span class="c">#    IP: 172.16.1.247</span>
<span class="c">#    RemoteAddr: 172.16.0.147:51692</span>
<span class="c">#    GET / HTTP/1.1</span>
<span class="c">#    Host: 172.16.1.247</span>
<span class="c">#    User-Agent: curl/8.7.1</span>
<span class="c">#    Accept: */*</span>
<span class="c">#    </span>
<span class="c">#    Hostname: webpod2</span>
<span class="c">#    IP: 127.0.0.1</span>
<span class="c">#    IP: 172.16.2.84</span>
<span class="c">#    RemoteAddr: 172.16.0.147:32796</span>
<span class="c">#    GET / HTTP/1.1</span>
<span class="c">#    Host: 172.16.2.84</span>
<span class="c">#    User-Agent: curl/8.7.1</span>
<span class="c">#    Accept: */*</span>
<span class="nv">$ </span>p0 curl <span class="nt">-s</span> <span class="nv">$WEBPOD1IP</span>:8080 <span class="p">;</span> p0 curl <span class="nt">-s</span> <span class="nv">$WEBPOD2IP</span>:8080
<span class="c"># =&gt; command terminated with exit code 7</span>
<span class="c">#    command terminated with exit code 7</span>
<span class="nv">$ </span>p0 ping <span class="nt">-c</span> 1 8.8.8.8 <span class="o">&amp;&amp;</span> p0 curl <span class="nt">-s</span> wttr.in/seoul
<span class="c"># =&gt; PING 8.8.8.8 (8.8.8.8) 56(84) bytes of data.</span>
<span class="c">#    64 bytes from 8.8.8.8: icmp_seq=1 ttl=61 time=39.1 ms</span>
<span class="c">#    </span>
<span class="c">#    --- 8.8.8.8 ping statistics ---</span>
<span class="c">#    1 packets transmitted, 1 received, 0% packet loss, time 0ms</span>
<span class="c">#    rtt min/avg/max/mdev = 39.095/39.095/39.095/0.000 ms</span>
<span class="c">#    Weather report: seoul</span>
<span class="c">#    </span>
<span class="c">#         \  /       Partly cloudy</span>
<span class="c">#       _ /&amp;quot;&amp;quot;.-.     16 °C</span>
<span class="c">#         \_(   ).   ← 4 km/h</span>
<span class="c">#         /(___(__)  10 km</span>
<span class="c">#                    0.0 mm</span>
<span class="c">#                                                           ┌─────────────┐</span>
<span class="c">#    ┌──────────────────────────────┬───────────────────────┤  Sat 26 Oct ├───────────────────────┬──────────────────────────────┐</span>
<span class="c">#    │            Morning           │             Noon      └──────┬──────┘     Evening           │             Night            │</span>
<span class="c">#    ├──────────────────────────────┼──────────────────────────────┼──────────────────────────────┼──────────────────────────────┤</span>
<span class="c">#    │     \   /     Sunny          │     \   /     Sunny          │               Overcast       │    \  /       Partly Cloudy  │</span>
<span class="c">#    │      .-.      16 °C          │      .-.      21 °C          │      .--.     20 °C          │  _ /&amp;quot;&amp;quot;.-.     19 °C          │</span>
<span class="c">#    │   ― (   ) ―   ↙ 5-7 km/h     │   ― (   ) ―   ↙ 5-6 km/h     │   .-(    ).   ← 4-6 km/h     │    \_(   ).   ↙ 4-7 km/h     │</span>
<span class="c">#    │      `-’      10 km          │      `-’      10 km          │  (___.__)__)  10 km          │    /(___(__)  10 km          │</span>
<span class="c">#    │     /   \     0.0 mm | 0%    │     /   \     0.0 mm | 0%    │               0.0 mm | 0%    │               0.0 mm | 0%    │</span>
<span class="c">#    └──────────────────────────────┴──────────────────────────────┴──────────────────────────────┴──────────────────────────────┘</span>
<span class="c">#                                                           ┌─────────────┐</span>
<span class="c">#    ┌──────────────────────────────┬───────────────────────┤  Sun 27 Oct ├───────────────────────┬──────────────────────────────┐</span>
<span class="c">#    │            Morning           │             Noon      └──────┬──────┘     Evening           │             Night            │</span>
<span class="c">#    ├──────────────────────────────┼──────────────────────────────┼──────────────────────────────┼──────────────────────────────┤</span>
<span class="c">#    │               Overcast       │               Cloudy         │     \   /     Clear          │     \   /     Clear          │</span>
<span class="c">#    │      .--.     16 °C          │      .--.     19 °C          │      .-.      19 °C          │      .-.      17 °C          │</span>
<span class="c">#    │   .-(    ).   ← 1 km/h       │   .-(    ).   ↓ 4-5 km/h     │   ― (   ) ―   ↘ 9-13 km/h    │   ― (   ) ―   ↘ 5-7 km/h     │</span>
<span class="c">#    │  (___.__)__)  10 km          │  (___.__)__)  10 km          │      `-’      10 km          │      `-’      10 km          │</span>
<span class="c">#    │               0.0 mm | 0%    │               0.0 mm | 0%    │     /   \     0.0 mm | 0%    │     /   \     0.0 mm | 0%    │</span>
<span class="c">#    └──────────────────────────────┴──────────────────────────────┴──────────────────────────────┴──────────────────────────────┘</span>
<span class="c">#                                                           ┌─────────────┐</span>
<span class="c">#    ┌──────────────────────────────┬───────────────────────┤  Mon 28 Oct ├───────────────────────┬──────────────────────────────┐</span>
<span class="c">#    │            Morning           │             Noon      └──────┬──────┘     Evening           │             Night            │</span>
<span class="c">#    ├──────────────────────────────┼──────────────────────────────┼──────────────────────────────┼──────────────────────────────┤</span>
<span class="c">#    │     \   /     Sunny          │    \  /       Partly Cloudy  │               Cloudy         │  _`/&amp;quot;&amp;quot;.-.     Patchy rain ne…│</span>
<span class="c">#    │      .-.      16 °C          │  _ /&amp;quot;&amp;quot;.-.     20 °C          │      .--.     20 °C          │   ,\_(   ).   19 °C          │</span>
<span class="c">#    │   ― (   ) ―   ← 5-6 km/h     │    \_(   ).   ← 6-7 km/h     │   .-(    ).   ↙ 6-9 km/h     │    /(___(__)  ← 8-10 km/h    │</span>
<span class="c">#    │      `-’      10 km          │    /(___(__)  10 km          │  (___.__)__)  10 km          │      ‘ ‘ ‘ ‘  10 km          │</span>
<span class="c">#    │     /   \     0.0 mm | 0%    │               0.0 mm | 0%    │               0.0 mm | 0%    │     ‘ ‘ ‘ ‘   0.0 mm | 67%   │</span>
<span class="c">#    └──────────────────────────────┴──────────────────────────────┴──────────────────────────────┴──────────────────────────────┘</span>
<span class="c">#    Location: 서울특별시, 대한민국 [37.5666791,126.9782914]</span>
<span class="c">#    </span>
<span class="c">#    Follow @igor_chubin for wttr.in updates</span>

<span class="nv">$ </span>p0 ip <span class="nt">-c</span> neigh
<span class="c"># =&gt; 172.16.0.227 dev eth0 lladdr 7e:77:fa:0a:d3:cc STALE</span>

<span class="c"># hubble cli 확인</span>
<span class="nv">$ </span>hubble observe <span class="nt">--pod</span> netpod
<span class="c"># =&gt; Oct 01 14:29:21.248: kube-system/coredns-7b98449c4-7kqtg:53 (ID:11970) &amp;lt;&amp;gt; default/netpod (ID:27629) pre-xlate-rev TRACED (UDP)</span>
<span class="c">#    Oct 01 14:29:21.248: kube-system/kube-dns:53 (world) &amp;lt;&amp;gt; default/netpod (ID:27629) post-xlate-rev TRANSLATED (UDP)</span>
<span class="c">#    Oct 01 14:29:21.248: default/netpod:34162 (ID:27629) -&amp;gt; 5.9.243.187:80 (world) to-network FORWARDED (TCP Flags: SYN)</span>
<span class="c">#    Oct 01 14:29:21.481: default/netpod:34162 (ID:27629) -&amp;gt; 5.9.243.187:80 (world) to-network FORWARDED (TCP Flags: ACK, PSH)</span>
<span class="c">#    Oct 01 14:29:21.481: default/netpod:34162 (ID:27629) &amp;lt;- 5.9.243.187:80 (world) to-endpoint FORWARDED (TCP Flags: SYN, ACK)</span>
<span class="c">#    Oct 01 14:29:21.481: default/netpod:34162 (ID:27629) -&amp;gt; 5.9.243.187:80 (world) to-network FORWARDED (TCP Flags: ACK)</span>
<span class="c">#    Oct 01 14:29:21.816: default/netpod:34162 (ID:27629) &amp;lt;- 5.9.243.187:80 (world) to-endpoint FORWARDED (TCP Flags: ACK, PSH)</span>
<span class="c">#    Oct 01 14:29:21.819: default/netpod:34162 (ID:27629) -&amp;gt; 5.9.243.187:80 (world) to-network FORWARDED (TCP Flags: ACK, FIN)</span>
<span class="c">#    Oct 01 14:29:22.052: default/netpod:34162 (ID:27629) &amp;lt;- 5.9.243.187:80 (world) to-endpoint FORWARDED (TCP Flags: ACK, FIN)</span>
<span class="c">#    Oct 01 14:29:22.052: default/netpod:34162 (ID:27629) -&amp;gt; 5.9.243.187:80 (world) to-network FORWARDED (TCP Flags: ACK)</span>
<span class="c">#    ...</span>
<span class="nv">$ </span>hubble observe <span class="nt">--pod</span> webpod1
<span class="c"># =&gt; Oct 01 14:33:40.392: default/netpod:60282 (ID:27629) -&amp;gt; default/webpod1:80 (ID:64309) to-endpoint FORWARDED (TCP Flags: SYN)</span>
<span class="c">#    Oct 01 14:33:40.392: default/netpod:60282 (ID:27629) &amp;lt;- default/webpod1:80 (ID:64309) to-network FORWARDED (TCP Flags: SYN, ACK)</span>
<span class="c">#    Oct 01 14:33:40.392: default/netpod:60282 (ID:27629) &amp;lt;&amp;gt; default/webpod1 (ID:64309) pre-xlate-rev TRACED (TCP)</span>
<span class="c">#    Oct 01 14:33:40.392: default/netpod:60282 (ID:27629) -&amp;gt; default/webpod1:80 (ID:64309) to-endpoint FORWARDED (TCP Flags: ACK)</span>
<span class="c">#    Oct 01 14:33:40.392: default/netpod:60282 (ID:27629) -&amp;gt; default/webpod1:80 (ID:64309) to-endpoint FORWARDED (TCP Flags: ACK, PSH)</span>
<span class="c">#    Oct 01 14:33:40.393: default/netpod:60282 (ID:27629) &amp;lt;- default/webpod1:80 (ID:64309) to-network FORWARDED (TCP Flags: ACK, PSH)</span>
<span class="c">#    Oct 01 14:33:40.393: default/netpod:60282 (ID:27629) -&amp;gt; default/webpod1:80 (ID:64309) to-endpoint FORWARDED (TCP Flags: ACK, FIN)</span>
<span class="c">#    Oct 01 14:33:40.393: default/netpod:60282 (ID:27629) &amp;lt;- default/webpod1:80 (ID:64309) to-network FORWARDED (TCP Flags: ACK, FIN)</span>
<span class="c">#    Oct 01 14:33:40.394: default/netpod:60282 (ID:27629) -&amp;gt; default/webpod1:80 (ID:64309) to-endpoint FORWARDED (TCP Flags: ACK)</span>
<span class="c">#    Oct 01 14:33:40.405: default/netpod:60282 (ID:27629) -&amp;gt; default/webpod1:80 (ID:64309) to-network FORWARDED (TCP Flags: SYN)</span>
<span class="c">#    Oct 01 14:33:40.406: default/netpod:60282 (ID:27629) &amp;lt;- default/webpod1:80 (ID:64309) to-endpoint FORWARDED (TCP Flags: SYN, ACK)</span>
<span class="c">#    Oct 01 14:33:40.406: default/netpod:60282 (ID:27629) -&amp;gt; default/webpod1:80 (ID:64309) to-network FORWARDED (TCP Flags: ACK)</span>
<span class="c">#    Oct 01 14:33:40.406: default/netpod:60282 (ID:27629) -&amp;gt; default/webpod1:80 (ID:64309) to-network FORWARDED (TCP Flags: ACK, PSH)</span>
<span class="c">#    Oct 01 14:33:40.407: default/netpod:60282 (ID:27629) &amp;lt;- default/webpod1:80 (ID:64309) to-endpoint FORWARDED (TCP Flags: ACK, PSH)</span>
<span class="c">#    Oct 01 14:33:40.407: default/netpod:60282 (ID:27629) -&amp;gt; default/webpod1:80 (ID:64309) to-network FORWARDED (TCP Flags: ACK, FIN)</span>
<span class="c">#    Oct 01 14:33:40.408: default/netpod:60282 (ID:27629) &amp;lt;- default/webpod1:80 (ID:64309) to-endpoint FORWARDED (TCP Flags: ACK, FIN)</span>
<span class="c">#    Oct 01 14:33:40.408: default/netpod:60282 (ID:27629) -&amp;gt; default/webpod1:80 (ID:64309) to-network FORWARDED (TCP Flags: ACK)</span>
<span class="c"># &lt;span style="color: green;"&gt;👉 80포트로 접속했을때 정상적인 응답 패킷들&lt;/span&gt;</span>

<span class="c">#    Oct 01 14:33:43.956: default/netpod:34488 (ID:27629) -&amp;gt; default/webpod1:8080 (ID:64309) to-endpoint FORWARDED (TCP Flags: SYN)</span>
<span class="c">#    Oct 01 14:33:43.956: default/netpod:34488 (ID:27629) &amp;lt;- default/webpod1:8080 (ID:64309) to-network FORWARDED (TCP Flags: ACK, RST)</span>
<span class="c">#    Oct 01 14:33:43.970: default/netpod:34488 (ID:27629) -&amp;gt; default/webpod1:8080 (ID:64309) to-network FORWARDED (TCP Flags: SYN)</span>
<span class="c">#    Oct 01 14:33:43.970: default/netpod:34488 (ID:27629) &amp;lt;- default/webpod1:8080 (ID:64309) to-endpoint FORWARDED (TCP Flags: ACK, RST)</span>
<span class="c"># &lt;span style="color: green;"&gt;👉 열려있지 않은 8080포트로 접속했을때 오류가 발생한 패킷들&lt;/span&gt;</span>

<span class="nv">$ </span>hubble observe <span class="nt">--pod</span> webpod2

<span class="c"># BPF maps : 목적지 파드와 통신 시 어느곳으로 보내야 될지 확인할 수 있다</span>
<span class="nv">$ </span>c0 map get cilium_ipcache
<span class="c"># =&gt; Key                 Value                                                                     State   Error</span>
<span class="c">#    172.16.2.182/32     identity=27624 encryptkey=0 tunnelendpoint=192.168.10.102, flags=&amp;lt;none&amp;gt;   sync</span>
<span class="c">#    172.16.2.8/32       identity=4 encryptkey=0 tunnelendpoint=192.168.10.102, flags=&amp;lt;none&amp;gt;       sync</span>
<span class="c">#    172.16.0.147/32     identity=27629 encryptkey=0 tunnelendpoint=0.0.0.0, flags=&amp;lt;none&amp;gt;          sync</span>
<span class="c">#    172.16.0.223/32     identity=37523 encryptkey=0 tunnelendpoint=0.0.0.0, flags=&amp;lt;none&amp;gt;          sync</span>
<span class="c">#    192.168.10.10/32    identity=1 encryptkey=0 tunnelendpoint=0.0.0.0, flags=&amp;lt;none&amp;gt;              sync</span>
<span class="c">#    172.16.0.26/32      identity=4 encryptkey=0 tunnelendpoint=0.0.0.0, flags=&amp;lt;none&amp;gt;              sync</span>
<span class="c">#    10.0.2.15/32        identity=1 encryptkey=0 tunnelendpoint=0.0.0.0, flags=&amp;lt;none&amp;gt;              sync</span>
<span class="c">#    192.168.10.102/32   identity=6 encryptkey=0 tunnelendpoint=0.0.0.0, flags=&amp;lt;none&amp;gt;              sync</span>
<span class="c">#    172.16.1.82/32      identity=6 encryptkey=0 tunnelendpoint=192.168.10.101, flags=&amp;lt;none&amp;gt;       sync</span>
<span class="c">#    172.16.2.237/32     identity=11970 encryptkey=0 tunnelendpoint=192.168.10.102, flags=&amp;lt;none&amp;gt;   sync</span>
<span class="c">#    172.16.2.216/32     identity=64309 encryptkey=0 tunnelendpoint=192.168.10.102, flags=&amp;lt;none&amp;gt;   sync</span>
<span class="c">#    0.0.0.0/0           identity=2 encryptkey=0 tunnelendpoint=0.0.0.0, flags=&amp;lt;none&amp;gt;              sync</span>
<span class="c">#    172.16.2.111/32     identity=11088 encryptkey=0 tunnelendpoint=192.168.10.102, flags=&amp;lt;none&amp;gt;   sync</span>
<span class="c">#    172.16.1.224/32     identity=4 encryptkey=0 tunnelendpoint=192.168.10.101, flags=&amp;lt;none&amp;gt;       sync</span>
<span class="c">#    172.16.2.25/32      identity=6 encryptkey=0 tunnelendpoint=192.168.10.102, flags=&amp;lt;none&amp;gt;       sync</span>
<span class="c">#    192.168.10.101/32   identity=6 encryptkey=0 tunnelendpoint=0.0.0.0, flags=&amp;lt;none&amp;gt;              sync</span>
<span class="c">#    172.16.0.148/32     identity=4124 encryptkey=0 tunnelendpoint=0.0.0.0, flags=&amp;lt;none&amp;gt;           sync</span>
<span class="c">#    172.16.1.247/32     identity=64309 encryptkey=0 tunnelendpoint=192.168.10.101, flags=&amp;lt;none&amp;gt;   sync</span>
<span class="c">#    172.16.0.227/32     identity=1 encryptkey=0 tunnelendpoint=0.0.0.0, flags=&amp;lt;none&amp;gt;              sync</span>
<span class="nv">$ </span>c0 map get cilium_ipcache | <span class="nb">grep</span> <span class="nv">$WEBPOD1IP</span>
<span class="c"># =&gt; 172.16.1.247/32     identity=64309 encryptkey=0 tunnelendpoint=192.168.10.101, flags=&amp;lt;none&amp;gt;   sync</span>

<span class="c"># netpod 의 LXC 변수 지정</span>
<span class="c">#$ LXC=&lt;k3s-m의 가장 나중에 lxc 이름&gt;</span>
<span class="nv">$ LXC</span><span class="o">=</span>lxcd551b3b4058f

<span class="c"># 파드와 veth pair 에 IP가 없습니다! proxy_arp 도 없습니다! 하지만 GW MAC 요청 시 lxc(veth)의 MAC 으로 응답이 옵니다! &gt;&gt; eBPF Magic!</span>
<span class="c"># Cilium hijacks ARP table of POD1, forces the next hop to be the peer end (host side) of the veth pair.</span>
<span class="nv">$ </span>ip <span class="nt">-c</span> addr show dev <span class="nv">$LXC</span>
<span class="c"># =&gt; 23: lxcd551b3b4058f@if22: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UP group default qlen 1000</span>
<span class="c">#        link/ether fa:32:54:4b:16:d7 brd ff:ff:ff:ff:ff:ff link-netns cni-8634ff07-8cb9-608a-1baf-dc929d510a84</span>
<span class="c">#        inet6 fe80::f832:54ff:fe4b:16d7/64 scope link</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w8/20241026_kans_w8_16.png" alt="img.png" class="image-center" />
<em class="image-caption">webpod1,2와 wttr.in 접속 테스트 후 Hubble UI</em></p>

<h4 id="서비스-통신-확인">서비스 통신 확인</h4>

<h5 id="소켓-기반-로드밸런싱-소개">소켓 기반 로드밸런싱 소개</h5>

<ul>
  <li>참고 : <a href="https://velog.io/@haruband/K8SCilium-Socket-Based-LoadBalancing-%EA%B8%B0%EB%B2%95">https://velog.io/@haruband/K8SCilium-Socket-Based-LoadBalancing-%EA%B8%B0%EB%B2%95</a></li>
  <li>네트워크 기반 로드밸런싱(왼쪽) vs 소켓 기반 로드밸런싱(오른쪽) 비교
<img src="/assets/2024/kans-3th/w8/20241026_kans_w8_17.png" alt="img.png" />
위의 그림에서 처럼 네트워크 기반 로드밸런싱은 서비스를 통해 DNAT 되는 과정을 거치는데, <strong>소켓기반 로드밸런싱은 DNAT하는 과정이 필요가 없습</strong>니다.
    <ul>
      <li>위의 그림을 풀어서 설명하자면 위의 예에서 Pod1에서 동작하는 앱이 <code class="language-plaintext highlighter-rouge">connect()</code> 시스템 콜을 이용해서 소켓을 연결할때 목적지 주소가
서비스 주소 (192.168.0.1)이면 소켓의 목적지 주소를 바로 백엔드 주소(10.0.0.2)으로 설정합니다. 이후 데이터 전송은 
<strong>바로 백엔드 주소(10.0.0.2)로 전송되기 때문</strong>에 <strong>DNAT 변환 및 역변환 과정이 필요없어</strong>집니다.</li>
      <li><em>이는 cilium이 L7에서 파드/서비스의 의미를 이해하고 처리하기 때문입니다!</em></li>
      <li>심지어 서비스 주소를 백엔드 주소로 변경하는 것은 <strong>시스템콜 레벨에서 이루어지며</strong>, 커널에서 패킷이 생성되기도 전입니다.</li>
    </ul>
  </li>
  <li>Socket Operations : BPF Socket Operations program은 root cgroup에 연결되며 TCP event(ESTABLISHED)에서 실행합니다.</li>
  <li>Socket send/recv : Socket send/recv 훅은 TCP socket의 모든 송수신 작업에서 실행되며, hook에서 검사/삭제/리다이렉션 할 수 있습니다.</li>
  <li><code class="language-plaintext highlighter-rouge">connect()</code>와 <code class="language-plaintext highlighter-rouge">sendto()</code> 소켓 함수에 연결된 프로그램(<code class="language-plaintext highlighter-rouge">connect4</code>, <code class="language-plaintext highlighter-rouge">sendmsg4</code>)에서는 소켓의 목적지 주소를 백엔드 주소와 포트로 변환하고, 
cilium_lb4_backends 맵에 백엔드 주소와 포트를 등록해놓습니다. 이후 <code class="language-plaintext highlighter-rouge">recvmsg()</code> 소켓 함수에 연결된 프로그램(<code class="language-plaintext highlighter-rouge">recvmsg4</code>)에서는
ilium_lb4_reverse_nat 맵을 이용해서 목적지 주소와 포트를 다시 서비스 주소와 포트로 변환합니다.
<img src="/assets/2024/kans-3th/w8/20241026_kans_w8_18.png" alt="img.png" class="image-center" />
<em class="image-caption">cilium의 소켓 기반 로드밸런싱 동작 방식 - <a href="https://k8s.networkop.co.uk/services/clusterip/dataplane/ebpf/">링크</a></em></li>
</ul>

<h5 id="서비스-생성-및-접속-확인">서비스 생성 및 접속 확인</h5>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 서비스 생성</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl create -f -
apiVersion: v1
kind: Service
metadata:
  name: svc
spec:
  ports:
    - name: svc-webport
      port: 80
      targetPort: 80
  selector:
    app: webpod
  type: ClusterIP
</span><span class="no">EOF
</span><span class="c"># =&gt; service/svc created</span>

<span class="c"># 서비스 생성 확인</span>
<span class="nv">$ </span>kubectl get svc,ep svc
<span class="c"># =&gt; NAME          TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE</span>
<span class="c">#    service/svc   ClusterIP   10.10.200.121   &amp;lt;none&amp;gt;        80/TCP    13s</span>
<span class="c">#    </span>
<span class="c">#    NAME            ENDPOINTS                         AGE</span>
<span class="c">#    endpoints/svc   172.16.1.247:80,172.16.2.216:80   13s</span>

<span class="c"># 노드에 iptables 더이상 KUBE-SVC rule 이 생성되지 않는다!</span>
<span class="nv">$ </span>iptables-save | <span class="nb">grep </span>KUBE-SVC
<span class="nv">$ </span>iptables-save | <span class="nb">grep </span>CILIUM

<span class="c"># 서비스IP를 변수에 지정</span>
<span class="nv">$ SVCIP</span><span class="o">=</span><span class="si">$(</span>kubectl get svc svc <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.spec.clusterIP}'</span><span class="si">)</span>

<span class="c"># Pod1 에서 Service(ClusterIP) 접속 트래픽 발생</span>
<span class="nv">$ </span>kubectl <span class="nb">exec </span>netpod <span class="nt">--</span> curl <span class="nt">-s</span> <span class="nv">$SVCIP</span>
<span class="c"># =&gt; Hostname: webpod2</span>
<span class="c">#    IP: 127.0.0.1</span>
<span class="c">#    IP: ::1</span>
<span class="c">#    IP: 172.16.2.216</span>
<span class="c">#    IP: fe80::844e:b2ff:fed0:7d5</span>
<span class="c">#    RemoteAddr: 172.16.0.147:36844</span>
<span class="c">#    GET / HTTP/1.1</span>
<span class="c">#    Host: 10.10.200.121</span>
<span class="c">#    User-Agent: curl/8.7.1</span>
<span class="c">#    Accept: */*</span>
<span class="nv">$ </span>kubectl <span class="nb">exec </span>netpod <span class="nt">--</span> curl <span class="nt">-s</span> <span class="nv">$SVCIP</span> | <span class="nb">grep </span>Hostname
<span class="c"># =&gt; Hostname: webpod1</span>

<span class="c"># 지속적으로 접속 트래픽 발생</span>
<span class="nv">$ SVCIP</span><span class="o">=</span><span class="si">$(</span>kubectl get svc svc <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.spec.clusterIP}'</span><span class="si">)</span>
<span class="nv">$ </span><span class="k">while </span><span class="nb">true</span><span class="p">;</span> <span class="k">do </span>kubectl <span class="nb">exec </span>netpod <span class="nt">--</span> curl <span class="nt">-s</span> <span class="nv">$SVCIP</span> | <span class="nb">grep </span>Hostname<span class="p">;</span><span class="nb">echo</span> <span class="s2">"-----"</span><span class="p">;</span><span class="nb">sleep </span>1<span class="p">;</span><span class="k">done</span>

<span class="nv">$ </span>kubectl <span class="nb">exec </span>netpod <span class="nt">--</span> tcpdump <span class="nt">-enni</span> any <span class="nt">-q</span>
<span class="c"># =&gt; 15:01:03.468433 eth0  Out ifindex 18 52:59:78:2f:c0:be 172.16.0.147.57744 &amp;gt; 172.16.1.247.80: tcp 0</span>
<span class="c">#    15:01:03.468761 eth0  In  ifindex 18 7e:77:fa:0a:d3:cc 172.16.1.247.80 &amp;gt; 172.16.0.147.57744: tcp 0</span>
<span class="c">#    15:01:03.468801 eth0  Out ifindex 18 52:59:78:2f:c0:be 172.16.0.147.57744 &amp;gt; 172.16.1.247.80: tcp 0</span>
<span class="c">#    15:01:03.469249 eth0  Out ifindex 18 52:59:78:2f:c0:be 172.16.0.147.57744 &amp;gt; 172.16.1.247.80: tcp 76</span>
<span class="c">#    15:01:03.469852 eth0  In  ifindex 18 7e:77:fa:0a:d3:cc 172.16.1.247.80 &amp;gt; 172.16.0.147.57744: tcp 0</span>
<span class="c">#    15:01:03.469852 eth0  In  ifindex 18 7e:77:fa:0a:d3:cc 172.16.1.247.80 &amp;gt; 172.16.0.147.57744: tcp 312</span>
<span class="c">#    15:01:03.469888 eth0  Out ifindex 18 52:59:78:2f:c0:be 172.16.0.147.57744 &amp;gt; 172.16.1.247.80: tcp 0</span>
<span class="c">#    ...</span>
<span class="c"># &lt;span style="color: green;"&gt;👉 파드에서 SVC(ClusterIP) 접속 시 tcpdump 로 확인 &gt;&gt; 파드 내부 캡쳐인데,&lt;/span&gt;</span>
<span class="c"># &lt;span style="color: green;"&gt;    SVC(10.10.200.121)는 보이지 않고, DNAT 된 web-pod 의 IP가 확인됩니다! It's Magic!&lt;/span&gt;</span>

<span class="nv">$ </span>kubectl <span class="nb">exec </span>netpod <span class="nt">--</span> sh <span class="nt">-c</span> <span class="s2">"ngrep -tW byline -d eth0 '' 'tcp port 80'"</span>
<span class="c"># =&gt; T 2024/10/01 15:02:58.406586 172.16.0.147:48018 -&amp;gt; 172.16.2.216:80 [AP] #4</span>
<span class="c">#    GET / HTTP/1.1.</span>
<span class="c">#    Host: 10.10.200.121.</span>
<span class="c">#    User-Agent: curl/8.7.1.</span>
<span class="c">#    Accept: */*.</span>
<span class="c">#    ...</span>

<span class="c"># 서비스 정보 확인</span>
<span class="nv">$ </span>c0 service list
<span class="c"># =&gt; ID   Frontend              Service Type   Backend</span>
<span class="c">#    ...</span>
<span class="c">#    11   10.10.200.121:80      ClusterIP      1 =&amp;gt; 172.16.1.247:80 (active)</span>
<span class="c">#                                              2 =&amp;gt; 172.16.2.216:80 (active)</span>
<span class="nv">$ </span>c0 bpf lb list
<span class="c"># =&gt; SERVICE ADDRESS           BACKEND ADDRESS (REVNAT_ID) (SLOT)</span>
<span class="c">#    0.0.0.0:30405 (0)         0.0.0.0:0 (8) (0) [NodePort, non-routable]</span>
<span class="c">#    ...</span>
<span class="c">#    10.10.200.121:80 (1)      172.16.1.247:80 (11) (1)</span>
<span class="c">#    10.10.200.121:80 (2)      172.16.2.216:80 (11) (2)</span>
<span class="c">#    ...</span>

<span class="c"># BPF maps</span>
<span class="nv">$ </span>c0 map list <span class="nt">--verbose</span>
<span class="nv">$ </span>c0 map list <span class="nt">--verbose</span> | <span class="nb">grep </span>lb
<span class="c"># =&gt; ## Map: cilium_lb4_backends_v3</span>
<span class="c">#    ## Map: cilium_lb_affinity_match</span>
<span class="c">#    ## Map: cilium_lb4_source_range</span>
<span class="c">#    ## Map: cilium_lb4_affinity</span>
<span class="c">#    ## Map: cilium_lb4_services_v2</span>
<span class="c">#    ## Map: cilium_lb4_reverse_nat</span>
<span class="c">#    ## Map: cilium_lb4_reverse_sk</span>
<span class="c">#    ## Map: cilium_skip_lb4</span>
<span class="nv">$ </span>c0 map get cilium_lb4_services_v2
<span class="c"># =&gt; Key                       Value                 State   Error</span>
<span class="c">#    ...</span>
<span class="c">#    10.10.200.121:80 (1)      20 0 (11) [0x0 0x0]   sync</span>
<span class="c">#    10.10.200.121:80 (0)      0 2 (11) [0x0 0x0]    sync</span>
<span class="c">#    10.10.200.121:80 (2)      21 0 (11) [0x0 0x0]   sync</span>
<span class="c">#    ...</span>
<span class="nv">$ </span>c0 map get cilium_lb4_backends_v3
<span class="c"># =&gt; Key                       Value                 State   Error</span>
<span class="c">#    ...</span>
<span class="c">#    21    ANY://172.16.2.216    sync</span>
<span class="c">#    20    ANY://172.16.1.247    sync</span>
<span class="c">#    ...</span>
<span class="nv">$ </span>c0 map get cilium_lb4_reverse_nat
<span class="c"># =&gt; Key                       Value                 State   Error</span>
<span class="c">#    ...</span>
<span class="c">#    11    10.10.200.121:80      sync</span>
<span class="c">#    ...</span>
<span class="nv">$ </span>c0 map get cilium_lb4_reverse_sk
<span class="c"># =&gt; Key                       Value                 State   Error</span>
<span class="c">#    ...</span>
<span class="c">#    [172.16.2.216]:20480, 22906     [10.10.200.121]:20480, 2816</span>
<span class="c">#    [172.16.1.247]:20480, 19024     [10.10.200.121]:20480, 2816</span>
<span class="c">#    [172.16.2.216]:20480, 30734     [10.10.200.121]:20480, 2816</span>
<span class="c">#    ...</span>
<span class="nv">$ </span>c0 map get cilium_lxc
<span class="c"># =&gt; Key              Value                                                                                            State   Error</span>
<span class="c">#    172.16.0.147:0   id=2724  sec_id=27629 flags=0x0000 ifindex=19  mac=52:59:78:2F:C0:BE nodemac=7E:77:FA:0A:D3:CC   sync</span>
<span class="c">#    172.16.0.223:0   id=640   sec_id=37523 flags=0x0000 ifindex=9   mac=AE:B8:81:A6:B1:28 nodemac=C2:21:B8:92:DA:FD   sync</span>
<span class="c">#    172.16.0.26:0    id=1606  sec_id=4     flags=0x0000 ifindex=21  mac=A2:52:92:B7:C0:D9 nodemac=C2:C0:3A:67:BE:B2   sync</span>
<span class="c">#    172.16.0.148:0   id=655   sec_id=4124  flags=0x0000 ifindex=23  mac=2E:AF:13:F1:DD:88 nodemac=FA:32:54:4B:16:D7   sync</span>
<span class="nv">$ </span>c0 map get cilium_ipcache
<span class="c"># =&gt; 172.16.2.216/32     identity=64309 encryptkey=0 tunnelendpoint=192.168.10.102, flags=&amp;lt;none&amp;gt;   sync</span>
<span class="c">#    172.16.1.247/32     identity=64309 encryptkey=0 tunnelendpoint=192.168.10.101, flags=&amp;lt;none&amp;gt;   sync</span>
</code></pre></div></div>

<h4 id="prometheus와-grafana를-통한-cilium-모니터링">Prometheus와 Grafana를 통한 Cilium 모니터링</h4>

<h5 id="설정">설정</h5>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 배포</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> https://raw.githubusercontent.com/cilium/cilium/1.16.3/examples/kubernetes/addons/prometheus/monitoring-example.yaml
<span class="c"># =&gt; namespace/cilium-monitoring created</span>
<span class="c">#    serviceaccount/prometheus-k8s created</span>
<span class="c">#    configmap/grafana-config created</span>
<span class="c">#    configmap/grafana-cilium-dashboard created</span>
<span class="c">#    configmap/grafana-cilium-operator-dashboard created</span>
<span class="c">#    configmap/grafana-hubble-dashboard created</span>
<span class="c">#    configmap/grafana-hubble-l7-http-metrics-by-workload created</span>
<span class="c">#    configmap/prometheus created</span>
<span class="c">#    clusterrole.rbac.authorization.k8s.io/prometheus created</span>
<span class="c">#    clusterrolebinding.rbac.authorization.k8s.io/prometheus created</span>
<span class="c">#    service/grafana created</span>
<span class="c">#    service/prometheus created</span>
<span class="c">#    deployment.apps/grafana created</span>
<span class="c">#    deployment.apps/prometheus created</span>
<span class="nv">$ </span>kubectl get all <span class="nt">-n</span> cilium-monitoring
<span class="c"># =&gt; NAME                              READY   STATUS    RESTARTS   AGE</span>
<span class="c">#    pod/grafana-65d4578dc4-zcmgz      1/1     Running   0          36s</span>
<span class="c">#    pod/prometheus-7cc8784659-bqchf   1/1     Running   0          36s</span>
<span class="c">#    </span>
<span class="c">#    NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE</span>
<span class="c">#    service/grafana      ClusterIP   10.10.200.223   &amp;lt;none&amp;gt;        3000/TCP   36s</span>
<span class="c">#    service/prometheus   ClusterIP   10.10.200.90    &amp;lt;none&amp;gt;        9090/TCP   36s</span>
<span class="c">#    </span>
<span class="c">#    NAME                         READY   UP-TO-DATE   AVAILABLE   AGE</span>
<span class="c">#    deployment.apps/grafana      1/1     1            1           36s</span>
<span class="c">#    deployment.apps/prometheus   1/1     1            1           36s</span>
<span class="c">#    </span>
<span class="c">#    NAME                                    DESIRED   CURRENT   READY   AGE</span>
<span class="c">#    replicaset.apps/grafana-65d4578dc4      1         1         1       36s</span>
<span class="c">#    replicaset.apps/prometheus-7cc8784659   1         1         1       36s</span>

<span class="c"># 파드와 서비스 확인</span>
<span class="nv">$ </span>kubectl get pod,svc,ep <span class="nt">-o</span> wide <span class="nt">-n</span> cilium-monitoring
<span class="c"># =&gt; NAME                              READY   STATUS    RESTARTS   AGE   IP             NODE     NOMINATED NODE   READINESS GATES</span>
<span class="c">#    pod/grafana-65d4578dc4-zcmgz      1/1     Running   0          49s   172.16.1.208   k3s-w1   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    pod/prometheus-7cc8784659-bqchf   1/1     Running   0          49s   172.16.1.248   k3s-w1   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    </span>
<span class="c">#    NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE   SELECTOR</span>
<span class="c">#    service/grafana      ClusterIP   10.10.200.223   &amp;lt;none&amp;gt;        3000/TCP   49s   app=grafana</span>
<span class="c">#    service/prometheus   ClusterIP   10.10.200.90    &amp;lt;none&amp;gt;        9090/TCP   49s   app=prometheus</span>
<span class="c">#    </span>
<span class="c">#    NAME                   ENDPOINTS           AGE</span>
<span class="c">#    endpoints/grafana      172.16.1.208:3000   49s</span>
<span class="c">#    endpoints/prometheus   172.16.1.248:9090   49s</span>

<span class="c"># NodePort 설정</span>
<span class="nv">$ </span>kubectl patch svc grafana <span class="nt">-n</span> cilium-monitoring <span class="nt">-p</span> <span class="s1">'{"spec": {"type": "NodePort"}}'</span>
<span class="nv">$ </span>kubectl patch svc prometheus <span class="nt">-n</span> cilium-monitoring <span class="nt">-p</span> <span class="s1">'{"spec": {"type": "NodePort"}}'</span>

<span class="c"># Grafana 웹 접속</span>
<span class="nv">$ GPT</span><span class="o">=</span><span class="si">$(</span>kubectl get svc <span class="nt">-n</span> cilium-monitoring grafana <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.spec.ports[0].nodePort<span class="o">}</span><span class="si">)</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"Grafana URL = http://</span><span class="si">$(</span>curl <span class="nt">-s</span> ipinfo.io/ip<span class="si">)</span><span class="s2">:</span><span class="nv">$GPT</span><span class="s2">"</span>
<span class="c"># =&gt; Grafana URL = http://54.180.146.116:32172</span>

<span class="c"># Prometheus 웹 접속 정보 확인</span>
<span class="nv">$ PPT</span><span class="o">=</span><span class="si">$(</span>kubectl get svc <span class="nt">-n</span> cilium-monitoring prometheus <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.spec.ports[0].nodePort<span class="o">}</span><span class="si">)</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"Prometheus URL = http://</span><span class="si">$(</span>curl <span class="nt">-s</span> ipinfo.io/ip<span class="si">)</span><span class="s2">:</span><span class="nv">$PPT</span><span class="s2">"</span>
<span class="c"># =&gt; Prometheus URL = http://54.180.146.116:30426</span>
</code></pre></div></div>

<h5 id="grafana--prometheus-nodeport-로-웹-접속-후-확인">grafana , prometheus NodePort 로 웹 접속 후 확인</h5>

<p><img src="/assets/2024/kans-3th/w8/20241026_kans_w8_19.png" alt="img.png" class="image-center" />
<em class="image-caption">Prometheus 모니터링 화면</em></p>

<p><img src="/assets/2024/kans-3th/w8/20241026_kans_w8_21.png" alt="img_1.png" class="image-center" />
<em class="image-caption">Grafana 모니터링 화면</em></p>

<h4 id="network-policy-l3-l4-l7">Network Policy (L3, L4, L7)</h4>

<h5 id="cilium-보안-소개">Cilium 보안 소개</h5>

<p>Cilium은 여러 레벨의 보안 기능을 제공합니다. <a href="https://docs.cilium.io/en/stable/security/network/intro/">문서</a>
그 중에서 다음 3가지를 알아보겠습니다.</p>

<ul>
  <li>ID 기반 (L3) : 엔드포인트 간의 연결 정책을 정의할 때, 엔드포인트의 ID를 사용합니다. 이 ID는 엔드포인트의 네트워크 주소와 무관하게 유지되며, 
k8s의 label을 통해 만들어집니다. 즉, 파드간에 공유할 수 있으며, 네트워크 주소가 변경되어도 유지됩니다.
<img src="/assets/2024/kans-3th/w8/20241026_kans_w8_22.png" alt="img.png" class="image-center w-80" />
<em class="image-caption"><a href="https://docs.cilium.io/en/stable/security/network/identity/">https://docs.cilium.io/en/stable/security/network/identity/</a></em></li>
  <li>포트기반 (L4) : 엔드포인트 간의 연결 정책을 정의할 때, 포트를 사용합니다. 이는 L3 정책과 함께 사용될 수 있어서,
<code class="language-plaintext highlighter-rouge">role=frontend</code>라는 레이블을 가진 엔드포인트는 443 포트로 outgoing 연결을 허용하고, <code class="language-plaintext highlighter-rouge">role=backend</code>라는 레이블을 가진 엔드포인트는 
443 포트로 incoming 연결을 허용하는 등의 정책을 정의할 수 있습니다.</li>
  <li>어플리케이션 (http) 기반 (L7) : HTTP통신과 RPC 프로토콜의 보안을 위해서 어플리케이션 레벨에서 정밀하게 정책을 정의할 수 있습니다.
이는 HTTP 헤더, 메소드, 경로, 쿼리 파라미터 등을 사용하여 정책을 정의할 수 있습니다.
    <ul>
      <li>프록시 주입 : Envoy - <a href="https://docs.cilium.io/en/stable/security/network/proxy/">Docs</a>, <a href="https://docs.cilium.io/en/stable/security/network/proxy/envoy/">Envoy</a>
        <ul>
          <li>Cilium은 모든 네트워크 연결에 대해 Layer 4 프록시(예) Envoy)를 주입시킬 수 있습니다.
이는 고차원의 네트워크 정책을 강제할 수 있는 기반이 됩니다.
<img src="/assets/2024/kans-3th/w8/20241026_kans_w8_23.png" alt="img.png" class="image-center" />
<img src="/assets/2024/kans-3th/w8/20241026_kans_w8_24.png" alt="img.png" class="image-center" /></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h5 id="network-policy-관련-ebpf-datapath">Network Policy 관련 eBPF Datapath</h5>

<p><img src="/assets/2024/kans-3th/w8/20241026_kans_w8_25.png" alt="img.png" class="image-center w-80" /></p>

<ul>
  <li>Prefilter : Prefilter는 XDP 프로그램을 통해 수행되며, 최고의 성능을 위해 네트워크 패킷을 필터링하는 prefilter 규칙들을 제공합니다.
특히 CIDR map들을 사용해 IP 주소를 필터링하는 등의 동작을 할 수 있습니다.</li>
  <li>Endpoint policy : 정책에 따라 패킷을 차단/전달하거나, 서비스로 전달하거나, L7로 정책 전달을 할 수 있습니다.
    <ul>
      <li>Cilium datapath는 L3와 L4 정책을 강제하거나, 패킷과 ID를 매핑하는 역할을 수행합니다.</li>
    </ul>
  </li>
  <li>L7 policy : L7 정책은 프록시 트래픽을 Cilium의 userspace proxy instance, 즉 Envoy로 전달합니다. Envoy 는 트래픽을 전달하거나
L7 정책에 의해 차단할 수 있습니다.
    <ul>
      <li>👉 L7 정책은 hook과 Userspace Proxy(envoy)를 사용하기 때문에 성능이 조금 떨어질 수 있습니다.</li>
    </ul>
  </li>
</ul>

<h5 id="실습">실습</h5>

<ul>
  <li>스타워즈에서 영감받은 예제를 통해 Network Policy를 적용해보겠습니다.
    <ul>
      <li>디플로이먼트(웹 서버, deathstar, replicas 2), 파드(xwing, tiefighter), 서비스(ClusterIP, service/deathstar)</li>
    </ul>
  </li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 배포</span>
<span class="nv">$ </span>kubectl create <span class="nt">-f</span> https://raw.githubusercontent.com/cilium/cilium/1.16.3/examples/minikube/http-sw-app.yaml
<span class="c"># =&gt; service/deathstar created</span>
<span class="c">#    deployment.apps/deathstar created</span>
<span class="c">#    pod/tiefighter created</span>
<span class="c">#    pod/xwing created</span>
<span class="nv">$ </span>kubectl get all
<span class="c"># =&gt; NAME                             READY   STATUS    RESTARTS   AGE</span>
<span class="c">#    pod/deathstar-689f66b57d-mm72v   1/1     Running   0          15s</span>
<span class="c">#    pod/deathstar-689f66b57d-pz56p   1/1     Running   0          15s</span>
<span class="c">#    pod/tiefighter                   1/1     Running   0          15s</span>
<span class="c">#    pod/xwing                        1/1     Running   0          15s</span>
<span class="c">#    </span>
<span class="c">#    NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE</span>
<span class="c">#    service/deathstar    ClusterIP   10.10.200.162   &amp;lt;none&amp;gt;        80/TCP    15s</span>
<span class="c">#    service/kubernetes   ClusterIP   10.10.200.1     &amp;lt;none&amp;gt;        443/TCP   8h</span>
<span class="c">#    </span>
<span class="c">#    NAME                        READY   UP-TO-DATE   AVAILABLE   AGE</span>
<span class="c">#    deployment.apps/deathstar   2/2     2            2           15s</span>
<span class="c">#    </span>
<span class="c">#    NAME                                   DESIRED   CURRENT   READY   AGE</span>
<span class="c">#    replicaset.apps/deathstar-689f66b57d   2         2         2       15s</span>

<span class="c"># 파드 라벨 확인</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">--show-labels</span>
<span class="c"># =&gt; NAME                         READY   STATUS    RESTARTS   AGE   LABELS</span>
<span class="c">#    deathstar-689f66b57d-mm72v   1/1     Running   0          24s   app.kubernetes.io/name=deathstar,class=deathstar,org=empire,pod-template-hash=689f66b57d</span>
<span class="c">#    deathstar-689f66b57d-pz56p   1/1     Running   0          24s   app.kubernetes.io/name=deathstar,class=deathstar,org=empire,pod-template-hash=689f66b57d</span>
<span class="c">#    tiefighter                   1/1     Running   0          24s   app.kubernetes.io/name=tiefighter,class=tiefighter,org=empire</span>
<span class="c">#    xwing                        1/1     Running   0          24s   app.kubernetes.io/name=xwing,class=xwing,org=alliance</span>

<span class="c"># cilium endpoint 확인</span>
<span class="nv">$ </span>kubectl get ciliumendpoints
<span class="c"># =&gt; NAME                         SECURITY IDENTITY   ENDPOINT STATE   IPV4           IPV6</span>
<span class="c">#    deathstar-689f66b57d-mm72v   391                 ready            172.16.2.35</span>
<span class="c">#    deathstar-689f66b57d-pz56p   391                 ready            172.16.1.232</span>
<span class="c">#    tiefighter                   9002                ready            172.16.0.5</span>
<span class="c">#    xwing                        10812               ready            172.16.2.31</span>

<span class="nv">$ </span>c0 endpoint list
<span class="c"># =&gt; ENDPOINT   POLICY (ingress)   POLICY (egress)   IDENTITY   LABELS (source:key[=value])                                                  IPv6   IPv4           STATUS</span>
<span class="c">#               ENFORCEMENT        ENFORCEMENT</span>
<span class="c">#    ...</span>
<span class="c">#    2481       Disabled           Disabled          9002       k8s:app.kubernetes.io/name=tiefighter                                               172.16.0.5     ready</span>
<span class="c">#                                                               k8s:class=tiefighter</span>
<span class="c">#                                                               k8s:org=empire</span>
<span class="nv">$ </span>c1 endpoint list
<span class="c"># =&gt; ENDPOINT   POLICY (ingress)   POLICY (egress)   IDENTITY   LABELS (source:key[=value])                                                        IPv6   IPv4           STATUS</span>
<span class="c">#               ENFORCEMENT        ENFORCEMENT</span>
<span class="c">#    ...</span>
<span class="c">#    1063       Disabled           Disabled          391        k8s:app.kubernetes.io/name=deathstar                                                      172.16.1.232   ready</span>
<span class="c">#                                                               k8s:class=deathstar</span>
<span class="c">#                                                               k8s:org=empire</span>
<span class="nv">$ </span>c2 endpoint list
<span class="c"># =&gt; ENDPOINT   POLICY (ingress)   POLICY (egress)   IDENTITY   LABELS (source:key[=value])                                                      IPv6   IPv4           STATUS</span>
<span class="c">#               ENFORCEMENT        ENFORCEMENT</span>
<span class="c">#    ...</span>
<span class="c">#    1695       Disabled           Disabled          391        k8s:app.kubernetes.io/name=deathstar                                                    172.16.2.35    ready</span>
<span class="c">#                                                               k8s:class=deathstar</span>
<span class="c">#                                                               k8s:org=empire</span>
<span class="c">#    2810       Disabled           Disabled          10812      k8s:app.kubernetes.io/name=xwing                                                        172.16.2.31    ready</span>
<span class="c">#                                                               k8s:class=xwing</span>
<span class="c">#                                                               k8s:org=alliance</span>

<span class="c"># 데스스타 SVC(ClusterIP) 접속하여 웹 파드 연결 확인 &gt;&gt; Hubble UI 에서 실시간 확인해보자!</span>
<span class="nv">$ </span>kubectl <span class="nb">exec </span>xwing <span class="nt">--</span> curl <span class="nt">-s</span> <span class="nt">-XPOST</span> deathstar.default.svc.cluster.local/v1/request-landing
<span class="c"># =&gt; Ship landed</span>

<span class="nv">$ </span>kubectl <span class="nb">exec </span>tiefighter <span class="nt">--</span> curl <span class="nt">-s</span> <span class="nt">-XPOST</span> deathstar.default.svc.cluster.local/v1/request-landing
<span class="c"># =&gt; Ship landed</span>

<span class="c"># 확인</span>
<span class="nv">$ </span>hubble observe
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w8/20241026_kans_w8_26.png" alt="img.png" /></p>

<ul>
  <li>Identity-Aware and HTTP-Aware Policy Enforcement <code class="language-plaintext highlighter-rouge">Apply an L3/L4 Policy</code> - <a href="https://docs.cilium.io/en/stable/gettingstarted/demo/#apply-an-l3-l4-policy">Link</a> &amp; Hubble CLI - <a href="https://docs.cilium.io/en/stable/gettingstarted/hubble_cli/">링크</a>
    <ul>
      <li>Cilium 에서는 Endpoint IP 대신, <strong>파드</strong>의 <strong>Labels(라벨)</strong>을 사용(기준)하여 <strong>보안 정책을 적용</strong>합니다.</li>
      <li><strong>IP/Port</strong> 필터링을 <strong>L3/L4 네트워크 정책</strong>이라고 합니다.</li>
      <li>아래 처럼 ‘org=empire’ Labels(라벨) 부착된 파드만 허용해보겠습니다.</li>
      <li>Cilium 은 <strong>stateful connection tracking</strong>을 지원하므로 리턴 트래픽은 자동으로 허용됩니다.</li>
    </ul>
  </li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># L3/L4 정책 생성</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl apply -f - 
apiVersion: "cilium.io/v2"
kind: CiliumNetworkPolicy
metadata:
  name: "rule1"
spec:
  description: "L3-L4 policy to restrict deathstar access to empire ships only"
  endpointSelector:
    matchLabels:
      org: empire
      class: deathstar
  ingress:
  - fromEndpoints:
    - matchLabels:
        org: empire
    toPorts:
    - ports:
      - port: "80"
        protocol: TCP
</span><span class="no">EOF
</span><span class="c"># =&gt; ciliumnetworkpolicy.cilium.io/rule1 created</span>

<span class="c"># 정책 확인</span>
<span class="nv">$ </span>kubectl get cnp
<span class="c"># =&gt; NAME    AGE</span>
<span class="c">#    rule1   9s</span>
<span class="nv">$ </span>kubectl describe cnp rule1
<span class="nv">$ </span>c0 policy get
<span class="c"># =&gt; [</span>
<span class="c">#      {</span>
<span class="c">#        &amp;quot;endpointSelector&amp;quot;: {</span>
<span class="c">#          &amp;quot;matchLabels&amp;quot;: {</span>
<span class="c">#            &amp;quot;any:class&amp;quot;: &amp;quot;deathstar&amp;quot;,</span>
<span class="c">#            &amp;quot;any:org&amp;quot;: &amp;quot;empire&amp;quot;,</span>
<span class="c">#            &amp;quot;k8s:io.kubernetes.pod.namespace&amp;quot;: &amp;quot;default&amp;quot;</span>
<span class="c">#          }</span>
<span class="c">#        },</span>
<span class="c">#        &amp;quot;ingress&amp;quot;: [</span>
<span class="c">#          {</span>
<span class="c">#            &amp;quot;fromEndpoints&amp;quot;: [</span>
<span class="c">#              {</span>
<span class="c">#                &amp;quot;matchLabels&amp;quot;: {</span>
<span class="c">#                  &amp;quot;any:org&amp;quot;: &amp;quot;empire&amp;quot;,</span>
<span class="c">#                  &amp;quot;k8s:io.kubernetes.pod.namespace&amp;quot;: &amp;quot;default&amp;quot;</span>
<span class="c">#                }</span>
<span class="c">#              }</span>
<span class="c">#            ],</span>
<span class="c">#            &amp;quot;toPorts&amp;quot;: [</span>
<span class="c">#              {</span>
<span class="c">#                &amp;quot;ports&amp;quot;: [</span>
<span class="c">#                  {</span>
<span class="c">#                    &amp;quot;port&amp;quot;: &amp;quot;80&amp;quot;,</span>
<span class="c">#                    &amp;quot;protocol&amp;quot;: &amp;quot;TCP&amp;quot;</span>
<span class="c">#                  }</span>
<span class="c">#                ]</span>
<span class="c">#              }</span>
<span class="c">#            ]</span>
<span class="c">#          }</span>
<span class="c">#        ],</span>
<span class="c">#        ...</span>
<span class="c">#        &amp;quot;enableDefaultDeny&amp;quot;: {</span>
<span class="c">#          &amp;quot;ingress&amp;quot;: true,</span>
<span class="c">#          &amp;quot;egress&amp;quot;: false</span>
<span class="c">#        },</span>
<span class="c">#        ...</span>
<span class="c">#      }</span>
<span class="c">#    ]</span>

<span class="c"># 파드 curl 접속 시도 시 파드 sh 접속 후 curl 시도하자!</span>
<span class="c"># 데스스타 SVC(ClusterIP) 접속하여 웹 파드 연결 확인 &gt;&gt; Hubble UI 에서 drop 확인!</span>
<span class="nv">$ </span>kubectl <span class="nb">exec </span>tiefighter <span class="nt">--</span> curl <span class="nt">-s</span> <span class="nt">-XPOST</span> deathstar.default.svc.cluster.local/v1/request-landing
<span class="c"># =&gt; Ship landed</span>

<span class="nv">$ </span>kubectl <span class="nb">exec </span>xwing <span class="nt">--</span> curl <span class="nt">-s</span> <span class="nt">-XPOST</span> deathstar.default.svc.cluster.local/v1/request-landing
<span class="c"># =&gt; (없음)</span>
<span class="c"># &lt;span style="color: green;"&gt;👉 접속이 drop 됩니다. xwing은 허용되는 `org=empire`인 엔드포인트가 아닌 `org=alliance`인 엔드포인트이기 때문입니다.&lt;/span&gt;</span>

<span class="c"># hubble cli 모니터링 </span>
<span class="nv">$ </span>hubble observe <span class="nt">--pod</span> xwing
<span class="nv">$ </span>hubble observe <span class="nt">--pod</span> tiefighter
<span class="nv">$ </span>hubble observe <span class="nt">--pod</span> deathstar
<span class="c"># =&gt; Oct 01 16:28:28.825: default/xwing:39224 (ID:10812) &amp;lt;&amp;gt; default/deathstar-689f66b57d-mm72v:80 (ID:391) policy-verdict:none INGRESS DENIED (TCP Flags: SYN)</span>
<span class="c">#    Oct 01 16:28:28.825: default/xwing:39224 (ID:10812) &amp;lt;&amp;gt; default/deathstar-689f66b57d-mm72v:80 (ID:391) Policy denied DROPPED (TCP Flags: SYN)</span>

<span class="nv">$ </span>hubble observe <span class="nt">--pod</span> deathstar <span class="nt">--verdict</span> DROPPED
<span class="c"># =&gt; Oct 01 16:27:50.894: default/xwing:43148 (ID:10812) &amp;lt;&amp;gt; default/deathstar-689f66b57d-pz56p:80 (ID:391) policy-verdict:none INGRESS DENIED (TCP Flags: SYN)</span>
<span class="c">#    Oct 01 16:27:50.894: default/xwing:43148 (ID:10812) &amp;lt;&amp;gt; default/deathstar-689f66b57d-pz56p:80 (ID:391) Policy denied DROPPED (TCP Flags: SYN)</span>
<span class="c">#    Oct 01 16:28:24.676: default/xwing:43148 (ID:10812) &amp;lt;&amp;gt; default/deathstar-689f66b57d-pz56p:80 (ID:391) policy-verdict:none INGRESS DENIED (TCP Flags: SYN)</span>
<span class="c">#    Oct 01 16:28:24.676: default/xwing:43148 (ID:10812) &amp;lt;&amp;gt; default/deathstar-689f66b57d-pz56p:80 (ID:391) Policy denied DROPPED (TCP Flags: SYN)</span>

<span class="c"># Inspecting the Policy</span>
<span class="c"># If we run cilium endpoint list again we will see that the pods with the label org=empire and class=deathstar</span>
<span class="c"># now have ingress policy enforcement enabled as per the policy above.</span>

<span class="c"># endpoint list 에서 정책 적용 확인</span>
<span class="nv">$ </span>c0 endpoint list
<span class="c"># =&gt; ENDPOINT   POLICY (ingress)   POLICY (egress)   IDENTITY   LABELS (source:key[=value])                                                  IPv6   IPv4           STATUS</span>
<span class="c">#               ENFORCEMENT        ENFORCEMENT</span>
<span class="c">#    2481       Disabled           Disabled          9002       k8s:app.kubernetes.io/name=tiefighter                                               172.16.0.5     ready</span>
<span class="c">#                                                               k8s:class=tiefighter</span>
<span class="c">#                                                               k8s:org=empire </span>
<span class="nv">$ </span>c1 endpoint list | <span class="nb">grep </span>deathstar
<span class="c"># =&gt; 1063       Enabled            Disabled          391        k8s:app.kubernetes.io/name=deathstar                                                      172.16.1.232   ready</span>
<span class="c">#                                                               k8s:class=deathstar</span>
<span class="nv">$ </span>c2 endpoint list
<span class="c"># =&gt; ENDPOINT   POLICY (ingress)   POLICY (egress)   IDENTITY   LABELS (source:key[=value])                                                      IPv6   IPv4           STATUS</span>
<span class="c">#               ENFORCEMENT        ENFORCEMENT</span>
<span class="c">#    1695       Enabled            Disabled          391        k8s:app.kubernetes.io/name=deathstar                                                    172.16.2.35    ready</span>
<span class="c">#                                                               k8s:class=deathstar</span>
<span class="c">#                                                               k8s:org=empire</span>
<span class="c">#    2810       Disabled           Disabled          10812      k8s:app.kubernetes.io/name=xwing                                                        172.16.2.31    ready</span>
<span class="c">#                                                               k8s:class=xwing</span>
<span class="c">#                                                               k8s:org=alliance</span>
</code></pre></div></div>

<ul>
  <li>Identity-Aware and HTTP-Aware Policy Enforcement Apply and Test HTTP-aware L7 Policy - <a href="https://docs.cilium.io/en/stable/gettingstarted/demo/#apply-and-test-http-aware-l7-policy">Docs</a>
    <ul>
      <li>HTTP L7 필터링을 적용 : PUT /v1/exhaust-port 요청을 차단해보겠습니다.</li>
    </ul>
  </li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 데스스타 SVC(ClusterIP) 접속</span>
<span class="nv">$ </span>kubectl <span class="nb">exec </span>tiefighter <span class="nt">--</span> curl <span class="nt">-s</span> <span class="nt">-XPUT</span> deathstar.default.svc.cluster.local/v1/exhaust-port
<span class="c"># =&gt; Panic: deathstar exploded</span>
<span class="c">#    ...</span>

<span class="c"># POST /v1/request-landing API 호출만 허용 정책으로 기존 정책 내용을 업데이트(configured)!</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl apply -f - 
apiVersion: "cilium.io/v2"
kind: CiliumNetworkPolicy
metadata:
  name: "rule1"
spec:
  description: "L7 policy to restrict access to specific HTTP call"
  endpointSelector:
    matchLabels:
      org: empire
      class: deathstar
  ingress:
  - fromEndpoints:
    - matchLabels:
        org: empire
    toPorts:
    - ports:
      - port: "80"
        protocol: TCP
      rules:
        http:
        - method: "POST"
          path: "/v1/request-landing"
</span><span class="no">EOF

</span><span class="c"># 정책 확인</span>
<span class="nv">$ </span>kubectl describe ciliumnetworkpolicies
<span class="c"># =&gt; Name:         rule1</span>
<span class="c">#    Namespace:    default</span>
<span class="c">#    Labels:       &amp;lt;none&amp;gt;</span>
<span class="c">#    Annotations:  &amp;lt;none&amp;gt;</span>
<span class="c">#    API Version:  cilium.io/v2</span>
<span class="c">#    Kind:         CiliumNetworkPolicy</span>
<span class="c">#    Metadata:</span>
<span class="c">#      Creation Timestamp:  2024-10-01T16:24:53Z</span>
<span class="c">#      Generation:          2</span>
<span class="c">#      Resource Version:    32642</span>
<span class="c">#      UID:                 d3527eec-5832-4273-b805-c006c728a8af</span>
<span class="c">#    Spec:</span>
<span class="c">#      Description:  L7 policy to restrict access to specific HTTP call</span>
<span class="c">#      Endpoint Selector:</span>
<span class="c">#        Match Labels:</span>
<span class="c">#          Class:  deathstar</span>
<span class="c">#          Org:    empire</span>
<span class="c">#      Ingress:</span>
<span class="c">#        From Endpoints:</span>
<span class="c">#          Match Labels:</span>
<span class="c">#            Org:  empire</span>
<span class="c">#        To Ports:</span>
<span class="c">#          Ports:</span>
<span class="c">#            Port:      80</span>
<span class="c">#            Protocol:  TCP</span>
<span class="c">#          Rules:</span>
<span class="c">#            Http:</span>
<span class="c">#              Method:  POST</span>
<span class="c">#              Path:    /v1/request-landing</span>
<span class="c">#    ...</span>
<span class="nv">$ </span>c0 policy get

<span class="c"># 모니터링</span>
<span class="nv">$ </span>c1 monitor <span class="nt">-v</span> <span class="nt">--type</span> l7
<span class="nv">$ </span>c2 monitor <span class="nt">-v</span> <span class="nt">--type</span> l7
<span class="c"># =&gt; &amp;lt;- Request http from 0 ([k8s:app.kubernetes.io/name=tiefighter k8s:class=tiefighter k8s:io.cilium.k8s.namespace.labels.kubernetes.io/metadata.name=default k8s:io.cilium.k8s.policy.cluster=default k8s:io.cilium.k8s.policy.serviceaccount=default k8s:io.kubernetes.pod.namespace=default k8s:org=empire]) to 1695 ([k8s:app.kubernetes.io/name=deathstar k8s:class=deathstar k8s:io.cilium.k8s.namespace.labels.kubernetes.io/metadata.name=default k8s:io.cilium.k8s.policy.cluster=default k8s:io.cilium.k8s.policy.serviceaccount=default k8s:io.kubernetes.pod.namespace=default k8s:org=empire]), identity 9002-&amp;gt;391, verdict Forwarded POST http://deathstar.default.svc.cluster.local/v1/request-landing =&amp;gt; 0</span>
<span class="c">#    &amp;lt;- Response http to 0 ([k8s:app.kubernetes.io/name=tiefighter k8s:class=tiefighter k8s:io.cilium.k8s.namespace.labels.kubernetes.io/metadata.name=default k8s:io.cilium.k8s.policy.cluster=default k8s:io.cilium.k8s.policy.serviceaccount=default k8s:io.kubernetes.pod.namespace=default k8s:org=empire]) from 1695 ([k8s:app.kubernetes.io/name=deathstar k8s:class=deathstar k8s:io.cilium.k8s.namespace.labels.kubernetes.io/metadata.name=default k8s:io.cilium.k8s.policy.cluster=default k8s:io.cilium.k8s.policy.serviceaccount=default k8s:io.kubernetes.pod.namespace=default k8s:org=empire]), identity 391-&amp;gt;9002, verdict Forwarded POST http://deathstar.default.svc.cluster.local/v1/request-landing =&amp;gt; 200</span>
<span class="nv">$ </span>hubble observe <span class="nt">--pod</span> deathstar
<span class="nv">$ </span>hubble observe <span class="nt">--pod</span> deathstar <span class="nt">--verdict</span> DROPPED

<span class="c"># 접근 테스트</span>
<span class="nv">$ </span>kubectl <span class="nb">exec </span>tiefighter <span class="nt">--</span> curl <span class="nt">-s</span> <span class="nt">-XPOST</span> deathstar.default.svc.cluster.local/v1/request-landing
<span class="c"># =&gt; Ship landed</span>

<span class="nv">$ </span>kubectl <span class="nb">exec </span>tiefighter <span class="nt">--</span> curl <span class="nt">-s</span> <span class="nt">-XPUT</span> deathstar.default.svc.cluster.local/v1/exhaust-port
<span class="c"># =&gt; Access denied</span>

<span class="c">## hubble cli 에 차단 로그 확인</span>
<span class="nv">$ </span>hubble observe <span class="nt">--pod</span> deathstar <span class="nt">--verdict</span> DROPPED
<span class="c"># =&gt; Oct 01 16:41:48.094: default/tiefighter:51860 (ID:9002) -&amp;gt; default/deathstar-689f66b57d-mm72v:80 (ID:391) http-request DROPPED (HTTP/1.1 PUT http://deathstar.default.svc.cluster.local/v1/exhaust-port)</span>
<span class="c">#    Oct 01 16:41:48.094: default/tiefighter:51860 (ID:9002) &amp;lt;- default/deathstar-689f66b57d-mm72v:80 (ID:391) http-response FORWARDED (HTTP/1.1 403 0ms (PUT http://deathstar.default.svc.cluster.local/v1/exhaust-port))</span>

<span class="nv">$ </span>hubble observe <span class="nt">--pod</span> deathstar <span class="nt">--protocol</span> http
<span class="c"># =&gt; Oct 01 16:39:57.041: default/tiefighter:57616 (ID:9002) &amp;lt;- default/deathstar-689f66b57d-mm72v:80 (ID:391) http-response FORWARDED (HTTP/1.1 200 2ms (POST http://deathstar.default.svc.cluster.local/v1/request-landing))</span>

<span class="c"># 삭제</span>
<span class="nv">$ </span>kubectl delete <span class="nt">-f</span> https://raw.githubusercontent.com/cilium/cilium/1.16.3/examples/minikube/http-sw-app.yaml
<span class="c"># =&gt; service &amp;quot;deathstar&amp;quot; deleted</span>
<span class="c">#    deployment.apps &amp;quot;deathstar&amp;quot; deleted</span>
<span class="c">#    pod &amp;quot;tiefighter&amp;quot; deleted</span>
<span class="c">#    pod &amp;quot;xwing&amp;quot; deleted</span>
<span class="nv">$ </span>kubectl delete cnp rule1
<span class="c"># =&gt; ciliumnetworkpolicy.cilium.io &amp;quot;rule1&amp;quot; deleted</span>
</code></pre></div></div>

<h4 id="bandwidth-manager">Bandwidth Manager</h4>

<h5 id="bandwidth-manager-소개">Bandwidth Manager 소개</h5>

<ul>
  <li>Cilium은 Bandwidth(네트워크 대역폭)과 Latency optimization(지연 시간 최적화)를 지원합니다. - <a href="https://docs.cilium.io/en/stable/network/kubernetes/bandwidth-manager/">Link</a> , <a href="https://cilium.io/use-cases/bandwidth-optimization/">Home</a> , <a href="https://www.youtube.com/watch?v=QTSS6ktK8hY">Youtube</a>
<img src="/assets/2024/kans-3th/w8/20241026_kans_w8_27.png" alt="img.png" class="image-center w-80" />
<em class="image-caption"><a href="https://cilium.io/use-cases/bandwidth-optimization/">https://cilium.io/use-cases/bandwidth-optimization/</a></em>
    <ul>
      <li>bandwidth manager는 TCP와 UDP 부하를 최적화 하고, 효율적으로 개별 파드의 접속률을 제한할 수 있습니다. - <strong>EDT</strong>(Earliest Departure Time) 와 <strong>eBPF</strong> 사용</li>
      <li><code class="language-plaintext highlighter-rouge">kubernetes.io/egress-bandwidth</code> Pod <strong>annotation</strong> 은 egress 트래픽에 대해 호스트 네트워크 장치의 대역폭 제한을 설정합니다.</li>
      <li><del><code class="language-plaintext highlighter-rouge">kubernetes.io/ingress-bandwidth</code></del> <strong>annotation</strong> 은 지원되지 않습니다.</li>
      <li>direct routing mode, tunneling mode 둘 다 지원합니다.</li>
      <li>Limitations : L7 정책과 함께 사용할 수 없습니다.
<img src="/assets/2024/kans-3th/w8/20241026_kans_w8_28.png" alt="img.png" class="image-center w-80" /></li>
    </ul>
  </li>
</ul>

<h5 id="설정-및-확인">설정 및 확인</h5>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 인터페이스 tc qdisc 확인</span>
<span class="nv">$ </span>tc qdisc show dev enp0s8
<span class="c"># =&gt; qdisc fq_codel 0: root refcnt 2 limit 10240p flows 1024 quantum 1514 target 5ms interval 100ms memory_limit 32Mb ecn drop_batch 64</span>
<span class="c">#    qdisc clsact ffff: parent ffff:fff1</span>

<span class="c"># 설정</span>
<span class="nv">$ </span>helm upgrade cilium cilium/cilium <span class="nt">--namespace</span> kube-system <span class="nt">--reuse-values</span> <span class="nt">--set</span> bandwidthManager.enabled<span class="o">=</span><span class="nb">true</span>
<span class="c"># =&gt; Release &amp;quot;cilium&amp;quot; has been upgraded. Happy Helming!</span>
<span class="c">#    ...</span>

<span class="c"># 적용 확인</span>
<span class="nv">$ </span>cilium config view | <span class="nb">grep </span>bandwidth
<span class="c"># =&gt; enable-bandwidth-manager                          true</span>

<span class="c"># egress bandwidth limitation 동작하는 인터페이스 확인</span>
<span class="nv">$ </span>c0 status | <span class="nb">grep  </span>BandwidthManager
<span class="c"># =&gt; BandwidthManager:        EDT with BPF [CUBIC] [enp0s3, enp0s8]</span>

<span class="c"># 인터페이스 tc qdisc 확인 : 설정 전후 옵션값들이 상당히 추가된다</span>
<span class="nv">$ </span>tc qdisc
<span class="nv">$ </span>tc qdisc show dev enp0s8
<span class="c"># =&gt; qdisc fq 8006: root refcnt 2 limit 10000p flow_limit 100p buckets 32768 orphan_mask 1023 quantum 3028b initial_quantum 15140b low_rate_threshold 550Kbit refill_delay 40ms timer_slack 10us horizon 2s horizon_cap</span>
<span class="c">#    qdisc clsact ffff: parent ffff:fff1</span>
</code></pre></div></div>

<h5 id="동작-및-확인">동작 및 확인</h5>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 테스트를 위한 트래픽 발생 서버/클라이언트 파드 생성</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl apply -f -
---
apiVersion: v1
kind: Pod
metadata:
  annotations:
    # Limits egress bandwidth to 10Mbit/s.
    kubernetes.io/egress-bandwidth: "10M"
  labels:
    # This pod will act as server.
    app.kubernetes.io/name: netperf-server
  name: netperf-server
spec:
  containers:
  - name: netperf
    image: cilium/netperf
    ports:
    - containerPort: 12865
---
apiVersion: v1
kind: Pod
metadata:
  # This Pod will act as client.
  name: netperf-client
spec:
  affinity:
    # Prevents the client from being scheduled to the
    # same node as the server.
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app.kubernetes.io/name
            operator: In
            values:
            - netperf-server
        topologyKey: kubernetes.io/hostname
  containers:
  - name: netperf
    args:
    - sleep
    - infinity
    image: cilium/netperf
</span><span class="no">EOF
</span><span class="c"># =&gt; pod/netperf-server created</span>
<span class="c">#    pod/netperf-client created</span>

<span class="c"># egress BW 제한 정보 확인</span>
<span class="nv">$ </span>kubectl describe pod netperf-server | <span class="nb">grep </span>Annotations:
<span class="c"># =&gt; Annotations:      kubernetes.io/egress-bandwidth: 10M</span>

<span class="c"># egress BW 제한이 설정된 파드가 있는 cilium pod 에서 제한 정보 확인</span>
<span class="nv">$ </span>c0 bpf bandwidth list
<span class="nv">$ </span>c1 bpf bandwidth list
<span class="nv">$ </span>c2 bpf bandwidth list
<span class="c"># =&gt; IDENTITY   EGRESS BANDWIDTH (BitsPerSec)</span>
<span class="c">#    1391       10M</span>

<span class="nv">$ </span>c0 endpoint list
<span class="nv">$ </span>c1 endpoint list
<span class="nv">$ </span>c2 endpoint list
<span class="c"># =&gt; ENDPOINT   POLICY (ingress)   POLICY (egress)   IDENTITY   LABELS (source:key[=value])                                                      IPv6   IPv4           STATUS</span>
<span class="c">#               ENFORCEMENT        ENFORCEMENT</span>
<span class="c">#    1391       Disabled           Disabled          8191       k8s:app.kubernetes.io/name=netperf-server                                               172.16.2.106   ready</span>
<span class="c">#    ...</span>

<span class="c"># 트래픽 발생 &gt;&gt; Hubble UI 에서 확인</span>
<span class="c"># egress traffic of the netperf-server Pod has been limited to 10Mbit per second. </span>
<span class="nv">$ NETPERF_SERVER_IP</span><span class="o">=</span><span class="si">$(</span>kubectl get pod netperf-server <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.status.podIP}'</span><span class="si">)</span>
<span class="nv">$ </span>kubectl <span class="nb">exec </span>netperf-client <span class="nt">--</span> netperf <span class="nt">-t</span> TCP_MAERTS <span class="nt">-H</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NETPERF_SERVER_IP</span><span class="k">}</span><span class="s2">"</span>
<span class="c"># =&gt; Recv   Send    Send</span>
<span class="c">#    Socket Socket  Message  Elapsed</span>
<span class="c">#    Size   Size    Size     Time     Throughput</span>
<span class="c">#    bytes  bytes   bytes    secs.    10^6bits/sec</span>
<span class="c">#    131072  16384  16384    10.01       9.13</span>
<span class="c"># &lt;span style="color: green;"&gt;👉 10Mbps 제한 확인!&lt;/span&gt;</span>

<span class="c"># 5M 제한 설정 후 테스트</span>
<span class="nv">$ </span>kubectl get pod netperf-server <span class="nt">-o</span> json | <span class="nb">sed</span> <span class="nt">-e</span> <span class="s1">'s|10M|5M|g'</span> | kubectl apply <span class="nt">-f</span> -
<span class="c"># =&gt; pod/netperf-server configured</span>
<span class="nv">$ </span>c0 bpf bandwidth list
<span class="nv">$ </span>c1 bpf bandwidth list
<span class="nv">$ </span>c2 bpf bandwidth list
<span class="c"># =&gt; IDENTITY   EGRESS BANDWIDTH (BitsPerSec)</span>
<span class="c">#    1391       5M</span>
<span class="nv">$ </span>kubectl <span class="nb">exec </span>netperf-client <span class="nt">--</span> netperf <span class="nt">-t</span> TCP_MAERTS <span class="nt">-H</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NETPERF_SERVER_IP</span><span class="k">}</span><span class="s2">"</span>
<span class="c"># =&gt; Recv   Send    Send</span>
<span class="c">#    Socket Socket  Message  Elapsed</span>
<span class="c">#    Size   Size    Size     Time     Throughput</span>
<span class="c">#    bytes  bytes   bytes    secs.    10^6bits/sec</span>
<span class="c">#    </span>
<span class="c">#    131072  16384  16384    10.01       4.59</span>
<span class="c"># &lt;span style="color: green;"&gt;👉 4.5Mbps 제한 확인!&lt;/span&gt;</span>

<span class="c"># 20M 제한 설정 후 테스트</span>
<span class="nv">$ </span>kubectl get pod netperf-server <span class="nt">-o</span> json | <span class="nb">sed</span> <span class="nt">-e</span> <span class="s1">'s|5M|20M|g'</span> | kubectl apply <span class="nt">-f</span> -
<span class="c"># =&gt; pod/netperf-server configured</span>
<span class="nv">$ </span>kubectl <span class="nb">exec </span>netperf-client <span class="nt">--</span> netperf <span class="nt">-t</span> TCP_MAERTS <span class="nt">-H</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NETPERF_SERVER_IP</span><span class="k">}</span><span class="s2">"</span>
<span class="c"># =&gt; Recv   Send    Send</span>
<span class="c">#    Socket Socket  Message  Elapsed</span>
<span class="c">#    Size   Size    Size     Time     Throughput</span>
<span class="c">#    bytes  bytes   bytes    secs.    10^6bits/sec</span>
<span class="c">#    131072  16384  16384    10.00      18.40</span>
<span class="c"># &lt;span style="color: green;"&gt;👉 19Mbps 제한 확인!&lt;/span&gt;</span>

<span class="nv">$ </span>tc qdisc show dev enp0s8
<span class="c"># =&gt; qdisc fq 8006: root refcnt 2 limit 10000p flow_limit 100p buckets 32768 orphan_mask 1023 quantum 3028b initial_quantum 15140b low_rate_threshold 550Kbit refill_delay 40ms timer_slack 10us horizon 2s horizon_cap</span>
<span class="c">#    qdisc clsact ffff: parent ffff:fff1</span>

<span class="c"># 삭제</span>
<span class="nv">$ </span>kubectl delete pod netperf-client netperf-server
</code></pre></div></div>

<h4 id="l2-announcements--l2-aware-lb-beta">L2 Announcements / L2 Aware LB (Beta)</h4>

<ul>
  <li>참고 링크 : <a href="https://docs.cilium.io/en/stable/network/l2-announcements/">Link</a> , <a href="https://isovalent.com/blog/post/migrating-from-metallb-to-cilium/">Blog</a></li>
  <li>L2 Announcements는 로컬 영역 네트워크에서 서비스를 표시하고 도달 가능하게 만드는 기능입니다. 
이 기능은 주로 사무실 또는 캠퍼스 네트워크와 같이 BGP 기반 라우팅이 없는 네트워크 내에서 온프레미스 배포를 위해 고안되었습니다.</li>
  <li>이 기능을 사용하면 ExternalIP 및 LoadBalancer IP에 대한 ARP 쿼리에 응답합니다. 
이러한 IP는 여러 노드의 가상 IP(네트워크 장치에 설치되지 않음)이므로 각 서비스에 대해 한 번에 한 노드가 
ARP 쿼리에 응답하고 MAC 주소로 응답합니다.
이 노드는 서비스 로드 밸런싱 기능으로 로드 밸런싱을 수행하여 북쪽/남쪽 로드 밸런서 역할을 합니다.</li>
  <li>NodePort 서비스에 비해 이 기능의 장점은 각 서비스가 고유한 IP를 사용할 수 있으므로 여러 서비스가 동일한 포트 번호를
사용할 수 있다는 것입니다. 
NodePort를 사용할 때 트래픽을 보낼 호스트를 결정하는 것은 클라이언트에게 달려 있으며 노드가 다운되면
IP+Port 콤보를 사용할 수 없게 됩니다. L2 공지를 사용하면 서비스 VIP가 다른 노드로 간단히 마이그레이션되고 계속 작동합니다.</li>
  <li>이를 통해 MetalLB와 같은 외부 로드 밸런서를 Cilium으로 대체할 수 있습니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w8/20241026_kans_w8_29.png" alt="img.png" class="image-center w-80" />
<em class="image-caption"><a href="https://isovalent.com/blog/post/migrating-from-metallb-to-cilium/">https://isovalent.com/blog/post/migrating-from-metallb-to-cilium/</a></em></p>

<h5 id="설정-및-확인-1">설정 및 확인</h5>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#</span>
<span class="nv">$ </span>helm upgrade cilium cilium/cilium <span class="nt">--namespace</span> kube-system <span class="nt">--reuse-values</span> <span class="se">\</span>
  <span class="nt">--set</span> l2announcements.enabled<span class="o">=</span><span class="nb">true</span> <span class="nt">--set</span> externalIPs.enabled<span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
  <span class="nt">--set</span> l2announcements.leaseDuration<span class="o">=</span>3s <span class="nt">--set</span> l2announcements.leaseRenewDeadline<span class="o">=</span>1s <span class="nt">--set</span> l2announcements.leaseRetryPeriod<span class="o">=</span>200ms
<span class="c"># =&gt; Release &amp;quot;cilium&amp;quot; has been upgraded. Happy Helming!</span>
<span class="c">#    ...</span>
 
<span class="c">#</span>
<span class="nv">$ </span>c0 config <span class="nt">--all</span> | <span class="nb">grep </span>L2
<span class="c"># =&gt; EnableL2Announcements             : true</span>
<span class="c">#    EnableL2NeighDiscovery            : true</span>

<span class="c"># CiliumL2AnnouncementPolicy 생성</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl apply -f - 
apiVersion: "cilium.io/v2alpha1"
kind: CiliumL2AnnouncementPolicy
metadata:
  name: policy1
spec:
  serviceSelector:
    matchLabels:
      color: blue
  nodeSelector:
    matchExpressions:
      - key: node-role.kubernetes.io/control-plane
        operator: DoesNotExist
  interfaces:
  - ^enp0s[0-9]+
  externalIPs: true
  loadBalancerIPs: true
</span><span class="no">EOF
</span><span class="c"># =&gt; ciliuml2announcementpolicy.cilium.io/policy1 created</span>

<span class="c"># 확인</span>
<span class="nv">$ </span>kubectl get ciliuml2announcementpolicy
<span class="c"># =&gt; NAME      AGE</span>
<span class="c">#    policy1   7s</span>
<span class="nv">$ </span>kubectl describe l2announcement

<span class="c">#</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl apply -f - 
apiVersion: "cilium.io/v2alpha1"
kind: CiliumLoadBalancerIPPool
metadata:
  name: "cilium-pool"
spec:
  allowFirstLastIPs: "No"
  blocks:
  - cidr: "10.10.200.0/29"
</span><span class="no">EOF
</span><span class="c"># =&gt; ciliumloadbalancerippool.cilium.io/cilium-pool created</span>

<span class="c"># cilium ip pool 조회</span>
<span class="nv">$ </span>kubectl get CiliumLoadBalancerIPPool
<span class="c"># =&gt; NAME          DISABLED   CONFLICTING   IPS AVAILABLE   AGE</span>
<span class="c">#    cilium-pool   false      False         6               9s</span>
</code></pre></div></div>

<h5 id="테스트용-파드-서비스-생성">테스트용 파드, 서비스 생성</h5>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: webpod1
  labels:
    app: webpod
spec:
  nodeName: k3s-w1
  containers:
  - name: container
    image: traefik/whoami
  terminationGracePeriodSeconds: 0
---
apiVersion: v1
kind: Pod
metadata:
  name: webpod2
  labels:
    app: webpod
spec:
  nodeName: k3s-w2
  containers:
  - name: container
    image: traefik/whoami
  terminationGracePeriodSeconds: 0
---
apiVersion: v1
kind: Service
metadata:
  name: svc1
spec:
  ports:
    - name: svc1-webport
      port: 80
      targetPort: 80
  selector:
    app: webpod
  type: LoadBalancer  # 👉 서비스 타입이 LoadBalancer
---
apiVersion: v1
kind: Service
metadata:
  name: svc2
spec:
  ports:
    - name: svc2-webport
      port: 80
      targetPort: 80
  selector:
    app: webpod
  type: LoadBalancer  # 👉 서비스 타입이 LoadBalancer
---
apiVersion: v1
kind: Service
metadata:
  name: svc3
spec:
  ports:
    - name: svc3-webport
      port: 80
      targetPort: 80
  selector:
    app: webpod
  type: LoadBalancer  # 👉 서비스 타입이 LoadBalancer
</span><span class="no">EOF
</span><span class="c"># =&gt; pod/webpod1 created</span>
<span class="c">#    pod/webpod2 created</span>
<span class="c">#    service/svc1 created</span>
<span class="c">#    service/svc2 created</span>
<span class="c">#    service/svc3 created</span>
</code></pre></div></div>

<h5 id="접속-확인">접속 확인</h5>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#</span>
<span class="nv">$ </span>kubectl get svc,ep
<span class="c"># =&gt; NAME                 TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE</span>
<span class="c">#    service/kubernetes   ClusterIP      10.10.200.1     &amp;lt;none&amp;gt;        443/TCP        10h</span>
<span class="c">#    service/svc1         LoadBalancer   10.10.200.214   10.10.200.1   80:30456/TCP   98s</span>
<span class="c">#    service/svc2         LoadBalancer   10.10.200.240   10.10.200.2   80:30367/TCP   98s</span>
<span class="c">#    service/svc3         LoadBalancer   10.10.200.155   10.10.200.3   80:31800/TCP   98s</span>
<span class="c">#    </span>
<span class="c">#    NAME                   ENDPOINTS                        AGE</span>
<span class="c">#    endpoints/kubernetes   192.168.10.10:6443               10h</span>
<span class="c">#    endpoints/svc1         172.16.1.32:80,172.16.2.115:80   98s</span>
<span class="c">#    endpoints/svc2         172.16.1.32:80,172.16.2.115:80   98s</span>
<span class="c">#    endpoints/svc3         172.16.1.32:80,172.16.2.115:80   98s</span>

<span class="c">#</span>
<span class="nv">$ </span>curl <span class="nt">-s</span> 10.10.200.1
<span class="c"># =&gt; Hostname: webpod1</span>
<span class="c">#    IP: 127.0.0.1</span>
<span class="c">#    IP: 172.16.1.32</span>
<span class="c">#    RemoteAddr: 192.168.10.10:58036</span>
<span class="c">#    GET / HTTP/1.1</span>
<span class="c">#    Host: 10.10.200.1</span>
<span class="c">#    User-Agent: curl/7.81.0</span>
<span class="c">#    Accept: */*</span>
<span class="nv">$ </span>curl <span class="nt">-s</span> 10.10.200.2
<span class="c"># =&gt; Hostname: webpod2</span>
<span class="c">#    IP: 127.0.0.1</span>
<span class="c">#    IP: 172.16.2.115</span>
<span class="c">#    RemoteAddr: 192.168.10.10:53496</span>
<span class="c">#    GET / HTTP/1.1</span>
<span class="c">#    Host: 10.10.200.2</span>
<span class="c">#    User-Agent: curl/7.81.0</span>
<span class="c">#    Accept: */*</span>
<span class="nv">$ </span>curl <span class="nt">-s</span> 10.10.200.3
<span class="c"># =&gt; Hostname: webpod1</span>
<span class="c">#    IP: 127.0.0.1</span>
<span class="c">#    IP: 172.16.1.32</span>
<span class="c">#    RemoteAddr: 192.168.10.10:54098</span>
<span class="c">#    GET / HTTP/1.1</span>
<span class="c">#    Host: 10.10.200.3</span>
<span class="c">#    User-Agent: curl/7.81.0</span>
<span class="c">#    Accept: */*</span>
</code></pre></div></div>

<p>이상과 같이 Cilium 만으로 Loadbalancer유형의 Service를 구성할 수 있었습니다.</p>

<hr />

<h2 id="마치며">마치며</h2>

<p>이번 글에서는 Cilium CNI의 다양한 기능들을 살펴보았습니다.
Cilium CNI은 알면 알수록 “끝판왕”, “It’s magic!”이라는 말이 계속 떠올랐습니다.
아래의 말과 참 맞닿는것 같습니다.</p>

<blockquote>
  <p>“충분히 발달한 과학 기술은 마법과 구별할 수 없다” - 아서 클라크</p>
</blockquote>

<p>스터디 중에도 가시다 님이 “됩니다”를 연발하셔서 “다 되는 페이” KB 페이 광고가 생각났습니다. 
정말 다 되는 CNI인것 같습니다.
물론 eBPF나 Envoy와 같은 기술들이 있었기에 Cilium이 가능한 것이지만, 참 대단합니다.
이번에 실습한것 외에도 더 다양한 기능들이 있어서, 더욱더 공부해야겠다는 생각이 듭니다.</p>

<p>어느덧 다음주가 마지막 주차입니다. 이런 저런 일들로 스터디 포기할까 하는 때도 있었는데 어찌저찌 잘 버텼습니다. 
다음주에는 좀 더 일찍 과제를 제출할 수 있기를 바라며
이만 포스팅을 마치겠습니다.</p>]]></content><author><name></name></author><category term="kans" /><category term="kubernetes," /><category term="network," /><category term="linux" /><summary type="html"><![CDATA[이번주에는 드디어 CNI의 끝판 왕(제가 아는 한에서)인 Cilium에 대해 알아보겠습니다.]]></summary></entry><entry><title type="html">[KANS 3기] Service Mesh : Istio - Mode (Sidecar, Ambient)</title><link href="https://sweetlittlebird.github.io/posts/2024-10-19-KANS-Study-Week7/" rel="alternate" type="text/html" title="[KANS 3기] Service Mesh : Istio - Mode (Sidecar, Ambient)" /><published>2024-10-19T01:00:18+09:00</published><updated>2024-10-19T01:00:18+09:00</updated><id>https://sweetlittlebird.github.io/posts/KANS%20Study%20-%20Week7</id><content type="html" xml:base="https://sweetlittlebird.github.io/posts/2024-10-19-KANS-Study-Week7/"><![CDATA[<h2 id="들어가며">들어가며</h2>

<p>이번주에는 Service Mesh와 대표적인 오픈소스 프로젝트인 Istio에 대해 알아 보겠습니다.
KANS 3기 7주차 스터디를 시작하겠습니다.</p>

<hr />

<h2 id="istio-소개">Istio 소개</h2>

<p>Istio는 서비스 메시를 구축하고 관리하기 위한 오픈소스 플랫폼입니다. Istio를 알아보기에 앞서 서비스 메시에 대해 알아보겠습니다.</p>

<h3 id="서비스-메시-service-mesh란">서비스 메시 (Service Mesh)란?</h3>

<ul>
  <li><strong>서비스 메시</strong>는 서비스 간 <strong>통신을 제어</strong>하고 <strong>모니터링</strong>하는 레이어를 제공하는 인프라스트럭처 계층입니다.</li>
  <li><strong>등장배경</strong> : MSA 환경에서 서비스가 많아지다 보니 서비스 간 통신이 복잡해지고, 이로 인해 서비스 간 통신을 관리하고 모니터링하는 것이 어려워졌습니다.
이로인해 장애가 발생하거나 병목 현상이 발생했을때 원인과 발생하는 구간을 찾기가 어려워졌습니다. 이것을 해결 하기 위해 등장한 것이 서비스 메시입니다.
<img src="/assets/2024/kans-3th/w7/20241019_kans_w7_1.png" alt="img.png" class="image-center w-80" /></li>
  <li><strong>개념</strong> : 마이크로 서비스 간에 통신이나 경로를 제어 - 예) istio, linkerd, consul, envoy, …</li>
  <li><strong>기본 동작</strong> : 파드간 통신경로에 프록시를 두고 트래픽을 모니터링하거나 컨트롤 합니다. 따라서 기존 어플리케이션을 수정하지 않고도 적용할 수 있습니다.
<img src="/assets/2024/kans-3th/w7/20241019_kans_w7_2.png" alt="img.png" class="image-center w-80" />
위의 그림 처럼 서비스 메시는 각 파드에 프록시를 두고 프록시를 통해 통신을 하도록한 다음 프록시를 통해 트래픽을 모니터링하거나 컨트롤 합니다.
    <ul>
      <li>이때 프록시는 <strong>Sidecar</strong> 모드로 동작하거나 <strong>Ambient</strong> 모드로 동작하며 대표적인 프록시로는 Envoys가 있습니다.</li>
      <li>Envoy는 구글, IBM, Lyft가 중심이 되어 개발하고 있는 오픈소스 프록시입니다.</li>
      <li>네트워크 투명성을 목표로 다양한 필터 체인 지원(L3, L4, L7), 동적 Configuration API를 제공하고, hot reload를 지원합니다.</li>
    </ul>
  </li>
  <li><strong>주요기능</strong>
    <ul>
      <li><strong>트래픽 모니터링</strong> : 요청의 에러율, 지연시간, 컨넥션 개수, 요청개수 등의 메트릭을 수집하여 모니터링하고, 서비스간 혹은 특정 요청 경로를 필터링 할 수 있습니다.
=&gt; 원인 파악 용이</li>
      <li><strong>트래픽 컨트롤</strong>
        <ul>
          <li>트래픽 시프팅(traffic shifting) : 트래픽을 서비스간에 분산시키는 기능으로, 특정 단말/사용자는 신규 어플리케이션에 연결하도록 하는 카나리 배포등에 활용할 수도 있습니다.</li>
          <li>서킷 브레이커(circuit breaker) : 특정 서비스에 문제가 있을때 접속을 차단하고, 출발지 서비스에 에러를 반환하도록 하는 기능입니다. (연쇄장애, 시스템 전체 장애 방지)</li>
          <li>플트 인젝션(fault injection) : 의도적으로 요청을 지연시키거나 실패하도록 할 수 있습니다. (비정상 상황 테스트)</li>
          <li>속도 제한(rate limiting) : 특정 서비스에 대한 요청 개수를 제한하는 기능입니다.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="envoy">Envoy</h3>

<ul>
  <li>지난주에 살짝 언급되었던 내용인데 이번 주에 좀 더 자세히 알아보겠습니다.</li>
  <li>Envoy는 구글, IBM, Lyft가 중심이 되어 개발하고 있는 오픈소스 프록시입니다.</li>
  <li>Istio의 핵심 기능들은 Envoy를 감싼 istio proxy를 통해 이루어지므로 Envoy에 대한 이해가 필요합니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_4.svg" alt="20241019_kans_w7_4.svg" class="image-center w-80" /></p>

<ul>
  <li>Envoy에서 사용하는 용어 들을 정리해 보았습니다.</li>
</ul>

<table>
  <thead>
    <tr>
      <th>용어</th>
      <th>설명</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Cluster</td>
      <td>envoy가 트래픽을 포워딩할 수 있는 논리적 서비스 (엔드포인트 셋트)</td>
    </tr>
    <tr>
      <td>Endpoint</td>
      <td>IP 주소와 포트 번호로 구성된 서비스의 실제 인스턴스. 엔드포인트가 모여서 하나의 Cluster를 이룸</td>
    </tr>
    <tr>
      <td>Listener</td>
      <td>클라이언트가 접속하는 포트, 유닉스 도메인 소켓 등을 노출하고, 다운스트림으로 부터 받은 요청을 처리</td>
    </tr>
    <tr>
      <td>Route</td>
      <td>Listener로 들어온 요청을 어떤 클러스터로 보낼지 정의</td>
    </tr>
    <tr>
      <td>Filter</td>
      <td>Listener로 부터 서비스에 트래픽 전달하기 전에 트래픽을 가공하거나 차단하는 역할을 하는 컴포넌트</td>
    </tr>
    <tr>
      <td>UpStream</td>
      <td>envoy 요청을 포워딩해서 연결하는 백엔드 네트워크 노드 - 사이드카일때는 application app, 아닐때는 원격 백엔드</td>
    </tr>
    <tr>
      <td>DownStream</td>
      <td>envoy로 연결하여 요청을 보내는 개체. 사이드카가 아닐때는 원격지의 클라이언트</td>
    </tr>
    <tr>
      <td>Host</td>
      <td>네트워크 통신이 가능한 개체 (PC, 서버, 휴대폰, 네트워크 어플리케이션 등)</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>많은 Service Mesh 솔루션이나, Gateway API 구현체들이 내부적으로 Envoy를 사용하고 있으며, Envoy가 제공하는 동적 구성을 위한 API(xDS Sync API)를 
이용하여 다양한 네트워크 정책을 구성하게 됩니다.</li>
  <li>Envoy의 xDS Sync API는 아래와 같은 레이어에서 동작합니다.
    <ul>
      <li>LDS - Listener Discovery Service</li>
      <li>RDS - Route Discovery Service</li>
      <li>CDS - Cluster Discovery Service</li>
      <li>EDS - Endpoint Discovery Service</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_5.png" alt="img.png" class="image-center w-80" /></p>

<p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_6.png" alt="img.png" class="image-center w-80" /></p>

<h4 id="envoy-실습">Envoy 실습</h4>

<ul>
  <li>test pc에 Envoy 설치</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 설치</span>
<span class="c"># echo "deb [signed-by=/etc/apt/keyrings/envoy-keyring.gpg] https://apt.envoyproxy.io focal main" | sudo tee /etc/apt/sources.list.d/envoy.list</span>
<span class="nv">$ </span>wget <span class="nt">-O-</span> https://apt.envoyproxy.io/signing.key | <span class="nb">sudo </span>gpg <span class="nt">--dearmor</span> <span class="nt">-o</span> /etc/apt/keyrings/envoy-keyring.gpg
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"deb [signed-by=/etc/apt/keyrings/envoy-keyring.gpg] https://apt.envoyproxy.io jammy main"</span> | <span class="nb">sudo tee</span> /etc/apt/sources.list.d/envoy.list
<span class="nv">$ </span><span class="nb">sudo </span>apt-get update <span class="o">&amp;&amp;</span> <span class="nb">sudo </span>apt-get <span class="nb">install </span>envoy <span class="nt">-y</span>

<span class="c"># 확인</span>
<span class="nv">$ </span>envoy <span class="nt">--version</span>
<span class="c"># =&gt; envoy  version: e3b4a6e9570da15ac1caffdded17a8bebdc7dfc9/1.32.0/Clean/RELEASE/BoringSSL</span>

<span class="c"># 도움말</span>
<span class="nv">$ </span>envoy <span class="nt">--help</span>
</code></pre></div></div>

<ul>
  <li>Envoy proxy 실습 - <a href="https://www.envoyproxy.io/docs/envoy/latest/start/quick-start/">Link</a>
    <ul>
      <li>envoy-demo.yml 작성
        <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># envoy-demo.yml</span>
<span class="na">static_resources</span><span class="pi">:</span>
    
  <span class="na">listeners</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">listener_0</span>
    <span class="na">address</span><span class="pi">:</span>
      <span class="na">socket_address</span><span class="pi">:</span>
        <span class="na">address</span><span class="pi">:</span> <span class="s">0.0.0.0</span>
        <span class="na">port_value</span><span class="pi">:</span> <span class="m">10000</span>
    <span class="na">filter_chains</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">filters</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">envoy.filters.network.http_connection_manager</span>
        <span class="na">typed_config</span><span class="pi">:</span>
          <span class="s2">"</span><span class="s">@type"</span><span class="err">:</span> <span class="s">type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager</span>
          <span class="s">stat_prefix</span><span class="err">:</span> <span class="s">ingress_http</span>
          <span class="s">access_log</span><span class="err">:</span>
          <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">envoy.access_loggers.stdout</span>
            <span class="na">typed_config</span><span class="pi">:</span>
              <span class="s2">"</span><span class="s">@type"</span><span class="err">:</span> <span class="s">type.googleapis.com/envoy.extensions.access_loggers.stream.v3.StdoutAccessLog</span>
          <span class="na">http_filters</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">envoy.filters.http.router</span>
          <span class="na">route_config</span><span class="pi">:</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">local_route</span>
            <span class="na">virtual_hosts</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">local_service</span>
              <span class="na">domains</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">*"</span><span class="pi">]</span>
              <span class="na">routes</span><span class="pi">:</span>
              <span class="pi">-</span> <span class="na">match</span><span class="pi">:</span>
                  <span class="na">prefix</span><span class="pi">:</span> <span class="s2">"</span><span class="s">/"</span>
                <span class="na">route</span><span class="pi">:</span>
                  <span class="na">host_rewrite_literal</span><span class="pi">:</span> <span class="s">www.envoyproxy.io</span>
                  <span class="na">cluster</span><span class="pi">:</span> <span class="s">service_envoyproxy_io</span>
    
  <span class="na">clusters</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">service_envoyproxy_io</span>
    <span class="na">type</span><span class="pi">:</span> <span class="s">LOGICAL_DNS</span>
    <span class="c1"># Comment out the following line to test on v6 networks</span>
    <span class="na">dns_lookup_family</span><span class="pi">:</span> <span class="s">V4_ONLY</span>
    <span class="na">connect_timeout</span><span class="pi">:</span> <span class="s">5s</span>
    <span class="na">load_assignment</span><span class="pi">:</span>
      <span class="na">cluster_name</span><span class="pi">:</span> <span class="s">service_envoyproxy_io</span>
      <span class="na">endpoints</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">lb_endpoints</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">endpoint</span><span class="pi">:</span>
            <span class="na">address</span><span class="pi">:</span>
              <span class="na">socket_address</span><span class="pi">:</span>
                <span class="na">address</span><span class="pi">:</span> <span class="s">www.envoyproxy.io</span>
                <span class="na">port_value</span><span class="pi">:</span> <span class="m">443</span>
    <span class="na">transport_socket</span><span class="pi">:</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">envoy.transport_sockets.tls</span>
      <span class="na">typed_config</span><span class="pi">:</span>
        <span class="s2">"</span><span class="s">@type"</span><span class="err">:</span> <span class="s">type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext</span>
        <span class="s">sni</span><span class="err">:</span> <span class="s">www.envoyproxy.io</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>실행</p>

        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c"># (터미널1) 데모 config 적용하여 실행</span>
  <span class="nv">$ </span>curl <span class="nt">-O</span> https://www.envoyproxy.io/docs/envoy/latest/_downloads/92dcb9714fb6bc288d042029b34c0de4/envoy-demo.yaml
  <span class="nv">$ </span>envoy <span class="nt">-c</span> envoy-demo.yaml
  <span class="c"># =&gt; [2024-10-01 16:41:51.547][4479][info][main] [source/server/server.cc:426] initializing epoch 0 (base id=0, hot restart version=11.104)</span>
  <span class="c">#    ...</span>
    
  <span class="c"># (터미널2) 정보 확인</span>
  <span class="nv">$ </span>ss <span class="nt">-tnlp</span>
  <span class="c"># =&gt; State           Recv-Q           Send-Q                     Local Address:Port                      Peer Address:Port          Process</span>
  <span class="c">#    LISTEN          0                4096                             0.0.0.0:10000                          0.0.0.0:*              users:((&amp;quot;envoy&amp;quot;,pid=4479,fd=35))</span>
  <span class="c">#    LISTEN          0                4096                             0.0.0.0:10000                          0.0.0.0:*              users:((&amp;quot;envoy&amp;quot;,pid=4479,fd=34))</span>
  <span class="c">#    LISTEN          0                4096                             0.0.0.0:10000                          0.0.0.0:*              users:((&amp;quot;envoy&amp;quot;,pid=4479,fd=33))</span>
  <span class="c">#    LISTEN          0                4096                             0.0.0.0:10000                          0.0.0.0:*              users:((&amp;quot;envoy&amp;quot;,pid=4479,fd=32))</span>
  <span class="c">#    ...</span>
    
  <span class="c"># 접속 테스트</span>
  <span class="nv">$ </span>curl <span class="nt">-s</span> http://127.0.0.1:10000 | <span class="nb">grep</span> <span class="nt">-o</span> <span class="s2">"&lt;title&gt;.*&lt;/title&gt;"</span>
  <span class="c"># =&gt; &amp;lt;title&amp;gt;Envoy proxy - home&amp;lt;/title&amp;gt;</span>
    
  <span class="c"># 외부 접속 정보 출력</span>
  <span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"http://</span><span class="si">$(</span>curl <span class="nt">-s</span> ipinfo.io/ip<span class="si">)</span><span class="s2">:10000"</span>
  <span class="c"># =&gt; http://54.123.42.212:10000</span>
    
  <span class="nt">--------------------</span>
  <span class="c"># 자신의 PC 웹브라우저에서 외부 접속 정보 접속 확인!</span>
    
  <span class="c"># k3s-m 에서 접속 테스트</span>
  <span class="nv">$ </span>curl <span class="nt">-s</span> http://192.168.56.104:10000 | <span class="nb">grep</span> <span class="nt">-o</span> <span class="s2">"&lt;title&gt;.*&lt;/title&gt;"</span>
  <span class="c"># =&gt; &amp;lt;title&amp;gt;Envoy proxy - home&amp;lt;/title&amp;gt; </span>
  <span class="nt">--------------------</span>
    
  <span class="c"># 연결 정보 확인</span>
  <span class="nv">$ </span>ss <span class="nt">-tnp</span>
    
  <span class="c"># (터미널1) envoy 실행 취소(CTRL+C) 후 (관리자페이지) 설정 덮어쓰기 - 링크</span>
  <span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOT</span><span class="sh">&gt; envoy-override.yaml
  admin:
    address:
      socket_address:
        address: 0.0.0.0
        port_value: 9902
</span><span class="no">  EOT
</span>  <span class="nv">$ </span>envoy <span class="nt">-c</span> envoy-demo.yaml <span class="nt">--config-yaml</span> <span class="s2">"</span><span class="si">$(</span><span class="nb">cat </span>envoy-override.yaml<span class="si">)</span><span class="s2">"</span>
    
  <span class="c"># envoy 관리페이지 외부 접속 정보 출력</span>
  <span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"http://</span><span class="si">$(</span>curl <span class="nt">-s</span> ipinfo.io/ip<span class="si">)</span><span class="s2">:9902"</span>
  <span class="c"># =&gt; http://54.123.42.212:9902</span>
    
  <span class="nt">--------------------</span>
  <span class="c"># 자신의 PC 웹브라우저에서 관리 페이지 외부 접속 정보 접속 확인!</span>
</code></pre></div>        </div>

        <p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_7.png" alt="img.png" class="image-center" />
  <em class="image-caption">PC에서 접속한 화면</em></p>
      </li>
    </ul>
  </li>
</ul>

<h3 id="istio-소개-1">Istio 소개</h3>

<ul>
  <li><strong>Istio</strong>는 서비스 메시를 구축하고 관리하기 위한 오픈소스 플랫폼입니다.</li>
  <li><strong>Istio의 구성</strong>
<img src="/assets/2024/kans-3th/w7/20241019_kans_w7_3.svg" alt="20241019_kans_w7_3.svg" class="image-center w-80" />
    <ul>
      <li>파일럿(Pilot) : 모든 Envoy 사이드카에서 프록시 라우팅 규칙을 관리하며, 서비스 디스커버리, 로드밸런싱 설정을 제공합니다.</li>
      <li>겔리(Galley) : Istio와 쿠버네티스를 연결하는 역할을 합니다. 서비스 메시 구성 데이터를 검증하고 변환합니다.</li>
      <li>시타델(Citadel) : 서비스 간의 인증과 보안을 관리합니다. 서비스 간의 TLS 통신을 제공하고, 서비스 간의 인증을 관리합니다.</li>
    </ul>
  </li>
  <li>Istio의 구성요소
    <ul>
      <li>istiod : Istio의 중앙 제어 플레인으로, Pilot, Citadel, Galley를 포함합니다.</li>
      <li>istio proxy : Envoy 기반의 프록시로, istiod와 통신하며, 서비스 트래픽을 통제하고 옵저빌리티를 위한 메트릭을 제공합니다.</li>
    </ul>
  </li>
  <li>특징
    <ul>
      <li>Istio는 각 파드안에서 사이드카로 동작하는 Envoy가 트래픽을 제어하고 모니터링합니다.</li>
      <li>모든 마이크로 서비스간 통신은 Envoy를 통해 이루어지며, 이를 통해 메트릭을 수집하거나 컨트롤 할 수 있습니다.</li>
      <li>트래픽을 컨트롤 하기 위해서 Envoy 프록시에 전송룰을 정의 합니다.</li>
      <li>마이크로 서비스간의 통신을 mutual TLS 인증(mTLS)을 통해 보안합니다.</li>
      <li>각 어플리케이션은 파드 내의 엔보이 프록시에 접속하기 위해 localhost에 TCP 접속을 합니다.</li>
    </ul>
  </li>
</ul>

<h3 id="istio-설치-sidecar-모드">Istio 설치 (Sidecar 모드)</h3>

<ul>
  <li>Istio 공식 문서 : <a href="https://istio.io/latest/docs/">Link</a>
    <ul>
      <li>Istio Sidecar mode 설치 : v1.23.2 - 
<a href="https://istio.io/latest/docs/releases/supported-releases/#support-status-of-istio-releases">버전</a>
<a href="https://istio.io/latest/docs/setup/getting-started/">설치</a>,</li>
      <li>without GwApi - <a href="https://istio.io/latest/docs/setup/additional-setup/getting-started-istio-apis/">Docs</a></li>
      <li>Operator 방식 설치 : https://istio.io/latest/docs/setup/install/operator/ (Istio Operator는 <strong>Deprecated</strong> 되었습니다.)</li>
    </ul>
  </li>
  <li>Istio 설치
```bash</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># istioctl 설치</span>
<span class="nv">$ </span><span class="nb">export </span><span class="nv">ISTIOV</span><span class="o">=</span>1.23.2
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"export ISTIOV=1.23.2"</span> <span class="o">&gt;&gt;</span> /etc/profile
<span class="nv">$ </span>curl <span class="nt">-s</span> <span class="nt">-L</span> https://istio.io/downloadIstio | <span class="nv">ISTIO_VERSION</span><span class="o">=</span><span class="nv">$ISTIOV</span> <span class="nv">TARGET_ARCH</span><span class="o">=</span>x86_64 sh -
<span class="nv">$ </span>tree istio-<span class="nv">$ISTIOV</span> <span class="nt">-L</span> 2 <span class="c"># sample yaml 포함</span>
<span class="c"># =&gt; istio-1.23.2</span>
<span class="c">#    ├── LICENSE</span>
<span class="c">#    ├── README.md</span>
<span class="c">#    ├── bin</span>
<span class="c">#    │   └── istioctl</span>
<span class="c">#    ├── manifest.yaml</span>
<span class="c">#    ├── manifests</span>
<span class="c">#    │   ├── charts</span>
<span class="c">#    │   └── profiles</span>
<span class="c">#    ├── samples</span>
<span class="c">#    │   ...</span>
<span class="c">#    └── tools</span>
<span class="c">#        ├── _istioctl</span>
<span class="c">#        ├── certs</span>
<span class="c">#        └── istioctl.bash</span>
<span class="nv">$ </span><span class="nb">cp </span>istio-<span class="nv">$ISTIOV</span>/bin/istioctl /usr/local/bin/istioctl
<span class="nv">$ </span>istioctl version <span class="nt">--remote</span><span class="o">=</span><span class="nb">false</span>
<span class="c"># =&gt; client version: 1.23.2</span>

<span class="c"># (demo 프로파일) 컨트롤 플레인 배포 - 링크 Customizing</span>
<span class="c"># The istioctl command supports the full IstioOperator API via command-line options for individual settings or for passing a yaml file containing an IstioOperator custom resource (CR).</span>
<span class="nv">$ </span>istioctl profile list
<span class="c"># =&gt; Istio configuration profiles:</span>
<span class="c">#        ambient</span>
<span class="c">#        default</span>
<span class="c">#        demo</span>
<span class="c">#        empty</span>
<span class="c">#        minimal</span>
<span class="c">#        openshift</span>
<span class="c">#        openshift-ambient</span>
<span class="c">#        preview</span>
<span class="c">#        remote</span>
<span class="c">#        stable</span>
<span class="nv">$ </span>istioctl profile dump default
<span class="nv">$ </span>istioctl profile dump <span class="nt">--config-path</span> components.ingressGateways
<span class="c"># =&gt; - enabled: true</span>
<span class="c">#      name: istio-ingressgateway</span>
<span class="nv">$ </span>istioctl profile dump <span class="nt">--config-path</span> values.gateways.istio-ingressgateway
<span class="c"># =&gt; {}</span>
<span class="nv">$ </span>istioctl profile dump demo
<span class="c"># =&gt; apiVersion: install.istio.io/v1alpha1</span>
<span class="c">#    kind: IstioOperator</span>
<span class="c">#    spec:</span>
<span class="c">#      components:</span>
<span class="c">#        base:</span>
<span class="c">#          enabled: true</span>
<span class="c">#        egressGateways:</span>
<span class="c">#        - enabled: true</span>
<span class="c">#          name: istio-egressgateway</span>
<span class="c">#        ingressGateways:</span>
<span class="c">#        - enabled: true</span>
<span class="c">#          name: istio-ingressgateway</span>
<span class="c">#        pilot:</span>
<span class="c">#          enabled: true</span>
<span class="c">#      hub: docker.io/istio</span>
<span class="c">#      profile: demo</span>
<span class="c">#      tag: 1.23.2</span>
<span class="c">#      values:</span>
<span class="c">#        defaultRevision: &amp;quot;&amp;quot;</span>
<span class="c">#        gateways:</span>
<span class="c">#          istio-egressgateway: {}</span>
<span class="c">#          istio-ingressgateway: {}</span>
<span class="c">#        global:</span>
<span class="c">#          configValidation: true</span>
<span class="c">#          istioNamespace: istio-system</span>
<span class="c">#        profile: demo</span>

<span class="nv">$ </span>istioctl profile dump demo <span class="o">&gt;</span> demo-profile.yaml
<span class="nv">$ </span>vi demo-profile.yaml <span class="c"># 복잡성을 줄이게 실습 시나리오 환경 맞춤</span>
<span class="nt">--------------------</span>
    egressGateways:
    - enabled: <span class="nb">false</span>
<span class="nt">--------------------</span>    

<span class="nv">$ </span>istioctl <span class="nb">install</span> <span class="nt">-f</span> demo-profile.yaml <span class="nt">-y</span>
<span class="c"># =&gt; ✔ Istio core installed ⛵️</span>
<span class="c">#    ✔ Istiod installed 🧠</span>
<span class="c">#    ✔ Ingress gateways installed 🛬</span>
<span class="c">#    ✔ Installation complete</span>
<span class="c">#    Made this installation the default for cluster-wide operations.</span>

<span class="c"># 설치 확인 : istiod, istio-ingressgateway</span>
<span class="nv">$ </span>kubectl get all,svc,ep,sa,cm,secret,pdb <span class="nt">-n</span> istio-system
<span class="c"># =&gt; NAME                                        READY   STATUS    RESTARTS      AGE</span>
<span class="c">#    pod/istio-ingressgateway-5f9f654d46-l7mqp   1/1     Running   0             14m</span>
<span class="c">#    pod/istiod-7f8b586864-8mc4c                 1/1     Running   1 (82s ago)   14m</span>
<span class="c">#    </span>
<span class="c">#    NAME                           TYPE           CLUSTER-IP      EXTERNAL-IP                                    PORT(S)                                                                      AGE</span>
<span class="c">#    service/istio-ingressgateway   LoadBalancer   10.10.200.171   192.168.10.101,192.168.10.102,192.168.10.103   15021:30953/TCP,80:31677/TCP,443:30737/TCP,31400:30617/TCP,15443:31668/TCP   14m</span>
<span class="c">#    service/istiod                 ClusterIP      10.10.200.215   &amp;lt;none&amp;gt;                                         15010/TCP,15012/TCP,443/TCP,15014/TCP                                        14m</span>
<span class="c">#    </span>
<span class="c">#    NAME                                   READY   UP-TO-DATE   AVAILABLE   AGE</span>
<span class="c">#    deployment.apps/istio-ingressgateway   1/1     1            1           14m</span>
<span class="c">#    deployment.apps/istiod                 1/1     1            1           14m</span>
<span class="c">#    </span>
<span class="c">#    NAME                                              DESIRED   CURRENT   READY   AGE</span>
<span class="c">#    replicaset.apps/istio-ingressgateway-5f9f654d46   1         1         1       14m</span>
<span class="c">#    replicaset.apps/istiod-7f8b586864                 1         1         1       14m</span>
<span class="c">#    </span>
<span class="c">#    NAME                             ENDPOINTS                                                           AGE</span>
<span class="c">#    endpoints/istio-ingressgateway   172.16.2.14:15443,172.16.2.14:15021,172.16.2.14:31400 + 2 more...   14m</span>
<span class="c">#    endpoints/istiod                 172.16.3.16:15012,172.16.3.16:15010,172.16.3.16:15017 + 1 more...   14m</span>
<span class="c">#    </span>
<span class="c">#    NAME                                                  SECRETS   AGE</span>
<span class="c">#    serviceaccount/istio-ingressgateway-service-account   0         14m</span>
<span class="c">#    serviceaccount/istio-reader-service-account           0         14m</span>
<span class="c">#    serviceaccount/istiod                                 0         14m</span>
<span class="c">#    ...</span>
<span class="c">#    </span>
<span class="c">#    NAME                                            DATA   AGE</span>
<span class="c">#    configmap/istio                                 2      14m</span>
<span class="c">#    configmap/istio-sidecar-injector                2      14m</span>
<span class="c">#    ...</span>
<span class="c">#    </span>
<span class="c">#    NAME                     TYPE               DATA   AGE</span>
<span class="c">#    secret/istio-ca-secret   istio.io/ca-root   5      14m</span>
<span class="c">#    </span>
<span class="c">#    NAME                                              MIN AVAILABLE   MAX UNAVAILABLE   ALLOWED DISRUPTIONS   AGE</span>
<span class="c">#    poddisruptionbudget.policy/istio-ingressgateway   1               N/A               0                     14m</span>
<span class="c">#    poddisruptionbudget.policy/istiod                 1               N/A               0                     14m</span>
<span class="nv">$ </span>kubectl get crd | <span class="nb">grep </span>istio.io | <span class="nb">sort</span>
<span class="c"># =&gt; authorizationpolicies.security.istio.io      2024-10-01T05:26:47Z</span>
<span class="c">#    destinationrules.networking.istio.io         2024-10-01T05:26:47Z</span>
<span class="c">#    envoyfilters.networking.istio.io             2024-10-01T05:26:47Z</span>
<span class="c">#    gateways.networking.istio.io                 2024-10-01T05:26:47Z</span>
<span class="c">#    peerauthentications.security.istio.io        2024-10-01T05:26:47Z</span>
<span class="c">#    proxyconfigs.networking.istio.io             2024-10-01T05:26:47Z</span>
<span class="c">#    requestauthentications.security.istio.io     2024-10-01T05:26:47Z</span>
<span class="c">#    serviceentries.networking.istio.io           2024-10-01T05:26:47Z</span>
<span class="c">#    sidecars.networking.istio.io                 2024-10-01T05:26:47Z</span>
<span class="c">#    telemetries.telemetry.istio.io               2024-10-01T05:26:48Z</span>
<span class="c">#    virtualservices.networking.istio.io          2024-10-01T05:26:48Z</span>
<span class="c">#    wasmplugins.extensions.istio.io              2024-10-01T05:26:48Z</span>
<span class="c">#    workloadentries.networking.istio.io          2024-10-01T05:26:48Z</span>
<span class="c">#    workloadgroups.networking.istio.io           2024-10-01T05:26:48Z</span>

<span class="c"># istio-ingressgateway 의 envoy 버전 확인</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deploy/istio-ingressgateway <span class="nt">-n</span> istio-system <span class="nt">-c</span> istio-proxy <span class="nt">--</span> envoy <span class="nt">--version</span>
<span class="c"># =&gt; envoy  version: 6c72b2179f5a58988b920a55b0be8346de3f7b35/1.31.2-dev/Clean/RELEASE/BoringSSL</span>

<span class="c"># istio-ingressgateway 서비스 NodePort로 변경</span>
<span class="nv">$ </span>kubectl patch svc <span class="nt">-n</span> istio-system istio-ingressgateway <span class="nt">-p</span> <span class="s1">'{"spec":{"type":"NodePort"}}'</span>
<span class="c"># =&gt; service/istio-ingressgateway patched</span>

<span class="c"># istio-ingressgateway 서비스 확인</span>
<span class="nv">$ </span>kubectl get svc,ep <span class="nt">-n</span> istio-system istio-ingressgateway
<span class="c"># =&gt; NAME                           TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                                      AGE</span>
<span class="c">#    service/istio-ingressgateway   NodePort   10.10.200.171   &amp;lt;none&amp;gt;        15021:30953/TCP,80:31677/TCP,443:30737/TCP,31400:30617/TCP,15443:31668/TCP   16m</span>
<span class="c">#    </span>
<span class="c">#    NAME                             ENDPOINTS                                                           AGE</span>
<span class="c">#    endpoints/istio-ingressgateway   172.16.2.14:15443,172.16.2.14:15021,172.16.2.14:31400 + 2 more...   16m</span>

<span class="c">## istio-ingressgateway 서비스 포트 정보 확인</span>
<span class="nv">$ </span>kubectl get svc <span class="nt">-n</span> istio-system istio-ingressgateway <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.spec.ports[<span class="k">*</span><span class="o">]}</span> | jq
<span class="c"># =&gt; {</span>
<span class="c">#      &amp;quot;name&amp;quot;: &amp;quot;https&amp;quot;,</span>
<span class="c">#      &amp;quot;nodePort&amp;quot;: 30737,</span>
<span class="c">#      &amp;quot;port&amp;quot;: 443,</span>
<span class="c">#      &amp;quot;protocol&amp;quot;: &amp;quot;TCP&amp;quot;,</span>
<span class="c">#      &amp;quot;targetPort&amp;quot;: 8443</span>
<span class="c">#    }</span>
<span class="c">#    {</span>
<span class="c">#      &amp;quot;name&amp;quot;: &amp;quot;tcp&amp;quot;,</span>
<span class="c">#      &amp;quot;nodePort&amp;quot;: 30617,</span>
<span class="c">#      &amp;quot;port&amp;quot;: 31400,</span>
<span class="c">#      &amp;quot;protocol&amp;quot;: &amp;quot;TCP&amp;quot;,</span>
<span class="c">#      &amp;quot;targetPort&amp;quot;: 31400</span>
<span class="c">#    }</span>
<span class="c">#    ...</span>

<span class="c">## istio-ingressgateway 디플로이먼트 파드의 포트 정보 확인 </span>
<span class="nv">$ </span>kubectl get deploy/istio-ingressgateway <span class="nt">-n</span> istio-system <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.spec.template.spec.containers[0].ports[<span class="k">*</span><span class="o">]}</span> | jq
<span class="c"># =&gt; ...</span>
<span class="c">#    {</span>
<span class="c">#      &amp;quot;containerPort&amp;quot;: 8443,</span>
<span class="c">#      &amp;quot;protocol&amp;quot;: &amp;quot;TCP&amp;quot;</span>
<span class="c">#    }</span>
<span class="c">#    {</span>
<span class="c">#      &amp;quot;containerPort&amp;quot;: 31400,</span>
<span class="c">#      &amp;quot;protocol&amp;quot;: &amp;quot;TCP&amp;quot;</span>
<span class="c">#    }</span>
<span class="c">#    ...</span>
<span class="nv">$ </span>kubectl get deploy/istio-ingressgateway <span class="nt">-n</span> istio-system <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.spec.template.spec.containers[0].readinessProbe<span class="o">}</span> | jq
<span class="c"># =&gt; {</span>
<span class="c">#      &amp;quot;failureThreshold&amp;quot;: 30,</span>
<span class="c">#      &amp;quot;httpGet&amp;quot;: {</span>
<span class="c">#        &amp;quot;path&amp;quot;: &amp;quot;/healthz/ready&amp;quot;,</span>
<span class="c">#        &amp;quot;port&amp;quot;: 15021,</span>
<span class="c">#        &amp;quot;scheme&amp;quot;: &amp;quot;HTTP&amp;quot;</span>
<span class="c">#      },</span>
<span class="c">#      &amp;quot;initialDelaySeconds&amp;quot;: 1,</span>
<span class="c">#      &amp;quot;periodSeconds&amp;quot;: 2,</span>
<span class="c">#      &amp;quot;successThreshold&amp;quot;: 1,</span>
<span class="c">#      &amp;quot;timeoutSeconds&amp;quot;: 1</span>
<span class="c">#    }</span>

<span class="c"># istiod(컨트롤플레인) 디플로이먼트 정보 확인</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deployment.apps/istiod <span class="nt">-n</span> istio-system <span class="nt">--</span> ss <span class="nt">-tnlp</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deployment.apps/istiod <span class="nt">-n</span> istio-system <span class="nt">--</span> ss <span class="nt">-tnp</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deployment.apps/istiod <span class="nt">-n</span> istio-system <span class="nt">--</span> ps <span class="nt">-ef</span>
<span class="c"># =&gt; UID          PID    PPID  C STIME TTY          TIME CMD</span>
<span class="c">#    istio-p+       1       0  0 05:39 ?        00:00:01 /usr/local/bin/pilot-discovery discovery --monitoringAddr=:15014 --log_output_level=default:info --domain cluster.local --ke</span>

<span class="c"># istio-ingressgateway 디플로이먼트 정보 확인</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deployment.apps/istio-ingressgateway <span class="nt">-n</span> istio-system <span class="nt">--</span> ss <span class="nt">-tnlp</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deployment.apps/istio-ingressgateway <span class="nt">-n</span> istio-system <span class="nt">--</span> ss <span class="nt">-tnp</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deployment.apps/istio-ingressgateway <span class="nt">-n</span> istio-system <span class="nt">--</span> ps <span class="nt">-ef</span>
<span class="c"># =&gt; UID          PID    PPID  C STIME TTY          TIME CMD</span>
<span class="c">#    istio-p+       1       0  0 05:27 ?        00:00:08 /usr/local/bin/pilot-agent proxy router --domain istio-system.svc.cluster.local</span>
<span class="c">#    istio-p+      16       1  0 05:27 ?        00:00:04 /usr/local/bin/envoy -c etc/istio/proxy/envoy-rev.json --drain-time-s 45 --drai</span>
<span class="c"># # &lt;span style="color: green;"&gt;👉 pilot-agent와 envoy가 동작 중입니다.&lt;/span&gt;</span>

<span class="c"># envoy 설정 확인</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deployment.apps/istio-ingressgateway <span class="nt">-n</span> istio-system <span class="nt">--</span> <span class="nb">cat</span> /etc/istio/proxy/envoy-rev.json
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deployment.apps/istio-ingressgateway <span class="nt">-n</span> istio-system <span class="nt">--</span> ss <span class="nt">-xnlp</span>
<span class="c"># =&gt; Netid      State       Recv-Q      Send-Q                                          Local Address:Port              Peer Address:Port      Process</span>
<span class="c">#    u_str      LISTEN      0           4096               var/run/secrets/workload-spiffe-uds/socket 32430                        * 0          users:((&amp;quot;pilot-agent&amp;quot;,pid=1,fd=9))</span>
<span class="c">#    u_str      LISTEN      0           4096                                      etc/istio/proxy/XDS 32431                        * 0          users:((&amp;quot;pilot-agent&amp;quot;,pid=1,fd=10))</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deployment.apps/istio-ingressgateway <span class="nt">-n</span> istio-system <span class="nt">--</span> ss <span class="nt">-xnp</span>
<span class="c"># =&gt; Netid State Recv-Q Send-Q                              Local Address:Port  Peer Address:Port Process</span>
<span class="c">#    u_str ESTAB 0      0                                               * 39978            * 37977 users:((&amp;quot;envoy&amp;quot;,pid=16,fd=19))</span>
<span class="c">#    u_str ESTAB 0      0                                               * 37501            * 37981 users:((&amp;quot;envoy&amp;quot;,pid=16,fd=32))</span>
<span class="c">#    u_str ESTAB 0      0                             etc/istio/proxy/XDS 37977            * 39978 users:((&amp;quot;pilot-agent&amp;quot;,pid=1,fd=11))</span>
<span class="c">#    u_str ESTAB 0      0      var/run/secrets/workload-spiffe-uds/socket 37981            * 37501 users:((&amp;quot;pilot-agent&amp;quot;,pid=1,fd=15))</span>
</code></pre></div></div>

<ul>
  <li>Auto Injection with namespace label</li>
  <li>
    <p>해당 네임스페이스에 생성되는 모든 파드들은 istio 사이드카가 자동으로 injection 됩니다.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># mutating Webhook admisstion controller 사용</span>
<span class="nv">$ </span>kubectl label namespace default istio-injection<span class="o">=</span>enabled
<span class="c"># =&gt; namespace/default labeled</span>
<span class="nv">$ </span>kubectl get ns <span class="nt">-L</span> istio-injection
<span class="c"># =&gt; NAME              STATUS   AGE     ISTIO-INJECTION</span>
<span class="c">#    default           Active   7d      enabled</span>
<span class="c">#    ...</span>
</code></pre></div>    </div>

    <p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_8.png" alt="img.png" class="image-center" /></p>
  </li>
  <li>Istio 접속 테스트를 위한 변수 지정 및 k3s-m에서 접속 테스트</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># k3s-m)</span>
<span class="c"># istio ingress gw NodePort(HTTP 접속용) 변수 지정 </span>
<span class="nv">$ </span><span class="nb">export </span><span class="nv">IGWHTTP</span><span class="o">=</span><span class="si">$(</span>kubectl get service <span class="nt">-n</span> istio-system istio-ingressgateway <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.spec.ports[1].nodePort}'</span><span class="si">)</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$IGWHTTP</span>
<span class="c"># =&gt; 31677</span>

<span class="c"># /etc/hosts 파일 수정</span>
<span class="c"># $ MYDOMAIN=&lt;각자 자신의 www 도메인&gt; # 단, 사용하고 있지 않는 공인 도메인을 사용 할 것</span>
<span class="c"># $ echo "&lt;istio-ingressgateway 파드가 있는 워커 노드&gt; $MYDOMAIN" &gt;&gt; /etc/hosts</span>

<span class="nv">$ </span><span class="nb">export </span><span class="nv">MYDOMAIN</span><span class="o">=</span>sweetlittlebird.com
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"192.168.10.10 </span><span class="nv">$MYDOMAIN</span><span class="s2">"</span> <span class="o">&gt;&gt;</span> /etc/hosts
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"export MYDOMAIN=</span><span class="nv">$MYDOMAIN</span><span class="s2">"</span> <span class="o">&gt;&gt;</span> /etc/profile

<span class="c"># istio ingress gw 접속 테스트 : 아직은 설정이 없어서 접속 실패가 됩니다.</span>
<span class="nv">$ </span>curl <span class="nt">-v</span> <span class="nt">-s</span> <span class="nv">$MYDOMAIN</span>:<span class="nv">$IGWHTTP</span>
<span class="c"># =&gt; *   Trying 192.168.10.10:31677...</span>
<span class="c">#    * connect to 192.168.10.10 port 31677 failed: Connection refused</span>
<span class="c">#    * Failed to connect to sweetlittlebird.com port 31677 after 3 ms: Connection refused</span>
<span class="c">#    * Closing connection 0</span>
</code></pre></div></div>

<ul>
  <li>testpc에서 접속 테스트</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 아래 변수는 각자 자신의 값을 직접 입력 할 것</span>
<span class="c"># $ IGWHTTP=&lt;각자 출력된 NodePort&gt;</span>
<span class="nv">$ IGWHTTP</span><span class="o">=</span>31677
<span class="nv">$ </span><span class="nb">export </span><span class="nv">MYDOMAIN</span><span class="o">=</span>sweetlittlebird.com
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"192.168.10.10 </span><span class="nv">$MYDOMAIN</span><span class="s2">"</span> <span class="o">&gt;&gt;</span> /etc/hosts
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"export MYDOMAIN=</span><span class="nv">$MYDOMAIN</span><span class="s2">"</span> <span class="o">&gt;&gt;</span> /etc/profile

<span class="c"># istio ingress gw 접속 테스트 : 아직은 설정이 없어서 접속 실패가 된다</span>
<span class="nv">$ </span>curl <span class="nt">-v</span> <span class="nt">-s</span> <span class="nv">$MYDOMAIN</span>:<span class="nv">$IGWHTTP</span>
<span class="c"># =&gt; * Host sweetlittlebird.com:31677 was resolved.</span>
<span class="c">#    * ...</span>
<span class="c">#    * Failed to connect to sweetlittlebird.com port 31677 after 2 ms: Couldn't connect to server</span>
<span class="c">#    * ...</span>
</code></pre></div></div>

<ul>
  <li>자신의 pc에서 접속 테스트</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 아래 변수는 각자 자신의 값을 직접 입력 할 것</span>
<span class="c"># $ IGWHTTP=&lt;각자 출력된 NodePort&gt;</span>
<span class="nv">$ IGWHTTP</span><span class="o">=</span>31677
<span class="c"># $ ISTIONODEIP=&lt;k3s-m 의 유동 공인 IP&gt;</span>
<span class="nv">$ ISTIONODEIP</span><span class="o">=</span>54.123.42.212

<span class="nv">$ </span><span class="nb">export </span><span class="nv">MYDOMAIN</span><span class="o">=</span>sweetlittlebird.com
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"192.168.10.10 </span><span class="nv">$MYDOMAIN</span><span class="s2">"</span> <span class="o">&gt;&gt;</span> /etc/hosts
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"export MYDOMAIN=</span><span class="nv">$MYDOMAIN</span><span class="s2">"</span> <span class="o">&gt;&gt;</span> /etc/profile

<span class="c"># istio ingress gw 접속 테스트 : 아직은 설정이 없어서 접속 실패가 된다</span>
<span class="nv">$ </span>curl <span class="nt">-v</span> <span class="nt">-s</span> <span class="nv">$MYDOMAIN</span>:<span class="nv">$IGWHTTP</span>
<span class="c"># =&gt; * Host sweetlittlebird.com:31677 was resolved.</span>
<span class="c">#    * ...</span>
<span class="c">#    * Failed to connect to sweetlittlebird.com port 31677 after 2 ms: Couldn't connect to server</span>
<span class="c">#    * ...</span>
</code></pre></div></div>

<h3 id="istio를-통한-외부-노출">Istio를 통한 외부 노출</h3>

<ul>
  <li>Nginx 디플로이먼트와 서비스 배포</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 로그 모니터링</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-n</span> istio-system <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>istiod
<span class="c"># =&gt; NAME                      READY   STATUS    RESTARTS      AGE</span>
<span class="c">#    istiod-7f8b586864-8mc4c   1/1     Running   1 (60m ago)   73m</span>
<span class="nv">$ </span>kubetail <span class="nt">-n</span> istio-system <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>istiod <span class="nt">-f</span>

<span class="nv">$ </span>kubectl get pod <span class="nt">-n</span> istio-system <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>istio-ingressgateway
<span class="c"># =&gt; NAME                                    READY   STATUS    RESTARTS   AGE</span>
<span class="c">#    istio-ingressgateway-5f9f654d46-l7mqp   1/1     Running   0          74m</span>
<span class="nv">$ </span>kubetail <span class="nt">-n</span> istio-system <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>istio-ingressgateway <span class="nt">-f</span>
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl create -f -
apiVersion: v1
kind: ServiceAccount
metadata:
  name: kans-nginx
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deploy-websrv
spec:
  replicas: 1
  selector:
    matchLabels:
      app: deploy-websrv
  template:
    metadata:
      labels:
        app: deploy-websrv
    spec:
      serviceAccountName: kans-nginx
      terminationGracePeriodSeconds: 0
      containers:
      - name: deploy-websrv
        image: nginx:alpine
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: svc-clusterip
spec:
  ports:
    - name: svc-webport
      port: 80
      targetPort: 80
  selector:
    app: deploy-websrv
  type: ClusterIP
</span><span class="no">EOF
</span><span class="c"># =&gt; serviceaccount/kans-nginx created</span>
<span class="c">#    deployment.apps/deploy-websrv created</span>
<span class="c">#    service/svc-clusterip created</span>

<span class="c"># 사이드카 컨테이너 배포 확인</span>
<span class="nv">$ </span>kubectl get pod,svc,ep,sa <span class="nt">-o</span> wide
<span class="c"># =&gt; NAME                                 READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES</span>
<span class="c">#    pod/deploy-websrv-778ffd6947-cxf5k   2/2     Running   0          50s   172.16.1.13   k3s-w2   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    </span>
<span class="c">#    NAME                    TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE     SELECTOR</span>
<span class="c">#    service/kubernetes      ClusterIP   10.10.200.1     &amp;lt;none&amp;gt;        443/TCP   6d16h   &amp;lt;none&amp;gt;</span>
<span class="c">#    service/svc-clusterip   ClusterIP   10.10.200.243   &amp;lt;none&amp;gt;        80/TCP    50s     app=deploy-websrv</span>
<span class="c">#    </span>
<span class="c">#    NAME                      ENDPOINTS            AGE</span>
<span class="c">#    endpoints/kubernetes      192.168.10.10:6443   6d16h</span>
<span class="c">#    endpoints/svc-clusterip   172.16.1.13:80       50s</span>
<span class="c">#    </span>
<span class="c">#    NAME                        SECRETS   AGE</span>
<span class="c">#    serviceaccount/default      0         7d1h</span>
<span class="c">#    serviceaccount/kans-nginx   0         50s</span>

<span class="nv">$ </span>kubectl describe pod
<span class="c"># =&gt; Name:             deploy-websrv-778ffd6947-cxf5k</span>
<span class="c">#    Namespace:        default</span>
<span class="c">#    Priority:         0</span>
<span class="c">#    Service Account:  kans-nginx</span>
<span class="c">#    Node:             k3s-w2/192.168.10.102</span>
<span class="c">#    Labels:           app=deploy-websrv</span>
<span class="c">#                      security.istio.io/tlsMode=istio</span>
<span class="c">#                      ...</span>
<span class="c">#    Annotations:      istio.io/rev: default</span>
<span class="c">#                      sidecar.istio.io/status:</span>
<span class="c">#                        {&amp;quot;initContainers&amp;quot;:[&amp;quot;istio-init&amp;quot;],&amp;quot;containers&amp;quot;:[&amp;quot;istio-proxy&amp;quot;],&amp;quot;volumes&amp;quot;:[&amp;quot;workload-socket&amp;quot;,&amp;quot;credential-socket&amp;quot;,&amp;quot;workload-certs&amp;quot;,&amp;quot;istio-env...</span>
<span class="c">#    Status:           Running</span>
<span class="c">#    ...</span>
<span class="c">#    Controlled By:  ReplicaSet/deploy-websrv-778ffd6947</span>
<span class="c">#    &lt;span style="color: red;"&gt;Init Containers:&lt;/span&gt;  # &lt;span style="color: green;"&gt;👉 init container가 파드내 iptables 셋팅&lt;/span&gt;</span>
<span class="c">#      istio-init:</span>
<span class="c">#        Container ID:  containerd://2a114fe0624581b35bda9ca257c6d3c831138e8a44900a6130e988bb51eb05da</span>
<span class="c">#        Image:         docker.io/istio/proxyv2:1.23.2</span>
<span class="c">#        Image ID:      docker.io/istio/proxyv2@sha256:2876cfc2fdf47e4b9665390ccc9ccf2bf913b71379325b8438135c9f35578e1a</span>
<span class="c">#        Port:          &amp;lt;none&amp;gt;</span>
<span class="c">#        Host Port:     &amp;lt;none&amp;gt;</span>
<span class="c">#        Args:</span>
<span class="c">#          &lt;span style="color: red;"&gt;istio-iptables&lt;/span&gt;</span>
<span class="c">#          -p</span>
<span class="c">#          15001</span>
<span class="c">#          -z</span>
<span class="c">#          15006</span>
<span class="c">#          -u</span>
<span class="c">#          1337</span>
<span class="c">#          -m</span>
<span class="c">#          REDIRECT</span>
<span class="c">#          -i</span>
<span class="c">#          *</span>
<span class="c">#          -x</span>
<span class="c">#    </span>
<span class="c">#          -b</span>
<span class="c">#          *</span>
<span class="c">#          -d</span>
<span class="c">#          15090,15021,15020</span>
<span class="c">#          --log_output_level=default:info</span>
<span class="c">#        State:          Terminated</span>
<span class="c">#          Reason:       Completed</span>
<span class="c">#        ...</span>
<span class="c">#    Containers:</span>
<span class="c">#      deploy-websrv:</span>
<span class="c">#        Container ID:   containerd://8918e0bb760bce8d090e84818bc189ae3ababdf9e74eb7dd3fb9709b356891f9</span>
<span class="c">#        Image:          nginx:alpine</span>
<span class="c">#        ...</span>
<span class="c">#      &lt;span style="color: red;"&gt;istio-proxy:&lt;/span&gt;  # &lt;span style="color: green;"&gt;👉 istio-proxy라는 컨테이너가 sidecar로 동작 중&lt;/span&gt;</span>
<span class="c">#        Container ID:  containerd://71d9e07a530dfce2ec34810d60a28dc3f9445b8eab714c2a7e204c459c59bcd3</span>
<span class="c">#        Image:         docker.io/istio/proxyv2:1.23.2</span>
<span class="c">#        Image ID:      docker.io/istio/proxyv2@sha256:2876cfc2fdf47e4b9665390ccc9ccf2bf913b71379325b8438135c9f35578e1a</span>
<span class="c">#        Port:          15090/TCP</span>
<span class="c">#        Host Port:     0/TCP</span>
<span class="c">#        Args:</span>
<span class="c">#          proxy</span>
<span class="c">#          sidecar</span>
<span class="c">#          --domain</span>
<span class="c">#          $(POD_NAMESPACE).svc.cluster.local</span>
<span class="c">#          --proxyLogLevel=warning</span>
<span class="c">#          --proxyComponentLogLevel=misc:error</span>
<span class="c">#          --log_output_level=default:info</span>
<span class="c">#        State:          Running</span>
<span class="c">#        ...</span>
</code></pre></div></div>

<ul>
  <li>Istio Gateway/VirtualService 설정 - Host 기반 트래픽 라우팅 설정 <a href="https://istio.io/latest/docs/setup/additional-setup/gateway/">링크</a>
<img src="/assets/2024/kans-3th/w7/20241019_kans_w7_9.png" alt="img.png" class="image-center w-80" />
    <ul>
      <li>클라이언트 PC → (Service:NodePort) Istio ingressgateway 파드 → (Gateway, VirtualService, Service 는 Bypass) → Endpoint(파드 : 사이드카 - Application 컨테이너)</li>
      <li>Gateway : 지정한 인그레스 게이트웨이로부터 트래픽이 인입, 프로토콜 및 포트, HOSTS, Proxy 등 설정 가능 <a href="https://istio.io/latest/docs/setup/additional-setup/gateway/">링크</a></li>
      <li>VirtualService : 인입 처리할 hosts 설정, L7 PATH 별 라우팅, 목적지에 대한 정책 설정 가능 (envoy route config)</li>
      <li>(참고) Introducing Istio v1 APIs - <a href="https://istio.io/latest/blog/2024/v1-apis/">Blog</a></li>
    </ul>
  </li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl create -f -
apiVersion: networking.istio.io/v1
kind: Gateway
metadata:
  name: test-gateway
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 80
      name: http
      protocol: HTTP
    hosts:
    - "*"
---
apiVersion: networking.istio.io/v1
kind: VirtualService
metadata:
  name: nginx-service
spec:
  hosts:
  - "</span><span class="nv">$MYDOMAIN</span><span class="sh">"
  gateways:
  - test-gateway
  http:
  - route:
    - destination:
        host: svc-clusterip
        port:
          number: 80
</span><span class="no">EOF
</span><span class="c"># =&gt; gateway.networking.istio.io/test-gateway created</span>
<span class="c">#    virtualservice.networking.istio.io/nginx-service created</span>

<span class="c"># Istio Gateway(=gw)/VirtualService(=vs) 설정 정보를 확인</span>
<span class="nv">$ </span>kubectl explain gateways.networking.istio.io
<span class="nv">$ </span>kubectl explain virtualservices.networking.istio.io
<span class="nv">$ </span>kubectl api-resources  | <span class="nb">grep </span>istio
<span class="c"># =&gt; wasmplugins                                      extensions.istio.io/v1alpha1      true         WasmPlugin</span>
<span class="c">#    destinationrules                    dr           networking.istio.io/v1            true         DestinationRule</span>
<span class="c">#    envoyfilters                                     networking.istio.io/v1alpha3      true         EnvoyFilter</span>
<span class="c">#    gateways                            gw           networking.istio.io/v1            true         Gateway</span>
<span class="c">#    proxyconfigs                                     networking.istio.io/v1beta1       true         ProxyConfig</span>
<span class="c">#    serviceentries                      se           networking.istio.io/v1            true         ServiceEntry</span>
<span class="c">#    sidecars                                         networking.istio.io/v1            true         Sidecar</span>
<span class="c">#    virtualservices                     vs           networking.istio.io/v1            true         VirtualService</span>
<span class="c">#    workloadentries                     we           networking.istio.io/v1            true         WorkloadEntry</span>
<span class="c">#    workloadgroups                      wg           networking.istio.io/v1            true         WorkloadGroup</span>
<span class="c">#    authorizationpolicies               ap           security.istio.io/v1              true         AuthorizationPolicy</span>
<span class="c">#    peerauthentications                 pa           security.istio.io/v1              true         PeerAuthentication</span>
<span class="c">#    requestauthentications              ra           security.istio.io/v1              true         RequestAuthentication</span>
<span class="c">#    telemetries                         telemetry    telemetry.istio.io/v1             true         Telemetry</span>

<span class="c"># virtual service 는 다른 네임스페이스의 서비스(ex. svc-nn.&lt;ns&gt;)도 참조할 수 있다</span>
<span class="nv">$ </span>kubectl get gw,vs
<span class="c"># =&gt; NAME                                       AGE</span>
<span class="c">#    gateway.networking.istio.io/test-gateway   105s</span>
<span class="c">#    </span>
<span class="c">#    NAME                                               GATEWAYS           HOSTS                     AGE</span>
<span class="c">#    virtualservice.networking.istio.io/nginx-service   [&amp;quot;test-gateway&amp;quot;]   [&amp;quot;sweetlittlebird.com&amp;quot;]   105s</span>

<span class="c"># Retrieves last sent and last acknowledged xDS sync from Istiod to each Envoy in the mesh</span>
<span class="c"># istioctl proxy-status command was improved to include the time since last change, and more relevant status values.</span>
<span class="nv">$ </span>istioctl proxy-status <span class="c"># 단축어 ps</span>
<span class="nv">$ </span>istioctl ps
<span class="c"># =&gt; NAME                                                   CLUSTER        CDS                LDS                EDS              RDS                ECDS        ISTIOD                      VERSION</span>
<span class="c">#    deploy-websrv-778ffd6947-cxf5k.default                 Kubernetes     SYNCED (22m)       SYNCED (22m)       SYNCED (22m)     SYNCED (22m)       IGNORED     istiod-7f8b586864-8mc4c     1.23.2</span>
<span class="c">#    istio-ingressgateway-5f9f654d46-l7mqp.istio-system     Kubernetes     SYNCED (2m14s)     SYNCED (2m14s)     SYNCED (22m)     SYNCED (2m14s)     IGNORED     istiod-7f8b586864-8mc4c     1.23.2</span>
</code></pre></div></div>

<ul>
  <li>Istio를 통한 Nginx 파드 접속 테스트
    <ul>
      <li>외부 (자신의 PC, test pc)에서 접속 테스트</li>
    </ul>
  </li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># istio ingress gw 를 통한 접속 테스트</span>
<span class="nv">$ </span>curl <span class="nt">-s</span> <span class="nv">$MYDOMAIN</span>:<span class="nv">$IGWHTTP</span> | <span class="nb">grep</span> <span class="nt">-o</span> <span class="s2">"&lt;title&gt;.*&lt;/title&gt;"</span>
<span class="c"># =&gt; &lt;title&gt;Welcome to nginx!&lt;/title&gt;</span>
<span class="nv">$ </span>curl <span class="nt">-v</span> <span class="nt">-s</span> <span class="nv">$MYDOMAIN</span>:<span class="nv">$IGWHTTP</span>
<span class="c"># =&gt; * Host sweetlittlebird.com:31677 was resolved.</span>
<span class="c">#    * IPv6: (none)</span>
<span class="c">#    * IPv4: 192.168.10.10, 192.168.10.10</span>
<span class="c">#    *   Trying 192.168.10.10:31677...</span>
<span class="c">#    * Connected to sweetlittlebird.com (192.168.10.10) port 31677</span>
<span class="c">#    &amp;gt; GET / HTTP/1.1</span>
<span class="c">#    &amp;gt; Host: sweetlittlebird.com:31677</span>
<span class="c">#    &amp;gt; User-Agent: curl/8.5.0</span>
<span class="c">#    &amp;gt; Accept: */*</span>
<span class="c">#    &amp;gt;</span>
<span class="c">#    &amp;lt; HTTP/1.1 200 OK</span>
<span class="c">#    &amp;lt; server: istio-envoy</span>
<span class="c">#    &amp;lt; date: Sat, 19 Oct 2024 07:14:25 GMT</span>
<span class="c">#    &amp;lt; content-type: text/html</span>
<span class="c">#    &amp;lt; content-length: 615</span>
<span class="c">#    &amp;lt; last-modified: Wed, 02 Oct 2024 16:07:39 GMT</span>
<span class="c">#    &amp;lt; etag: &amp;quot;66fd6fcb-267&amp;quot;</span>
<span class="c">#    &amp;lt; accept-ranges: bytes</span>
<span class="c">#    &amp;lt; x-envoy-upstream-service-time: 1</span>
<span class="c">#    ...</span>
<span class="c"># $ curl -v -s &lt;유동공인이IP&gt;:$IGWHTTP</span>
<span class="nv">$ </span>curl <span class="nt">-v</span> <span class="nt">-s</span> 54.123.42.212:<span class="nv">$IGWHTTP</span>
</code></pre></div></div>

<ul>
  <li>출력 로그 정보 확인</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubetail <span class="nt">-n</span> istio-system <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>istio-ingressgateway <span class="nt">-f</span>
<span class="c"># =&gt; [istio-ingressgateway-5f9f654d46-l7mqp] [2024-10-01T07:49:20.833Z] &amp;quot;GET / HTTP/1.1&amp;quot; 200 - via_upstream - &amp;quot;-&amp;quot; 0 615 6 5 &amp;quot;172.16.0.0&amp;quot; &amp;quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Safari/605.1.15&amp;quot; &amp;quot;6f246b4e-d675-971e-9a9d-dd809f560c6f&amp;quot; &amp;quot;sweetlittlebird.com:31677&amp;quot; &amp;quot;172.16.1.13:80&amp;quot; </span>
<span class="c">#    &lt;span style="color: red;"&gt;outbound|80||svc-clusterip.default.svc.cluster.local&lt;/span&gt; 172.16.2.14:60786 172.16.2.14:8080 172.16.0.0:8773 - -</span>
<span class="nv">$ </span>kubetail <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>deploy-websrv
<span class="c"># =&gt; [deploy-websrv-778ffd6947-cxf5k istio-proxy] [2024-10-01T07:49:20.866Z] &amp;quot;GET / HTTP/1.1&amp;quot; 200 - via_upstream - &amp;quot;-&amp;quot; 0 615 2 1 &amp;quot;172.16.0.0&amp;quot; &amp;quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Safari/605.1.15&amp;quot; &amp;quot;6f246b4e-d675-971e-9a9d-dd809f560c6f&amp;quot; &amp;quot;sweetlittlebird.com:31677&amp;quot; &amp;quot;172.16.1.13:80&amp;quot; </span>
<span class="c">#    &lt;span style="color: red;"&gt;inbound|80||&lt;/span&gt; 127.0.0.6:40337 172.16.1.13:80 172.16.0.0:0 invalid:outbound_.80_._.svc-clusterip.default.svc.cluster.local default</span>
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_10.png" alt="img.png" class="image-center" /></p>

<ul>
  <li>istioctl 정보 확인</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#</span>
<span class="nv">$ </span>istioctl proxy-status
<span class="c"># =&gt; NAME                                                   CLUSTER        CDS              LDS              EDS              RDS              ECDS        ISTIOD                      VERSION</span>
<span class="c">#    deploy-websrv-778ffd6947-cxf5k.default                 Kubernetes     SYNCED (19m)     SYNCED (19m)     SYNCED (19m)     SYNCED (19m)     IGNORED     istiod-7f8b586864-8mc4c     1.23.2</span>
<span class="c">#    istio-ingressgateway-5f9f654d46-l7mqp.istio-system     Kubernetes     SYNCED (24m)     SYNCED (24m)     SYNCED (24m)     SYNCED (24m)     IGNORED     istiod-7f8b586864-8mc4c     1.23.2</span>

<span class="c"># Envoy config dump : all, cluster, endpoint, listener 등</span>
<span class="nv">$ </span>istioctl proxy-config <span class="nt">--help</span> 
<span class="nv">$ </span>istioctl proxy-config all deploy-websrv-778ffd6947-cxf5k
<span class="nv">$ </span>istioctl proxy-config all deploy-websrv-778ffd6947-cxf5k <span class="nt">-o</span> json | jq
<span class="nv">$ </span>istioctl proxy-config route deploy-websrv-778ffd6947-cxf5k <span class="nt">-o</span> json | jq
</code></pre></div></div>

<ul>
  <li>pilot : istio-proxy내 uds로 envoy와 grpc통신, istiod에서 받아온 dynamic config를 envoy에 전달</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># istio-proxy 사용자 정보 확인 : uid(1337):gid(1337) 확인 -&gt; iptables rule 에서 사용됨</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deploy/deploy-websrv <span class="nt">-c</span> istio-proxy <span class="nt">--</span> <span class="nb">tail</span> <span class="nt">-n</span> 3 /etc/passwd
<span class="c"># =&gt; ubuntu:x:1000:1000:Ubuntu:/home/ubuntu:/bin/bash</span>
<span class="c">#    tcpdump:x:100:102::/nonexistent:/usr/sbin/nologin</span>
<span class="c">#    istio-proxy:x:1337:1337::/home/istio-proxy:/bin/sh</span>

<span class="c"># envoy 설정 정보 확인 : dynamic_resources , static_resources - listeners  </span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deploy/deploy-websrv <span class="nt">-c</span> istio-proxy <span class="nt">--</span> <span class="nb">cat</span> /etc/istio/proxy/envoy-rev.json
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deploy/deploy-websrv <span class="nt">-c</span> istio-proxy <span class="nt">--</span> ss <span class="nt">-nlp</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deploy/deploy-websrv <span class="nt">-c</span> istio-proxy <span class="nt">--</span> ss <span class="nt">-np</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deploy/deploy-websrv <span class="nt">-c</span> istio-proxy <span class="nt">--</span> netstat <span class="nt">-np</span>
<span class="c"># =&gt; Active Internet connections (w/o servers)</span>
<span class="c">#    Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name</span>
<span class="c">#    tcp        0      0 127.0.0.1:33168         127.0.0.1:15020         ESTABLISHED 13/envoy</span>
<span class="c">#    tcp        0      0 127.0.0.1:33156         127.0.0.1:15020         ESTABLISHED 13/envoy</span>
<span class="c">#    tcp        0      0 172.16.1.13:15006       172.16.2.14:33006       ESTABLISHED 13/envoy</span>
<span class="c">#    tcp        0      0 172.16.1.13:59774       10.10.200.215:15012     ESTABLISHED 1/pilot-agent</span>
<span class="c">#    tcp        0      0 172.16.1.13:15006       172.16.2.14:60786       ESTABLISHED 13/envoy</span>
<span class="c"># &lt;span style="color: green;"&gt;👉 172.16.1.13 : deploy-websrv 파드 ip&lt;/span&gt;</span>
<span class="c"># &lt;span style="color: green;"&gt;    172.16.2.14 : istio-ingressgateway 파드 ip&lt;/span&gt;</span>
<span class="c"># &lt;span style="color: green;"&gt;    10.10.200.215 : istiod 서비스의 Cluster-IP&lt;/span&gt;</span>
<span class="c">#    ...</span>
<span class="c">#    Active UNIX domain sockets (w/o servers)</span>
<span class="c">#    Proto RefCnt Flags       Type       State         I-Node   PID/Program name     Path</span>
<span class="c">#    unix  3      [ ]         STREAM     CONNECTED     54162    13/envoy</span>
<span class="c">#    unix  3      [ ]         STREAM     CONNECTED     54709    1/pilot-agent        var/run/secrets/workload-spiffe-uds/socket</span>
<span class="c">#    unix  3      [ ]         STREAM     CONNECTED     71034    1/pilot-agent        etc/istio/proxy/XDS</span>
<span class="c">#    unix  3      [ ]         STREAM     CONNECTED     72979    13/envoy</span>
<span class="c">#    ...</span>

<span class="c">#</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deploy/deploy-websrv <span class="nt">-c</span> istio-proxy <span class="nt">--</span> ps <span class="nt">-ef</span>
<span class="c"># =&gt; UID          PID    PPID  C STIME TTY          TIME CMD</span>
<span class="c">#    istio-p+       1       0  0 06:43 ?        00:00:01 /usr/local/bin/pilot-agent proxy sidecar --domain default.svc.cluster.local --proxyLogLevel=warning --proxyComponentLogLevel=mi</span>
<span class="c">#    istio-p+      13       1  0 06:43 ?        00:00:34 /usr/local/bin/envoy -c etc/istio/proxy/envoy-rev.json --drain-time-s 45 --drain-strategy immediate --local-address-ip-version</span>

<span class="c"># </span>
<span class="nv">$ </span>kubectl get pod,svc <span class="nt">-A</span> <span class="nt">-owide</span>
<span class="c"># =&gt; NAMESPACE      NAME                                            READY   STATUS    RESTARTS        AGE     IP            NODE     NOMINATED NODE   READINESS GATES</span>
<span class="c">#    default        pod/deploy-websrv-778ffd6947-cxf5k              2/2     Running   0               91m     172.16.1.13   k3s-w2   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    istio-system   pod/istio-ingressgateway-5f9f654d46-l7mqp       1/1     Running   0               167m    172.16.2.14   k3s-w3   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    istio-system   pod/istiod-7f8b586864-8mc4c                     1/1     Running   1 (154m ago)    167m    172.16.3.16   k3s-w1   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    ...</span>
<span class="c">#    </span>
<span class="c">#    NAMESPACE      NAME                                         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                                      AGE     SELECTOR</span>
<span class="c">#    default        service/kubernetes                           ClusterIP   10.10.200.1     &amp;lt;none&amp;gt;        443/TCP                                                                      6d17h   &amp;lt;none&amp;gt;</span>
<span class="c">#    default        service/svc-clusterip                        ClusterIP   10.10.200.243   &amp;lt;none&amp;gt;        80/TCP                                                                       91m     app=deploy-websrv</span>
<span class="c">#    istio-system   service/istio-ingressgateway                 NodePort    10.10.200.171   &amp;lt;none&amp;gt;        15021:30953/TCP,80:31677/TCP,443:30737/TCP,31400:30617/TCP,15443:31668/TCP   167m    app=istio-ingressgateway,istio=ingressgateway</span>
<span class="c">#    istio-system   service/istiod                               ClusterIP   10.10.200.215   &amp;lt;none&amp;gt;        15010/TCP,15012/TCP,443/TCP,15014/TCP                                        167m    app=istiod,istio=pilot</span>
<span class="c">#    ...</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deploy/deploy-websrv <span class="nt">-c</span> istio-proxy <span class="nt">--</span> netstat <span class="nt">-antp</span>
<span class="c"># =&gt; Active Internet connections (servers and established)</span>
<span class="c">#    Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name</span>
<span class="c">#    tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      -</span>
<span class="c">#    ...</span>
<span class="c">#    tcp        0      0 127.0.0.1:33168         127.0.0.1:15020         ESTABLISHED 13/envoy</span>
<span class="c">#    tcp        0      0 127.0.0.1:33156         127.0.0.1:15020         ESTABLISHED 13/envoy</span>
<span class="c">#    tcp        0      0 172.16.1.13:15006       172.16.2.14:33006       ESTABLISHED 13/envoy</span>
<span class="c">#    tcp        0      0 172.16.1.13:59774       10.10.200.215:15012     ESTABLISHED 1/pilot-agent</span>
<span class="c">#    tcp        0      0 172.16.1.13:15006       172.16.2.14:60786       ESTABLISHED 13/envoy</span>
<span class="c">#    tcp6       0      0 127.0.0.1:15020         127.0.0.1:33168         ESTABLISHED 1/pilot-agent</span>
<span class="c">#    tcp6       0      0 127.0.0.1:15020         127.0.0.1:33156         ESTABLISHED 1/pilot-agent</span>

<span class="c"># istiod 정보 같이 확인 : 출력되는 IP가 누구인지 확인 해보자</span>
<span class="nv">$ </span>kubectl get pod,svc <span class="nt">-A</span> <span class="nt">-owide</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deploy/istiod <span class="nt">-n</span> istio-system <span class="nt">--</span> ps <span class="nt">-ef</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deploy/istiod <span class="nt">-n</span> istio-system <span class="nt">--</span> netstat <span class="nt">-antp</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deploy/istiod <span class="nt">-n</span> istio-system <span class="nt">--</span> ss <span class="nt">-nlp</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deploy/istiod <span class="nt">-n</span> istio-system <span class="nt">--</span> ss <span class="nt">-np</span>
<span class="c"># =&gt; Netid  State  Recv-Q  Send-Q         Local Address:Port           Peer Address:Port   Process</span>
<span class="c">#    tcp    ESTAB  0       0                172.16.3.16:33552           10.10.200.1:443     users:((&amp;quot;pilot-discovery&amp;quot;,pid=1,fd=7))</span>
<span class="c">#    tcp    ESTAB  0       0       [::ffff:172.16.3.16]:15012  [::ffff:172.16.2.14]:37102   users:((&amp;quot;pilot-discovery&amp;quot;,pid=1,fd=13))</span>
<span class="c">#    tcp    ESTAB  0       0       [::ffff:172.16.3.16]:15012  [::ffff:172.16.1.13]:56560   users:((&amp;quot;pilot-discovery&amp;quot;,pid=1,fd=14))</span>
</code></pre></div></div>

<ul>
  <li>istio-proxy, istiod가 각각 사용하는 포트 정보 <a href="https://istio.io/latest/docs/ops/deployment/application-requirements/">링크</a>
    <ul>
      <li>
        <p>istio-proxy</p>

        <table>
          <thead>
            <tr>
              <th>Port</th>
              <th>Protocol</th>
              <th>Description</th>
              <th>Pod-internal only</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>15000</td>
              <td>TCP</td>
              <td>Envoy admin port (commands/diagnostics)</td>
              <td>Yes</td>
            </tr>
            <tr>
              <td>15001</td>
              <td>TCP</td>
              <td>Envoy outbound</td>
              <td>No</td>
            </tr>
            <tr>
              <td>15004</td>
              <td>HTTP</td>
              <td>Debug port</td>
              <td>Yes</td>
            </tr>
            <tr>
              <td>15006</td>
              <td>TCP</td>
              <td>Envoy inbound</td>
              <td>No</td>
            </tr>
            <tr>
              <td>15008</td>
              <td>HTTP2</td>
              <td>HBONE mTLS tunnel port</td>
              <td>No</td>
            </tr>
            <tr>
              <td>15020</td>
              <td>HTTP</td>
              <td>Merged Prometheus telemetry from Istio agent, Envoy, and application No</td>
              <td>No</td>
            </tr>
            <tr>
              <td>15021</td>
              <td>HTTP</td>
              <td>Health checks</td>
              <td>No</td>
            </tr>
            <tr>
              <td>15053</td>
              <td>DNS</td>
              <td>DNS port, if capture is enabled</td>
              <td>Yes</td>
            </tr>
            <tr>
              <td>15090</td>
              <td>HTTP</td>
              <td>Envoy Prometheus telemetry</td>
              <td>No</td>
            </tr>
          </tbody>
        </table>
      </li>
      <li>
        <p>istiod (컨트롤플레인)</p>

        <table>
          <thead>
            <tr>
              <th>Port</th>
              <th>Protocol</th>
              <th>Description</th>
              <th>Pod-internal only</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>443</td>
              <td>HTTPS</td>
              <td>Webhooks service port</td>
              <td>No</td>
            </tr>
            <tr>
              <td>8080</td>
              <td>HTTP</td>
              <td>Debug interface (deprecated, container port only)</td>
              <td>No</td>
            </tr>
            <tr>
              <td>15010</td>
              <td>GRPC</td>
              <td>XDS and CA services (Plaintext, only for secure networks)</td>
              <td>No</td>
            </tr>
            <tr>
              <td>15012</td>
              <td>GRPC</td>
              <td>XDS and CA services (TLS and mTLS, recommended for production use)</td>
              <td>No</td>
            </tr>
            <tr>
              <td>15014</td>
              <td>HTTP</td>
              <td>Control plane monitoring</td>
              <td>No</td>
            </tr>
            <tr>
              <td>15017</td>
              <td>HTTPS</td>
              <td>Webhook container port, forwarded from 443</td>
              <td>No</td>
            </tr>
          </tbody>
        </table>
      </li>
    </ul>
  </li>
  <li>Istio - Istio proxy와 Envoy 프로세스간 유닉스 도메인 소켓 통신 확인</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 파드 확인</span>
<span class="nv">$ </span>kubectl get pod
<span class="c"># =&gt; NAME                             READY   STATUS    RESTARTS   AGE</span>
<span class="c">#    deploy-websrv-778ffd6947-cxf5k   2/2     Running   0          106m</span>

<span class="c"># istio 컨테이너 접속</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deploy/deploy-websrv <span class="nt">-c</span> istio-proxy <span class="nt">--</span> bash
<span class="nt">---------------------------------------------------------------</span>
<span class="c"># SDS, XDS 는 소켓 타입</span>
<span class="nv">$ </span><span class="nb">ls</span> <span class="nt">-al</span> /etc/istio/proxy
<span class="c"># =&gt; total 24</span>
<span class="c">#    drwxrwxrwt 2 root        root          100 Oct 19 06:43 .</span>
<span class="c">#    drwxr-xr-x 4 root        root         4096 Oct 19 06:43 ..</span>
<span class="c">#    srw-rw-rw- 1 istio-proxy istio-proxy     0 Oct 19 06:43 XDS</span>
<span class="c">#    -rw-r--r-- 1 istio-proxy istio-proxy 13644 Oct 19 06:43 envoy-rev.json</span>
<span class="c">#    -rw-r--r-- 1 istio-proxy istio-proxy  2747 Oct 19 06:43 grpc-bootstrap.json</span>

<span class="c"># .json 파일 확인</span>
<span class="nv">$ </span>more /etc/istio/proxy/envoy-rev.json
<span class="c"># =&gt; {</span>
<span class="c">#      &amp;quot;node&amp;quot;: {</span>
<span class="c">#        &amp;quot;id&amp;quot;: &amp;quot;sidecar~172.16.1.13~deploy-websrv-778ffd6947-cxf5k.default~default.svc.cluster.local&amp;quot;,</span>
<span class="c">#        &amp;quot;cluster&amp;quot;: &amp;quot;deploy-websrv.default&amp;quot;,</span>
<span class="c">#    ...</span>
<span class="c">#      &amp;quot;admin&amp;quot;: {</span>
<span class="c">#        &amp;quot;access_log&amp;quot;: [</span>
<span class="c">#          {</span>
<span class="c">#            &amp;quot;name&amp;quot;: &amp;quot;envoy.access_loggers.file&amp;quot;,</span>
<span class="c">#            &amp;quot;typed_config&amp;quot;: {</span>
<span class="c">#              &amp;quot;@type&amp;quot;: &amp;quot;type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog&amp;quot;,</span>
<span class="c">#              &amp;quot;path&amp;quot;: &amp;quot;/dev/null&amp;quot;</span>
<span class="c">#            }</span>
<span class="c">#          }</span>
<span class="c">#        ],</span>
<span class="c">#        &amp;quot;profile_path&amp;quot;: &amp;quot;/var/lib/istio/data/envoy.prof&amp;quot;,</span>
<span class="c">#        &amp;quot;address&amp;quot;: {</span>
<span class="c">#          &amp;quot;socket_address&amp;quot;: {</span>
<span class="c">#            &amp;quot;address&amp;quot;: &amp;quot;127.0.0.1&amp;quot;,</span>
<span class="c">#            &amp;quot;port_value&amp;quot;: 15000</span>
<span class="c">#          }</span>
<span class="c">#        }</span>
<span class="c">#      },</span>
<span class="c">#    ...</span>
<span class="c">#    &amp;quot;dynamic_resources&amp;quot;: {</span>
<span class="c">#        &amp;quot;lds_config&amp;quot;: {</span>
<span class="c">#          &amp;quot;ads&amp;quot;: {},</span>
<span class="c">#          &amp;quot;initial_fetch_timeout&amp;quot;: &amp;quot;0s&amp;quot;,</span>
<span class="c">#          &amp;quot;resource_api_version&amp;quot;: &amp;quot;V3&amp;quot;</span>
<span class="c">#        },</span>
<span class="c">#        &amp;quot;cds_config&amp;quot;: {</span>
<span class="c">#          &amp;quot;ads&amp;quot;: {},</span>
<span class="c">#          &amp;quot;initial_fetch_timeout&amp;quot;: &amp;quot;0s&amp;quot;,</span>
<span class="c">#          &amp;quot;resource_api_version&amp;quot;: &amp;quot;V3&amp;quot;</span>
<span class="c">#        },</span>
<span class="c">#        &amp;quot;ads_config&amp;quot;: {</span>
<span class="c">#          &amp;quot;api_type&amp;quot;: &amp;quot;GRPC&amp;quot;,</span>
<span class="c">#          &amp;quot;set_node_on_first_message_only&amp;quot;: true,</span>
<span class="c">#          &amp;quot;transport_api_version&amp;quot;: &amp;quot;V3&amp;quot;,</span>
<span class="c">#          &amp;quot;grpc_services&amp;quot;: [</span>
<span class="c">#            {</span>
<span class="c">#              &amp;quot;envoy_grpc&amp;quot;: {</span>
<span class="c">#                &amp;quot;cluster_name&amp;quot;: &amp;quot;xds-grpc&amp;quot;</span>
<span class="c">#    ...</span>
<span class="c">#    &amp;quot;static_resources&amp;quot;: {</span>
<span class="c">#        &amp;quot;clusters&amp;quot;: [</span>
<span class="c">#          {</span>
<span class="c">#            ...</span>
<span class="c">#            &amp;quot;name&amp;quot;: &amp;quot;xds-grpc&amp;quot;,</span>
<span class="c">#            &amp;quot;alt_stat_name&amp;quot;: &amp;quot;xds-grpc;&amp;quot;,</span>
<span class="c">#            &amp;quot;type&amp;quot; : &amp;quot;STATIC&amp;quot;,</span>
<span class="c">#            &amp;quot;connect_timeout&amp;quot;: &amp;quot;1s&amp;quot;,</span>
<span class="c">#            &amp;quot;lb_policy&amp;quot;: &amp;quot;ROUND_ROBIN&amp;quot;,</span>
<span class="c">#            &amp;quot;load_assignment&amp;quot;: {</span>
<span class="c">#              &amp;quot;cluster_name&amp;quot;: &amp;quot;xds-grpc&amp;quot;,</span>
<span class="c">#              &amp;quot;endpoints&amp;quot;: [{</span>
<span class="c">#                &amp;quot;lb_endpoints&amp;quot;: [{</span>
<span class="c">#                  &amp;quot;endpoint&amp;quot;: {</span>
<span class="c">#                    &amp;quot;address&amp;quot;:{</span>
<span class="c">#                      &amp;quot;pipe&amp;quot;: {</span>
<span class="c">#                        &amp;quot;path&amp;quot;: &amp;quot;./etc/istio/proxy/XDS&amp;quot;</span>
<span class="c">#                      }</span>
<span class="c">#    ...</span>
<span class="c">#    &amp;quot;listeners&amp;quot;:[</span>
<span class="c">#      ...</span>
<span class="c">#      &amp;quot;address&amp;quot;: {</span>
<span class="c">#        &amp;quot;socket_address&amp;quot;: {</span>
<span class="c">#          &amp;quot;protocol&amp;quot;: &amp;quot;TCP&amp;quot;,</span>
<span class="c">#          &amp;quot;address&amp;quot;: &amp;quot;0.0.0.0&amp;quot;,</span>
<span class="c">#          &amp;quot;port_value&amp;quot;: 15090</span>
<span class="c">#        }</span>
<span class="c">#      },</span>
<span class="c">#     &amp;quot;filter_chains&amp;quot;: [</span>
<span class="c">#                &amp;quot;filters&amp;quot;: [</span>
<span class="c">#                  {</span>
<span class="c">#                    &amp;quot;name&amp;quot;: &amp;quot;envoy.filters.network.http_connection_manager&amp;quot;,</span>
<span class="c">#                    &amp;quot;typed_config&amp;quot;: {</span>
<span class="c">#                      &amp;quot;@type&amp;quot;: &amp;quot;type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager&amp;quot;,</span>
<span class="c">#                      &amp;quot;codec_type&amp;quot;: &amp;quot;AUTO&amp;quot;,</span>
<span class="c">#                      &amp;quot;stat_prefix&amp;quot;: &amp;quot;agent&amp;quot;,</span>
<span class="c">#                      &amp;quot;route_config&amp;quot;: {</span>
<span class="c">#                        &amp;quot;virtual_hosts&amp;quot;: [</span>
<span class="c">#                          {</span>
<span class="c">#                            &amp;quot;name&amp;quot;: &amp;quot;backend&amp;quot;,</span>
<span class="c">#                            &amp;quot;domains&amp;quot;: [</span>
<span class="c">#                              &amp;quot;*&amp;quot;</span>
<span class="c">#                            ],</span>
<span class="c">#                            &amp;quot;routes&amp;quot;: [</span>
<span class="c">#                              {</span>
<span class="c">#                                &amp;quot;match&amp;quot;: {</span>
<span class="c">#                                  &amp;quot;prefix&amp;quot;: &amp;quot;/healthz/ready&amp;quot;</span>
<span class="c">#                                },</span>
<span class="c">#                                &amp;quot;route&amp;quot;: {</span>
<span class="c">#                                  &amp;quot;cluster&amp;quot;: &amp;quot;agent&amp;quot;</span>
<span class="c">#    ...</span>

<span class="nv">$ </span>more /etc/istio/proxy/grpc-bootstrap.json
<span class="c"># =&gt; {</span>
<span class="c">#      &amp;quot;xds_servers&amp;quot;: [</span>
<span class="c">#        {</span>
<span class="c">#          &amp;quot;server_uri&amp;quot;: &lt;span style="color: red;"&gt;&amp;quot;unix:///etc/istio/proxy/XDS&amp;quot;,&lt;/span&gt;</span>
<span class="c">#          ...</span>
<span class="c">#        }</span>
<span class="c">#      ],</span>
<span class="c">#      &amp;quot;node&amp;quot;: {</span>
<span class="c">#        &amp;quot;id&amp;quot;: &amp;quot;sidecar~172.16.1.13~deploy-websrv-778ffd6947-cxf5k.default~default.svc.cluster.local&amp;quot;,</span>
<span class="c">#        &amp;quot;metadata&amp;quot;: {</span>
<span class="c">#          &amp;quot;ANNOTATIONS&amp;quot;: {</span>
<span class="c">#            &amp;quot;istio.io/rev&amp;quot;: &amp;quot;default&amp;quot;,</span>
<span class="c">#            &amp;quot;kubectl.kubernetes.io/default-container&amp;quot;: &amp;quot;deploy-websrv&amp;quot;,</span>
<span class="c">#            &amp;quot;kubectl.kubernetes.io/default-logs-container&amp;quot;: &amp;quot;deploy-websrv&amp;quot;,</span>
<span class="c">#            ...</span>
<span class="c">#            &amp;quot;sidecar.istio.io/status&amp;quot;: &amp;quot;{\&amp;quot;initContainers\&amp;quot;:[\&amp;quot;istio-init\&amp;quot;],\&amp;quot;containers\&amp;quot;:[\&amp;quot;istio-proxy\&amp;quot;],\&amp;quot;volumes\&amp;quot;:[\&amp;quot;workload-socket\&amp;quot;,\&amp;quot;credential-socket\&amp;quot;,\&amp;quot;workload-certs\&amp;quot;</span>
<span class="c">#    ,\&amp;quot;istio-envoy\&amp;quot;,\&amp;quot;istio-data\&amp;quot;,\&amp;quot;istio-podinfo\&amp;quot;,\&amp;quot;istio-token\&amp;quot;,\&amp;quot;istiod-ca-cert\&amp;quot;],\&amp;quot;imagePullSecrets\&amp;quot;:null,\&amp;quot;revision\&amp;quot;:\&amp;quot;default\&amp;quot;}&amp;quot;</span>
<span class="c">#          },</span>
<span class="c">#          ...</span>
<span class="c">#          &amp;quot;SERVICE_ACCOUNT&amp;quot;: &amp;quot;kans-nginx&amp;quot;,</span>
<span class="c">#          &amp;quot;WORKLOAD_NAME&amp;quot;: &amp;quot;deploy-websrv&amp;quot;</span>
<span class="c">#        },</span>
<span class="c">#        &amp;quot;locality&amp;quot;: {},</span>
<span class="c">#        &amp;quot;UserAgentVersionType&amp;quot;: null</span>
<span class="c">#      },</span>
<span class="c">#      &amp;quot;server_listener_resource_name_template&amp;quot;: &amp;quot;xds.istio.io/grpc/lds/inbound/%s&amp;quot;</span>
<span class="c">#    }</span>

<span class="c"># display only Unix domain sockets : Listener 과 ESTAB 상태 정보 확인</span>
<span class="nv">$ </span>ss <span class="nt">-xpl</span>
<span class="c"># =&gt; Netid State  Recv-Q Send-Q                              Local Address:Port  Peer Address:PortProcess</span>
<span class="c">#    u_str LISTEN 0      4096                          etc/istio/proxy/XDS 54694            * 0    users:((&amp;quot;pilot-agent&amp;quot;,pid=1,fd=11))</span>
<span class="c">#    u_str LISTEN 0      4096   var/run/secrets/workload-spiffe-uds/socket 54693            * 0    users:((&amp;quot;pilot-agent&amp;quot;,pid=1,fd=9))</span>
<span class="nv">$ </span>ss <span class="nt">-xp</span>
<span class="c"># =&gt; Netid State Recv-Q Send-Q                              Local Address:Port  Peer Address:Port Process</span>
<span class="c">#    u_str ESTAB 0      0                             etc/istio/proxy/XDS 79304            * 82228 users:((&amp;quot;pilot-agent&amp;quot;,pid=1,fd=10))</span>
<span class="c">#    u_str ESTAB 0      0      var/run/secrets/workload-spiffe-uds/socket 54709            * 54162 users:((&amp;quot;pilot-agent&amp;quot;,pid=1,fd=16))</span>
<span class="c">#    u_str ESTAB 0      0                                               * 82228            * 79304 users:((&amp;quot;envoy&amp;quot;,pid=13,fd=19))</span>
<span class="c">#    u_str ESTAB 0      0                                               * 54162            * 54709 users:((&amp;quot;envoy&amp;quot;,pid=13,fd=32))</span>

<span class="c"># display only TCP sockets and display only IP version 4 sockets : TCP 상태 정보 확인</span>
<span class="nv">$ </span>ss <span class="nt">-4tpl</span>
<span class="c"># =&gt; LISTEN 0      4096         0.0.0.0:15090      0.0.0.0:*    users:((&amp;quot;envoy&amp;quot;,pid=13,fd=21))</span>
<span class="c">#    LISTEN 0      4096         0.0.0.0:15090      0.0.0.0:*    users:((&amp;quot;envoy&amp;quot;,pid=13,fd=20))</span>
<span class="c">#    LISTEN 0      4096         0.0.0.0:15021      0.0.0.0:*    users:((&amp;quot;envoy&amp;quot;,pid=13,fd=23))</span>
<span class="c">#    LISTEN 0      4096         0.0.0.0:15021      0.0.0.0:*    users:((&amp;quot;envoy&amp;quot;,pid=13,fd=22))</span>
<span class="c">#    LISTEN 0      4096         0.0.0.0:15001      0.0.0.0:*    users:((&amp;quot;envoy&amp;quot;,pid=13,fd=35))</span>
<span class="c">#    LISTEN 0      4096         0.0.0.0:15001      0.0.0.0:*    users:((&amp;quot;envoy&amp;quot;,pid=13,fd=34))</span>
<span class="c">#    LISTEN 0      4096         0.0.0.0:15006      0.0.0.0:*    users:((&amp;quot;envoy&amp;quot;,pid=13,fd=37))</span>
<span class="c">#    LISTEN 0      4096         0.0.0.0:15006      0.0.0.0:*    users:((&amp;quot;envoy&amp;quot;,pid=13,fd=36))</span>
<span class="c">#    LISTEN 0      4096       127.0.0.1:15000      0.0.0.0:*    users:((&amp;quot;envoy&amp;quot;,pid=13,fd=18))</span>
<span class="c">#    LISTEN 0      4096       127.0.0.1:15004      0.0.0.0:*    users:((&amp;quot;pilot-agent&amp;quot;,pid=1,fd=13))</span>
<span class="c">#    LISTEN 0      511          0.0.0.0:http       0.0.0.0:*</span>
</code></pre></div></div>

<h3 id="bookinfo-실습-및-istio-기능-확인">Bookinfo 실습 및 Istio 기능 확인</h3>

<h4 id="bookinfo">Bookinfo</h4>

<ul>
  <li>Bookinfo는 istio의 기능을 설명하기위한 MSA(Microservices Architecture)  기반의 예제 어플리케이션입니다.
<img src="/assets/2024/kans-3th/w7/20241019_kans_w7_11.png" alt="img.png" class="image-center" />
<em class="image-caption">Bookinfo 어플리케이션 구성</em>
    <ul>
      <li>productpage, details, reviews, ratings 서비스로 구성됩니다. <a href="https://istio.io/latest/docs/examples/bookinfo/">소개 링크</a></li>
      <li><strong>ProductPage</strong> 페이지에서 요청을 받으면, 도서 리뷰를 보여주는 <strong>Reviews</strong> 서비스와 도서 상세 정보를 보여주는 <strong>Details</strong> 서비스에 접속하고,</li>
      <li>ProductPage 는 <strong>Reviews</strong> 와 <strong>Details</strong> 결과를 사용자에게 응답합니다.</li>
      <li><strong>Reviews</strong> 서비스는 v1, v2, v3 세 개의 버전이 있고 v2, v3 버전의 경우 <strong>Ratings</strong> 서비스에 접소갛여 도서에 대한 5단계 평가를 가져옵니다.</li>
      <li>Reviews 서비스의 차이는, v1은 Rating 이 <strong>없고</strong>, v2는 <strong>검은색</strong> 별로 Ratings 가 표시되며, v3는 <strong>색깔이</strong> 있는 별로 Ratings 가 표시됩니다.</li>
    </ul>
  </li>
  <li>설치 및 확인</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 모니터링</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s1">'kubectl get pod -owide;echo;kubectl get svc'</span>

<span class="c"># Bookinfo 애플리케이션 배포</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$ISTIOV</span>
<span class="c"># =&gt; 1.23.2</span>
<span class="nv">$ </span><span class="nb">cat</span> ~/istio-<span class="nv">$ISTIOV</span>/samples/bookinfo/platform/kube/bookinfo.yaml
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> ~/istio-<span class="nv">$ISTIOV</span>/samples/bookinfo/platform/kube/bookinfo.yaml
<span class="c"># =&gt; service/details created</span>
<span class="c">#    serviceaccount/bookinfo-details created</span>
<span class="c">#    deployment.apps/details-v1 created</span>
<span class="c">#    service/ratings created</span>
<span class="c">#    serviceaccount/bookinfo-ratings created</span>
<span class="c">#    deployment.apps/ratings-v1 created</span>
<span class="c">#    service/reviews created</span>
<span class="c">#    serviceaccount/bookinfo-reviews created</span>
<span class="c">#    deployment.apps/reviews-v1 created</span>
<span class="c">#    deployment.apps/reviews-v2 created</span>
<span class="c">#    deployment.apps/reviews-v3 created</span>
<span class="c">#    service/productpage created</span>
<span class="c">#    serviceaccount/bookinfo-productpage created</span>
<span class="c">#    deployment.apps/productpage-v1 created</span>

<span class="c"># 확인</span>
<span class="nv">$ </span>kubectl get all,sa
<span class="c"># =&gt; NAME                                 READY   STATUS    RESTARTS   AGE</span>
<span class="c">#    pod/details-v1-65cfcf56f9-8465x      2/2     Running   0          87s</span>
<span class="c">#    pod/productpage-v1-d5789fdfb-f8gdf   2/2     Running   0          86s</span>
<span class="c">#    pod/ratings-v1-7c9bd4b87f-s9gxs      2/2     Running   0          87s</span>
<span class="c">#    pod/reviews-v1-6584ddcf65-gnc9j      2/2     Running   0          87s</span>
<span class="c">#    pod/reviews-v2-6f85cb9b7c-cfc68      2/2     Running   0          87s</span>
<span class="c">#    pod/reviews-v3-6f5b775685-cw7tc      2/2     Running   0          87s</span>
<span class="c">#    </span>
<span class="c">#    NAME                    TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE</span>
<span class="c">#    service/details         ClusterIP   10.10.200.54    &amp;lt;none&amp;gt;        9080/TCP   87s</span>
<span class="c">#    service/productpage     ClusterIP   10.10.200.184   &amp;lt;none&amp;gt;        9080/TCP   87s</span>
<span class="c">#    service/ratings         ClusterIP   10.10.200.163   &amp;lt;none&amp;gt;        9080/TCP   87s</span>
<span class="c">#    service/reviews         ClusterIP   10.10.200.214   &amp;lt;none&amp;gt;        9080/TCP   87s</span>
<span class="c">#    </span>
<span class="c">#    NAME                             READY   UP-TO-DATE   AVAILABLE   AGE</span>
<span class="c">#    deployment.apps/details-v1       1/1     1            1           87s</span>
<span class="c">#    deployment.apps/productpage-v1   1/1     1            1           86s</span>
<span class="c">#    deployment.apps/ratings-v1       1/1     1            1           87s</span>
<span class="c">#    deployment.apps/reviews-v1       1/1     1            1           87s</span>
<span class="c">#    deployment.apps/reviews-v2       1/1     1            1           87s</span>
<span class="c">#    deployment.apps/reviews-v3       1/1     1            1           87s</span>
<span class="c">#    </span>
<span class="c">#    NAME                                       DESIRED   CURRENT   READY   AGE</span>
<span class="c">#    replicaset.apps/details-v1-65cfcf56f9      1         1         1       87s</span>
<span class="c">#    replicaset.apps/productpage-v1-d5789fdfb   1         1         1       86s</span>
<span class="c">#    replicaset.apps/ratings-v1-7c9bd4b87f      1         1         1       87s</span>
<span class="c">#    replicaset.apps/reviews-v1-6584ddcf65      1         1         1       87s</span>
<span class="c">#    replicaset.apps/reviews-v2-6f85cb9b7c      1         1         1       87s</span>
<span class="c">#    replicaset.apps/reviews-v3-6f5b775685      1         1         1       87s</span>
<span class="c">#    </span>
<span class="c">#    NAME                                  SECRETS   AGE</span>
<span class="c">#    serviceaccount/bookinfo-details       0         87s</span>
<span class="c">#    serviceaccount/bookinfo-productpage   0         86s</span>
<span class="c">#    serviceaccount/bookinfo-ratings       0         87s</span>
<span class="c">#    serviceaccount/bookinfo-reviews       0         87s</span>

<span class="c"># product 웹 접속 확인</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="s2">"</span><span class="si">$(</span>kubectl get pod <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>ratings <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.items[0].metadata.name}'</span><span class="si">)</span><span class="s2">"</span> <span class="nt">-c</span> ratings <span class="nt">--</span> curl <span class="nt">-sS</span> productpage:9080/productpage | <span class="nb">grep</span> <span class="nt">-o</span> <span class="s2">"&lt;title&gt;.*&lt;/title&gt;"</span>
<span class="c"># =&gt; &amp;lt;title&amp;gt;Simple Bookstore App&amp;lt;/title&amp;gt;</span>

<span class="c"># 로그</span>
<span class="nv">$ </span>kubetail <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>productpage <span class="nt">-f</span>
</code></pre></div></div>

<h4 id="istio를-통한-인입-기본-설정">Istio를 통한 인입 기본 설정</h4>

<h5 id="istio-gatewayvirtualservice-설정">Istio Gateway/VirtualService 설정</h5>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Istio Gateway/VirtualService 설정</span>
<span class="nv">$ </span><span class="nb">cat</span> ~/istio-<span class="nv">$ISTIOV</span>/samples/bookinfo/networking/bookinfo-gateway.yaml
<span class="c"># =&gt; apiVersion: networking.istio.io/v1alpha3</span>
<span class="c">#    kind: Gateway</span>
<span class="c">#    metadata:</span>
<span class="c">#      name: bookinfo-gateway</span>
<span class="c">#    spec:</span>
<span class="c">#      # The selector matches the ingress gateway pod labels.</span>
<span class="c">#      # If you installed Istio using Helm following the standard documentation, this would be &amp;quot;istio=ingress&amp;quot;</span>
<span class="c">#      selector:</span>
<span class="c">#        istio: ingressgateway # use istio default controller</span>
<span class="c">#      servers:</span>
<span class="c">#      - port:</span>
<span class="c">#          number: 8080</span>
<span class="c">#          name: http</span>
<span class="c">#          protocol: HTTP</span>
<span class="c">#        hosts:</span>
<span class="c">#        - &amp;quot;*&amp;quot;</span>
<span class="c">#    ---</span>
<span class="c">#    apiVersion: networking.istio.io/v1alpha3</span>
<span class="c">#    kind: VirtualService</span>
<span class="c">#    metadata:</span>
<span class="c">#      name: bookinfo</span>
<span class="c">#    spec:</span>
<span class="c">#      hosts:</span>
<span class="c">#      - &amp;quot;*&amp;quot;</span>
<span class="c">#      gateways:</span>
<span class="c">#      - bookinfo-gateway</span>
<span class="c">#      http:</span>
<span class="c">#      - match:</span>
<span class="c">#        - uri:</span>
<span class="c">#            exact: /productpage</span>
<span class="c">#        - uri:</span>
<span class="c">#            prefix: /static</span>
<span class="c">#        - uri:</span>
<span class="c">#            exact: /login</span>
<span class="c">#        - uri:</span>
<span class="c">#            exact: /logout</span>
<span class="c">#        - uri:</span>
<span class="c">#            prefix: /api/v1/products</span>
<span class="c">#        route:</span>
<span class="c">#        - destination:</span>
<span class="c">#            host: productpage</span>
<span class="c">#            port:</span>
<span class="c">#              number: 9080</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> ~/istio-<span class="nv">$ISTIOV</span>/samples/bookinfo/networking/bookinfo-gateway.yaml
<span class="c"># =&gt; gateway.networking.istio.io/bookinfo-gateway created</span>
<span class="c">#    virtualservice.networking.istio.io/bookinfo created</span>

<span class="c"># 확인</span>
<span class="nv">$ </span>kubectl get gw,vs
<span class="nv">$ </span>istioctl proxy-status
<span class="c"># =&gt; NAME                                                   CLUSTER        CDS                LDS                EDS                RDS                ECDS        ISTIOD                      VERSION</span>
<span class="c">#    deploy-websrv-778ffd6947-cxf5k.default                 Kubernetes     SYNCED (5m9s)      SYNCED (5m9s)      SYNCED (4m24s)     SYNCED (5m9s)      IGNORED     istiod-7f8b586864-8mc4c     1.23.2</span>
<span class="c">#    details-v1-65cfcf56f9-8465x.default                    Kubernetes     SYNCED (4m43s)     SYNCED (4m43s)     SYNCED (4m24s)     SYNCED (4m43s)     IGNORED     istiod-7f8b586864-8mc4c     1.23.2</span>
<span class="c">#    istio-ingressgateway-5f9f654d46-l7mqp.istio-system     Kubernetes     SYNCED (10s)       SYNCED (10s)       SYNCED (4m24s)     SYNCED (10s)       IGNORED     istiod-7f8b586864-8mc4c     1.23.2</span>
<span class="c">#    productpage-v1-d5789fdfb-f8gdf.default                 Kubernetes     SYNCED (4m53s)     SYNCED (4m53s)     SYNCED (4m24s)     SYNCED (4m53s)     IGNORED     istiod-7f8b586864-8mc4c     1.23.2</span>
<span class="c">#    ratings-v1-7c9bd4b87f-s9gxs.default                    Kubernetes     SYNCED (4m24s)     SYNCED (4m24s)     SYNCED (4m24s)     SYNCED (4m24s)     IGNORED     istiod-7f8b586864-8mc4c     1.23.2</span>
<span class="c">#    reviews-v1-6584ddcf65-gnc9j.default                    Kubernetes     SYNCED (4m47s)     SYNCED (4m47s)     SYNCED (4m24s)     SYNCED (4m47s)     IGNORED     istiod-7f8b586864-8mc4c     1.23.2</span>
<span class="c">#    reviews-v2-6f85cb9b7c-cfc68.default                    Kubernetes     SYNCED (4m41s)     SYNCED (4m41s)     SYNCED (4m24s)     SYNCED (4m41s)     IGNORED     istiod-7f8b586864-8mc4c     1.23.2</span>
<span class="c">#    reviews-v3-6f5b775685-cw7tc.default                    Kubernetes     SYNCED (4m43s)     SYNCED (4m43s)     SYNCED (4m24s)     SYNCED (4m43s)     IGNORED     istiod-7f8b586864-8mc4c     1.23.2</span>

<span class="c"># productpage 파드의 istio-proxy 로그 확인 Access log 가 출력 - Default access log format : 링크</span>
<span class="nv">$ </span>kubetail <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>productpage <span class="nt">-c</span> istio-proxy <span class="nt">-f</span>
</code></pre></div></div>

<ul>
  <li><strong>k3s-m</strong> NodePort 접속 확인</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#</span>
<span class="nv">$ </span><span class="nb">export </span><span class="nv">IGWHTTP</span><span class="o">=</span><span class="si">$(</span>kubectl get service <span class="nt">-n</span> istio-system istio-ingressgateway <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.spec.ports[1].nodePort}'</span><span class="si">)</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$IGWHTTP</span>
<span class="c"># =&gt; 31677</span>

<span class="c"># 접속 확인</span>
<span class="nv">$ </span>kubectl get svc <span class="nt">-n</span> istio-system istio-ingressgateway
<span class="c"># =&gt; NAME                   TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                                      AGE</span>
<span class="c">#    istio-ingressgateway   NodePort   10.10.200.171   &amp;lt;none&amp;gt;        15021:30953/TCP,80:31677/TCP,443:30737/TCP,31400:30617/TCP,15443:31668/TCP   3h31m</span>
<span class="nv">$ </span>curl <span class="nt">-s</span> http://localhost:<span class="nv">$IGWHTTP</span>/productpage
<span class="nv">$ </span>curl <span class="nt">-s</span> http://192.168.10.101:<span class="nv">$IGWHTTP</span>/productpage
<span class="nv">$ </span>curl <span class="nt">-s</span> http://192.168.10.102:<span class="nv">$IGWHTTP</span>/productpage
<span class="c"># =&gt; &amp;lt;meta charset=&amp;quot;utf-8&amp;quot;&amp;gt;</span>
<span class="c">#    &amp;lt;meta http-equiv=&amp;quot;X-UA-Compatible&amp;quot; content=&amp;quot;IE=edge&amp;quot;&amp;gt;</span>
<span class="c">#    &amp;lt;meta name=&amp;quot;viewport&amp;quot; content=&amp;quot;width=device-width, initial-scale=1.0&amp;quot;&amp;gt;</span>
<span class="c">#    </span>
<span class="c">#    &amp;lt;title&amp;gt;Simple Bookstore App&amp;lt;/title&amp;gt;</span>
<span class="c">#    ...</span>

<span class="c"># 정보 확인</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$MYDOMAIN</span>
<span class="c"># =&gt; sweetlittlebird.com</span>
<span class="nv">$ </span><span class="nb">cat</span> /etc/hosts
<span class="c"># =&gt; ...</span>
<span class="c">#    127.0.2.1 k3s-m k3s-m</span>
<span class="c">#    192.168.10.10 k3s-m</span>
<span class="c">#    192.168.10.101 k3s-w1</span>
<span class="c">#    192.168.10.102 k3s-w2</span>
<span class="c">#    192.168.10.103 k3s-w3</span>
<span class="c">#    192.168.10.10 sweetlittlebird.com</span>

<span class="c">#</span>
<span class="nv">$ </span>curl <span class="nt">-s</span> http://<span class="nv">$MYDOMAIN</span>:<span class="nv">$IGWHTTP</span>/productpage
<span class="c"># =&gt; &amp;lt;meta charset=&amp;quot;utf-8&amp;quot;&amp;gt;</span>
<span class="c">#    &amp;lt;meta http-equiv=&amp;quot;X-UA-Compatible&amp;quot; content=&amp;quot;IE=edge&amp;quot;&amp;gt;</span>
<span class="c">#    &amp;lt;meta name=&amp;quot;viewport&amp;quot; content=&amp;quot;width=device-width, initial-scale=1.0&amp;quot;&amp;gt;</span>
<span class="c">#    </span>
<span class="c">#    &amp;lt;title&amp;gt;Simple Bookstore App&amp;lt;/title&amp;gt;</span>
<span class="c">#    ...</span>
</code></pre></div></div>

<ul>
  <li>자신의 PC에서 접속 확인</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$MYDOMAIN</span> <span class="nv">$IGWHTTP</span>
<span class="c"># =&gt; sweetlittlebird.com 31677</span>
<span class="nv">$ </span><span class="nb">cat</span> /etc/hosts

<span class="c">#</span>
<span class="nv">$ </span>curl <span class="nt">-v</span> <span class="nt">-s</span> <span class="nv">$MYDOMAIN</span>:<span class="nv">$IGWHTTP</span>/productpage
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_12.png" alt="img.png" class="image-center" />
<em class="image-caption">Bookinfo 접속 결과 - 새로고침 할 때 마다 다른 파드에 접속되면서 리뷰가 달라짐</em></p>

<ul>
  <li><strong>testpc 에서 접속 실행</strong></li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># istio ingress gw 를 통한 접속 테스트</span>
<span class="nv">$ </span>curl <span class="nt">-s</span> <span class="nv">$MYDOMAIN</span>:<span class="nv">$IGWHTTP</span>/productpage | <span class="nb">grep</span> <span class="nt">-o</span> <span class="s2">"&lt;title&gt;.*&lt;/title&gt;"</span>
<span class="c"># =&gt; &amp;lt;title&amp;gt;Simple Bookstore App&amp;lt;/title&amp;gt;</span>
<span class="nv">$ </span><span class="k">while </span><span class="nb">true</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$MYDOMAIN</span>:<span class="nv">$IGWHTTP</span>/productpage | <span class="nb">grep</span> <span class="nt">-o</span> <span class="s2">"&lt;title&gt;.*&lt;/title&gt;"</span> <span class="p">;</span> <span class="nb">echo</span> <span class="s2">"--------------"</span> <span class="p">;</span> <span class="nb">sleep </span>1<span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; &amp;lt;title&amp;gt;Simple Bookstore App&amp;lt;/title&amp;gt;</span>
<span class="c">#    --------------</span>
<span class="c">#    &amp;lt;title&amp;gt;Simple Bookstore App&amp;lt;/title&amp;gt;</span>
<span class="c">#    --------------</span>
<span class="c">#    ...</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span>  <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$MYDOMAIN</span>:<span class="nv">$IGWHTTP</span>/productpage | <span class="nb">grep</span> <span class="nt">-o</span> <span class="s2">"&lt;title&gt;.*&lt;/title&gt;"</span> <span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; &amp;lt;title&amp;gt;Simple Bookstore App&amp;lt;/title&amp;gt;</span>
<span class="c">#    &amp;lt;title&amp;gt;Simple Bookstore App&amp;lt;/title&amp;gt;</span>
<span class="c">#    ...</span>
</code></pre></div></div>

<h4 id="모니터링">모니터링</h4>

<ul>
  <li>옵저빌리티 연동 문서 : <a href="https://istio.io/latest/docs/ops/integrations/">링크</a></li>
</ul>

<h5 id="kiali-키알리-소개">Kiali (키알리) 소개</h5>

<ul>
  <li>Kiali는 Istio 서비스 메시의 모니터링 및 시각화 도구입니다.</li>
  <li>주 데이터 소스는 Prometheus와 Jaeger 등입니다.</li>
  <li>특히 Jaeger와 연동하여 서비스 간의 호출 관계를 시각화하여 볼 수 있습니다.</li>
  <li>Istiod의 health 상태를 확인하기 위해 istiod 파드를 직접 접속합니다. (기본 15014포트)</li>
</ul>

<h5 id="kiali-설치-및-확인">Kiali 설치 및 확인</h5>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Install Kiali and the other addons and wait for them to be deployed. : Kiali dashboard, along with Prometheus, Grafana, and Jaeger.</span>
<span class="nv">$ </span>tree ~/istio-<span class="nv">$ISTIOV</span>/samples/addons/
<span class="c"># =&gt; /root/istio-1.23.2/samples/addons/</span>
<span class="c">#    ├── README.md</span>
<span class="c">#    ├── extras</span>
<span class="c">#    │   ├── prometheus-operator.yaml</span>
<span class="c">#    │   ├── skywalking.yaml</span>
<span class="c">#    │   └── zipkin.yaml</span>
<span class="c">#    ├── grafana.yaml</span>
<span class="c">#    ├── jaeger.yaml</span>
<span class="c">#    ├── kiali.yaml</span>
<span class="c">#    ├── loki.yaml</span>
<span class="c">#    └── prometheus.yaml</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> ~/istio-<span class="nv">$ISTIOV</span>/samples/addons <span class="c"># 디렉터리에 있는 모든 yaml 자원을 생성</span>
<span class="c"># =&gt; deployment.apps/grafana created</span>
<span class="c">#    deployment.apps/jaeger created</span>
<span class="c">#    deployment.apps/kiali created</span>
<span class="c">#    deployment.apps/prometheus created</span>
<span class="c">#    ...</span>
<span class="nv">$ </span>kubectl rollout status deployment/kiali <span class="nt">-n</span> istio-system
<span class="c"># =&gt; deployment &amp;quot;kiali&amp;quot; successfully rolled out</span>

<span class="c"># 확인</span>
<span class="nv">$ </span>kubectl get all,sa,cm <span class="nt">-n</span> istio-system
<span class="nv">$ </span>kubectl get svc,ep <span class="nt">-n</span> istio-system
<span class="c"># =&gt; NAME                           TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                                      AGE</span>
<span class="c">#    service/grafana                ClusterIP   10.10.200.178   &amp;lt;none&amp;gt;        3000/TCP                                                                     69s</span>
<span class="c">#    service/istio-ingressgateway   NodePort    10.10.200.171   &amp;lt;none&amp;gt;        15021:30953/TCP,80:31677/TCP,443:30737/TCP,31400:30617/TCP,15443:31668/TCP   5h48m</span>
<span class="c">#    service/istiod                 ClusterIP   10.10.200.215   &amp;lt;none&amp;gt;        15010/TCP,15012/TCP,443/TCP,15014/TCP                                        5h48m</span>
<span class="c">#    service/jaeger-collector       ClusterIP   10.10.200.121   &amp;lt;none&amp;gt;        14268/TCP,14250/TCP,9411/TCP,4317/TCP,4318/TCP                               69s</span>
<span class="c">#    service/kiali                  ClusterIP   10.10.200.19    &amp;lt;none&amp;gt;        20001/TCP,9090/TCP                                                           68s</span>
<span class="c">#    service/loki                   ClusterIP   10.10.200.227   &amp;lt;none&amp;gt;        3100/TCP,9095/TCP                                                            68s</span>
<span class="c">#    service/loki-headless          ClusterIP   None            &amp;lt;none&amp;gt;        3100/TCP                                                                     68s</span>
<span class="c">#    service/loki-memberlist        ClusterIP   None            &amp;lt;none&amp;gt;        7946/TCP                                                                     68s</span>
<span class="c">#    service/prometheus             ClusterIP   10.10.200.148   &amp;lt;none&amp;gt;        9090/TCP                                                                     68s</span>
<span class="c">#    service/tracing                ClusterIP   10.10.200.133   &amp;lt;none&amp;gt;        80/TCP,16685/TCP                                                             69s</span>
<span class="c">#    service/zipkin                 ClusterIP   10.10.200.29    &amp;lt;none&amp;gt;        9411/TCP                                                                     69s</span>
<span class="c">#    </span>
<span class="c">#    NAME                             ENDPOINTS                                                           AGE</span>
<span class="c">#    endpoints/grafana                172.16.2.17:3000                                                    69s</span>
<span class="c">#    endpoints/istio-ingressgateway   172.16.2.14:15443,172.16.2.14:15021,172.16.2.14:31400 + 2 more...   5h48m</span>
<span class="c">#    endpoints/istiod                 172.16.3.16:15012,172.16.3.16:15010,172.16.3.16:15017 + 1 more...   5h48m</span>
<span class="c">#    endpoints/jaeger-collector       172.16.3.19:9411,172.16.3.19:14250,172.16.3.19:4317 + 2 more...     69s</span>
<span class="c">#    endpoints/kiali                  172.16.1.16:9090,172.16.1.16:20001                                  68s</span>
<span class="c">#    endpoints/loki                                                                                       68s</span>
<span class="c">#    endpoints/loki-headless                                                                              68s</span>
<span class="c">#    endpoints/loki-memberlist                                                                            68s</span>
<span class="c">#    endpoints/prometheus             172.16.3.20:9090                                                    67s</span>
<span class="c">#    endpoints/tracing                172.16.3.19:16685,172.16.3.19:16686                                 69s</span>
<span class="c">#    endpoints/zipkin                 172.16.3.19:9411                                                    69s</span>

<span class="c"># kiali 서비스 변경</span>
<span class="nv">$ </span>kubectl patch svc <span class="nt">-n</span> istio-system kiali <span class="nt">-p</span> <span class="s1">'{"spec":{"type":"NodePort"}}'</span>
<span class="c"># =&gt; service/kiali patched</span>

<span class="c"># kiali 웹 접속 주소 확인</span>
<span class="nv">$ KIALINodePort</span><span class="o">=</span><span class="si">$(</span>kubectl get svc <span class="nt">-n</span> istio-system kiali <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.spec.ports[0].nodePort<span class="o">}</span><span class="si">)</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"KIALI UI URL = http://</span><span class="si">$(</span>curl <span class="nt">-s</span> ipinfo.io/ip<span class="si">)</span><span class="s2">:</span><span class="nv">$KIALINodePort</span><span class="s2">"</span>
<span class="c"># =&gt; KIALI UI URL = http://54.123.42.212:31274</span>

<span class="c"># Grafana 서비스 변경</span>
<span class="nv">$ </span>kubectl patch svc <span class="nt">-n</span> istio-system grafana <span class="nt">-p</span> <span class="s1">'{"spec":{"type":"NodePort"}}'</span>
<span class="c"># =&gt; service/grafana patched</span>

<span class="c"># Grafana 웹 접속 주소 확인 : 7개의 대시보드</span>
<span class="nv">$ GRAFANANodePort</span><span class="o">=</span><span class="si">$(</span>kubectl get svc <span class="nt">-n</span> istio-system grafana <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.spec.ports[0].nodePort<span class="o">}</span><span class="si">)</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"Grafana URL = http://</span><span class="si">$(</span>curl <span class="nt">-s</span> ipinfo.io/ip<span class="si">)</span><span class="s2">:</span><span class="nv">$GRAFANANodePort</span><span class="s2">"</span>
<span class="c"># =&gt; Grafana URL = http://54.123.42.212:30266</span>

<span class="c"># Prometheus 서비스 변경</span>
<span class="nv">$ </span>kubectl patch svc <span class="nt">-n</span> istio-system prometheus <span class="nt">-p</span> <span class="s1">'{"spec":{"type":"NodePort"}}'</span>
<span class="c"># =&gt; service/prometheus patched</span>

<span class="c"># Prometheus 웹 접속 주소 확인</span>
<span class="nv">$ PROMENodePort</span><span class="o">=</span><span class="si">$(</span>kubectl get svc <span class="nt">-n</span> istio-system prometheus <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.spec.ports[0].nodePort<span class="o">}</span><span class="si">)</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"Prometheus URL = http://</span><span class="si">$(</span>curl <span class="nt">-s</span> ipinfo.io/ip<span class="si">)</span><span class="s2">:</span><span class="nv">$PROMENodePort</span><span class="s2">"</span>
<span class="c"># =&gt; Prometheus URL = http://54.123.42.212:30506</span>
</code></pre></div></div>

<ul>
  <li>Prometheus : Targets - 파드별로 tcp/15020의 /stats/prometheus를 통해 수집</li>
</ul>

<p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_13.png" alt="img.png" class="image-center" /></p>

<p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_14.png" alt="img.png" class="image-center" /></p>

<p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_15.png" alt="img.png" class="image-center" /></p>

<ul>
  <li>Grafana : 7개의 대시보드</li>
</ul>

<p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_17.png" alt="img.png" class="image-center" /></p>

<p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_16.png" alt="img.png" class="image-center" /></p>

<ul>
  <li>Kiali : 서비스간의 호출 관계를 시각화</li>
</ul>

<p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_18.png" alt="img.png" class="image-center" /></p>

<h5 id="kiali-키알리-대시보드-둘러보기">Kiali (키알리) 대시보드 둘러보기</h5>

<ul>
  <li>Namespace 를 default 로 선택 후 Graph (Traffic, Versioned app graph) 에서 Display 옵션 중 ‘Traffic Distribution’과
‘Traffic Animation’ 활성화, Security 체크 해서 확인해보겠습니다.</li>
  <li>트래픽을 발생시켜서 Kiali 대시보드를 확인해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># testpc 에서 아래 실행</span>
<span class="c"># 반복 접속 테스트</span>
<span class="nv">$ </span><span class="k">while </span><span class="nb">true</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$MYDOMAIN</span>:<span class="nv">$IGWHTTP</span>/productpage | <span class="nb">grep</span> <span class="nt">-o</span> <span class="s2">"&lt;title&gt;.*&lt;/title&gt;"</span> <span class="p">;</span> <span class="nb">echo</span> <span class="s2">"--------------"</span> <span class="p">;</span> <span class="nb">sleep </span>1<span class="p">;</span> <span class="k">done</span>
<span class="nv">$ </span><span class="k">while </span><span class="nb">true</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$MYDOMAIN</span>:<span class="nv">$IGWHTTP</span>/productpage | <span class="nb">grep</span> <span class="nt">-o</span> <span class="s2">"&lt;title&gt;.*&lt;/title&gt;"</span> <span class="p">;</span> <span class="nb">echo</span> <span class="s2">"--------------"</span> <span class="p">;</span> <span class="nb">sleep </span>0.1<span class="p">;</span> <span class="k">done</span>
<span class="nv">$ </span><span class="k">while </span><span class="nb">true</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$MYDOMAIN</span>:<span class="nv">$IGWHTTP</span>/productpage | <span class="nb">grep</span> <span class="nt">-o</span> <span class="s2">"&lt;title&gt;.*&lt;/title&gt;"</span> <span class="p">;</span> <span class="nb">echo</span> <span class="s2">"--------------"</span> <span class="p">;</span> <span class="nb">sleep </span>0.5<span class="p">;</span> <span class="k">done</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span>  <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$MYDOMAIN</span>:<span class="nv">$IGWHTTP</span>/productpage | <span class="nb">grep</span> <span class="nt">-o</span> <span class="s2">"&lt;title&gt;.*&lt;/title&gt;"</span> <span class="p">;</span> <span class="k">done</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..1000<span class="o">}</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$MYDOMAIN</span>:<span class="nv">$IGWHTTP</span>/productpage | <span class="nb">grep</span> <span class="nt">-o</span> <span class="s2">"&lt;title&gt;.*&lt;/title&gt;"</span> <span class="p">;</span> <span class="k">done</span>
</code></pre></div></div>

<ul>
  <li><strong>Traffic Graph</strong>에서는 트래픽 흐름을 확인할 수 있습니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_19.png" alt="img.png" class="image-center" /></p>

<ul>
  <li><strong>Workloads</strong>에서는 Log 등을 확인할 수 있고, Envoy 관련 설정 정보(Listener, Cluster, Route, Endpoint 등)를 확인할 수 있습니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_20.png" alt="img.png" class="image-center" /></p>

<ul>
  <li><strong>Istio Config</strong>에서 Istio 관련 설정을 볼 수 있고, <strong>Action</strong> 으로 Istio 관련 오브젝트를 설정/삭제 할 수 있습니다.</li>
</ul>

<h4 id="traffic-management">Traffic Management</h4>

<ul>
  <li><strong>동작 소개</strong> : 클라이언트 PC → Istio <strong>ingressgateway</strong> 파드 → (Gateway, <strong>VirtualService</strong> + <strong>DestinationRule</strong>) → Cluster(<strong>Endpoint</strong> - 파드)
    <ul>
      <li><strong>Gateway</strong> : 지정한 인그레스 게이트웨이로부터 트래픽이 인입, 프로토콜 및 포트, HOSTS, Proxy 등 설정이 가능합니다.</li>
      <li><strong>VirtualService</strong> : 인입 처리할 hosts 설정, L7 PATH 별 라우팅, 목적지에 대한 정책 설정 가능합니다. (envoy route config) - <a href="https://istio.io/latest/docs/concepts/traffic-management/#virtual-services">링크</a>
        <ul>
          <li>VirtualService 는 DestinationRule 에서 설정된 <strong>서브셋(subset)</strong>을 사용하여 <strong>트래픽 컨트롤</strong>을 할 수 있습니다.</li>
          <li><strong>hosts 필드</strong> : 목적지 주소 - IP address, a DNS name (FQDN), 혹은 k8s svc 이름, wildcard (”*”) prefixes</li>
          <li><strong>Routing rules</strong> : HTTP 경우 - Match 필드(예) 헤더), Destination(istio/envoy 에 등록된 대상, subnet 에 DestinationRule 활용)
            <ul>
              <li><strong>HTTPRoute</strong> : redirect , rewrite , fault(장애 주입) , mirror(복제, 기본 100%) , corsPolicy(CORS 삽입) , headers(헤더 조작) 등 - <a href="https://istio.io/latest/docs/reference/config/networking/virtual-service/#HTTPRoute">링크</a></li>
            </ul>
          </li>
          <li>Routing rule precedence : Routing rules are evaluated in sequential order from top to bottom - 위에서 순차적 적용</li>
        </ul>
      </li>
      <li>DestinationRule : 실제 도착지(서비스와 1:1 연결)의 정교한 정책(부하분산, 연결 옵션, 서킷 브레이크, TLS 등)을 설정 - <a href="https://istio.io/latest/docs/concepts/traffic-management/#destination-rules">링크</a>
        <ul>
          <li><strong>Load balancing options</strong> : Round robin(기본값) , Random , Weighted , Least requests - <a href="https://www.envoyproxy.io/docs/envoy/v1.5.0/intro/arch_overview/load_balancing">링크</a>
            <ul>
              <li><strong>Destination Rule</strong> : TrafficPolicy , Subset , ConnectionPoolSettings 등 - <a href="https://istio.io/latest/docs/reference/config/networking/destination-rule/">링크</a></li>
              <li>서브셋(subsets)을 정의할 수 있어 마이크로서비스 <strong>버전별로 라우팅</strong>할 때 사용한다</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h5 id="request-routing-실습">Request Routing 실습</h5>

<ul>
  <li>실습전 기본 DestinationRule 적용</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 샘플 파일들 확인</span>
<span class="nv">$ </span>tree ~/istio-<span class="nv">$ISTIOV</span>/samples/bookinfo/networking
<span class="c"># =&gt; /root/istio-1.23.2/samples/bookinfo/networking</span>
<span class="c">#    ├── bookinfo-gateway.yaml</span>
<span class="c">#    ├── certmanager-gateway.yaml</span>
<span class="c">#    ├── destination-rule-all-mtls.yaml</span>
<span class="c">#    ├── destination-rule-all.yaml</span>
<span class="c">#    ├── destination-rule-reviews.yaml</span>
<span class="c">#    ├── egress-rule-google-apis.yaml</span>
<span class="c">#    ├── fault-injection-details-v1.yaml</span>
<span class="c">#    ├── virtual-service-all-v1.yaml</span>
<span class="c">#    ├── virtual-service-details-v2.yaml</span>
<span class="c">#    ├── virtual-service-ratings-db.yaml</span>
<span class="c">#    ├── virtual-service-ratings-mysql-vm.yaml</span>
<span class="c">#    ├── virtual-service-ratings-mysql.yaml</span>
<span class="c">#    ├── virtual-service-ratings-test-abort.yaml</span>
<span class="c">#    ├── virtual-service-ratings-test-delay.yaml</span>
<span class="c">#    ├── virtual-service-reviews-50-v3.yaml</span>
<span class="c">#    ├── virtual-service-reviews-80-20.yaml</span>
<span class="c">#    ├── virtual-service-reviews-90-10.yaml</span>
<span class="c">#    ├── virtual-service-reviews-jason-v2-v3.yaml</span>
<span class="c">#    ├── virtual-service-reviews-test-v2.yaml</span>
<span class="c">#    ├── virtual-service-reviews-v2-v3.yaml</span>
<span class="c">#    └── virtual-service-reviews-v3.yaml</span>

<span class="c"># 기본 DestinationRule 적용</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> ~/istio-<span class="nv">$ISTIOV</span>/samples/bookinfo/networking/destination-rule-all.yaml
<span class="c"># =&gt; destinationrule.networking.istio.io/productpage created</span>
<span class="c">#    destinationrule.networking.istio.io/reviews created</span>
<span class="c">#    destinationrule.networking.istio.io/ratings created</span>
<span class="c">#    destinationrule.networking.istio.io/details created</span>

<span class="c"># DestinationRule 확인 dr(=destinationrules) : KIALI Services 확인 시 GW, VS, DR 확인</span>
<span class="nv">$ </span>kubectl get dr
<span class="c"># =&gt; NAME          HOST          AGE</span>
<span class="c">#    details       details       31s</span>
<span class="c">#    productpage   productpage   31s</span>
<span class="c">#    ratings       ratings       31s</span>
<span class="c">#    reviews       reviews       31s</span>
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_21.png" alt="img.png" /></p>

<ul>
  <li><strong>virtual-service-all-v1.yaml</strong> : 4개 서비스 모두 v1 의 서브셋(subset) 에 전송하는 정책 테스트</li>
</ul>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># virtual-service-all-v1.yaml</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">networking.istio.io/v1alpha3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualService</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">productpage</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">hosts</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">productpage</span>
  <span class="na">http</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">route</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">destination</span><span class="pi">:</span>
        <span class="na">host</span><span class="pi">:</span> <span class="s">productpage</span>
        <span class="na">subset</span><span class="pi">:</span> <span class="s">v1</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">networking.istio.io/v1alpha3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualService</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">reviews</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">hosts</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">reviews</span>
  <span class="na">http</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">route</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">destination</span><span class="pi">:</span>
        <span class="na">host</span><span class="pi">:</span> <span class="s">reviews</span>
        <span class="na">subset</span><span class="pi">:</span> <span class="s">v1</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">networking.istio.io/v1alpha3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualService</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">ratings</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">hosts</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">ratings</span>
  <span class="na">http</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">route</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">destination</span><span class="pi">:</span>
        <span class="na">host</span><span class="pi">:</span> <span class="s">ratings</span>
        <span class="na">subset</span><span class="pi">:</span> <span class="s">v1</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">networking.istio.io/v1alpha3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualService</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">details</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">hosts</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">details</span>
  <span class="na">http</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">route</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">destination</span><span class="pi">:</span>
        <span class="na">host</span><span class="pi">:</span> <span class="s">details</span>
        <span class="na">subset</span><span class="pi">:</span> <span class="s">v1</span>
<span class="nn">---</span>
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># istio vs(virtualservices) 확인</span>
<span class="nv">$ </span>kubectl get vs
<span class="c"># =&gt; NAME       GATEWAYS               HOSTS   AGE</span>
<span class="c">#    bookinfo   [&amp;quot;bookinfo-gateway&amp;quot;]   [&amp;quot;*&amp;quot;]   3h30m</span>

<span class="c"># 모든 마이크로서비스에 대해 v1 의 서브셋(subset) 에 전송되게 virtualservices 적용</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> virtual-service-all-v1.yaml
<span class="c"># =&gt; virtualservice.networking.istio.io/productpage created</span>
<span class="c">#    virtualservice.networking.istio.io/reviews created</span>
<span class="c">#    virtualservice.networking.istio.io/ratings created</span>
<span class="c">#    virtualservice.networking.istio.io/details created</span>

<span class="c"># istio vs(virtualservices) 확인 &gt;&gt; KIALI 에서 reviews v2,v3 향하는 트래픽 경로가 사라진다!</span>
<span class="nv">$ </span>kubectl get virtualservices
<span class="c"># =&gt; NAME          GATEWAYS               HOSTS             AGE</span>
<span class="c">#    bookinfo      [&amp;quot;bookinfo-gateway&amp;quot;]   [&amp;quot;*&amp;quot;]             3h30m</span>
<span class="c">#    details                              [&amp;quot;details&amp;quot;]       10s</span>
<span class="c">#    productpage                          [&amp;quot;productpage&amp;quot;]   10s</span>
<span class="c">#    ratings                              [&amp;quot;ratings&amp;quot;]       10s</span>
<span class="c">#    reviews                              [&amp;quot;reviews&amp;quot;]       10s</span>
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_22.png" alt="img.png" /></p>

<ul>
  <li>
    <p>모든 트래픽이 v1으로 향하게 되어서 브라우저를 새로고침해도 v1만 나오게 됩니다.</p>
  </li>
  <li>
    <p><strong>virtual-service-reviews-test-v2.yaml</strong> : User Identity 기반 라우팅, end-user 커스텀 헤더에 <strong>jason</strong> 매칭 시 <strong>reviews v2</strong> 로 전달</p>
    <ul>
      <li>Match 조건에는 완전 일치(exact) , 전방 일치(prefix) , 정규 표현(regex) - 3가지 패턴을 선택할 수 있다</li>
    </ul>
  </li>
</ul>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># virtual-service-reviews-test-v2.yaml</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">networking.istio.io/v1alpha3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualService</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">reviews</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">hosts</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">reviews</span>
  <span class="na">http</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">match</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">headers</span><span class="pi">:</span>
        <span class="na">end-user</span><span class="pi">:</span>
          <span class="na">exact</span><span class="pi">:</span> <span class="s">jason</span>
    <span class="na">route</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">destination</span><span class="pi">:</span>
        <span class="na">host</span><span class="pi">:</span> <span class="s">reviews</span>
        <span class="na">subset</span><span class="pi">:</span> <span class="s">v2</span>
  <span class="pi">-</span> <span class="na">route</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">destination</span><span class="pi">:</span>
        <span class="na">host</span><span class="pi">:</span> <span class="s">reviews</span>
        <span class="na">subset</span><span class="pi">:</span> <span class="s">v1</span>
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 모든 마이크로서비스에 대해 v1 의 서브셋(subset) 에 전송되게 virtualservices 적용</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> virtual-service-reviews-test-v2.yaml
<span class="c"># =&gt; virtualservice.networking.istio.io/reviews configured</span>

<span class="c"># jason 로그인 시 로그 확인</span>
<span class="nv">$ </span>kubetail <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>productpage <span class="nt">-f</span>
<span class="c"># =&gt; [productpage-v1-d5789fdfb-f8gdf productpage] DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): details:9080</span>
<span class="c">#    [productpage-v1-d5789fdfb-f8gdf productpage] send: b'GET /details/0 HTTP/1.1\r\nHost: details:9080\r\nuser-agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Safari/605.1.15\r\nAccept-Encoding: gzip, deflate\r\nAccept: */*\r\nConnection: keep-alive\r\nx-request-id: 3aa3c711-d014-930f-8c2a-563c3dbbd8b5\r\n\r\n'</span>
<span class="c">#    [productpage-v1-d5789fdfb-f8gdf productpage] reply: 'HTTP/1.1 200 OK\r\n'</span>
<span class="c">#    [productpage-v1-d5789fdfb-f8gdf productpage] header: content-type: application/json</span>
<span class="c">#    [productpage-v1-d5789fdfb-f8gdf productpage] header: server: envoy</span>
<span class="c">#    [productpage-v1-d5789fdfb-f8gdf productpage] header: date: Sat, 19 Oct 2024 14:32:09 GMT</span>
<span class="c">#    [productpage-v1-d5789fdfb-f8gdf productpage] header: content-length: 178</span>
<span class="c">#    [productpage-v1-d5789fdfb-f8gdf productpage] header: x-envoy-upstream-service-time: 6</span>
<span class="c">#    [productpage-v1-d5789fdfb-f8gdf productpage] DEBUG:urllib3.connectionpool:http://details:9080 &amp;quot;GET /details/0 HTTP/1.1&amp;quot; 200 178</span>
<span class="c">#    [productpage-v1-d5789fdfb-f8gdf productpage] DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): reviews:9080</span>
<span class="c">#    [productpage-v1-d5789fdfb-f8gdf productpage] send: b'GET /reviews/0 HTTP/1.1</span>
<span class="c">#    Host: reviews:9080</span>
<span class="c">#    user-agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Safari/605.1.15</span>
<span class="c">#    Accept-Encoding: gzip, deflate</span>
<span class="c">#    Accept: */*</span>
<span class="c">#    Connection: keep-alive</span>
<span class="c">#    x-request-id: 3aa3c711-d014-930f-8c2a-563c3dbbd8b5\r\n\r\n'</span>
<span class="c">#    [productpage-v1-d5789fdfb-f8gdf productpage] reply: 'HTTP/1.1 200 OK\r\n'</span>
<span class="c">#    [productpage-v1-d5789fdfb-f8gdf productpage] header: x-powered-by: Servlet/3.1</span>
<span class="c">#    [productpage-v1-d5789fdfb-f8gdf productpage] header: content-type: application/json</span>
<span class="c">#    [productpage-v1-d5789fdfb-f8gdf productpage] header: date: Sat, 19 Oct 2024 14:32:09 GMT</span>
<span class="c">#    [productpage-v1-d5789fdfb-f8gdf productpage] header: content-language: en-US</span>
<span class="c">#    [productpage-v1-d5789fdfb-f8gdf productpage] header: content-length: 358</span>
<span class="c">#    [productpage-v1-d5789fdfb-f8gdf productpage] header: x-envoy-upstream-service-time: 1</span>
<span class="c">#    [productpage-v1-d5789fdfb-f8gdf productpage] header: server: envoy</span>
<span class="c">#    [productpage-v1-d5789fdfb-f8gdf productpage] DEBUG:urllib3.connectionpool:http://reviews:9080 &amp;quot;GET /reviews/0 HTTP/1.1&amp;quot; 200 358</span>
<span class="c">#    [productpage-v1-d5789fdfb-f8gdf productpage] INFO:werkzeug:::ffff:127.0.0.6 - - [19/Oct/2024 14:32:09] &amp;quot;GET /productpage HTTP/1.1&amp;quot; 200 -</span>
<span class="c">#    ...</span>
</code></pre></div></div>

<ul>
  <li>웹브라우저에서 productpage로 접속 후 새로고침을 해보겠습니다.
    <ul>
      <li>로그인 전에는 v1으로 접속 됩니다.
<img src="/assets/2024/kans-3th/w7/20241019_kans_w7_23.png" alt="img.png" /></li>
    </ul>
  </li>
  <li>오른쪽 상단의 Sign in 클릭 후 jason으로 로그인 후 새로고침을 해보겠습니다.
    <ul>
      <li>로그인 후에는 v2로 접속 됩니다.
<img src="/assets/2024/kans-3th/w7/20241019_kans_w7_24.png" alt="img_1.png" /></li>
      <li>
        <p>헤더에는 end-user:jason 이 추가되어 있습니다.</p>

        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 로그인 후 헤더헤더</span>
<span class="c"># =&gt; [productpage-v1-d5789fdfb-f8gdf productpage] send: b'GET /reviews/0 HTTP/1.1</span>
<span class="c">#    Host: reviews:9080</span>
<span class="c">#    user-agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Safari/605.1.15</span>
<span class="c">#    Accept-Encoding: gzip, deflate</span>
<span class="c">#    Accept: */*</span>
<span class="c">#    Connection: keep-alive</span>
<span class="c">#    &lt;span style="color: red;"&gt;end-user: jason&lt;/span&gt;</span>
<span class="c">#    x-request-id: 03366677-7032-9291-a4b9-7009a6257394</span>
<span class="c">#    cookie: session=eyJ1c2VyIjoiamFzb24ifQ.ZxPUxA.3MJkXTFH8zJtg_YlXlvzq8xArpc'</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
</ul>

<h5 id="fault-injection-실습">Fault Injection 실습</h5>

<ul>
  <li><strong>virtual-service-ratings-test-delay.yaml</strong> : end-user 가 jason 는 ratings v1 에 7초 지연 발생, 그외 사용자는 ratings v1 정상 연결</li>
</ul>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># virtual-service-ratings-test-delay.yaml</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">networking.istio.io/v1alpha3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualService</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">ratings</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">hosts</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">ratings</span>
  <span class="na">http</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">match</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">headers</span><span class="pi">:</span>
            <span class="na">end-user</span><span class="pi">:</span>
              <span class="na">exact</span><span class="pi">:</span> <span class="s">jason</span>
      <span class="na">fault</span><span class="pi">:</span>
        <span class="na">delay</span><span class="pi">:</span>
          <span class="na">percentage</span><span class="pi">:</span>
            <span class="na">value</span><span class="pi">:</span> <span class="m">100.0</span>
          <span class="na">fixedDelay</span><span class="pi">:</span> <span class="s">7s</span>
      <span class="na">route</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">destination</span><span class="pi">:</span>
            <span class="na">host</span><span class="pi">:</span> <span class="s">ratings</span>
            <span class="na">subset</span><span class="pi">:</span> <span class="s">v1</span>
    <span class="pi">-</span> <span class="na">route</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">destination</span><span class="pi">:</span>
            <span class="na">host</span><span class="pi">:</span> <span class="s">ratings</span>
            <span class="na">subset</span><span class="pi">:</span> <span class="s">v1</span>
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># virtualservices 적용</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> virtual-service-ratings-test-delay.yaml
<span class="c"># =&gt; virtualservice.networking.istio.io/ratings configured</span>

<span class="c"># 로그 확인 : product 입장에서 접속 사용자(clinet) 연결을 끊어버림 0 DC downstream_remote_disconnect</span>
<span class="nv">$ </span>kubetail <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>productpage <span class="nt">-f</span>
</code></pre></div></div>

<ul>
  <li>웹브라우저에서 jason으로 로그인된 상태에서 접속시 6~7초 지연이 발생하는것을 확인할 수 있습니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_25.png" alt="img.png" /></p>

<ul>
  <li><strong>virtual-service-ratings-test-abort.yaml</strong> : end-user 가 jason 는 ratings v1 에 500 에러 리턴, 그외 사용자는 ratings v1 정상 연결</li>
</ul>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># virtual-service-ratings-test-abort.yaml</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">networking.istio.io/v1alpha3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualService</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">ratings</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">hosts</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">ratings</span>
  <span class="na">http</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">match</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">headers</span><span class="pi">:</span>
            <span class="na">end-user</span><span class="pi">:</span>
              <span class="na">exact</span><span class="pi">:</span> <span class="s">jason</span>
      <span class="na">fault</span><span class="pi">:</span>
        <span class="na">abort</span><span class="pi">:</span>
          <span class="na">percentage</span><span class="pi">:</span>
            <span class="na">value</span><span class="pi">:</span> <span class="m">100.0</span>
          <span class="na">httpStatus</span><span class="pi">:</span> <span class="m">500</span>
      <span class="na">route</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">destination</span><span class="pi">:</span>
            <span class="na">host</span><span class="pi">:</span> <span class="s">ratings</span>
            <span class="na">subset</span><span class="pi">:</span> <span class="s">v1</span>
    <span class="pi">-</span> <span class="na">route</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">destination</span><span class="pi">:</span>
            <span class="na">host</span><span class="pi">:</span> <span class="s">ratings</span>
            <span class="na">subset</span><span class="pi">:</span> <span class="s">v1</span>
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># virtualservices 적용</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> virtual-service-ratings-test-abort.yaml
<span class="c"># =&gt; virtualservice.networking.istio.io/ratings configured</span>

<span class="c"># 로그 확인</span>
<span class="nv">$ </span>kubetail <span class="nt">-l</span> <span class="nv">version</span><span class="o">=</span>v2 <span class="nt">-f</span>
</code></pre></div></div>

<p>jason으로 로그인 했을때 Rating 서비스에 500 에러가 발생하는것을 확인할 수 있습니다.</p>

<p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_26.png" alt="img.png" /></p>

<p>또한 kiali에서도 어느 구간에서 오류가 발생했는지 확인할 수 있으며, Flags도 확인할 수 있습니다. <a href="https://www.envoyproxy.io/docs/envoy/latest/configuration/observability/access_log/usage#config-access-log-format-response-flags">링크</a><br />
(이경우 FI는 Fault Injection을 의미으로 일부러 오류를 일으킨 것을 확인 할 수 있습니다.)</p>

<p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_27.png" alt="img.png" /></p>

<h5 id="traffic-shifting-실습">Traffic Shifting 실습</h5>

<ul>
  <li>
    <p>카나라 배포 전략 등 활용 - <a href="https://istio.io/latest/docs/tasks/traffic-management/traffic-shifting/">링크</a></p>
  </li>
  <li>
    <p><strong>virtual-service-reviews-50-v3.yaml</strong> : 가중치 (weight-based routing), reviews 에 v1(50%), v3(50%)</p>
  </li>
</ul>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">networking.istio.io/v1alpha3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualService</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">reviews</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">hosts</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">reviews</span>
  <span class="na">http</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">route</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">destination</span><span class="pi">:</span>
        <span class="na">host</span><span class="pi">:</span> <span class="s">reviews</span>
        <span class="na">subset</span><span class="pi">:</span> <span class="s">v1</span>
      <span class="na">weight</span><span class="pi">:</span> <span class="m">50</span>
    <span class="pi">-</span> <span class="na">destination</span><span class="pi">:</span>
        <span class="na">host</span><span class="pi">:</span> <span class="s">reviews</span>
        <span class="na">subset</span><span class="pi">:</span> <span class="s">v3</span>
      <span class="na">weight</span><span class="pi">:</span> <span class="m">50</span>
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># virtualservices 적용</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> virtual-service-reviews-50-v3.yaml
<span class="c"># =&gt; virtualservice.networking.istio.io/reviews configured</span>

<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span>  <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$MYDOMAIN</span>:<span class="nv">$IGWHTTP</span>/productpage | <span class="nb">grep</span> <span class="nt">-m</span> 1 <span class="s2">"reviews-v</span><span class="se">\?</span><span class="s2">"</span> <span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span>
<span class="c"># =&gt;      53                   reviews-v1-6584ddcf65-gnc9j</span>
<span class="c">#         47                   reviews-v3-6f5b775685-cw7tc</span>
</code></pre></div></div>

<p>대략 50%의 확률로 v1과 v3로 접속되는것을 확인할 수 있습니다.</p>

<ul>
  <li><strong>virtual-service-reviews-80-20.yaml</strong> : 가중치 (weight-based routing), reviews 에 v1(80%), v2(20%)</li>
</ul>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">networking.istio.io/v1alpha3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualService</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">reviews</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">hosts</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">reviews</span>
  <span class="na">http</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">route</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">destination</span><span class="pi">:</span>
        <span class="na">host</span><span class="pi">:</span> <span class="s">reviews</span>
        <span class="na">subset</span><span class="pi">:</span> <span class="s">v1</span>
      <span class="na">weight</span><span class="pi">:</span> <span class="m">80</span>
    <span class="pi">-</span> <span class="na">destination</span><span class="pi">:</span>
        <span class="na">host</span><span class="pi">:</span> <span class="s">reviews</span>
        <span class="na">subset</span><span class="pi">:</span> <span class="s">v2</span>
      <span class="na">weight</span><span class="pi">:</span> <span class="m">20</span>
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># virtualservices 적용</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> virtual-service-reviews-80-20.yaml
<span class="c"># =&gt; virtualservice.networking.istio.io/reviews configured</span>

<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span>  <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$MYDOMAIN</span>:<span class="nv">$IGWHTTP</span>/productpage | <span class="nb">grep</span> <span class="nt">-m</span> 1 <span class="s2">"reviews-v</span><span class="se">\?</span><span class="s2">"</span> <span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span>
<span class="c"># =&gt;      79                   reviews-v1-6584ddcf65-gnc9j</span>
<span class="c">#         21                   reviews-v2-6f85cb9b7c-cfc68</span>
</code></pre></div></div>

<p>대략 80%의 확률로 v1과 20%의 확률로 v2로 접속되는것을 확인할 수 있습니다.</p>

<p>kiali에서도 어느 구간에서 어떠한 비중으로 트래픽이 흘러가는지 확인할 수 있습니다.
<img src="/assets/2024/kans-3th/w7/20241019_kans_w7_28.png" alt="img.png" /></p>

<h4 id="security-보안">Security (보안)</h4>

<ul>
  <li>요구사항
    <ul>
      <li>MITM (Man-In-The-Middle) 공격 방지를 위해 모든 트래픽은 mTLS로 암호화 되어야 합니다.</li>
      <li>또한 접근 제어 정책이 필요하며, 감사 로깅을 통해 보안 이슈를 식별이 가능 해야 합니다.</li>
    </ul>
  </li>
  <li>목표
    <ul>
      <li>기본 셋팅을 안전하게 하기 : 별도의 셋팅이 없어도 보안을 유지할 수 있도록 설정</li>
      <li>깊은 방어 : 기존에 존재하는 보안 시스템과 통합되어, 다층 방어를 구성</li>
      <li><strong>Zero-trust network</strong> : 네트워크를 신뢰하지 않음으로써 보안 강화 <a href="https://genians.co.kr/genians-nac/zt/">https://genians.co.kr/genians-nac/zt/</a></li>
    </ul>
  </li>
  <li>구성요소
    <ul>
      <li>Certification Authority (CA) : 인증서 발급, 관리, 갱신</li>
      <li>보안 정책 (인증정책, 인가정책 등) 관련 설정을 각 프록시에 전달하는 API 서버</li>
      <li>사이드카와 프록시를 정책 강제 지점(Policy Enforcement Point-PEPs)으로 사용하여 클라이언트와 서버간의 통신을 보호</li>
      <li>Envoy Proxy 확장 기능을 통해 telemetry와 감사 로깅 수집</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_29.svg" alt="20241019_kans_w7_29.svg" class="image-center" />
<em class="image-caption">istio 보안 아키텍쳐</em></p>

<ul>
  <li>TLS와 mTLS
    <ul>
      <li><strong>TLS (Transport Layer Security)</strong> : 통신 보안을 위한 프로토콜로, 인터넷 상에서 데이터를 암호화하는 표준화된 방법입니다.
기본적으로 서버의 인증서만 확인하는 방식을 사용합니다.</li>
      <li><strong>mTLS (mutual TLS)</strong> : 서버와 클라이언트가 서로 인증을 하고 서로 신뢰할 수 있는지 확인하는 방식입니다.</li>
    </ul>
  </li>
  <li>Authentication (인증), Authorization (인가) (Auto mTLS)
    <ul>
      <li>Istio는 모든 워크로드에 X.509 인증서를 부여하고, 서로 인증을 통해 통신을 보호합니다.</li>
      <li>Envoy proxy와 함께 실행되는 Istio agent는 istiod와 함께 동작하면서 자동으로 인증서를 갱신합니다. 
 <img src="/assets/2024/kans-3th/w7/20241019_kans_w7_30.svg" alt="20241019_kans_w7_30.svg" class="image-center" />
        <ol>
          <li>istiod는 CSR(인증서 서명 요청)을 수행하기 위해 gRPC 서비스를 제공합니다.</li>
          <li>Envoy는 SDS(Secret Discovery Serice) API를 통해 인증서와 키 요청을 보냅니다.</li>
          <li>istio-agent는 SDs 요청을 받으면 Private Key와 CSR을 생성한 후 자격증명
   (credential)과 함께 CSR istiod에 전송하여 서명을 요청합니다.</li>
          <li>CA는 CSR에 포함된 자격증명(credential) 의 유효성을 검사하고 CSR에 서명하여 인증서를 생성합니다.</li>
          <li>istio-agent는 istiod로부터 받은 인증서(certiftcate)와 개인키 private Key)를 Envoy SDS
   API를 통해 Envoy에게 보냅니다.</li>
          <li>위의 CSR 프로세스는 인증서 및 키 순환을 위해 주기적으로 반복됩니다</li>
        </ol>
      </li>
    </ul>
  </li>
  <li>Authentication (인증) : 2가지 타입 제공
<img src="/assets/2024/kans-3th/w7/20241019_kans_w7_31.svg" alt="20241019_kans_w7_31.svg" />
    <ol>
      <li>Peer Authentication :
        <ul>
          <li>서비스간 인증에 사용되며 Client가 연결을 확인하는데 사용. 서비스 코드 변경없이 mTLS 제공</li>
        </ul>
      </li>
      <li>Request Authentication :
        <ul>
          <li>Request에 첨부된 자격증명(Credential)을 통해 최종 사용자 인증에 사용</li>
          <li>istio는 JWT (JSON Web Token)을 지원하여 최종 사용자 인증을 제공</li>
          <li>커스텀 인증 제공자를 비롯하여 OpenID Connect, Keycloak, Auth0 등 다양한 인증 방식을 지원</li>
        </ul>
      </li>
    </ol>
  </li>
  <li>Authorization (인가)
<img src="/assets/2024/kans-3th/w7/20241019_kans_w7_32.svg" alt="20241019_kans_w7_32.svg" />
    <ul>
      <li>istio는 mesh 단위, 네임스페이스 단위, 워크로드 단위로 접근을 제어 할 수 있는 인가 기능을 제공합니다. 다음과 같은 이점이 있습니다.
        <ul>
          <li>워크로드와 워크로드간, 또는 사용자와 워크로드간 인가 제공</li>
          <li>단일한 AuthorizationPolicy CRD를 사용한 단순한 API 제공</li>
          <li>유연한 정책 제공 : 커스텀 조건을 등록할 수 있고, CUSTOM, DENY, ALLOW 액션 지원</li>
          <li>고성능 : Envoy Proxy를 통해 인가 정책을 적용하므로 성능 저하가 없음</li>
          <li>높은 호환성 : gRPC, HTTP, HTTPS 등을 지원하며, 일반적인 TCP 프로토콜도 지원</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h5 id="authentication-auto-mtls-실습">Authentication (Auto mTLS) 실습</h5>

<ul>
  <li>
    <p>기존 파드에 로그에서 인증서 등 보안 관련 내용 확인</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c"># CA Endpoint, CA(/var/run/secret/istio/root-cert,pem), citadelclient, SDS server 등등</span>
  <span class="nv">$ </span>kubectl logs ratings-v1-7c9bd4b87f-s9gxs <span class="nt">-c</span> istio-proxy <span class="nt">-f</span>
  <span class="nv">$ </span>kubetail
  
  <span class="c"># 인증정책 확인</span>
  <span class="nv">$ </span>kubectl get peerauthentications.security.istio.io
  <span class="c"># =&gt; No resources found </span>
    
  <span class="c"># envoy 에 cert 정보 확인 : istio-proxy 에 admin페이지 접속 or kaila 에서 envoy 에서 확인    </span>
</code></pre></div>    </div>
  </li>
  <li>bookinfo → kiali → product 계속 접속</li>
  <li>kiali 에서 Display(Security 체크) 후 자물쇠 클릭하면 오른쪽 창에서 보안설정을 확이할 수 있습니다. : mTLS Enabled, spiffe(Secure name)</li>
</ul>

<p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_33.png" alt="img.png" /></p>

<ul>
  <li>test 네임스페이스 생성 후 파드 생성(sidecar 미적용) 후 ratings 접속</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 네임스페이스 생성</span>
<span class="nv">$ </span>kubectl create ns <span class="nb">test</span>
<span class="c"># =&gt; namespace/test created</span>

<span class="c"># 파드 생성</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl create -f -
apiVersion: v1
kind: Pod
metadata:
  name: netpod
  namespace: test
spec:
  containers:
  - name: netshoot-pod
    image: nicolaka/netshoot
    command: ["tail"]
    args: ["-f", "/dev/null"]
  terminationGracePeriodSeconds: 0
</span><span class="no">EOF

</span><span class="c"># 확인 : sidecar 미적용</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-n</span> <span class="nb">test</span>
<span class="c"># =&gt; NAME     READY   STATUS    RESTARTS   AGE</span>
<span class="c">#    netpod   1/1     Running   0          19s</span>

<span class="c"># ratings 서비스 확인</span>
<span class="nv">$ </span>kubectl get svc ratings
<span class="c"># =&gt; NAME      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE</span>
<span class="c">#    ratings   ClusterIP   10.10.200.163   &amp;lt;none&amp;gt;        9080/TCP   8h</span>

<span class="c"># ratings 접속 시도 : 성공</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> <span class="nt">-n</span> <span class="nb">test </span>netpod <span class="nt">--</span> curl ratings.default:9080/health <span class="p">;</span><span class="nb">echo</span>
<span class="nv">$ </span><span class="o">{</span><span class="s2">"status"</span>:<span class="s2">"Ratings is healthy"</span><span class="o">}</span>

<span class="c"># 로그 확인</span>
<span class="nv">$ </span>kubetail <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>ratings <span class="nt">-f</span>
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_34.png" alt="img.png" class="image-center w-80" />
<em class="image-caption">NS(default, test 체크) netpod 에서 접속 시 unknown 으로 표기되며, 접근 성공(녹색) 확인</em></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Peer authentication 설정 변경 : PERMISSIVE(mTLS 사용/미사용 모두 허용) → STRICT(반드시 mTLS 사용, 미사용 시 거부)</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl create -f -
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: default-strict
spec:
  mtls:
    mode: STRICT
</span><span class="no">EOF

</span><span class="c"># ratings 접속 시도 : 실패!</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> <span class="nt">-n</span> <span class="nb">test </span>netpod <span class="nt">--</span> curl ratings.default:9080/health <span class="p">;</span><span class="nb">echo</span>
<span class="c"># =&gt; curl: (56) Recv failure: Connection reset by peer</span>
<span class="c">#    command terminated with exit code 56</span>

<span class="nv">$ </span>kubetail <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>ratings <span class="nt">-f</span>
<span class="c"># =&gt; [ratings-v1-7c9bd4b87f-s9gxs istio-proxy] [2024-10-01T17:21:12.708Z] &amp;quot;- - -&amp;quot; 0 NR filter_chain_not_found - &amp;quot;-&amp;quot; 0 0 0 - &amp;quot;-&amp;quot; &amp;quot;-&amp;quot; &amp;quot;-&amp;quot; &amp;quot;-&amp;quot; &amp;quot;-&amp;quot; - - 172.16.3.17:9080 172.16.1.17:34938 - -</span>
</code></pre></div></div>

<ul>
  <li>실습 자원 삭제</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl delete PeerAuthentication default-strict
<span class="c"># =&gt; peerauthentication.security.istio.io &amp;quot;default-strict&amp;quot; deleted</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> <span class="nt">-n</span> <span class="nb">test </span>netpod <span class="nt">--</span> curl ratings.default:9080/health <span class="p">;</span><span class="nb">echo</span>
<span class="c"># =&gt; {&amp;quot;status&amp;quot;:&amp;quot;Ratings is healthy&amp;quot;}</span>
<span class="c"># &lt;span style="color: green;"&gt;👉 인증정책 강제 정책 삭제시 다시 통신이 됩니다.&lt;/span&gt;</span>

<span class="nv">$ </span>kubectl delete pod <span class="nt">-n</span> <span class="nb">test </span>netpod
<span class="nv">$ </span>kubectl delete ns <span class="nb">test</span>
</code></pre></div></div>

<h3 id="istio-통신-흐름">Istio 통신 흐름</h3>

<p>istio 사용시 트래픽은 호스트의 tcp/ip 스택과 iptables, 파드내의 iptables와 envoy를 경유하게 됩니다.
istio는 강력하고 다양한 기능들을 제공하지만 비용(지연추가, 프로세서 사용량 추가, 복잡한 구조)이 필요합니다.</p>

<p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_35.png" alt="img.png" class="image-center w-80" /></p>

<p>외부 클라이언트(PC 등)에서 파드로 접속되는 과정은 다음과 같습니다.</p>

<p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_36.png" alt="img_1.png" /></p>

<p>위의 그림에서 처럼 iptables를 여러번 거치고, DNAT등을 통해 포트번호등이 80 (http) =&gt; 15006 (istio-proxy) 로 변경되는 등의 작업을 여러번 거칩니다.
또한 파드와 호스트간 통신 envoy를 요청을 받을때와 응답할때 모두 거쳐가는 것을 확인할 수 있습니다.</p>

<ul>
  <li>파드 내 Iptables 적용 흐름</li>
</ul>

<p><img src="/assets/2024/kans-3th/w7/20241019_kans_w7_37.png" alt="img.png" class="image-center" />
<em class="image-caption">출처 : <a href="https://jimmysong.io/en/blog/sidecar-injection-iptables-and-traffic-routing/#understand-outbound-handler">Jimmy song</a> 블로그</em></p>

<p>다음 블로그에서 자세한 내용을 확인할 수 있습니다.
<a href="https://jimmysong.io/en/blog/sidecar-injection-iptables-and-traffic-routing/#understand-outbound-handler">Understanding the Sidecar Injection, Traffic Intercepting &amp; Routing Process in Istio</a></p>

<hr />

<h2 id="마치며">마치며</h2>

<p>1주차 컨테이너 격리 이후에 또 다시 뇌정지가 찾아온 주차였습니다.
이번주에 학습한것도 많은데, 이것이 일부만 살펴본것이라니 놀랍습니다.
정말 만든 분들도, 쓰는 분들도, 스터디를 진행해주시는 분들도 대단합니다. :thumbsup:</p>

<p>인증이나 인가 등 gateway api에서 아쉬웠던 부분들이 나와서 좋았습니다.
찾던 기능인데 마침 이번주에 다루게 되어서 좋았습니다. 
Kiali를 통한 트래픽 시각화도 정말 유용하게 쓰일 것 같습니다.
복잡하긴 하지만 좋은 기능들이 많았습니다.</p>

<p>시간이 모자라서 미처 실습하지 못한 부분들과 Ambient Mesh도 바쁜일이 지나가면 다시 살펴봐야겠습니다.
스터디를 준비해주신 가시다님과 참여하신 분들 감사합니다. :pray:</p>]]></content><author><name></name></author><category term="kans" /><category term="kubernetes," /><category term="network," /><category term="linux" /><summary type="html"><![CDATA[이번주에는 Service Mesh와 대표적인 오픈소스 프로젝트인 Istio에 대해 알아 보겠습니다.]]></summary></entry><entry><title type="html">[KANS 3기] Ingress &amp;amp; Gateway API</title><link href="https://sweetlittlebird.github.io/posts/2024-10-13-KANS-Study-Week6/" rel="alternate" type="text/html" title="[KANS 3기] Ingress &amp;amp; Gateway API" /><published>2024-10-13T01:00:18+09:00</published><updated>2024-10-13T01:00:18+09:00</updated><id>https://sweetlittlebird.github.io/posts/KANS%20Study%20-%20Week6</id><content type="html" xml:base="https://sweetlittlebird.github.io/posts/2024-10-13-KANS-Study-Week6/"><![CDATA[<h2 id="들어가며">들어가며</h2>

<p>이번주에는 ingress와 gateway api 에대해 알아 보겠습니다.
KANS 3기 6주차 스터디를 시작하겠습니다.</p>

<hr />

<h2 id="ingress">Ingress</h2>

<h3 id="ingress란">Ingress란?</h3>

<ul>
  <li>Ingress는 클러스터 외부에서 클러스터 내부로 HTTP 및 HTTPS 트래픽을 라우팅하는 Web Proxy 역할을 수행합니다.</li>
  <li>지난주에 스터디했던 LoadBalancer와 비슷한 역할을 수행하지만, LoadBalancer는 Layer 4에서 동작하는 반면 Ingress는 Layer 7에서 동작한다는 차이가 있습니다.</li>
  <li>Ingress는 HTTP와 HTTPS를 이해하기 때문에 호스트명, 경로 등에 따라 트래픽을 라우팅할 수도 있고, SSL Offloading 등의 기능도 제공합니다.</li>
  <li>이렇게 다양한 기능이 있지만 <strong>Ingress는 동결처리</strong> 되었으며, <strong>신규 기능들은 Gateway API라는 다른 API에 추가되고 있고</strong>, 향후에는 Ingress 대신 Gateway API를 사용하는 것이 권장될것으로 보입니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w6/20241012_kans_w6_1.png" alt="img.png" class="image-center" /></p>

<ul>
  <li>특이한 점은 Ingress 를 통한 트래픽은 서비스를 통하지 않고, 서비스를 통해서 파드의 IP를 확인하고, 위의 그림과 같이 서비스를 거치지 않고 파드와 직접 통신합니다.</li>
</ul>

<h3 id="ingress-controller의-종류">Ingress Controller의 종류</h3>

<ul>
  <li>Ingress는 Kubernetes에 내장된 기능이 아니어서 별도의 Ingress Controller를 설치해야만 사용할 수 있습니다. 
많이 사용되는 Ingress Controller는 다음과 같습니다.</li>
</ul>

<table>
  <thead>
    <tr>
      <th>구분</th>
      <th>특징</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Pomerium</td>
      <td>보안에 특화 된 Ingress로 Identity-Aware 접근이 가능하며 Zero Trust 모델을 지원합니다.</td>
    </tr>
    <tr>
      <td>NGINX Ingress Controller</td>
      <td>신뢰할 수 있는 안정적으로, 라우팅이 유연하고, Lua 스크립트 등으로 기능확장이 가능합니다.</td>
    </tr>
    <tr>
      <td>Traefik</td>
      <td>Auto-discovery를 제공하고, 실시간으로 업데이트 되며, 관리 대시보드를 제공합니다. 동적으로 운영하기 좋습니다.</td>
    </tr>
    <tr>
      <td>HAProxy Ingress</td>
      <td>고성능이며, 다양한 고급 기능을 제공합니다.</td>
    </tr>
    <tr>
      <td>Envoy</td>
      <td>확장성이 있으며 재시도, 서킷 브레이커, 레이트 제한 등 다양한 기능을 제공합니다.</td>
    </tr>
    <tr>
      <td>Istio Ingress Gateway</td>
      <td>트래픽 관리에 강점이 있으며, Istio 서비스 메시와 연동하기 좋습니다.</td>
    </tr>
    <tr>
      <td>Contour</td>
      <td>HTTP/2와 gRPC를 지원하는 경량의 고성능을 제공합니다.</td>
    </tr>
    <tr>
      <td>Kong Ingress Controller</td>
      <td>Kong은 API Gateway로 널리 알려져있지만 Ingress Controller 기능도 제공합니다. NGINX Ingress Controller의 기능에 추가적인 기능을 제공하지만 학습 곡선이 높은 편입니다.</td>
    </tr>
  </tbody>
</table>

<p>이외에도 다양한 Ingress Controller가 존재하며 다음의 링크에서 확인 할 수 있습니다. <a href="https://docs.google.com/spreadsheets/d/191WWNpjJ2za6-nbG4ZoUMXMpUK8KlCIosvQB0f-oq3k/">Kubernetes Ingress Controllers 비교</a></p>

<h3 id="실습-환경-준비">실습 환경 준비</h3>

<ul>
  <li>이번 실습에는 k3s라는 경량 Kubernetes 클러스터를 사용하겠습니다. k3s는 Rancher에서 개발한 경량 Kubernetes 클러스터로, 쉽게 설치가 가능하고, 
전체가 100MB보다 적을 정도로 적은 자원으로도 Kubernetes를 사용할 수 있습니다.</li>
  <li>하지만 K8S와는 기능 차이가 있기 때문에, 이러한 부분을 감안하고 사용하시면 됩니다.</li>
</ul>

<h4 id="k3s-특징">k3s 특징</h4>

<ul>
  <li>k3s의 특징은 다음과 같습니다
    <ul>
      <li>단일 바이너리 또는 최소 컨테이너 이미지로 배포됩니다.</li>
      <li>기본 저장소 백엔드로 sqlite3를 기반으로 한 경량 데이터 저장소가 사용됩니다. etcd, MySQL 및 Postgres도 사용할 수 있습니다.</li>
      <li>TLS 및 옵션의 복잡성을 처리하는 런처에 포함되어 있습니다.</li>
      <li>경량 환경에 적합한 합리적인 기본값으로 보안에 신경을 썼습니다.</li>
      <li>모든 Kubernetes 컨트롤 플레인 구성 요소의 작동이 단일 바이너리 및 프로세스에 캡슐화되어 있고, k3s가 인증서 배포와 같은 복잡한 클러스터 작업을 자동화합니다.</li>
      <li>외부 종속성이 최소화되었습니다. 필요한 것은 최신 커널과 cgroup 마운트뿐입니다.</li>
      <li>손쉬운 클러스터 생성을 위해 필요한 패키지를 기본 제공합니다:
        <ul>
          <li>containerd / cri-dockerd 컨테이너 런타임 (CRI)</li>
          <li>Flannel 컨테이너 네트워크 인터페이스 (CNI)</li>
          <li>CoreDNS 클러스터 DNS</li>
          <li>Traefik Ingress 컨트롤러</li>
          <li>ServiceLB 로드 밸런서 컨트롤러</li>
          <li>Kube-router 네트워크 정책 컨트롤러</li>
          <li>Local-path-provisioner 영구 볼륨 컨트롤러</li>
          <li>Spegel 분산 컨테이너 이미지 레지스트리 미러</li>
          <li>호스트 유틸리티 (iptables, socat 등)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h4 id="k3s의-아키텍쳐">k3s의 아키텍쳐</h4>

<ul>
  <li>k3s는 서버 (Control Plane)와 에이전트 (Worker Node)로 구성되어 있습니다.
    <ul>
      <li>서버 노드는 Kubernetes의 <code class="language-plaintext highlighter-rouge">k3s server</code> 명령으로 실행되며 모든 컨트롤 플레인 구성 요소와 데이터 저장 컴포넌트를 실행하며 k3s가 관리합니다.</li>
      <li>에이전트 노드는 <code class="language-plaintext highlighter-rouge">k3s agent</code> 명령으로 실행되며 컨트롤 플레인 요소등 없이 워커 노드로 동작합니다.</li>
      <li>모든 서버와 에이전트는 kublet, 컨테이너 런타임, CNI 등을 포함한 모든 Kubernetes 구성 요소를 실행합니다.</li>
      <li>더 자세한 내용은 다음 링크를 참고하세요. <a href="https://docs.k3s.io/advanced#running-agentless-servers-experimental">링크</a>
<img src="/assets/2024/kans-3th/w6/20241012_kans_w6_2.svg" alt="20241012_kans_w6_2.svg" /></li>
    </ul>
  </li>
  <li>단일 서버 구성 : 1대 K3S 서버(경량 DB = SQLite), 필요한 만큼의 K3S Agents (Worker Node) 구성
<img src="/assets/2024/kans-3th/w6/20241012_kans_w6_3.png" alt="img.png" /></li>
  <li>고가용성 구성 : Embedded DB (etcd 등), 외부 DB (MySQL, PostgreSQL 등) 사용 가능
<img src="/assets/2024/kans-3th/w6/20241012_kans_w6_4.png" alt="img_1.png" /></li>
</ul>

<h4 id="k3s-설치">k3s 설치</h4>

<ul>
  <li>k3s는 기본적으로 <code class="language-plaintext highlighter-rouge">traefik</code>을 Ingress Controller로 사용하는데 이번 실습에서는 nginx ingress controller를 사용할 것이기 때문에 <code class="language-plaintext highlighter-rouge">traefik</code>을 설치하지 않겠습니다.</li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">INSTALL_K3S_EXEC=" --disable=traefik"</code> 옵션을 사용하여 traefik을 설치하지 않을 수 있습니다.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Install k3s-server</span>
<span class="nv">$ </span>curl <span class="nt">-sfL</span> https://get.k3s.io | <span class="nv">INSTALL_K3S_EXEC</span><span class="o">=</span><span class="s2">" --disable=traefik"</span>  sh <span class="nt">-s</span> - server <span class="nt">--token</span> <span class="o">[[</span>인증토큰]] <span class="nt">--cluster-cidr</span> <span class="s2">"172.16.0.0/16"</span> <span class="nt">--service-cidr</span> <span class="s2">"10.10.200.0/24"</span> <span class="nt">--write-kubeconfig-mode</span> 644 
  
<span class="c"># Install k3s-agent</span>
<span class="nv">$ </span>curl <span class="nt">-sfL</span> https://get.k3s.io | <span class="nv">K3S_URL</span><span class="o">=</span>https://192.168.10.10:6443 <span class="nv">K3S_TOKEN</span><span class="o">=[[</span>인증토큰]]  sh <span class="nt">-s</span> -
</code></pre></div>    </div>
  </li>
  <li>k3s 설치 후, k3의 설정을 확인해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 노드 확인</span>
<span class="nv">$ </span>kubectl get node <span class="nt">-owide</span>
<span class="c"># =&gt; NAME     STATUS   ROLES                  AGE     VERSION        INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION       CONTAINER-RUNTIME</span>
<span class="c">#    k3s-m    Ready    control-plane,master   30m     v1.30.5+k3s1   10.0.2.15     &amp;lt;none&amp;gt;        Ubuntu 22.04.5 LTS   5.15.0-119-generic   containerd://1.7.21-k3s2</span>
<span class="c">#    k3s-w1   Ready    &amp;lt;none&amp;gt;                 4m24s   v1.30.5+k3s1   10.0.2.15     &amp;lt;none&amp;gt;        Ubuntu 22.04.5 LTS   5.15.0-119-generic   containerd://1.7.21-k3s2</span>
<span class="c">#    k3s-w2   Ready    &amp;lt;none&amp;gt;                 26m     v1.30.5+k3s1   10.0.2.15     &amp;lt;none&amp;gt;        Ubuntu 22.04.5 LTS   5.15.0-119-generic   containerd://1.7.21-k3s2</span>
<span class="c">#    k3s-w3   Ready    &amp;lt;none&amp;gt;                 24m     v1.30.5+k3s1   10.0.2.15     &amp;lt;none&amp;gt;        Ubuntu 22.04.5 LTS   5.15.0-119-generic   containerd://1.7.21-k3s2</span>

<span class="nv">$ </span>kubectl describe node k3s-m | <span class="nb">grep </span>Taint  <span class="c"># Taints 없음</span>
<span class="c"># =&gt; Taints:             &amp;lt;none&amp;gt;</span>
<span class="nv">$ </span>kubectl describe node k3s-w1 | <span class="nb">grep </span>Taint  <span class="c"># Taints 없음</span>
<span class="c"># =&gt; Taints:             &amp;lt;none&amp;gt;</span>

<span class="c"># 파드 확인</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-n</span> kube-system
<span class="c"># =&gt; NAME                                      READY   STATUS    RESTARTS   AGE</span>
<span class="c">#    coredns-7b98449c4-8l64d                   1/1     Running   0          31m</span>
<span class="c">#    local-path-provisioner-6795b5f9d8-b5gt6   1/1     Running   0          31m</span>
<span class="c">#    metrics-server-cdcc87586-d87gv            1/1     Running   0          31m</span>

<span class="c">#</span>
<span class="nv">$ </span>kubectl top node
<span class="c"># =&gt; NAME     CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%</span>
<span class="c">#    k3s-m    147m         3%     1128Mi          28%</span>
<span class="c">#    k3s-w1   147m         3%     1128Mi          57%</span>
<span class="c">#    k3s-w2   147m         3%     1128Mi          57%</span>
<span class="c">#    k3s-w3   147m         3%     1128Mi          57%</span>
<span class="nv">$ </span>kubectl top pod <span class="nt">-A</span> <span class="nt">--sort-by</span><span class="o">=</span><span class="s1">'cpu'</span>
<span class="c"># =&gt; NAMESPACE     NAME                                      CPU(cores)   MEMORY(bytes)</span>
<span class="c">#    kube-system   metrics-server-cdcc87586-d87gv            15m          19Mi</span>
<span class="c">#    kube-system   coredns-7b98449c4-8l64d                   4m           13Mi</span>
<span class="c">#    kube-system   local-path-provisioner-6795b5f9d8-b5gt6   1m           6Mi</span>
<span class="nv">$ </span>kubectl top pod <span class="nt">-A</span> <span class="nt">--sort-by</span><span class="o">=</span><span class="s1">'memory'</span>
<span class="nv">$ </span>kubectl get storageclass
<span class="c"># =&gt; NAME                   PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE</span>
<span class="c">#    local-path (default)   rancher.io/local-path   Delete          WaitForFirstConsumer   false                  32m</span>

<span class="c"># config 정보(위치) 확인</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-v</span><span class="o">=</span>6
<span class="c"># =&gt; I1012 05:55:56.507623    6817 loader.go:395] Config loaded from file:  /etc/rancher/k3s/k3s.yaml</span>
<span class="c">#    I1012 05:55:56.518338    6817 round_trippers.go:553] GET https://127.0.0.1:6443/api/v1/namespaces/default/pods?limit=500 200 OK in 5 milliseconds</span>
<span class="c">#    No resources found in default namespace.</span>

<span class="nv">$ </span><span class="nb">cat</span> /etc/rancher/k3s/k3s.yaml
<span class="c"># =&gt; apiVersion: v1</span>
<span class="c">#    clusters:</span>
<span class="c">#    - cluster:</span>
<span class="c">#        certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJlRENDQVIyZ0F3SUJBZ0lCQURBS0JnZ3Foa2pPUFFRREFqQWpNU0V3SHdZRFZRUUREQmhyTTNNdGMyVnkKZG1WeUxXTmhRREUzTWpnM01UQTJNREF3SGhjTk1qUXhNREV5TURVeU16SXdXaGNOTXpReE1ERXdNRFV5TXpJdwpXakFqTVNFd0h3WURWUVFEREJock0zTXRjMlZ5ZG1WeUxXTmhRREUzTWpnM01UQTJNREF3V1RBVEJnY3Foa2pPClBRSUJCZ2dxaGtqT1BRTUJCd05DQUFReEdLOFFEcHMvNmNHdE45RWRCYmZJRmg2UjBpQlFLYUhHYWhVQXVMdjUKWHhpd1JjTVdia1FZNmxBdWM1RC9zWWYrTmhZYUFjcmNzMk01LzAyTkQ5bERvMEl3UURBT0JnTlZIUThCQWY4RQpCQU1DQXFRd0R3WURWUjBUQVFIL0JBVXdBd0VCL3pBZEJnTlZIUTRFRmdRVXJzL1ZVODFCZEJnS3N2YmJDRmhjCkJ5aStxUTB3Q2dZSUtvWkl6ajBFQXdJRFNRQXdSZ0loQUxwWXpzZkVMdjZScG56OGdqcDZXYkZuUFk2S3FrQ2gKTWYwRWZvMnRzM2d5QWlFQXhkaDM4akJCMWJrTWlwWDNSMTFyTnBtZmc2S2huZzliNUJDTUs0M3UyTjA9Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K</span>
<span class="c">#        server: https://127.0.0.1:6443</span>
<span class="c">#      name: default</span>
<span class="c">#    ...</span>
<span class="nv">$ </span><span class="nb">export</span> | <span class="nb">grep </span>KUBECONFIG
<span class="c"># =&gt; (공백)</span>

<span class="c"># 네트워크 정보 확인 : flannel CNI(vxlan mode), podCIDR</span>
<span class="nv">$ </span>ip <span class="nt">-c</span> addr
<span class="c"># =&gt; ...</span>
<span class="c">#    4: flannel.1: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1450 qdisc noqueue state UNKNOWN group default</span>
<span class="c">#        link/ether 02:21:77:da:a3:91 brd ff:ff:ff:ff:ff:ff</span>
<span class="c">#        inet 172.16.0.0/32 scope global flannel.1</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    5: cni0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1450 qdisc noqueue state UP group default qlen 1000</span>
<span class="c">#        link/ether ca:30:a0:c8:5c:cd brd ff:ff:ff:ff:ff:ff</span>
<span class="c">#        inet 172.16.0.1/24 brd 172.16.0.255 scope global cni0</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    6: veth41d9e3b2@if2: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1450 qdisc noqueue master cni0 state UP group default</span>
<span class="c">#        link/ether fa:47:5c:4a:8d:af brd ff:ff:ff:ff:ff:ff link-netns cni-9c26655e-b22f-97a1-f97c-db88daccc77f</span>
<span class="c">#    7: veth5c3de18a@if2: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1450 qdisc noqueue master cni0 state UP group default</span>
<span class="c">#        link/ether 6a:5e:39:72:4f:4c brd ff:ff:ff:ff:ff:ff link-netns cni-cff25bf8-d23b-790a-91d0-ed5c4ee526d5</span>
<span class="c">#    8: vethfaeebb1c@if2: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1450 qdisc noqueue master cni0 state UP group default</span>
<span class="c">#        link/ether c6:f5:31:a1:56:21 brd ff:ff:ff:ff:ff:ff link-netns cni-06a3672f-70dc-7445-48c9-8cf8c26e7fb3</span>
<span class="nv">$ </span>ip <span class="nt">-c</span> route
<span class="c"># =&gt; default via 10.0.2.2 dev enp0s3 proto dhcp src 10.0.2.15 metric 100</span>
<span class="c">#    ...</span>
<span class="c">#    172.16.0.0/24 dev cni0 proto kernel scope link src 172.16.0.1</span>
<span class="c">#    172.16.1.0/24 via 172.16.1.0 dev flannel.1 onlink</span>
<span class="c">#    172.16.2.0/24 via 172.16.2.0 dev flannel.1 onlink</span>
<span class="c">#    172.16.3.0/24 via 172.16.3.0 dev flannel.1 onlink</span>
<span class="c">#    192.168.10.0/24 dev enp0s8 proto kernel scope link src 192.168.10.10</span>
<span class="nv">$ </span><span class="nb">cat</span> /run/flannel/subnet.env
<span class="c"># =&gt; FLANNEL_NETWORK=172.16.0.0/16</span>
<span class="c">#    FLANNEL_SUBNET=172.16.0.1/24</span>
<span class="c">#    FLANNEL_MTU=1450</span>
<span class="c">#    FLANNEL_IPMASQ=true</span>
<span class="nv">$ </span>kubectl get nodes <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.items[*].spec.podCIDR}'</span> <span class="p">;</span><span class="nb">echo</span>
<span class="c"># =&gt; 172.16.0.0/24 172.16.3.0/24 172.16.1.0/24 172.16.2.0/24</span>
<span class="nv">$ </span>kubectl describe node | <span class="nb">grep</span> <span class="nt">-A3</span> Annotations
<span class="c"># =&gt; Annotations:        alpha.kubernetes.io/provided-node-ip: 10.0.2.15</span>
<span class="c">#                        flannel.alpha.coreos.com/backend-data: {&amp;quot;VNI&amp;quot;:1,&amp;quot;VtepMAC&amp;quot;:&amp;quot;02:21:77:da:a3:91&amp;quot;}</span>
<span class="c">#                        flannel.alpha.coreos.com/backend-type: vxlan</span>
<span class="c">#                        flannel.alpha.coreos.com/kube-subnet-manager: true</span>
<span class="c">#    --</span>
<span class="c">#    Annotations:        alpha.kubernetes.io/provided-node-ip: 10.0.2.15</span>
<span class="c">#                        flannel.alpha.coreos.com/backend-data: {&amp;quot;VNI&amp;quot;:1,&amp;quot;VtepMAC&amp;quot;:&amp;quot;72:95:9e:3d:c6:35&amp;quot;}</span>
<span class="c">#                        flannel.alpha.coreos.com/backend-type: vxlan</span>
<span class="c">#                        flannel.alpha.coreos.com/kube-subnet-manager: true</span>
<span class="c">#    --</span>
<span class="c">#    Annotations:        alpha.kubernetes.io/provided-node-ip: 10.0.2.15</span>
<span class="c">#                        flannel.alpha.coreos.com/backend-data: {&amp;quot;VNI&amp;quot;:1,&amp;quot;VtepMAC&amp;quot;:&amp;quot;ae:28:43:65:df:f4&amp;quot;}</span>
<span class="c">#                        flannel.alpha.coreos.com/backend-type: vxlan</span>
<span class="c">#                        flannel.alpha.coreos.com/kube-subnet-manager: true</span>
<span class="c">#    --</span>
<span class="c">#    Annotations:        alpha.kubernetes.io/provided-node-ip: 10.0.2.15</span>
<span class="c">#                        flannel.alpha.coreos.com/backend-data: {&amp;quot;VNI&amp;quot;:1,&amp;quot;VtepMAC&amp;quot;:&amp;quot;8e:91:37:7d:c1:d7&amp;quot;}</span>
<span class="c">#                        flannel.alpha.coreos.com/backend-type: vxlan</span>
<span class="c">#                        flannel.alpha.coreos.com/kube-subnet-manager: true</span>
<span class="nv">$ </span>brctl show
<span class="c"># =&gt; bridge name     bridge id               STP enabled     interfaces</span>
<span class="c">#    cni0            8000.ca30a0c85ccd       no              veth41d9e3b2</span>
<span class="c">#                                                            veth5c3de18a</span>
<span class="c">#                                                            vethfaeebb1c</span>

<span class="c"># 서비스와 엔드포인트 확인</span>
<span class="nv">$ </span>kubectl get svc,ep <span class="nt">-A</span>
<span class="c"># =&gt; NAMESPACE     NAME                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                  AGE</span>
<span class="c">#    default       service/kubernetes       ClusterIP   10.10.200.1     &amp;lt;none&amp;gt;        443/TCP                  38m</span>
<span class="c">#    kube-system   service/kube-dns         ClusterIP   10.10.200.10    &amp;lt;none&amp;gt;        53/UDP,53/TCP,9153/TCP   38m</span>
<span class="c">#    kube-system   service/metrics-server   ClusterIP   10.10.200.103   &amp;lt;none&amp;gt;        443/TCP                  38m</span>
<span class="c">#    </span>
<span class="c">#    NAMESPACE     NAME                       ENDPOINTS                                     AGE</span>
<span class="c">#    default       endpoints/kubernetes       10.0.2.15:6443                                38m</span>
<span class="c">#    kube-system   endpoints/kube-dns         172.16.0.4:53,172.16.0.4:53,172.16.0.4:9153   38m</span>
<span class="c">#    kube-system   endpoints/metrics-server   172.16.0.3:10250                              38m</span>

<span class="c"># iptables 정보 확인</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> filter <span class="nt">-S</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> mangle <span class="nt">-S</span>

<span class="c"># tcp listen 포트 정보 확인</span>
<span class="nv">$ </span>ss <span class="nt">-tnlp</span>
</code></pre></div></div>

<ul>
  <li>flannel CNI를 사용하고 있고, 클러스터 IP는 172.16.0.0/16이며, 컨트롤 플레인의기능들이 많이 내장되어있어 실행중인 파드가 적음을 확인 할 수 있습니다.</li>
</ul>

<h3 id="nginx-ingress-controller-설치">Nginx Ingress Controller 설치</h3>

<ul>
  <li>Nginx Ingress Controller는 가장 많이 사용되는 Ingress Controller 중 하나로 Ingress 실습을 위해 설치해보겠습니다.</li>
  <li>먼저 NGINX Ingress 의 특징을 살펴보겠습니다.
    <ul>
      <li>NGINX Ingress는 고성능 웹서버인 NGINX를 기반으로 동작하며, Layer 7에서 동작합니다.</li>
      <li>k8s의 configmap 설정을 lua 스크립트로 가공하여 nginx config로 변환하여 사용합니다.</li>
      <li>설정을 변경하면 내부의 nginx가 reload 되면서 자동으로 적용되며, 설정을 쉽게 변경할 수 있습니다.</li>
    </ul>
  </li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Ingress-Nginx 컨트롤러 생성</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOT</span><span class="sh">&gt; ingress-nginx-values.yaml
controller:
  service:
    type: NodePort
    nodePorts:
      http: 30080
      https: 30443
  nodeSelector:
    kubernetes.io/hostname: "k3s-s"
  metrics:
    enabled: true
  serviceMonitor:
      enabled: true
</span><span class="no">EOT

</span><span class="nv">$ </span>helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
<span class="c"># =&gt; "ingress-nginx" has been added to your repositories</span>
<span class="nv">$ </span>helm repo update
<span class="c"># =&gt; ...Successfully got an update from the "ingress-nginx" chart repository</span>

<span class="nv">$ </span>kubectl create ns ingress
<span class="c"># =&gt; namespace/ingress created</span>
<span class="nv">$ </span>helm <span class="nb">install </span>ingress-nginx ingress-nginx/ingress-nginx <span class="nt">-f</span> ingress-nginx-values.yaml <span class="nt">--namespace</span> ingress <span class="nt">--version</span> 4.11.2
<span class="c"># =&gt; Release &amp;quot;ingress-nginx&amp;quot; has been upgraded. Happy Helming!</span>
<span class="c">#    NAME: ingress-nginx</span>
<span class="c">#    NAMESPACE: ingress</span>
<span class="c">#    STATUS: deployed</span>
<span class="c">#    ...</span>
<span class="c">#    The ingress-nginx controller has been installed.</span>
<span class="c">#    Get the application URL by running these commands:</span>
<span class="c">#      export HTTP_NODE_PORT=30080</span>
<span class="c">#      export HTTPS_NODE_PORT=30443</span>
<span class="c">#      export NODE_IP=&amp;quot;$(kubectl get nodes --output jsonpath=&amp;quot;{.items[0].status.addresses[1].address}&amp;quot;)&amp;quot;</span>
<span class="c">#    </span>
<span class="c">#      echo &amp;quot;Visit http://${NODE_IP}:${HTTP_NODE_PORT} to access your application via HTTP.&amp;quot;</span>
<span class="c">#      echo &amp;quot;Visit https://${NODE_IP}:${HTTPS_NODE_PORT} to access your application via HTTPS.&amp;quot;</span>
<span class="c">#    ...</span>

<span class="c"># 확인</span>
<span class="nv">$ </span>kubectl get all <span class="nt">-n</span> ingress
<span class="c"># =&gt; NAME                                            READY   STATUS    RESTARTS   AGE</span>
<span class="c">#    pod/ingress-nginx-controller-7b67846f8f-jdt65   1/1     Running   0          47s</span>
<span class="c">#    </span>
<span class="c">#    NAME                                         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE</span>
<span class="c">#    service/ingress-nginx-controller             NodePort    10.10.200.113   &amp;lt;none&amp;gt;        80:30080/TCP,443:30443/TCP   47s</span>
<span class="c">#    service/ingress-nginx-controller-admission   ClusterIP   10.10.200.176   &amp;lt;none&amp;gt;        443/TCP                      47s</span>
<span class="c">#    service/ingress-nginx-controller-metrics     ClusterIP   10.10.200.218   &amp;lt;none&amp;gt;        10254/TCP                    47s</span>
<span class="c">#    </span>
<span class="c">#    NAME                                       READY   UP-TO-DATE   AVAILABLE   AGE</span>
<span class="c">#    deployment.apps/ingress-nginx-controller   1/1     1            1           47s</span>
<span class="c">#    </span>
<span class="c">#    NAME                                                  DESIRED   CURRENT   READY   AGE</span>
<span class="c">#    replicaset.apps/ingress-nginx-controller-7b67846f8f   1         1         1       47s</span>

<span class="nv">$ </span>kubectl describe svc <span class="nt">-n</span> ingress ingress-nginx-controller
<span class="c"># =&gt; Name:                     ingress-nginx-controller</span>
<span class="c">#    Namespace:                ingress</span>
<span class="c">#    Labels:                   app.kubernetes.io/component=controller</span>
<span class="c">#                              app.kubernetes.io/instance=ingress-nginx</span>
<span class="c">#    ...</span>
<span class="c">#    Selector:                 app.kubernetes.io/component=controller,app.kubernetes.io/instance=ingress-nginx,app.kubernetes.io/name=ingress-nginx</span>
<span class="c">#    Type:                     NodePort</span>
<span class="c">#    IP Family Policy:         SingleStack</span>
<span class="c">#    IP Families:              IPv4</span>
<span class="c">#    IP:                       10.10.200.113</span>
<span class="c">#    ...</span>
<span class="c">#    Port:                     http  80/TCP</span>
<span class="c">#    TargetPort:               http/TCP</span>
<span class="c">#    NodePort:                 http  30080/TCP</span>
<span class="c">#    Endpoints:                172.16.0.16:80</span>
<span class="c">#    Port:                     https  443/TCP</span>
<span class="c">#    TargetPort:               https/TCP</span>
<span class="c">#    NodePort:                 https  30443/TCP</span>
<span class="c">#    Endpoints:                172.16.0.16:443</span>
<span class="c">#    ...</span>

<span class="c"># externalTrafficPolicy 설정</span>
<span class="nv">$ </span>kubectl patch svc <span class="nt">-n</span> ingress ingress-nginx-controller <span class="nt">-p</span> <span class="s1">'{"spec":{"externalTrafficPolicy": "Local"}}'</span>
<span class="c"># =&gt; service/ingress-nginx-controller patched</span>

<span class="c"># 기본 nginx conf 파일 확인</span>
<span class="nv">$ </span>kubectl describe cm <span class="nt">-n</span> ingress ingress-nginx-controller
<span class="c"># =&gt; ...</span>
<span class="c">#    Data</span>
<span class="c">#    ====</span>
<span class="c">#    allow-snippet-annotations:</span>
<span class="c">#    ----</span>
<span class="c">#    false</span>
<span class="c">#    ...</span>
<span class="nv">$ </span>kubectl <span class="nb">exec </span>deploy/ingress-nginx-controller <span class="nt">-n</span> ingress <span class="nt">-it</span> <span class="nt">--</span> <span class="nb">cat</span> /etc/nginx/nginx.conf
<span class="c"># =&gt; # Configuration checksum: 13054992059071414660</span>
<span class="c">#    # setup custom paths that do not require root access</span>
<span class="c">#    pid /tmp/nginx/nginx.pid;</span>
<span class="c">#    </span>
<span class="c">#    daemon off;</span>
<span class="c">#    worker_processes 4;</span>
<span class="c">#    worker_rlimit_nofile 1047552;</span>
<span class="c">#    worker_shutdown_timeout 240s ;</span>
<span class="c">#    </span>
<span class="c">#    events {</span>
<span class="c">#            multi_accept        on;</span>
<span class="c">#            worker_connections  16384;</span>
<span class="c">#            use                 epoll;</span>
<span class="c">#    }</span>
<span class="c">#    </span>
<span class="c">#    http {</span>
<span class="c">#            lua_package_path &amp;quot;/etc/nginx/lua/?.lua;;&amp;quot;;</span>
<span class="c">#            lua_shared_dict balancer_ewma 10M;</span>
<span class="c">#    ...</span>

<span class="c"># 관련된 정보 확인 : 포드(Nginx 서버), 서비스, 디플로이먼트, 리플리카셋, 컨피그맵, 롤, 클러스터롤, 서비스 어카운트 등</span>
<span class="nv">$ </span>kubectl get all,sa,cm,secret,roles <span class="nt">-n</span> ingress
<span class="c"># =&gt; NAME                                            READY   STATUS    RESTARTS   AGE</span>
<span class="c">#    pod/ingress-nginx-controller-7b67846f8f-jdt65   1/1     Running   0          4m21s</span>
<span class="c">#    </span>
<span class="c">#    NAME                                         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE</span>
<span class="c">#    service/ingress-nginx-controller             NodePort    10.10.200.113   &amp;lt;none&amp;gt;        80:30080/TCP,443:30443/TCP   4m21s</span>
<span class="c">#    service/ingress-nginx-controller-admission   ClusterIP   10.10.200.176   &amp;lt;none&amp;gt;        443/TCP                      4m21s</span>
<span class="c">#    service/ingress-nginx-controller-metrics     ClusterIP   10.10.200.218   &amp;lt;none&amp;gt;        10254/TCP                    4m21s</span>
<span class="c">#    </span>
<span class="c">#    NAME                                       READY   UP-TO-DATE   AVAILABLE   AGE</span>
<span class="c">#    deployment.apps/ingress-nginx-controller   1/1     1            1           4m21s</span>
<span class="c">#    </span>
<span class="c">#    NAME                                                  DESIRED   CURRENT   READY   AGE</span>
<span class="c">#    replicaset.apps/ingress-nginx-controller-7b67846f8f   1         1         1       4m21s</span>
<span class="c">#    </span>
<span class="c">#    NAME                           SECRETS   AGE</span>
<span class="c">#    serviceaccount/default         0         4m29s</span>
<span class="c">#    serviceaccount/ingress-nginx   0         4m21s</span>
<span class="c">#    </span>
<span class="c">#    NAME                                 DATA   AGE</span>
<span class="c">#    configmap/ingress-nginx-controller   1      4m21s</span>
<span class="c">#    configmap/kube-root-ca.crt           1      4m30s</span>
<span class="c">#    </span>
<span class="c">#    NAME                                         TYPE                 DATA   AGE</span>
<span class="c">#    secret/ingress-nginx-admission               Opaque               3      4m24s</span>
<span class="c">#    secret/sh.helm.release.v1.ingress-nginx.v1   helm.sh/release.v1   1      4m26s</span>
<span class="c">#    </span>
<span class="c">#    NAME                                           CREATED AT</span>
<span class="c">#    role.rbac.authorization.k8s.io/ingress-nginx   2024-01-01T08:53:04Z</span>
<span class="nv">$ </span>kubectl describe clusterroles ingress-nginx
<span class="nv">$ </span>kubectl get pod,svc,ep <span class="nt">-n</span> ingress <span class="nt">-o</span> wide <span class="nt">-l</span> app.kubernetes.io/component<span class="o">=</span>controller

<span class="c"># 버전 정보 확인</span>
<span class="nv">$ POD_NAMESPACE</span><span class="o">=</span>ingress
<span class="nv">$ POD_NAME</span><span class="o">=</span><span class="si">$(</span>kubectl get pods <span class="nt">-n</span> <span class="nv">$POD_NAMESPACE</span> <span class="nt">-l</span> app.kubernetes.io/name<span class="o">=</span>ingress-nginx <span class="nt">--field-selector</span><span class="o">=</span>status.phase<span class="o">=</span>Running <span class="nt">-o</span> name<span class="si">)</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nv">$POD_NAME</span> <span class="nt">-n</span> <span class="nv">$POD_NAMESPACE</span> <span class="nt">--</span> /nginx-ingress-controller <span class="nt">--version</span>
<span class="c"># =&gt; -------------------------------------------------------------------------------</span>
<span class="c">#    NGINX Ingress controller</span>
<span class="c">#      Release:       v1.11.2</span>
<span class="c">#      Build:         46e76e5916813cfca2a9b0bfdc34b69a0000f6b9</span>
<span class="c">#      Repository:    https://github.com/kubernetes/ingress-nginx</span>
<span class="c">#      nginx version: nginx/1.25.5</span>
<span class="c">#    -------------------------------------------------------------------------------</span>
</code></pre></div></div>

<ul>
  <li>Ingress Controller가 설치되었으며, NodePort로 서비스가 생성된것을 확인할 수 있습니다.</li>
  <li>
    <p>또한 Nginx Ingress Controller의 경우 내부적으로는 일반적인 <strong>nginx 서버가 동일하게 동작</strong>하고, <strong>lua 스크립트를 사용하여 configmap의 설정이 적용/관리</strong>되고 있음을 확인할 수 있습니다.</p>
  </li>
  <li>(옵션) kubectl krew 설치 - <a href="https://krew.sigs.k8s.io/docs/user-guide/setup/install/">링크</a> &amp; ingress-nginx plugin 설치 - <a href="https://kubernetes.github.io/ingress-nginx/kubectl-plugin/">링크</a></li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># (참고) 운영체제 확인 : linux</span>
<span class="nv">$ OS</span><span class="o">=</span><span class="s2">"</span><span class="si">$(</span><span class="nb">uname</span> | <span class="nb">tr</span> <span class="s1">'[:upper:]'</span> <span class="s1">'[:lower:]'</span><span class="si">)</span><span class="s2">"</span>
<span class="c"># (참고)  CPU 아키텍처 확인 : amd64</span>
<span class="nv">$ ARCH</span><span class="o">=</span><span class="s2">"</span><span class="si">$(</span><span class="nb">uname</span> <span class="nt">-m</span> | <span class="nb">sed</span> <span class="nt">-e</span> <span class="s1">'s/x86_64/amd64/'</span> <span class="nt">-e</span> <span class="s1">'s/\(arm\)\(64\)\?.*/\1\2/'</span> <span class="nt">-e</span> <span class="s1">'s/aarch64$/arm64/'</span><span class="si">)</span><span class="s2">"</span>
<span class="c"># (참고)  KREW 지정 : krew-linux_amd64</span>
<span class="nv">$ KREW</span><span class="o">=</span><span class="s2">"krew-</span><span class="k">${</span><span class="nv">OS</span><span class="k">}</span><span class="s2">_</span><span class="k">${</span><span class="nv">ARCH</span><span class="k">}</span><span class="s2">"</span>

<span class="c"># kubectl krew 설치</span>
<span class="c"># curl -fsSLO "https://github.com/kubernetes-sigs/krew/releases/latest/download/${KREW}.tar.gz"</span>
<span class="nv">$ </span>curl <span class="nt">-fsSLO</span> <span class="s2">"https://github.com/kubernetes-sigs/krew/releases/latest/download/krew-linux_amd64.tar.gz"</span> <span class="o">&amp;&amp;</span> <span class="nb">tar </span>zxvf krew-linux_amd64.tar.gz <span class="o">&amp;&amp;</span> ./krew-linux_amd64 <span class="nb">install </span>krew
<span class="nv">$ </span><span class="nb">export </span><span class="nv">PATH</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">KREW_ROOT</span><span class="k">:-</span><span class="nv">$HOME</span><span class="p">/.krew</span><span class="k">}</span><span class="s2">/bin:</span><span class="nv">$PATH</span><span class="s2">"</span>

<span class="c"># 플러그인 정보 업데이트 후 확인 - 링크</span>
<span class="nv">$ </span>kubectl krew update
<span class="nv">$ </span>kubectl krew search

<span class="c"># ingress-nginx 플러그인 설치</span>
<span class="nv">$ </span>kubectl krew <span class="nb">install </span>ingress-nginx
<span class="c"># =&gt; (아쉽게도 옛날 버전이라서 설치가 안 됩니다.) </span>

<span class="c"># ingress-nginx 플러그인 명령어 실행(도움말 출력)</span>
<span class="nv">$ </span>kubectl ingress-nginx

<span class="c"># nginx ctrl 의 backends 설정 정보 출력</span>
<span class="nv">$ </span>kubectl ingress-nginx backends <span class="nt">-n</span> ingress-nginx <span class="nt">--list</span>
<span class="nv">$ </span>kubectl ingress-nginx backends <span class="nt">-n</span> ingress-nginx

<span class="c"># conf 출력</span>
<span class="nv">$ </span>kubectl ingress-nginx conf <span class="nt">-n</span> ingress-nginx
<span class="c">## 특정 호스트(도메인) 설정 확인</span>
<span class="nv">$ </span>kubectl ingress-nginx conf <span class="nt">-n</span> ingress-nginx <span class="nt">--host</span> gasida.cndk.link
<span class="nv">$ </span>kubectl ingress-nginx conf <span class="nt">-n</span> ingress-nginx <span class="nt">--host</span> nasida.cndk.link

<span class="c"># 정보 보기 편함!</span>
<span class="nv">$ </span>kubectl ingress-nginx ingresses
<span class="nv">$ </span>kubectl ingress-nginx ingresses <span class="nt">--all-namespaces</span>
</code></pre></div></div>

<h3 id="인그레스ingress-실습-및-통신-흐름-확인">인그레스(Ingress) 실습 및 통신 흐름 확인</h3>

<ul>
  <li>실습 구성도
    <ul>
      <li>컨트롤플레인 노드에 인그레스 컨트롤러(Nginx) 파드를 생성하고, NodePort 로 외부에 노출합니다.</li>
      <li>인그레스 정책 설정 : Host/Path routing, 실습의 편리를 위해서 도메인 없이 IP로 접속 설정 가능하도록 합니다.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/2024/kans-3th/w6/20241012_kans_w6_5.png" alt="img.png" /></p>

<p><img src="/assets/2024/kans-3th/w6/20241012_kans_w6_6.png" alt="img_1.png" /></p>

<h4 id="deployment와-service-생성">deployment와 service 생성</h4>

<ul>
  <li>svc1-pod.yml 생성
    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># svc1-pod.yml</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">deploy1-websrv</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">websrv</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">app</span><span class="pi">:</span> <span class="s">websrv</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">containers</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">pod-web</span>
        <span class="na">image</span><span class="pi">:</span> <span class="s">nginx</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">svc1-web</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">ports</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">web-port</span>
      <span class="na">port</span><span class="pi">:</span> <span class="m">9001</span>
      <span class="na">targetPort</span><span class="pi">:</span> <span class="m">80</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">websrv</span>
  <span class="na">type</span><span class="pi">:</span> <span class="s">ClusterIP</span>
</code></pre></div>    </div>
  </li>
  <li>svc2-pod.yml 생성
    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># svc2-pod.yml </span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">deploy2-guestsrv</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">2</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">guestsrv</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">app</span><span class="pi">:</span> <span class="s">guestsrv</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">containers</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">pod-guest</span>
        <span class="na">image</span><span class="pi">:</span> <span class="s">gcr.io/google-samples/kubernetes-bootcamp:v1</span>
        <span class="na">ports</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">8080</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">svc2-guest</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">ports</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">guest-port</span>
      <span class="na">port</span><span class="pi">:</span> <span class="m">9002</span>
      <span class="na">targetPort</span><span class="pi">:</span> <span class="m">8080</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">guestsrv</span>
  <span class="na">type</span><span class="pi">:</span> <span class="s">NodePort</span>
</code></pre></div>    </div>
  </li>
  <li>svc3-pod.yml 생성
    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># svc3-pod.yml</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">deploy3-adminsrv</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">3</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">adminsrv</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">app</span><span class="pi">:</span> <span class="s">adminsrv</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">containers</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">pod-admin</span>
        <span class="na">image</span><span class="pi">:</span> <span class="s">k8s.gcr.io/echoserver:1.5</span>
        <span class="na">ports</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">8080</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">svc3-admin</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">ports</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">admin-port</span>
      <span class="na">port</span><span class="pi">:</span> <span class="m">9003</span>
      <span class="na">targetPort</span><span class="pi">:</span> <span class="m">8080</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">adminsrv</span>
</code></pre></div>    </div>
  </li>
  <li>생성 및 확인
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 모니터링</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s1">'kubectl get ingress,svc,ep,pod -owide'</span>
  
<span class="c"># 생성</span>
<span class="nv">$ </span>kubectl taint nodes k3s-m <span class="nv">role</span><span class="o">=</span>controlplane:NoSchedule
<span class="c"># &lt;span style="color: green;"&gt;&lt;/span&gt;</span>
  
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> svc1-pod.yml,svc2-pod.yml,svc3-pod.yml
<span class="c"># =&gt; deployment.apps/deploy1-websrv created</span>
<span class="c">#    service/svc1-web created</span>
<span class="c">#    deployment.apps/deploy2-guestsrv created</span>
<span class="c">#    service/svc2-guest created</span>
<span class="c">#    deployment.apps/deploy3-adminsrv created</span>
<span class="c">#    service/svc3-admin created</span>
  
<span class="c"># 확인 : svc1, svc3 은 ClusterIP 로 클러스터 외부에서는 접속할 수 없다 &gt;&gt; Ingress 는 연결 가능!</span>
<span class="nv">$ </span>kubectl get pod,svc,ep
<span class="c"># =&gt; NAME                                    READY   STATUS    RESTARTS   AGE</span>
<span class="c">#    pod/deploy1-websrv-5c6b88bd77-ht5hl     1/1     Running   0          34s</span>
<span class="c">#    pod/deploy2-guestsrv-649875f78b-8wh8r   1/1     Running   0          34s</span>
<span class="c">#    pod/deploy2-guestsrv-649875f78b-jcvrf   1/1     Running   0          34s</span>
<span class="c">#    pod/deploy3-adminsrv-7c8f8b8c87-4mzv7   1/1     Running   0          34s</span>
<span class="c">#    pod/deploy3-adminsrv-7c8f8b8c87-4sqmh   1/1     Running   0          34s</span>
<span class="c">#    pod/deploy3-adminsrv-7c8f8b8c87-ztltl   1/1     Running   0          34s</span>
<span class="c">#    </span>
<span class="c">#    NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE</span>
<span class="c">#    service/kubernetes   ClusterIP   10.10.200.1     &amp;lt;none&amp;gt;        443/TCP          5h52m</span>
<span class="c">#    service/svc1-web     ClusterIP   10.10.200.141   &amp;lt;none&amp;gt;        9001/TCP         34s</span>
<span class="c">#    service/svc2-guest   NodePort    10.10.200.60    &amp;lt;none&amp;gt;        9002:30901/TCP   34s</span>
<span class="c">#    service/svc3-admin   ClusterIP   10.10.200.171   &amp;lt;none&amp;gt;        9003/TCP         34s</span>
<span class="c">#    &lt;span style="color: green;"&gt;# ingress는 pod 정보로 바로 접근 가능하므로 서비스가 ClusterIP이든 NodePort 타입이든 관계 없습니다.&lt;/span&gt;</span>
<span class="c">#    </span>
<span class="c">#    NAME                   ENDPOINTS                                         AGE</span>
<span class="c">#    endpoints/kubernetes   192.168.10.10:6443                                5h52m</span>
<span class="c">#    endpoints/svc1-web     172.16.1.8:80                                     34s</span>
<span class="c">#    endpoints/svc2-guest   172.16.2.8:8080,172.16.3.7:8080                   34s</span>
<span class="c">#    endpoints/svc3-admin   172.16.1.7:8080,172.16.2.9:8080,172.16.3.8:8080   34s</span>
</code></pre></div>    </div>
  </li>
</ul>

<h4 id="인그레스정책-생성">인그레스(정책) 생성</h4>

<p><img src="/assets/2024/kans-3th/w6/20241012_kans_w6_7.png" alt="img.png" class="image-center" />
<em class="image-caption">ingress 정책 적용 구조 (<a href="https://kschoi728.tistory.com/266">출처</a>)</em></p>

<ul>
  <li>ingress1.yml 파일 생성</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOT</span><span class="sh">&gt; ingress1.yml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-1
  annotations:
    #nginx.ingress.kubernetes.io/upstream-hash-by: "true"
spec:
  ingressClassName: nginx
  rules:
  - http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: svc1-web
            port:
              number: 80
      - path: /guest
        pathType: Prefix
        backend:
          service:
            name: svc2-guest
            port:
              number: 8080
      - path: /admin
        pathType: Prefix
        backend:
          service:
            name: svc3-admin
            port:
              number: 8080
</span><span class="no">EOT
</span></code></pre></div></div>

<ul>
  <li>ingress 정책 생성</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 모니터링</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s1">'kubectl get ingress,svc,ep,pod -owide'</span>

<span class="c"># 생성</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> ingress1.yml
<span class="c"># =&gt; ingress.networking.k8s.io/ingress-1 created</span>

<span class="c"># 확인</span>
<span class="nv">$ </span>kubectl get ingress
<span class="c"># =&gt; NAME        CLASS   HOSTS   ADDRESS   PORTS   AGE</span>
<span class="c">#    ingress-1   nginx   *                 80      11s</span>
<span class="nv">$ </span>kubectl describe ingress ingress-1
<span class="c"># =&gt; ...</span>
<span class="c">#    Rules:</span>
<span class="c">#      Host        Path  Backends</span>
<span class="c">#      ----        ----  --------</span>
<span class="c">#      *</span>
<span class="c">#                  /        svc1-web:80 ()</span>
<span class="c">#                  /guest   svc2-guest:8080 ()</span>
<span class="c">#                  /admin   svc3-admin:8080 ()</span>
<span class="c">#    ...</span>

<span class="c"># 설정이 반영된 nginx conf 파일 확인</span>
<span class="nv">$ </span>kubectl <span class="nb">exec </span>deploy/ingress-nginx-controller <span class="nt">-n</span> ingress <span class="nt">-it</span> <span class="nt">--</span> <span class="nb">cat</span> /etc/nginx/nginx.conf
<span class="nv">$ </span>kubectl <span class="nb">exec </span>deploy/ingress-nginx-controller <span class="nt">-n</span> ingress <span class="nt">-it</span> <span class="nt">--</span> <span class="nb">cat</span> /etc/nginx/nginx.conf | <span class="nb">grep</span> <span class="s1">'location /'</span> <span class="nt">-A5</span>
<span class="c"># =&gt;      location /guest/ {</span>
<span class="c">#    </span>
<span class="c">#         set $namespace      &amp;quot;default&amp;quot;;</span>
<span class="c">#         set $ingress_name   &amp;quot;ingress-1&amp;quot;;</span>
<span class="c">#         set $service_name   &amp;quot;svc2-guest&amp;quot;;</span>
<span class="c">#         set $service_port   &amp;quot;8080&amp;quot;;</span>
<span class="c">#    --</span>
<span class="c">#         location /admin/ {</span>
<span class="c">#    </span>
<span class="c">#         set $namespace      &amp;quot;default&amp;quot;;</span>
<span class="c">#         set $ingress_name   &amp;quot;ingress-1&amp;quot;;</span>
<span class="c">#         set $service_name   &amp;quot;svc3-admin&amp;quot;;</span>
<span class="c">#         set $service_port   &amp;quot;8080&amp;quot;;</span>
<span class="c">#    --</span>
<span class="c">#         location / {</span>
<span class="c">#    </span>
<span class="c">#         set $namespace      &amp;quot;default&amp;quot;;</span>
<span class="c">#         set $ingress_name   &amp;quot;ingress-1&amp;quot;;</span>
<span class="c">#         set $service_name   &amp;quot;svc1-web&amp;quot;;</span>
<span class="c">#         set $service_port   &amp;quot;80&amp;quot;;</span>
<span class="c">#    ...</span>
</code></pre></div></div>

<h4 id="ingress를-통한-내부-접속">ingress를 통한 내부 접속</h4>

<ul>
  <li>
    <p>Nginx ingress controller를 통해 접속시 서비스는 파드의 엔드포인트의 정보만 참조되고, 서비스를 거치지 않고 바로 파드로 전달됩니다.
<img src="/assets/2024/kans-3th/w6/20241012_kans_w6_8.png" alt="img.png" class="w-80 image-center" />
<em class="image-caption">인그레스 접속 경로(서비스 Bypass) : Ingress → 애플리케이션(Deploy, Pod 등)</em></p>
  </li>
  <li>참고 : URI(Uniform Resource Identifier)는 RFC 3986에 정의된 통합 자원 식별자로, 흔히 사용되는 URL(Uniform Resource Locator)과 URN(Uniform Resource Name)을 포함합니다.
    <ul>
      <li>Request URI는 서버 주소나 파일이름, 파라미터 등 다양한 리소스를 식별하기 위해 사용되는 문자열입니다.</li>
      <li>절대 URI(absolute URI)는 스키마와 호스트를 포함한 완전한 URI를 의미하며, 상대 URI(relative URI)는 스키마와 호스트를 포함하지 않고 현재 위치에서 상대적인 위치를 기록한 URI를 의미합니다.
        <ul>
          <li>URI의 구조는 아래와 같습니다.
<img src="/assets/2024/kans-3th/w6/20241012_kans_w6_9.png" alt="img.png" class="image-center" />
<em class="image-caption">책 ‘그림으로 공부하는 TCP/IP 구조’ 중 발췌</em></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>참고 : X-Forwarded-For 헤더, X-Forwarded-Proto 헤더
    <ul>
      <li>X-Forwarded-For 헤더는 송신지 IP 주소가 변환되는 환경(장비, 서버, 솔루션 등)에서, 변환 전 송신지(클라이언트) IP 주소를 저장하는 헤더입니다.
        <ul>
          <li>여러 장비나 솔루션을 거칠 경우 <code class="language-plaintext highlighter-rouge">,</code>로 구분하여 여러 건이 넘어올 수도 있습니다. 그럴 경우 가장 왼쪽 것이 클라이언트 IP이고, 오른쪽으로 갈 수록 나중에 처리된 장비/솔루션의 IP가 됩니다.</li>
        </ul>
      </li>
      <li>X-Forwarded-Proto 헤더는 변환 전 프로토콜을 저장합니다. (예. SSL Offload 환경에서 서버 측에서 클라이언트가 요청 시 사용한 원래 프로토콜을 확인)</li>
      <li>이러한 헤더는 클라이언트의 IP 주소를 확인하거나, 프로토콜을 확인하는 등의 용도로 사용되며, 어플리케이션에서 NodePort나 LoadBalancer를 통해서 접속되었을때도 원래의 클라이언트의 IP를 확인할 수 있게 해줍니다. (<code class="language-plaintext highlighter-rouge">externalTrafficPolicy: Local</code>를 사용할 필요가 줄어듭니다!)</li>
      <li>원래의 IP를 가져오는 방법은 다음의 방법들이 있습니다.
        <ul>
          <li>Http request header 중 다음 값들에서 원래의 IP 찾기
            <ol>
              <li>X-Forwarded-For : HTTP RFC 표준에는 없지만 사실상 표준!!!</li>
              <li>Proxy-Client-IP : 특정 웹 어플리케이션에서 사용 (예. WebLogic Connector - mod_wl)</li>
              <li>WL-Proxy-Client-IP : 특정 웹 어플리케이션에서 사용 (예. WebLogic Connector - mod_wl)</li>
              <li>CLIENT_IP</li>
            </ol>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>인그레스(Nginx 인그레스 컨트롤러)를 통한 접속(HTTP 인입)을 확인해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># (krew 플러그인 설치 시) 인그레스 정책 확인</span>
<span class="c"># $ kubectl ingress-nginx ingresses</span>
<span class="c"># INGRESS NAME   HOST+PATH   ADDRESSES       TLS   SERVICE      SERVICE PORT   ENDPOINTS</span>
<span class="c"># ingress-1      /           192.168.10.10   NO    svc1-web     80             1</span>
<span class="c"># ingress-1      /guest      192.168.10.10   NO    svc2-guest   8080           2</span>
<span class="c"># ingress-1      /admin      192.168.10.10   NO    svc3-admin   8080           3</span>

<span class="c">#</span>
<span class="nv">$ </span>kubectl get ingress
<span class="c"># =&gt; NAME        CLASS   HOSTS   ADDRESS         PORTS   AGE</span>
<span class="c">#    ingress-1   nginx   *       10.10.200.113   80      18m</span>
 
<span class="nv">$ </span>kubectl describe ingress ingress-1 | <span class="nb">sed</span> <span class="nt">-n</span> <span class="s2">"5, </span><span class="se">\$</span><span class="s2">p"</span>
<span class="c"># =&gt; ...</span>
<span class="c">#    Rules:</span>
<span class="c">#      Host        Path  Backends</span>
<span class="c">#      ----        ----  --------</span>
<span class="c">#      *</span>
<span class="c">#                  /        svc1-web:80 ()</span>
<span class="c">#                  /guest   svc2-guest:8080 ()</span>
<span class="c">#                  /admin   svc3-admin:8080 ()</span>
<span class="c">#    ...</span>

<span class="c"># 접속 로그 확인 : kubetail 설치되어 있음 - 출력되는 nginx 의 로그의 IP 확인</span>
<span class="nv">$ </span>kubetail <span class="nt">-n</span> ingress <span class="nt">-l</span> app.kubernetes.io/component<span class="o">=</span>controller

<span class="nt">-------------------------------</span>
<span class="c"># 자신의 집 PC에서 인그레스를 통한 접속 : 각각 </span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"Ingress1 sv1-web URL = http://</span><span class="si">$(</span>curl <span class="nt">-s</span> ipinfo.io/ip<span class="si">)</span><span class="s2">:30080"</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"Ingress1 sv2-guest URL = http://</span><span class="si">$(</span>curl <span class="nt">-s</span> ipinfo.io/ip<span class="si">)</span><span class="s2">:30080/guest"</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"Ingress1 sv3-admin URL = http://</span><span class="si">$(</span>curl <span class="nt">-s</span> ipinfo.io/ip<span class="si">)</span><span class="s2">:30080/admin"</span>

<span class="c"># svc1-web 접속</span>
<span class="c"># $ MYIP=&lt;EC2 공인 IP 또는 컨트롤플레인 node ip&gt;</span>
<span class="nv">$ MYIP</span><span class="o">=</span>192.168.10.10
<span class="nv">$ </span>curl <span class="nt">-s</span> <span class="nv">$MYIP</span>:30080
<span class="c"># =&gt; ...</span>
<span class="c">#    &amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;</span>
<span class="c">#    &amp;lt;p&amp;gt;If you see this page, the nginx web server is successfully installed and</span>
<span class="c">#    working. Further configuration is required.&amp;lt;/p&amp;gt;</span>
<span class="c">#    </span>
<span class="c">#    &amp;lt;p&amp;gt;For online documentation and support please refer to</span>
<span class="c">#    &amp;lt;a href=&amp;quot;http://nginx.org/&amp;quot;&amp;gt;nginx.org&amp;lt;/a&amp;gt;.&amp;lt;br/&amp;gt;</span>
<span class="c">#    Commercial support is available at</span>
<span class="c">#    &amp;lt;a href=&amp;quot;http://nginx.com/&amp;quot;&amp;gt;nginx.com&amp;lt;/a&amp;gt;.&amp;lt;/p&amp;gt;</span>
<span class="c">#    </span>
<span class="c">#    &amp;lt;p&amp;gt;&amp;lt;em&amp;gt;Thank you for using nginx.&amp;lt;/em&amp;gt;&amp;lt;/p&amp;gt;</span>
<span class="c">#    ...</span>

<span class="c"># svc2-guest 접속</span>
<span class="nv">$ </span>curl <span class="nt">-s</span> <span class="nv">$MYIP</span>:30080/guest
<span class="c"># =&gt; Hello Kubernetes bootcamp! | Running on: deploy2-guestsrv-649875f78b-jcvrf | v=1</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$MYIP</span>:30080/guest <span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
<span class="c"># =&gt;      51 Hello Kubernetes bootcamp! | Running on: deploy2-guestsrv-649875f78b-8wh8r | v=1</span>
<span class="c">#         49 Hello Kubernetes bootcamp! | Running on: deploy2-guestsrv-649875f78b-jcvrf | v=1</span>

<span class="c"># svc3-admin 접속 &gt; 기본적으로 Nginx 는 라운드로빈 부하분산 알고리즘을 사용 &gt;&gt; Client_address 와 XFF 주소는 어떤 주소인가요?</span>
<span class="nv">$ </span>curl <span class="nt">-s</span> <span class="nv">$MYIP</span>:30080/admin
<span class="nv">$ </span>curl <span class="nt">-s</span> <span class="nv">$MYIP</span>:30080/admin | egrep <span class="s1">'(client_address|x-forwarded-for)'</span>
<span class="c"># =&gt;  client_address=172.16.0.16</span>
<span class="c">#     x-forwarded-for=172.16.0.1</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$MYIP</span>:30080/admin | <span class="nb">grep </span>Hostname <span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
<span class="c"># =&gt;      34 Hostname: deploy3-adminsrv-7c8f8b8c87-4sqmh</span>
<span class="c">#         33 Hostname: deploy3-adminsrv-7c8f8b8c87-ztltl</span>
<span class="c">#         33 Hostname: deploy3-adminsrv-7c8f8b8c87-4mzv7</span>

<span class="c"># (옵션) 디플로이먼트의 파드 갯수를 증가/감소 설정 후 접속 테스트 해보자</span>
<span class="nv">$ </span>kubectl scale deployment deploy3-adminsrv <span class="nt">--replicas</span> 2   <span class="c"># svc3-admin의 파드 갯수를 2개로 감소</span>
<span class="c"># =&gt; deployment.apps/deploy3-adminsrv scaled</span>
<span class="nv">$ </span>kubectl get deploy deploy3-adminsrv
<span class="c"># =&gt; NAME               READY   UP-TO-DATE   AVAILABLE   AGE</span>
<span class="c">#    deploy3-adminsrv   2/2     2            2           80m</span>
<span class="c"># &lt;span style="color: green;"&gt;파드수가 3개 =&gt; 2개로 줄었습니다.&lt;/span&gt;</span>
 
<span class="c"># 접속 테스트</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$MYIP</span>:30080/admin | <span class="nb">grep </span>Hostname <span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
<span class="c"># =&gt;      50 Hostname: deploy3-adminsrv-7c8f8b8c87-4sqmh</span>
<span class="c">#         50 Hostname: deploy3-adminsrv-7c8f8b8c87-4mzv7</span>
<span class="c"># &lt;span style="color: green;"&gt;2개로 줄어든 파드수만큼 2개의 파드에 부하가 분산 되는것을 확인하였습니다.&lt;/span&gt;</span>
</code></pre></div></div>

<ul>
  <li>노드에서 패킷 캡쳐 확인 : flannel vxlan의 파드간 통신시 IP정보 확인</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># ngrep을 이용해 패킷 캡쳐</span>
<span class="nv">$ </span>ngrep <span class="nt">-tW</span> byline <span class="nt">-d</span> enp0s8 <span class="s1">''</span> udp port 8472 or tcp port 80
<span class="c"># =&gt; interface: enp0s8 (192.168.10.0/255.255.255.0)</span>
<span class="c">#    filter: ( udp port 8472 or tcp port 80 ) and ((ip || ip6) || (vlan &amp;amp;&amp;amp; (ip || ip6)))</span>
<span class="c">#    #</span>
<span class="c">#    U 2024/10/12 12:42:39.071289 192.168.10.10:39828 -&amp;gt; 192.168.10.102:8472 #1</span>
<span class="c">#    .........(Ce...!w.....E..&amp;lt;..@.?............|.P...........\...........</span>
<span class="c">#    d9?.........</span>
<span class="c">#    #</span>
<span class="c">#    U 2024/10/12 12:42:39.072521 192.168.10.102:37126 -&amp;gt; 192.168.10.10:8472 #2</span>
<span class="c">#    .........!w....(Ce....E..&amp;lt;..@.?............P.|(3c@.......4.y.........</span>
<span class="c">#    ....d9?.....</span>
<span class="c">#    #</span>
<span class="c">#    U 2024/10/12 12:42:39.072734 192.168.10.10:39828 -&amp;gt; 192.168.10.102:8472 #3</span>
<span class="c">#    .........(Ce...!w.....E..4..@.?............|.P....(3cA.....K.....</span>
<span class="c">#    d9?.....</span>
<span class="c">#    #</span>
<span class="c">#    U 2024/10/12 12:42:39.072855 192.168.10.10:39828 -&amp;gt; 192.168.10.102:8472 #4</span>
<span class="c">#    .........(Ce...!w.....E..c..@.?..Y.........|.P....(3cA...........</span>
<span class="c">#    d9?.....GET / HTTP/1.1.</span>
<span class="c">#    Host: localhost:30080.</span>
<span class="c">#    X-Request-ID: e8aa4e70150ae6ae8de5a34637e294e6.</span>
<span class="c">#    X-Real-IP: 172.16.0.1.</span>
<span class="c">#    X-Forwarded-For: 172.16.0.1.</span>
<span class="c">#    X-Forwarded-Host: localhost:30080.</span>
<span class="c">#    X-Forwarded-Port: 80.</span>
<span class="c">#    X-Forwarded-Proto: http.</span>
<span class="c">#    X-Forwarded-Scheme: http.</span>
<span class="c">#    X-Scheme: http.</span>
<span class="c">#    User-Agent: curl/7.81.0.</span>
<span class="c">#    Accept: */*.</span>
<span class="c">#    ...</span>

<span class="c"># tcp dump를 이용해 vxlan(udp 8472) 통신 확인</span>
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> enp0s8 udp port 8472 <span class="nt">-nn</span>
<span class="c"># =&gt; tcpdump: verbose output suppressed, use -v[v]... for full protocol decode</span>
<span class="c">#    listening on enp0s8, link-type EN10MB (Ethernet), snapshot length 262144 bytes</span>
<span class="c">#    12:42:13.504948 IP 192.168.10.10.55617 &amp;gt; 192.168.10.102.8472: OTV, flags [I] (0x08), overlay 0, instance 1</span>
<span class="c">#    IP 172.16.0.16.57692 &amp;gt; 172.16.1.8.80: Flags [S], seq 911277209, win 64860, options [mss 1410,sackOK,TS val 1681447926 ecr 0,nop,wscale 7], length 0</span>
<span class="c">#    ...</span>

<span class="c"># vethY는 각자 k3s-s 의 가장 마지막 veth 를 지정</span>
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> vethY tcp port 8080 <span class="nt">-nn</span>
<span class="c"># =&gt; tcpdump: verbose output suppressed, use -v[v]... for full protocol decode</span>
<span class="c">#    listening on veth5ae3dd58, link-type EN10MB (Ethernet), snapshot length 262144 bytes</span>
<span class="c">#    12:44:11.609334 IP 172.16.0.16.41240 &amp;gt; 172.16.2.9.8080: Flags [S], seq 1593288127, win 64860, options [mss 1410,sackOK,TS val 3487210526 ecr 0,nop,wscale 7], length 0</span>
<span class="c">#    12:44:11.610875 IP 172.16.2.9.8080 &amp;gt; 172.16.0.16.41240: Flags [S.], seq 1820942288, ack 1593288128, win 64308, options [mss 1410,sackOK,TS val 257720908 ecr 3487210526,nop,wscale 7], length 0</span>
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> vethY tcp port 8080 <span class="nt">-w</span> /tmp/ingress-nginx.pcap

<span class="nt">---</span> 

<span class="c"># 다른 터미널에서 svc3-admin 접속</span>
<span class="nv">$ </span>curl <span class="nt">-s</span> <span class="nv">$MYIP</span>:30080/admin 

<span class="nt">---</span>

<span class="c"># 자신의 PC에서 k3s-s EC2 공인 IP로 pcap 다운로드</span>
<span class="c"># $ scp ubuntu@&lt;k3s-s EC2 공인 IP&gt;:/tmp/ingress-nginx.pcap ~/Downloads</span>
<span class="nv">$ </span>scp ubuntu@43.202.1.177:/tmp/ingress-nginx.pcap ~/Downloads
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w6/20241012_kans_w6_10.png" alt="20241012_kans_w6_10.png" class="image-center" />
<em class="image-caption">인그레스를 통한 접속 흐름</em></p>

<ul>
  <li>패킷 캡쳐 결과 ingress controller에서 파드의 ip로 바로 접속 됨을 확인할 수 있었습니다.</li>
  <li>
    <p>또한, flannel CNI를 사용하기 때문에 vxlan을 통해 통신이 이루어지고 있음을 확인할 수 있습니다.</p>
  </li>
  <li>Nginx 파드가 endpoint 정보 등을 모니터링 가능한 이유는 클러스터롤과 롤(엔드포인트 list, watch)를 바인딩된 서비스 어카운트를 파드가 사용하기 때문입니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl describe deployments.apps <span class="nt">-n</span> ingress ingress-nginx-controller | <span class="nb">grep</span> <span class="s1">'Service Account'</span>
<span class="c"># =&gt;   Service Account:  ingress-nginx</span>

<span class="nv">$ </span>kubectl get <span class="nt">-n</span> ingress clusterrolebindings ingress-nginx
<span class="c"># =&gt; NAME            ROLE                        AGE</span>
<span class="c">#    ingress-nginx   ClusterRole/ingress-nginx   4h9m</span>

<span class="nv">$ </span>kubectl get <span class="nt">-n</span> ingress rolebindings ingress-nginx
<span class="c"># =&gt; NAME            ROLE                 AGE</span>
<span class="c">#    ingress-nginx   Role/ingress-nginx   4h9m</span>

<span class="nv">$ </span>kubectl describe clusterrole ingress <span class="nt">-n</span> ingress | egrep <span class="s1">'(Verbs|endpoints)'</span>
<span class="c"># =&gt;   Resources                           Non-Resource URLs  Resource Names  Verbs</span>
<span class="c">#      endpointslices.discovery.k8s.io     []                 []              [list watch get]</span>
<span class="c">#      endpoints                           []                 []              [list watch]</span>

<span class="nv">$ </span>kubectl describe roles ingress-nginx <span class="nt">-n</span> ingress | egrep <span class="s1">'(Verbs|endpoints)'</span>
<span class="c"># =&gt;   Resources                           Non-Resource URLs  Resource Names          Verbs</span>
<span class="c">#      endpoints                           []                 []                      [get list watch]</span>
<span class="c">#      endpointslices.discovery.k8s.io     []                 []                      [list watch get]</span>
</code></pre></div></div>

<h4 id="패킷-분석">패킷 분석</h4>

<ul>
  <li>클러스터 외부에서 접속 후 내부로 접속하는 패킷을 분석해보겠습니다.</li>
  <li>위의 실습과 동일하지만 veth에서 8080을 캡쳐하고 노드의 nic에서 8472 (vxnet)를 캡쳐하여 병합(merge)하여 확인하였습니다.</li>
  <li>또한, 클라이언트의 IP 주소를 확인하기 위해 X-Forwarded-For 헤더를 확인하였습니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w6/20241012_kans_w6_11.png" alt="20241012_kans_w6_11.png" /></p>

<ul>
  <li>위의 그림과 같이 프로토콜의 정보를 그림으로 보려면 아래와 같이 환경설정에서 Appearance &gt; Layout에서 Pane 3에 
“Packet Diagram”을 선택하시면 됩니다.
<img src="/assets/2024/kans-3th/w6/20241012_kans_w6_12.png" alt="20241012_kans_w6_12.png" class="w-80 image-center" /></li>
</ul>

<h4 id="nginx-분산-알고리즘-변경">Nginx 분산 알고리즘 변경</h4>

<ul>
  <li>nginx는 기본 RR(Round Robin) 방식으로 부하분산을 수행하지만, IP-Hash나 Session Cookie 설정으로 변경할 수 있습니다.</li>
  <li>특히 IP-Hash 나 Session Cookie를 사용하면 각 클라이언트에서 대상 파드를 고정할 수 있습니다.</li>
  <li>이를 변경하기 위해서는 <code class="language-plaintext highlighter-rouge">nginx.ingress.kubernetes.io/upstream-hash-by</code> annotation을 사용하여 변경하여야 하는데 실습을 통해 확인해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># mypc</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$MYIP</span>:30080/admin | <span class="nb">grep </span>Hostname <span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
<span class="c"># =&gt;   51 Hostname: deploy3-adminsrv-7c8f8b8c87-4sqmh</span>
<span class="c">#      49 Hostname: deploy3-adminsrv-7c8f8b8c87-4mzv7</span>
<span class="nv">$ </span><span class="k">while </span><span class="nb">true</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nt">--connect-timeout</span> 1 <span class="nv">$MYIP</span>:30080/admin | <span class="nb">grep </span>Hostname <span class="p">;</span> <span class="nb">date</span> <span class="s2">"+%Y-%m-%d %H:%M:%S"</span> <span class="p">;</span> <span class="nb">echo</span> <span class="s2">"--------------"</span> <span class="p">;</span> <span class="nb">sleep </span>1<span class="p">;</span> <span class="k">done</span>

<span class="c"># 아래 ingress 설정 중 IP-Hash 설정 &gt; # 주석 제거</span>
<span class="nv">$ </span><span class="nb">sed</span> <span class="nt">-i</span> <span class="s1">'s/#nginx.ingress/nginx.ingress/g'</span> ingress1.yml
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> ingress1.yml
<span class="c"># =&gt; ingress.networking.k8s.io/ingress-1 configured</span>

<span class="c"># 접속 확인</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$MYIP</span>:30080/admin | <span class="nb">grep </span>Hostname <span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
<span class="c"># =&gt;  100 Hostname: deploy3-adminsrv-7c8f8b8c87-4sqmh</span>
<span class="nv">$ </span><span class="k">while </span><span class="nb">true</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nt">--connect-timeout</span> 1 <span class="nv">$MYIP</span>:30080/admin | <span class="nb">grep </span>Hostname <span class="p">;</span> <span class="nb">date</span> <span class="s2">"+%Y-%m-%d %H:%M:%S"</span> <span class="p">;</span> <span class="nb">echo</span> <span class="s2">"--------------"</span> <span class="p">;</span> <span class="nb">sleep </span>1<span class="p">;</span> <span class="k">done</span>

<span class="c"># 다시 원복(라운드 로빈) &gt; # 주석 추가</span>
<span class="nv">$ </span><span class="nb">sed</span> <span class="nt">-i</span> <span class="s1">'s/nginx.ingress/#nginx.ingress/g'</span> ingress1.yml
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> ingress1.yml
<span class="c"># =&gt; ingress.networking.k8s.io/ingress-1 configured</span>

<span class="c"># 접속 확인</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$MYIP</span>:30080/admin | <span class="nb">grep </span>Hostname <span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
<span class="c"># =&gt;   50 Hostname: deploy3-adminsrv-7c8f8b8c87-4sqmh</span>
<span class="c">#      50 Hostname: deploy3-adminsrv-7c8f8b8c87-4mzv7</span>
<span class="nv">$ </span><span class="k">while </span><span class="nb">true</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nt">--connect-timeout</span> 1 <span class="nv">$MYIP</span>:30080/admin | <span class="nb">grep </span>Hostname <span class="p">;</span> <span class="nb">date</span> <span class="s2">"+%Y-%m-%d %H:%M:%S"</span> <span class="p">;</span> <span class="nb">echo</span> <span class="s2">"--------------"</span> <span class="p">;</span> <span class="nb">sleep </span>1<span class="p">;</span> <span class="k">done</span>
</code></pre></div></div>

<ul>
  <li>ip-hash 설정을 통해 클라이언트의 IP 주소를 해싱하여 특정 파드로 접속되는 것을 확인할 수 있습니다.</li>
  <li>오브젝트 삭제</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl delete deployments,svc,ingress <span class="nt">--all</span>
</code></pre></div></div>

<h4 id="host-기반-라우팅">Host 기반 라우팅</h4>

<ul>
  <li>ingress2.yml 파일 생성</li>
</ul>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">networking.k8s.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Ingress</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">ingress-2</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">ingressClassName</span><span class="pi">:</span> <span class="s">nginx</span>
  <span class="na">rules</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">host</span><span class="pi">:</span> <span class="s">kans.com</span>
    <span class="na">http</span><span class="pi">:</span>
      <span class="na">paths</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">path</span><span class="pi">:</span> <span class="s">/</span>
        <span class="na">pathType</span><span class="pi">:</span> <span class="s">Prefix</span>
        <span class="na">backend</span><span class="pi">:</span>
          <span class="na">service</span><span class="pi">:</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">svc3-admin</span>
            <span class="na">port</span><span class="pi">:</span>
              <span class="na">number</span><span class="pi">:</span> <span class="m">8080</span>
  <span class="pi">-</span> <span class="na">host</span><span class="pi">:</span> <span class="s2">"</span><span class="s">*.kans.com"</span>
    <span class="na">http</span><span class="pi">:</span>
      <span class="na">paths</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">path</span><span class="pi">:</span> <span class="s">/echo</span>
        <span class="na">pathType</span><span class="pi">:</span> <span class="s">Prefix</span>
        <span class="na">backend</span><span class="pi">:</span>
          <span class="na">service</span><span class="pi">:</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">svc3-admin</span>
            <span class="na">port</span><span class="pi">:</span>
              <span class="na">number</span><span class="pi">:</span> <span class="m">8080</span>
</code></pre></div></div>

<ul>
  <li>ingress 정책 생성</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 터미널1</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s1">'kubectl get ingresses,svc,ep,pod -owide'</span>

<span class="c"># 도메인 변경</span>
<span class="c"># $ MYDOMAIN1=&lt;각자 자신의 닉네임의 도메인&gt; 예시) gasida.com</span>
<span class="nv">$ MYDOMAIN1</span><span class="o">=</span>sweetlittlebird.com
<span class="nv">$ </span><span class="nb">sed</span> <span class="nt">-i</span> <span class="s2">"s/kans.com/</span><span class="nv">$MYDOMAIN1</span><span class="s2">/g"</span> ingress2.yaml

<span class="c"># 생성</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> ingress2.yaml,svc3-pod.yaml
<span class="c"># =&gt; ingress.networking.k8s.io/ingress-2 created</span>
<span class="c">#    deployment.apps/deploy3-adminsrv created</span>
<span class="c">#    service/svc3-admin created</span>

<span class="c"># 확인</span>
<span class="nv">$ </span>kubectl get ingress
<span class="c"># =&gt; NAME        CLASS   HOSTS                                       ADDRESS         PORTS   AGE</span>
<span class="c">#    ingress-2   nginx   sweetlittlebird.com,*.sweetlittlebird.com   10.10.200.113   80      14s</span>
<span class="nv">$ </span>kubectl describe ingress ingress-2
<span class="nv">$ </span>kubectl describe ingress ingress-2 | <span class="nb">sed</span> <span class="nt">-n</span> <span class="s2">"5, </span><span class="se">\$</span><span class="s2">p"</span>
<span class="c"># =&gt; ...</span>
<span class="c">#    Default backend:  &amp;lt;default&amp;gt;</span>
<span class="c">#    Rules:</span>
<span class="c">#      Host                   Path    Backends</span>
<span class="c">#      ----                   ----    --------</span>
<span class="c">#      sweetlittlebird.com    /       svc3-admin:8080 ()</span>
<span class="c">#      *.sweetlittlebird.com  /echo   svc3-admin:8080 ()</span>
<span class="c">#    ...</span>
</code></pre></div></div>

<ul>
  <li>Host 기반 라우팅을 통해 서비스에 접속해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 로그 모니터링</span>
<span class="nv">$ </span>kubetail <span class="nt">-n</span> ingress <span class="nt">-l</span> app.kubernetes.io/component<span class="o">=</span>controller
<span class="c"># =&gt; Will tail 1 logs...</span>
<span class="c">#    ingress-nginx-controller-7b67846f8f-jdt65</span>
<span class="c"># =&gt; [ingress-nginx-controller-7b67846f8f-jdt65] 192.168.10.1 - - [01/Jan/2024:13:52:42 +0000] &amp;quot;GET / HTTP/1.1&amp;quot; 200 677 &amp;quot;-&amp;quot; &amp;quot;curl/8.5.0&amp;quot; 88 0.002 [default-svc3-admin-8080] [] 172.16.3.10:8080 852 0.002 200 f22ddba305f55138796fc866f7416890</span>
<span class="c">#    [ingress-nginx-controller-7b67846f8f-jdt65] 192.168.10.1 - - [01/Jan/2024:13:53:47 +0000] &amp;quot;GET /admin HTTP/1.1&amp;quot; 200 687 &amp;quot;-&amp;quot; &amp;quot;curl/8.5.0&amp;quot; 93 0.002 [default-svc3-admin-8080] [] 172.16.2.10:8080 863 0.002 200 966f7a55060f1497eed20818d4bef890</span>
<span class="c">#    ...</span>

<span class="nt">------------</span>
<span class="c"># 자신의 PC 에서 접속 테스트</span>
<span class="c"># svc3-admin 접속 &gt; 결과 확인 : 왜 접속이 되지 않는가? HTTP 헤더에 Host 필드를 잘 확인해보자!</span>
<span class="nv">$ </span>curl <span class="nv">$MYIP</span>:30080 <span class="nt">-v</span>
<span class="nv">$ </span>curl <span class="nv">$MYIP</span>:30080/echo <span class="nt">-v</span>

<span class="c"># mypc에서 접속을 위한 설정</span>
<span class="c">## /etc/hosts 수정 : 도메인 이름으로 접속하기 위해서 변수 지정</span>
<span class="c">## 윈도우 C:\Windows\System32\drivers\etc\hosts</span>
<span class="c">## 맥 sudo vim /etc/hosts</span>
<span class="c"># $ MYDOMAIN1=&lt;각자 자신의 닉네임의 도메인&gt;</span>
<span class="c"># $ MYDOMAIN2=&lt;test.각자 자신의 닉네임의 도메인&gt;</span>
<span class="nv">$ MYDOMAIN1</span><span class="o">=</span>sweetlittlebird.com
<span class="nv">$ MYDOMAIN2</span><span class="o">=</span>test.sweetlittlebird.com
<span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$MYIP</span> <span class="nv">$MYDOMAIN1</span> <span class="nv">$MYDOMAIN2</span>
<span class="c"># =&gt; 192.168.10.10 sweetlittlebird.com test.sweetlittlebird.com</span>

<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"</span><span class="nv">$MYIP</span><span class="s2"> </span><span class="nv">$MYDOMAIN1</span><span class="s2">"</span> | <span class="nb">sudo tee</span> <span class="nt">-a</span> /etc/hosts
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"</span><span class="nv">$MYIP</span><span class="s2"> </span><span class="nv">$MYDOMAIN2</span><span class="s2">"</span> | <span class="nb">sudo tee</span> <span class="nt">-a</span> /etc/hosts
<span class="nv">$ </span><span class="nb">cat</span> /etc/hosts | <span class="nb">grep</span> <span class="nv">$MYDOMAIN1</span>
<span class="c"># =&gt; 192.168.10.10 sweetlittlebird.com</span>
<span class="c">#    192.168.10.10 test.sweetlittlebird.com</span>

<span class="c"># svc3-admin 접속 &gt; 결과 확인</span>
<span class="nv">$ </span>curl <span class="nv">$MYDOMAIN1</span>:30080 <span class="nt">-v</span>
<span class="c"># =&gt; * Host sweetlittlebird.com:30080 was resolved.</span>
<span class="c">#    * IPv4: 192.168.10.10</span>
<span class="c">#    *   Trying 192.168.10.10:30080...</span>
<span class="c">#    * Connected to sweetlittlebird.com (192.168.10.10) port 30080</span>
<span class="c">#    &amp;gt; GET / HTTP/1.1</span>
<span class="c">#    &amp;gt; Host: sweetlittlebird.com:30080</span>
<span class="c">#    &amp;gt; User-Agent: curl/8.5.0</span>
<span class="c">#    &amp;gt; Accept: */*</span>
<span class="c">#    &amp;gt;</span>
<span class="c">#    &amp;lt; HTTP/1.1 200 OK</span>
<span class="c">#    &amp;lt; Date: Sat, 01 Jan 2024 13:52:42 GMT</span>
<span class="c">#    &amp;lt; Content-Type: text/plain</span>
<span class="c">#    &amp;lt; Transfer-Encoding: chunked</span>
<span class="c">#    &amp;lt; Connection: keep-alive</span>
<span class="c">#    &amp;lt;</span>
<span class="c">#    </span>
<span class="c">#    Hostname: deploy3-adminsrv-7c8f8b8c87-48xwp</span>
<span class="c">#    </span>
<span class="c">#    Pod Information:</span>
<span class="c">#            -no pod information available-</span>
<span class="c">#    </span>
<span class="c">#    Server values:</span>
<span class="c">#            server_version=nginx: 1.13.0 - lua: 10008</span>
<span class="c">#    </span>
<span class="c">#    Request Information:</span>
<span class="c">#            client_address=172.16.0.16</span>
<span class="c">#            method=GET</span>
<span class="c">#            real path=/</span>
<span class="c">#            query=</span>
<span class="c">#            request_version=1.1</span>
<span class="c">#            request_uri=http://sweetlittlebird.com:8080/</span>
<span class="c">#    </span>
<span class="c">#    Request Headers:</span>
<span class="c">#            accept=*/*</span>
<span class="c">#            host=sweetlittlebird.com:30080</span>
<span class="c">#            user-agent=curl/8.5.0</span>
<span class="c">#            x-forwarded-for=192.168.10.1</span>
<span class="c">#            x-forwarded-host=sweetlittlebird.com:30080</span>
<span class="c">#            x-forwarded-port=80</span>
<span class="c">#            x-forwarded-proto=http</span>
<span class="c">#            ...</span>
<span class="nv">$ </span>curl <span class="nv">$MYDOMAIN1</span>:30080/admin
<span class="c"># =&gt; </span>
<span class="c">#    </span>
<span class="c">#    Hostname: deploy3-adminsrv-7c8f8b8c87-bm7dq</span>
<span class="c">#            ...</span>
<span class="c">#            client_address=172.16.0.16</span>
<span class="c">#            method=GET</span>
<span class="c">#            real path=/admin</span>
<span class="c">#            ...</span>
<span class="c">#            request_uri=http://sweetlittlebird.com:8080/admin</span>
<span class="c">#    </span>
<span class="c">#    Request Headers:</span>
<span class="c">#            accept=*/*</span>
<span class="c">#            host=sweetlittlebird.com:30080</span>
<span class="c">#            user-agent=curl/8.5.0</span>
<span class="c">#            x-forwarded-for=192.168.10.1</span>
<span class="c">#            x-forwarded-host=sweetlittlebird.com:30080</span>
<span class="c">#            x-forwarded-port=80</span>
<span class="c">#            x-forwarded-proto=http</span>
<span class="c">#            x-forwarded-scheme=http</span>
<span class="c">#            ...</span>
<span class="nv">$ </span>curl <span class="nv">$MYDOMAIN1</span>:30080/echo
<span class="c"># =&gt; Hostname: deploy3-adminsrv-7c8f8b8c87-ndwkj</span>
<span class="c">#            ...</span>
<span class="c">#            client_address=172.16.0.16</span>
<span class="c">#            method=GET</span>
<span class="c">#            real path=/echo</span>
<span class="c">#            ...</span>
<span class="c">#            request_uri=http://sweetlittlebird.com:8080/echo</span>
<span class="c">#    </span>
<span class="c">#    Request Headers:</span>
<span class="c">#            accept=*/*</span>
<span class="c">#            host=sweetlittlebird.com:30080</span>
<span class="c">#            user-agent=curl/8.5.0</span>
<span class="c">#            x-forwarded-for=192.168.10.1</span>
<span class="c">#            x-forwarded-host=sweetlittlebird.com:30080</span>
<span class="c">#            x-forwarded-port=80</span>
<span class="c">#            x-forwarded-proto=http</span>
<span class="c">#            x-forwarded-scheme=http</span>
<span class="c">#            ...    </span>
<span class="nv">$ </span>curl <span class="nv">$MYDOMAIN1</span>:30080/echo/1
<span class="c"># =&gt; Hostname: deploy3-adminsrv-7c8f8b8c87-48xwp</span>
<span class="c">#    </span>
<span class="c">#    Pod Information:</span>
<span class="c">#            -no pod information available-</span>
<span class="c">#    </span>
<span class="c">#    Server values:</span>
<span class="c">#            server_version=nginx: 1.13.0 - lua: 10008</span>
<span class="c">#    </span>
<span class="c">#    Request Information:</span>
<span class="c">#            client_address=172.16.0.16</span>
<span class="c">#            method=GET</span>
<span class="c">#            real path=/echo/1</span>
<span class="c">#            query=</span>
<span class="c">#            request_version=1.1</span>
<span class="c">#            request_uri=http://sweetlittlebird.com:8080/echo/1</span>
<span class="c">#            ...</span>

<span class="nv">$ </span>curl <span class="nv">$MYDOMAIN2</span>:30080 <span class="nt">-v</span>
<span class="c"># =&gt; * Host test.sweetlittlebird.com:30080 was resolved.</span>
<span class="c">#    * IPv4: 192.168.10.10</span>
<span class="c">#    *   Trying 192.168.10.10:30080...</span>
<span class="c">#    * Connected to test.sweetlittlebird.com (192.168.10.10) port 30080</span>
<span class="c">#    &amp;gt; GET / HTTP/1.1</span>
<span class="c">#    &amp;gt; Host: test.sweetlittlebird.com:30080</span>
<span class="c">#    ...</span>
<span class="c">#    &amp;lt; HTTP/1.1 404 Not Found</span>
<span class="c">#    ...</span>
<span class="nv">$ </span>curl <span class="nv">$MYDOMAIN2</span>:30080/admin
<span class="c"># =&gt; &amp;lt;html&amp;gt;</span>
<span class="c">#    &amp;lt;head&amp;gt;&amp;lt;title&amp;gt;404 Not Found&amp;lt;/title&amp;gt;&amp;lt;/head&amp;gt;</span>
<span class="c">#    &amp;lt;body&amp;gt;</span>
<span class="c">#    &amp;lt;center&amp;gt;&amp;lt;h1&amp;gt;404 Not Found&amp;lt;/h1&amp;gt;&amp;lt;/center&amp;gt;</span>
<span class="c">#    &amp;lt;hr&amp;gt;&amp;lt;center&amp;gt;nginx&amp;lt;/center&amp;gt;</span>
<span class="c">#    &amp;lt;/body&amp;gt;</span>
<span class="c">#    &amp;lt;/html&amp;gt;</span>
<span class="nv">$ </span>curl <span class="nv">$MYDOMAIN2</span>:30080/echo
<span class="c"># =&gt; Hostname: deploy3-adminsrv-7c8f8b8c87-ndwkj</span>
<span class="c">#    </span>
<span class="c">#    Pod Information:</span>
<span class="c">#            -no pod information available-</span>
<span class="c">#    </span>
<span class="c">#    Server values:</span>
<span class="c">#            server_version=nginx: 1.13.0 - lua: 10008</span>
<span class="c">#    </span>
<span class="c">#    Request Information:</span>
<span class="c">#            client_address=172.16.0.16</span>
<span class="c">#            method=GET</span>
<span class="c">#            real path=/echo</span>
<span class="c">#            ...</span>
<span class="c">#            request_uri=http://test.sweetlittlebird.com:8080/echo</span>
<span class="c">#    </span>
<span class="c">#    Request Headers:</span>
<span class="c">#            accept=*/*</span>
<span class="c">#            host=test.sweetlittlebird.com:30080</span>
<span class="c">#            user-agent=curl/8.5.0</span>
<span class="c">#            x-forwarded-for=192.168.10.1</span>
<span class="c">#            x-forwarded-host=test.sweetlittlebird.com:30080</span>
<span class="c">#            x-forwarded-port=80</span>
<span class="c">#            x-forwarded-proto=http</span>
<span class="c">#            x-forwarded-scheme=http</span>
<span class="c">#            ...</span>
<span class="nv">$ </span>curl <span class="nv">$MYDOMAIN2</span>:30080/echo/1
<span class="nv">$ </span>curl <span class="nv">$MYDOMAIN2</span>:30080/echo/1/2

<span class="c">## (옵션) /etc/hosts 파일 변경 없이 접속 방안</span>
<span class="nv">$ </span>curl <span class="nt">-H</span> <span class="s2">"host: </span><span class="nv">$MYDOMAIN1</span><span class="s2">"</span> <span class="nv">$MYIP</span>:30080
<span class="c"># =&gt; (정상)</span>
<span class="nv">$ </span>curl <span class="nt">-H</span> <span class="s2">"host: </span><span class="nv">$MYDOMAIN2</span><span class="s2">"</span> <span class="nv">$MYIP</span>:30080
<span class="c"># =&gt; (404 에러)</span>
<span class="nv">$ </span>curl <span class="nt">-H</span> <span class="s2">"host: </span><span class="nv">$MYDOMAIN2</span><span class="s2">"</span> <span class="nv">$MYIP</span>:30080/echo
<span class="c"># =&gt; (정상 응답 옴)</span>
</code></pre></div></div>

<ul>
  <li>실습결과 sweetlittlebird.com으로는 모든 응답이 200 OK 응답이 오고,
test.sweetlittlebird.com으로 접속시에는 /echo 경로로 접속해야만 200 OK 응답이 오고, 그 외의 경로로 접속시에는 404 에러가 발생하는 것을 확인할 수 있습니다.</li>
  <li>아래의 룰대로 잘 접속이 되는 것을 확인할 수 있습니다.
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># sweetlittlebird.com이라는 호스트로 접속시 모든 경로에 대해서 200 OK 응답</span>
sweetlittlebird.com    /       svc3-admin:8080   
  
<span class="c"># test.sweetlittlebird.com 처럼 서브 도메인이 있는 호스트명으로 접속시 /echo 경로로만 200 OK 응답</span>
<span class="k">*</span>.sweetlittlebird.com  /echo   svc3-admin:8080   
</code></pre></div>    </div>
  </li>
  <li>오브젝트 삭제
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl delete deployments,svc,ingress <span class="nt">--all</span>
</code></pre></div>    </div>
  </li>
</ul>

<h4 id="카나리-업데이트">카나리 업데이트</h4>

<ul>
  <li>카나리 업데이트는 새로운 버전의 파드를  배포하고, 일부 트래픽만 새로운 버전으로 전환하고, 새로운 버전의 정상동작 확인 후 전체를 새로운 버전으로 전환하는 업데이트 방식입니다.</li>
  <li>배포 자동화시 최소 중단/무중단으로 하는 방법을 몇가지 살펴보겠습니다.
    <ol>
      <li>롤링 업데이트
  <img src="/assets/2024/kans-3th/w6/20241012_kans_w6_13.png" alt="img.png" class="w-80 image-center" />
        <ul>
          <li>파드를 하나씩 새로운 버전으로 교체하는 방식으로, 기존 버전의 파드가 정상동작하는지 확인 후 다음 파드로 교체하는 방식입니다.</li>
        </ul>
      </li>
      <li>카나리 업데이트
  <img src="/assets/2024/kans-3th/w6/20241012_kans_w6_14.png" alt="img_1.png" class="w-80 image-center" />
        <ul>
          <li>일부 트래픽을 새로운 버전으로 전환하고, 정상동작 확인 후 전체로 전환하는 방식입니다.</li>
        </ul>
      </li>
      <li>블루/그린 업데이트
  <img src="/assets/2024/kans-3th/w6/20241012_kans_w6_15.png" alt="img_2.png" class="w-80 image-center" />
        <ul>
          <li>새로운 버전의 파드를 새로운 서비스로 배포하고, 모든 파드 배포 후, 하나씩 전환하는 롤링 업데이트와는 다르게 전체 트래픽을 한꺼번에 새로운 서비스로 전환하는 방식입니다.</li>
        </ul>
      </li>
    </ol>
  </li>
  <li>실습을 통해 nginx ingress controller를 이용한 카나리 업데이트를 진행해보겠습니다.
    <ul>
      <li>
        <p>canary-svc1-pod.yml 파일 생성</p>

        <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">dp-v1</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">3</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">svc-v1</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">app</span><span class="pi">:</span> <span class="s">svc-v1</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">containers</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">pod-v1</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">k8s.gcr.io/echoserver:1.5</span>
          <span class="na">ports</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">8080</span>
      <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">svc-v1</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">ports</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">web-port</span>
      <span class="na">port</span><span class="pi">:</span> <span class="m">9001</span>
      <span class="na">targetPort</span><span class="pi">:</span> <span class="m">8080</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">svc-v1</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>canary-svc2-pod.yml 파일 생성</p>

        <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">dp-v2</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">3</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">svc-v2</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">app</span><span class="pi">:</span> <span class="s">svc-v2</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">containers</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">pod-v2</span>
        <span class="na">image</span><span class="pi">:</span> <span class="s">k8s.gcr.io/echoserver:1.6</span>
        <span class="na">ports</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">8080</span>
      <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">svc-v2</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">ports</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">web-port</span>
      <span class="na">port</span><span class="pi">:</span> <span class="m">9001</span>
      <span class="na">targetPort</span><span class="pi">:</span> <span class="m">8080</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">svc-v2</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>생성 및 확인</p>

        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 터미널1</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s1">'kubectl get ingress,svc,ep,pod -owide'</span>
    
<span class="c"># 생성</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> canary-svc1-pod.yml,canary-svc2-pod.yml
<span class="c"># =&gt; deployment.apps/dp-v1 created</span>
<span class="c">#    service/svc-v1 created</span>
<span class="c">#    deployment.apps/dp-v2 created</span>
<span class="c">#    service/svc-v2 created</span>
    
<span class="c"># 확인</span>
<span class="nv">$ </span>kubectl get svc,ep,pod
<span class="c"># =&gt; NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE</span>
<span class="c">#    service/kubernetes   ClusterIP   10.10.200.1     &amp;lt;none&amp;gt;        443/TCP    12m</span>
<span class="c">#    service/svc-v1       ClusterIP   10.10.200.231   &amp;lt;none&amp;gt;        9001/TCP   18s</span>
<span class="c">#    service/svc-v2       ClusterIP   10.10.200.216   &amp;lt;none&amp;gt;        9001/TCP   18s</span>
<span class="c">#    </span>
<span class="c">#    NAME                   ENDPOINTS                                            AGE</span>
<span class="c">#    endpoints/kubernetes   192.168.10.10:6443                                   12m</span>
<span class="c">#    endpoints/svc-v1       172.16.1.10:8080,172.16.2.12:8080,172.16.3.11:8080   18s</span>
<span class="c">#    endpoints/svc-v2       172.16.1.11:8080,172.16.2.11:8080,172.16.3.12:8080   18s</span>
<span class="c">#    </span>
<span class="c">#    NAME                         READY   STATUS    RESTARTS   AGE</span>
<span class="c">#    pod/dp-v1-8684d45558-22nbv   1/1     Running   0          18s</span>
<span class="c">#    pod/dp-v1-8684d45558-59pnl   1/1     Running   0          18s</span>
<span class="c">#    pod/dp-v1-8684d45558-87xrs   1/1     Running   0          18s</span>
<span class="c">#    pod/dp-v2-7757c4bdc-5xmcm    1/1     Running   0          18s</span>
<span class="c">#    pod/dp-v2-7757c4bdc-bm2gq    1/1     Running   0          18s</span>
<span class="c">#    pod/dp-v2-7757c4bdc-h7fzl    1/1     Running   0          18s</span>
    
<span class="c"># 파드 버전 확인: 1.13.0 vs 1.13.1</span>
<span class="nv">$ </span><span class="k">for </span>pod <span class="k">in</span> <span class="si">$(</span>kubectl get pod <span class="nt">-o</span> wide <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>svc-v1 |awk <span class="s1">'NR&gt;1 {print $6}'</span><span class="si">)</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$pod</span>:8080 | egrep <span class="s1">'(Hostname|nginx)'</span><span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; Hostname: dp-v1-8684d45558-22nbv</span>
<span class="c">#     server_version=nginx: 1.13.0 - lua: 10008</span>
<span class="c">#    ...</span>
<span class="nv">$ </span><span class="k">for </span>pod <span class="k">in</span> <span class="si">$(</span>kubectl get pod <span class="nt">-o</span> wide <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>svc-v2 |awk <span class="s1">'NR&gt;1 {print $6}'</span><span class="si">)</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$pod</span>:8080 | egrep <span class="s1">'(Hostname|nginx)'</span><span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; Hostname: dp-v2-7757c4bdc-5xmcm</span>
<span class="c">#     server_version=nginx: 1.13.1 - lua: 10008</span>
<span class="c">#    ...		</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>canary-ingress1.yml</p>

        <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">networking.k8s.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Ingress</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">ingress-canary-v1</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">ingressClassName</span><span class="pi">:</span> <span class="s">nginx</span>
  <span class="na">rules</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">host</span><span class="pi">:</span> <span class="s">kans.com</span>
    <span class="na">http</span><span class="pi">:</span>
      <span class="na">paths</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">path</span><span class="pi">:</span> <span class="s">/</span>
        <span class="na">pathType</span><span class="pi">:</span> <span class="s">Prefix</span>
        <span class="na">backend</span><span class="pi">:</span>
          <span class="na">service</span><span class="pi">:</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">svc-v1</span>
            <span class="na">port</span><span class="pi">:</span>
              <span class="na">number</span><span class="pi">:</span> <span class="m">8080</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>canary-ingress2.yml</p>

        <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">networking.k8s.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Ingress</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">ingress-canary-v2</span>
  <span class="na">annotations</span><span class="pi">:</span>
    <span class="na">nginx.ingress.kubernetes.io/canary</span><span class="pi">:</span> <span class="s2">"</span><span class="s">true"</span>
    <span class="na">nginx.ingress.kubernetes.io/canary-weight</span><span class="pi">:</span> <span class="s2">"</span><span class="s">10"</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">ingressClassName</span><span class="pi">:</span> <span class="s">nginx</span>
  <span class="na">rules</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">host</span><span class="pi">:</span> <span class="s">kans.com</span>
    <span class="na">http</span><span class="pi">:</span>
      <span class="na">paths</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">path</span><span class="pi">:</span> <span class="s">/</span>
        <span class="na">pathType</span><span class="pi">:</span> <span class="s">Prefix</span>
        <span class="na">backend</span><span class="pi">:</span>
          <span class="na">service</span><span class="pi">:</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">svc-v2</span>
            <span class="na">port</span><span class="pi">:</span>
              <span class="na">number</span><span class="pi">:</span> <span class="m">8080</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>카나리 업그레이드 확인</p>
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 터미널1</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s1">'kubectl get ingress,svc,ep'</span>
    
<span class="c"># 도메인 변경</span>
<span class="c"># $ MYDOMAIN1=&lt;각자 자신의 닉네임의 도메인&gt; 예시) gasida.com</span>
<span class="nv">$ MYDOMAIN1</span><span class="o">=</span>sweetlittlebird.com
<span class="nv">$ </span><span class="nb">sed</span> <span class="nt">-i</span> <span class="s2">"s/kans.com/</span><span class="nv">$MYDOMAIN1</span><span class="s2">/g"</span> canary-ingress1.yml
<span class="nv">$ </span><span class="nb">sed</span> <span class="nt">-i</span> <span class="s2">"s/kans.com/</span><span class="nv">$MYDOMAIN1</span><span class="s2">/g"</span> canary-ingress2.yml
    
<span class="c"># 생성</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> canary-ingress1.yml,canary-ingress2.yml
<span class="c"># =&gt; ingress.networking.k8s.io/ingress-canary-v1 created</span>
<span class="c">#    ingress.networking.k8s.io/ingress-canary-v2 created</span>
    
<span class="c"># 로그 모니터링</span>
<span class="nv">$ </span>kubetail <span class="nt">-n</span> ingress <span class="nt">-l</span> app.kubernetes.io/component<span class="o">=</span>controller
    
<span class="c"># 접속 테스트</span>
<span class="nv">$ </span>curl <span class="nt">-s</span> <span class="nv">$MYDOMAIN1</span>:30080
<span class="nv">$ </span>curl <span class="nt">-s</span> <span class="nv">$MYDOMAIN1</span>:30080 | <span class="nb">grep </span>nginx
<span class="c"># =&gt; &amp;lt;hr&amp;gt;&amp;lt;center&amp;gt;nginx&amp;lt;/center&amp;gt;</span>
    
<span class="c"># 접속 시 v1 v2 버전별 비율이 어떻게 되나요? 왜 이렇게 되나요?</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span>  <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$MYDOMAIN1</span>:30080 | <span class="nb">grep </span>nginx <span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
<span class="c"># =&gt;      84         server_version=nginx: 1.13.0 - lua: 10008</span>
<span class="c">#         16         server_version=nginx: 1.13.1 - lua: 10008</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..1000<span class="o">}</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$MYDOMAIN1</span>:30080 | <span class="nb">grep </span>nginx <span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
<span class="c"># =&gt;     919         server_version=nginx: 1.13.0 - lua: 10008</span>
<span class="c">#         81         server_version=nginx: 1.13.1 - lua: 10008</span>
<span class="nv">$ </span><span class="k">while </span><span class="nb">true</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nt">--connect-timeout</span> 1 <span class="nv">$MYDOMAIN1</span>:30080 | <span class="nb">grep </span>Hostname <span class="p">;</span> <span class="nb">echo</span> <span class="s2">"--------------"</span> <span class="p">;</span> <span class="nb">date</span> <span class="s2">"+%Y-%m-%d %H:%M:%S"</span> <span class="p">;</span> <span class="nb">sleep </span>1<span class="p">;</span> <span class="k">done</span>
<span class="c"># &lt;span style="color: green;"&gt;👉 v2의 canary 비율을 10%로 두어서 그렇습니다.&lt;/span&gt;</span>
    
<span class="c"># 비율 조정하여 절반을 v2로 전환하겠습니다. &gt;&gt; 개발 배포 버전 전략에 유용하다!</span>
<span class="nv">$ </span>kubectl annotate <span class="nt">--overwrite</span> ingress ingress-canary-v2 nginx.ingress.kubernetes.io/canary-weight<span class="o">=</span>50
    
<span class="c"># 접속 테스트</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span>  <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$MYDOMAIN1</span>:30080 | <span class="nb">grep </span>nginx <span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
<span class="c"># =&gt;      53         server_version=nginx: 1.13.1 - lua: 10008</span>
<span class="c">#         47         server_version=nginx: 1.13.0 - lua: 10008</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..1000<span class="o">}</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$MYDOMAIN1</span>:30080 | <span class="nb">grep </span>nginx <span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
<span class="c"># =&gt;     526         server_version=nginx: 1.13.1 - lua: 10008</span>
<span class="c">#        474         server_version=nginx: 1.13.0 - lua: 10008</span>
        
<span class="c"># (옵션) 비율 조정 &lt;&lt; 어떻게 비율이 조정될까요?</span>
<span class="nv">$ </span>kubectl annotate <span class="nt">--overwrite</span> ingress ingress-canary-v2 nginx.ingress.kubernetes.io/canary-weight<span class="o">=</span>100
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span>  <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$MYDOMAIN1</span>:30080 | <span class="nb">grep </span>nginx <span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
<span class="c"># =&gt;    100         server_version=nginx: 1.13.1 - lua: 10008</span>
<span class="c"># &lt;span style="color: green;"&gt;👉 카나리 비율을 100%로 하니 v2버전으로 100% 전환 되었습니다.&lt;/span&gt;</span>
    
<span class="c"># (옵션) 비율 조정 &lt;&lt; 어떻게 비율이 조정될까요?</span>
<span class="nv">$ </span>kubectl annotate <span class="nt">--overwrite</span> ingress ingress-canary-v2 nginx.ingress.kubernetes.io/canary-weight<span class="o">=</span>0
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span>  <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$MYDOMAIN1</span>:30080 | <span class="nb">grep </span>nginx <span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
<span class="c"># =&gt;     100         server_version=nginx: 1.13.0 - lua: 10008</span>
<span class="c"># &lt;span style="color: green;"&gt;👉 카나리 비율을 0%로 하니 v2버전으로 0% 로 전환 되고 모든 트래픽이 v1으로 전달되었습니다.&lt;/span&gt;</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>오브젝트 삭제
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl delete deployments,svc,ingress <span class="nt">--all</span>
</code></pre></div>    </div>
  </li>
</ul>

<hr />

<h2 id="gateway-api">Gateway API</h2>

<h3 id="gateway-api-소개">Gateway API 소개</h3>

<p>앞서 Ingress를 살펴볼때 말씀드린것 처럼 Ingress는 Frozen 되어서 더이상 업데이트 되지 않고, Gateway API에 기능을 추가할 계획이라고 합니다.
이어서 Gateway API에 대해 알아보겠습니다.</p>

<p>Gateway API는 Kubernetes에서 API Gateway를 정의하고 구성하기 위한 API를 제공하는 프로젝트입니다. <a href="https://medium.com/@disha.20.10/introduction-to-the-gateway-api-revolutionizing-kubernetes-networking-7b0c9a696038">Gateway API 소개</a>
Gateway API는 서비스 메시(예) istio 등)에서 제공하는 풍부한 기능 중 일부 기능들과 운영 관리에 필요한 기능들을 추가하였습니다.
추가된 기능의 예로는 헤더 기반 라우팅, 헤더 변조, 트래픽 미러링(쉽게 트래픽 복제), 역할 기반 접근 제어 등이 있습니다.</p>

<p><img src="/assets/2024/kans-3th/w6/20241012_kans_w6_16.png" alt="img.png" class="w-80 image-center" /></p>

<p>Gateway API는 이를 통해 동적 인프라 구성을 지원하고, 고급 트래픽 라우팅을 지원합니다.</p>

<ul>
  <li>Gateway API의 주요 기능은 다음과 같습니다.
    <ol>
      <li>
        <dl>
          <dt><strong>개선된 리소스 모델</strong></dt>
          <dd>API는 GatewayClass, Gateway 및 Route(HTTPRoute, TCPRoute 등)와 같은 새로운 사용자 정의 리소스를 도입하여 라우팅 규칙을 정의하는 보다 세부적이고 표현력 있는 방법을 제공합니다.</dd>
        </dl>
      </li>
      <li>
        <dl>
          <dt><strong>프로토콜 독립적</strong></dt>
          <dd>주로 HTTP용으로 설계된 Ingress와 달리 Gateway API는 TCP, UDP, TLS를 포함한 여러 프로토콜을 지원합니다.</dd>
        </dl>
      </li>
      <li>
        <dl>
          <dt><strong>강화된 보안</strong></dt>
          <dd>TLS 구성 및 보다 세부적인 액세스 제어에 대한 기본 제공 지원.</dd>
        </dl>
      </li>
      <li>
        <dl>
          <dt><strong>교차 네임스페이스 지원</strong></dt>
          <dd>서로 다른 네임스페이스의 서비스로 트래픽을 라우팅하여 보다 유연한 아키텍처를 구축할 수 있는 기능을 제공합니다.</dd>
        </dl>
      </li>
      <li>
        <dl>
          <dt><strong>확장성</strong></dt>
          <dd>API는 사용자 정의 리소스 및 정책으로 쉽게 확장할 수 있도록 설계되었습니다.</dd>
        </dl>
      </li>
      <li>
        <dl>
          <dt><strong>역할 지향</strong></dt>
          <dd>클러스터 운영자, 애플리케이션 개발자, 보안 팀 간의 우려를 명확하게 분리합니다.</dd>
        </dl>
      </li>
    </ol>
  </li>
  <li>다음의 구성요소 (Resource)를 가집니다.
    <ul>
      <li>GatewayClass, Gateway, HTTPRoute, TCPRoute, Service
<img src="/assets/2024/kans-3th/w6/20241012_kans_w6_17.png" alt="img.png" />
        <ul>
          <li><strong>GatewayClass:</strong> 공통 구성을 가진 게이트웨이 세트를 정의하고 클래스를 구현하는 컨트롤러에 의해 관리됩니다.</li>
          <li><strong>Gateway:</strong> 클라우드 로드 밸런서와 같은 트래픽 처리 인프라의 인스턴스를 정의합니다.</li>
          <li><strong>HTTPRoute:</strong> Gateway 리스너에서 백엔드 네트워크 엔드포인트의 표현으로 트래픽을 매핑하기 위한 HTTP 전용 규칙을 정의합니다. 이러한 엔드포인트는 종종 <a href="https://kubernetes.io/docs/concepts/services-networking/service/">Service</a>로 표현됩니다<br />
<img src="/assets/2024/kans-3th/w6/20241012_kans_w6_18.png" alt="img.png" class="image-center" />
<em class="image-caption">출처 : <a href="https://gateway-api.sigs.k8s.io/">https://gateway-api.sigs.k8s.io/</a></em></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>리퀘스트 흐름
<img src="/assets/2024/kans-3th/w6/20241012_kans_w6_19.svg" alt="20241012_kans_w6_19.svg" /></li>
  <li>role-oriented  API 가 중요한 이유
    <ul>
      <li>담당 업무의 역할에 따라서 동작/권한을 유연하게 제공할 수 있습니다.</li>
      <li>아래 그림 처럼 ‘스토어 개발자’는 Store 네임스페이스내에서 해당 store PATH 라우팅 관련 정책을 스스로 관리 할 수 있습니다.
<img src="/assets/2024/kans-3th/w6/20241012_kans_w6_20.png" alt="img.png" /></li>
      <li>역할의 예시입니다.
        <ul>
          <li><strong>인프라 제공자:</strong> 여러 격리된 클러스터가 여러 테넌트를 서비스할 수 있도록 인프라를 관리합니다. 예: 클라우드 제공자.</li>
          <li><strong>클러스터 운영자:</strong> 클러스터를 관리하며 주로 정책, 네트워크 접근, 애플리케이션 권한 등을 관리합니다.</li>
          <li><strong>애플리케이션 개발자:</strong> 클러스터에서 실행되는 애플리케이션을 관리하며 주로 애플리케이션 수준의 구성 및 <a href="https://kubernetes.io/docs/concepts/services-networking/service/">서비스</a> 구성에 관심이 있습니다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>추천글
    <ul>
      <li><a href="https://www.anyflow.net/sw-engineer/kubernetes-gateway-api-1">Ingress + API Gateway = Kubernetes Gateway API</a></li>
      <li><a href="https://www.anyflow.net/sw-engineer/kubernetes-gateway-api-2">API Gateway + Service Mesh = Kubernetes Gateway API</a></li>
    </ul>
  </li>
</ul>

<h3 id="gloo-gateway">Gloo Gateway</h3>

<p>Gloo Gateway는 Solo.io에서 개발한 API Gateway로, Gateway API를 구현한 대표적인 제품 중 하나입니다. 
Gloo Gateway는 다양한 환경에서 사용할 수 있도록 설계되어 있으며, 다음과 같은 특징을 가지고 있습니다.</p>

<ul>
  <li><strong>Envoy Proxy 기반</strong> : Gloo Gateway는 고성능의 Envoy Proxy를 기반으로 하여 뛰어난 확장성과 성능을 제공합니다.</li>
  <li><strong>API 관리 및 라우팅</strong> : 다양한 API 라우팅 옵션을 제공하며, REST, gRPC, GraphQL 등의 다양한 프로토콜을 지원합니다. 이를 통해 복잡한 트래픽 관리와 라우팅이 가능합니다.</li>
  <li><strong>보안 기능</strong> : 인증, 인가, TLS 암호화, OAuth, OpenID Connect 등 다양한 보안 기능을 제공합니다. 이를 통해 API를 안전하게 보호할 수 있습니다.</li>
  <li><strong>확장성</strong> : 플러그인 아키텍처를 통해 쉽게 확장할 수 있으며, 다양한 서드파티 통합을 지원합니다. 필요에 따라 기능을 확장하거나 사용자 정의 기능을 추가할 수 있습니다.</li>
  <li><strong>서비스 디스커버리</strong> : Kubernetes, Consul, EC2 등 다양한 서비스 디스커버리 메커니즘을 지원하여 동적 환경에서도 효율적으로 작동합니다.</li>
  <li><strong>Observability</strong> : 트래픽 모니터링, 로깅, 트레이싱 등의 기능을 제공하여 운영 중인 시스템의 상태를 쉽게 파악하고 문제를 해결할 수 있습니다.</li>
  <li><strong>유연한</strong> 배포 : 클라우드, 온프레미스, 하이브리드 환경 등 다양한 배포 옵션을 지원합니다. 이를 통해 다양한 인프라 환경에 맞춰 유연하게 배포할 수 있습니다.</li>
</ul>

<h4 id="gloo-gateway-architecture">Gloo Gateway Architecture</h4>

<p><img src="/assets/2024/kans-3th/w6/20241012_kans_w6_21.png" alt="img.png" /></p>

<ul>
  <li>Envoy를 통해서 Http의 L7 라우팅을 지원하며, Gloo의 역할은 라우팅 규칙을 관리하고 Envoy에 전달하는 역할을 합니다.</li>
</ul>

<p>Gloo Gateway는 내용이 방대하기 때문에 아래의 링크들로 설명을 대체하겠습니다.</p>

<ul>
  <li><a href="https://www.solo.io/blog/">Gloo Blog</a></li>
  <li><a href="https://docs.solo.io/gateway/latest/quickstart/">Docs</a></li>
  <li><a href="https://www.solo.io/blog/gateway-api-tutorial-blog/">https://www.solo.io/blog/gateway-api-tutorial-blog/</a></li>
  <li><a href="https://www.solo.io/blog/gateway-api-workshop/">https://www.solo.io/blog/gateway-api-workshop/</a></li>
  <li><a href="https://www.solo.io/blog/fast-and-furious-gateway-api-at-scale-with-envoy-proxy-and-gloo-gateway/">https://www.solo.io/blog/fast-and-furious-gateway-api-at-scale-with-envoy-proxy-and-gloo-gateway/</a></li>
  <li><a href="https://www.solo.io/blog/getting-the-most-out-of-gateway-api-lessons-learned-from-gloo-migrations/">https://www.solo.io/blog/getting-the-most-out-of-gateway-api-lessons-learned-from-gloo-migrations/</a></li>
  <li><a href="https://www.solo.io/blog/solving-an-information-leakage-problem-with-the-envoy-extproc-filter-and-kube-gateway-api/">https://www.solo.io/blog/solving-an-information-leakage-problem-with-the-envoy-extproc-filter-and-kube-gateway-api/</a></li>
  <li><a href="https://www.solo.io/blog/gateway-api-gitops-argo-tutorial-blog/">https://www.solo.io/blog/gateway-api-gitops-argo-tutorial-blog/</a></li>
</ul>

<h4 id="실습">실습</h4>

<p>실습을 통해 Gloo Gateway를 설치하고, Gateway API를 사용해보겠습니다.
실습은 <a href="https://www.solo.io/blog/gateway-api-tutorial-blog/">[Tutorial] Hands-On with the Kubernetes Gateway API and Envoy Proxy</a>를 참고하였습니다.
kind를 통해서 실습할 수 있도록 잘 구성되었고 30분 정도면 따라할 수 있다고 합니다.</p>

<h5 id="install">Install</h5>

<p><strong>Install KinD Cluster</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOT</span><span class="sh">&gt; kind-1node.yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  extraPortMappings:
  - containerPort: 30000
    hostPort: 30000
  - containerPort: 30001
    hostPort: 30001
  - containerPort: 30002
    hostPort: 30002
</span><span class="no">EOT

</span><span class="c"># Install KinD Cluster</span>
<span class="nv">$ </span>kind create cluster <span class="nt">--image</span> kindest/node:v1.30.0 <span class="nt">--config</span> kind-1node.yaml <span class="nt">--name</span> myk8s

<span class="c"># 노드에 기본 툴 설치</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane sh <span class="nt">-c</span> <span class="s1">'apt update &amp;&amp; apt install tree psmisc lsof wget bsdmainutils bridge-utils net-tools dnsutils tcpdump ngrep iputils-ping git vim -y'</span>

<span class="c"># 노드/파드 확인</span>
<span class="nv">$ </span>kubectl get nodes <span class="nt">-o</span> wide
<span class="c"># =&gt; NAME                  STATUS   ROLES           AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                         KERNEL-VERSION     CONTAINER-RUNTIME</span>
<span class="c">#    myk8s-control-plane   Ready    control-plane   32s   v1.30.0   172.20.0.2    &amp;lt;none&amp;gt;        Debian GNU/Linux 12 (bookworm)   5.10.76-linuxkit   containerd://1.7.15</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-A</span>
<span class="c"># =&gt; NAMESPACE            NAME                                          READY   STATUS    RESTARTS   AGE</span>
<span class="c">#    kube-system          coredns-7db6d8ff4d-45mzg                      1/1     Running   0          38s</span>
<span class="c">#    kube-system          coredns-7db6d8ff4d-gc4zp                      1/1     Running   0          38s</span>
<span class="c">#    kube-system          etcd-myk8s-control-plane                      1/1     Running   0          52s</span>
<span class="c">#    kube-system          kindnet-h4dwk                                 1/1     Running   0          38s</span>
<span class="c">#    kube-system          kube-apiserver-myk8s-control-plane            1/1     Running   0          54s</span>
<span class="c">#    kube-system          kube-controller-manager-myk8s-control-plane   1/1     Running   0          52s</span>
<span class="c">#    kube-system          kube-proxy-sptf6                              1/1     Running   0          38s</span>
<span class="c">#    kube-system          kube-scheduler-myk8s-control-plane            1/1     Running   0          52s</span>
<span class="c">#    local-path-storage   local-path-provisioner-988d74bc-gl679         1/1     Running   0          38s</span>
</code></pre></div></div>

<p><strong>Install Gateway API CRDs</strong> : The Kubernetes Gateway API abstractions are expressed using Kubernetes CRDs.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># CRDs 설치 및 확인</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> https://github.com/kubernetes-sigs/gateway-api/releases/download/v1.0.0/standard-install.yaml
<span class="nv">$ </span>kubectl get crd
<span class="c"># =&gt; NAME                                        CREATED AT</span>
<span class="c">#    gatewayclasses.gateway.networking.k8s.io    2024-10-01T15:50:32Z</span>
<span class="c">#    gateways.gateway.networking.k8s.io          2024-10-01T15:50:32Z</span>
<span class="c">#    httproutes.gateway.networking.k8s.io        2024-10-01T15:50:32Z</span>
<span class="c">#    referencegrants.gateway.networking.k8s.io   2024-10-01T15:50:32Z</span>
</code></pre></div></div>

<p><strong>Install Glooctl Utility</strong> : GLOOCTL is a command-line utility that allows users to view, manage, and debug Gloo Gateway deployments - <a href="https://docs.solo.io/gloo-edge/latest/installation/glooctl_setup/">Link</a></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># [신규 터미널] 아래 bash 진입 후 glooctl 툴 사용</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane bash
<span class="nt">----------------------------------------</span>
<span class="c"># Install Glooctl Utility</span>
<span class="c">## glooctl install gateway     # install gloo's function gateway functionality into the 'gloo-system' namespace</span>
<span class="c">## glooctl install ingress     # install very basic Kubernetes Ingress support with Gloo into namespace gloo-system</span>
<span class="c">## glooctl install knative     # install Knative serving with Gloo configured as the default cluster ingress</span>
<span class="c">## curl -sL https://run.solo.io/gloo/install | sh</span>
<span class="nv">$ </span>curl <span class="nt">-sL</span> https://run.solo.io/gloo/install | <span class="nv">GLOO_VERSION</span><span class="o">=</span>v1.17.7 sh
<span class="nv">$ </span><span class="nb">export </span><span class="nv">PATH</span><span class="o">=</span><span class="nv">$HOME</span>/.gloo/bin:<span class="nv">$PATH</span>

<span class="c"># 버전 확인</span>
<span class="nv">$ </span>glooctl version
<span class="c"># =&gt; Server: version undefined, could not find any version of gloo running</span>
<span class="c">#    </span>
<span class="c">#    {</span>
<span class="c">#      &amp;quot;client&amp;quot;: {</span>
<span class="c">#        &amp;quot;version&amp;quot;: &amp;quot;1.17.7&amp;quot;</span>
<span class="c">#      },</span>
<span class="c">#      &amp;quot;kubernetesCluster&amp;quot;: {</span>
<span class="c">#        &amp;quot;major&amp;quot;: &amp;quot;1&amp;quot;,</span>
<span class="c">#        &amp;quot;minor&amp;quot;: &amp;quot;30&amp;quot;,</span>
<span class="c">#        &amp;quot;gitVersion&amp;quot;: &amp;quot;v1.30.0&amp;quot;,</span>
<span class="c">#        &amp;quot;buildDate&amp;quot;: &amp;quot;2024-05-13T22:02:25Z&amp;quot;,</span>
<span class="c">#        &amp;quot;platform&amp;quot;: &amp;quot;linux/arm64&amp;quot;</span>
<span class="c">#      }</span>
<span class="c">#    }</span>

<span class="c"># &lt;span style="color: green;"&gt;👉 서버가 설치되지 않았기 때문에 클라이언트 정보만 나옵니다.&lt;/span&gt;</span>
<span class="nt">----------------------------------------</span>
</code></pre></div></div>

<p><strong>Install Gloo Gateway : 오픈소스 버전</strong></p>

<p><strong>rosetta 비활성화 방법</strong></p>

<ul>
  <li>
    <p>[macOS m시리즈] <strong>Docker Desktop</strong> : 아래 옵션 Uncheck 해둘 것 → Apply &amp; restart</p>

    <p><img src="/assets/2024/kans-3th/w6/20241012_kans_w6_22.png" alt="img.png" /></p>
  </li>
  <li>
    <p>[macOS m시리즈] <strong>Orbstack</strong> : 터미널에서 아래 입력</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c"># rosetta 비활성화 </span>
  <span class="nv">$ </span>orb config <span class="nb">set </span>rosetta <span class="nb">false</span>
    
  <span class="c">#  orb 설정 확인 </span>
  <span class="nv">$ </span>orb config show
    
  <span class="c"># orbstack 재시작 </span>
  <span class="nv">$ </span>orb stop 
  <span class="nv">$ </span>orb start 
</code></pre></div>    </div>
  </li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># [신규 터미널] 모니터링</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> kubectl get pod,svc,endpointslices,ep <span class="nt">-n</span> gloo-system

<span class="c"># Install Gloo Gateway</span>
<span class="c">## --set kubeGateway.enabled=true: Kubernetes Gateway 기능을 활성화합니다.</span>
<span class="c">## --set gloo.disableLeaderElection=true: Gloo의 리더 선출 기능을 비활성화합니다. (단일 인스턴스에서 Gloo를 실행 시 유용)</span>
<span class="c">## --set discovery.enabled=false: 서비스 디스커버리 기능을 비활성화합니다.</span>
<span class="nv">$ </span>helm repo add gloo https://storage.googleapis.com/solo-public-helm
<span class="c"># =&gt; &amp;quot;gloo&amp;quot; has been added to your repositories</span>
<span class="nv">$ </span>helm repo update
<span class="nv">$ </span>helm <span class="nb">install</span> <span class="nt">-n</span> gloo-system gloo-gateway gloo/gloo <span class="se">\</span>
<span class="nt">--create-namespace</span> <span class="se">\</span>
<span class="nt">--version</span> 1.17.7 <span class="se">\</span>
<span class="nt">--set</span> kubeGateway.enabled<span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
<span class="nt">--set</span> gloo.disableLeaderElection<span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
<span class="nt">--set</span> discovery.enabled<span class="o">=</span><span class="nb">false</span>
<span class="c"># =&gt; NAME: gloo-gateway</span>
<span class="c">#    LAST DEPLOYED: Sun Oct 13 00:57:27 2024</span>
<span class="c">#    NAMESPACE: gloo-system</span>
<span class="c">#    STATUS: deployed</span>
<span class="c">#    REVISION: 1</span>
<span class="c">#    TEST SUITE: None</span>

<span class="c"># Confirm that the Gloo control plane has successfully been deployed using this command</span>
<span class="nv">$ </span>kubectl rollout status deployment/gloo <span class="nt">-n</span> gloo-system
<span class="c"># =&gt; deployment &amp;quot;gloo&amp;quot; successfully rolled out</span>

<span class="c"># 설치 확인</span>
<span class="nv">$ </span>kubectl get crd | <span class="nb">grep</span> <span class="s1">'networking.k8s.io'</span>
<span class="c"># =&gt; gatewayclasses.gateway.networking.k8s.io    2024-10-01T15:50:32Z</span>
<span class="c">#    gateways.gateway.networking.k8s.io          2024-10-01T15:50:32Z</span>
<span class="c">#    httproutes.gateway.networking.k8s.io        2024-10-01T15:50:32Z</span>
<span class="c">#    referencegrants.gateway.networking.k8s.io   2024-10-01T15:50:32Z</span>
<span class="nv">$ </span>kubectl get crd | <span class="nb">grep</span> <span class="nt">-v</span> <span class="s1">'networking.k8s.io'</span>
<span class="nv">$ </span>kubectl get pod,svc,endpointslices <span class="nt">-n</span> gloo-system
<span class="c"># =&gt; NAME                                    READY   STATUS      RESTARTS   AGE</span>
<span class="c">#    pod/gateway-proxy-57c49d4f48-xm8vv      1/1     Running     0          87s</span>
<span class="c">#    pod/gloo-748d877c4-24ngk                1/1     Running     0          87s</span>
<span class="c">#    pod/gloo-resource-rollout-5bt7d         0/1     Completed   0          87s</span>
<span class="c">#    pod/gloo-resource-rollout-check-xwxd4   0/1     Completed   0          86s</span>
<span class="c">#    </span>
<span class="c">#    NAME                    TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                AGE</span>
<span class="c">#    service/gateway-proxy   LoadBalancer   10.96.69.126    &amp;lt;pending&amp;gt;     80:30172/TCP,443:32484/TCP                             87s</span>
<span class="c">#    service/gloo            ClusterIP      10.96.100.145   &amp;lt;none&amp;gt;        9977/TCP,9976/TCP,9988/TCP,9966/TCP,9979/TCP,443/TCP   87s</span>
<span class="c">#    </span>
<span class="c">#    NAME                                                 ADDRESSTYPE   PORTS                        ENDPOINTS    AGE</span>
<span class="c">#    endpointslice.discovery.k8s.io/gateway-proxy-n7f7v   IPv4          8080,8443                    10.244.0.7   87s</span>
<span class="c">#    endpointslice.discovery.k8s.io/gloo-9bf7g            IPv4          9979,9988,9966 + 3 more...   10.244.0.8   87s</span>

<span class="c">#</span>
<span class="nv">$ </span>kubectl explain gatewayclasses
<span class="nv">$ </span>kubectl get gatewayclasses
<span class="c"># =&gt; NAME           CONTROLLER             ACCEPTED   AGE</span>
<span class="c">#    gloo-gateway   solo.io/gloo-gateway   True       2m18s</span>

<span class="nv">$ </span>kubectl get gatewayclasses <span class="nt">-o</span> yaml
<span class="c"># =&gt; apiVersion: v1</span>
<span class="c">#    items:</span>
<span class="c">#    - apiVersion: gateway.networking.k8s.io/v1</span>
<span class="c">#      kind: GatewayClass</span>
<span class="c">#      metadata:</span>
<span class="c">#        labels:</span>
<span class="c">#          app: gloo</span>
<span class="c">#        name: gloo-gateway</span>
<span class="c">#      spec:</span>
<span class="c">#        controllerName: solo.io/gloo-gateway</span>
<span class="c">#    ...</span>
</code></pre></div></div>

<p><strong>Install Httpbin Application</strong> : A simple HTTP Request &amp; Response Service - <a href="https://httpbin.org/">Link</a></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> kubectl get pod,svc,endpointslices,ep <span class="nt">-n</span> httpbin

<span class="c"># Install Httpbin Application</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> https://raw.githubusercontent.com/solo-io/solo-blog/main/gateway-api-tutorial/01-httpbin-svc.yaml
<span class="c"># =&gt; namespace/httpbin created</span>
<span class="c">#    serviceaccount/httpbin created</span>
<span class="c">#    service/httpbin created</span>
<span class="c">#    deployment.apps/httpbin created</span>

<span class="c"># 설치 확인</span>
<span class="nv">$ </span>kubectl get deploy,pod,svc,endpointslices,sa <span class="nt">-n</span> httpbin
<span class="c"># =&gt; NAME                      READY   UP-TO-DATE   AVAILABLE   AGE</span>
<span class="c">#    deployment.apps/httpbin   0/1     1            0           10s</span>
<span class="c">#    </span>
<span class="c">#    NAME                           READY   STATUS              RESTARTS   AGE</span>
<span class="c">#    pod/httpbin-5855dc8bdd-xh2vf   0/1     ContainerCreating   0          10s</span>
<span class="c">#    </span>
<span class="c">#    NAME              TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE</span>
<span class="c">#    service/httpbin   ClusterIP   10.96.169.139   &amp;lt;none&amp;gt;        8000/TCP   10s</span>
<span class="c">#    </span>
<span class="c">#    NAME                                           ADDRESSTYPE   PORTS     ENDPOINTS   AGE</span>
<span class="c">#    endpointslice.discovery.k8s.io/httpbin-6zhsk   IPv4          &amp;lt;unset&amp;gt;   &amp;lt;unset&amp;gt;     10s</span>
<span class="c">#    </span>
<span class="c">#    NAME                     SECRETS   AGE</span>
<span class="c">#    serviceaccount/default   0         10s</span>
<span class="c">#    serviceaccount/httpbin   0         10s</span>
<span class="nv">$ </span>kubectl rollout status deploy/httpbin <span class="nt">-n</span> httpbin
<span class="c"># =&gt; deployment &amp;quot;httpbin&amp;quot; successfully rolled out</span>

<span class="c"># (옵션) NodePort 설정</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  labels:
    app: httpbin
    service: httpbin
  name: httpbin
  namespace: httpbin
spec:
  type: NodePort
  ports:
  - name: http
    port: 8000
    targetPort: 80
    nodePort: 30000
  selector:
    app: httpbin
</span><span class="no">EOF
</span><span class="c"># =&gt; service/httpbin configured</span>

<span class="c"># (옵션) 로컬 접속 확인</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"httpbin web - http://localhost:30000"</span>     <span class="c"># macOS 사용자</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"httpbin web - http://192.168.50.10:30000"</span> <span class="c"># Windows 사용자</span>
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w6/20241012_kans_w6_23.png" alt="img.png" class="w-80 image-center" />
<em class="image-caption">httpbin 설치결과</em></p>

<p><strong>Gateway API 종류</strong> - <a href="https://kubernetes.io/docs/concepts/services-networking/gateway/#resource-model">Docs</a></p>

<ul>
  <li><strong>GatewayClass:</strong> Defines a set of gateways with <strong>common configuration</strong> and managed by a controller that implements the <strong>class</strong>. - 예) 인프라 엔지니어가 관리</li>
  <li><strong>Gateway:</strong> Defines an instance of traffic handling <strong>infrastructure</strong>, such as cloud load balancer. - 예) 데브옵스 엔지니어가 관리</li>
  <li><strong>HTTPRoute:</strong> Defines <strong>HTTP-specific rules</strong> for mapping traffic from a Gateway listener to a representation of backend network endpoints. These endpoints are often represented as a <a href="https://kubernetes.io/docs/concepts/services-networking/service/">Service</a>. - 예) 개발자가 관리</li>
</ul>

<p><img src="/assets/2024/kans-3th/w6/20241012_kans_w6_24.png" alt="img.png" class="image-center" /></p>

<h5 id="control--envoy-data-plane-and-the-gloo-control-plane">Control : <strong>Envoy</strong> data plane and the <strong>Gloo</strong> control plane.</h5>

<ul>
  <li>Now we’ll configure a <strong>Gateway listener</strong>, establish external access to <strong>Gloo Gateway,</strong> and test the <strong>routing</strong> <strong>rules</strong> that are the core of the proxy configuration.</li>
</ul>

<p><strong>Configure a Gateway Listener</strong></p>

<ul>
  <li>Let’s begin by establishing a Gateway resource that sets up an HTTP listener on port 8080 to expose routes from all our namespaces. Gateway custom resources like this are part of the Gateway API standard.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 02-gateway.yaml</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> &gt; 02-gateway.yaml
kind: Gateway
apiVersion: gateway.networking.k8s.io/v1
metadata:
  name: http
  namespace: gloo-system
spec:
  gatewayClassName: gloo-gateway
  listeners:
  - protocol: HTTP
    port: 8080
    name: http
    allowedRoutes:
      namespaces:
        from: All
</span><span class="no">EOF

</span><span class="c"># gateway 리소스 생성</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> 02-gateway.yaml
<span class="c"># =&gt; gateway.gateway.networking.k8s.io/http created</span>

<span class="c"># 확인 : Now we can confirm that the Gateway has been activated</span>
<span class="nv">$ </span>kubectl get gateway <span class="nt">-n</span> gloo-system
<span class="c"># =&gt; NAME   CLASS          ADDRESS   PROGRAMMED   AGE</span>
<span class="c">#    http   gloo-gateway             True         8s</span>
<span class="nv">$ </span>kubectl get gateway <span class="nt">-n</span> gloo-system <span class="nt">-o</span> yaml
<span class="c"># =&gt; apiVersion: v1</span>
<span class="c">#    items:</span>
<span class="c">#    - apiVersion: gateway.networking.k8s.io/v1</span>
<span class="c">#      kind: Gateway</span>
<span class="c">#      metadata:</span>
<span class="c">#        name: http</span>
<span class="c">#        namespace: gloo-system</span>
<span class="c">#      spec:</span>
<span class="c">#        gatewayClassName: gloo-gateway</span>
<span class="c">#        listeners:</span>
<span class="c">#        - allowedRoutes:</span>
<span class="c">#            namespaces:</span>
<span class="c">#              from: All</span>
<span class="c">#          name: http</span>
<span class="c">#          port: 8080</span>
<span class="c">#          protocol: HTTP</span>
<span class="c">#    ...</span>

<span class="c"># You can also confirm that Gloo Gateway has spun up an Envoy proxy instance in response to the creation of this Gateway object by deploying gloo-proxy-http:</span>
<span class="nv">$ </span>kubectl get deployment gloo-proxy-http <span class="nt">-n</span> gloo-system
<span class="c"># =&gt; NAME              READY   UP-TO-DATE   AVAILABLE   AGE</span>
<span class="c">#    gloo-proxy-http   1/1     1            1           66s</span>

<span class="c"># envoy 사용 확인</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-n</span> gloo-system
<span class="c"># =&gt; NAME                               READY   STATUS    RESTARTS   AGE</span>
<span class="c">#    gateway-proxy-57c49d4f48-xm8vv     1/1     Running   0          13m</span>
<span class="c">#    gloo-748d877c4-24ngk               1/1     Running   0          13m</span>
<span class="c">#    gloo-proxy-http-587765f6b6-mpnt5   1/1     Running   0          78s</span>
<span class="nv">$ </span>kubectl describe pod <span class="nt">-n</span> gloo-system  |grep Image:
<span class="c"># =&gt;     Image:         quay.io/solo-io/gloo-envoy-wrapper:1.17.7 # &lt;span style="color: green;"&gt;👉 이름에서 알 수 있듯이 envoy가 들어있고 감싸고 있는것으로 보입니다.&lt;/span&gt;</span>
<span class="c">#        Image:          quay.io/solo-io/gloo:1.17.7</span>
<span class="c">#        Image:         quay.io/solo-io/gloo-envoy-wrapper:1.17.7</span>

<span class="c"># gloo-proxy-http 서비스는 External-IP는 Pending 상태</span>
<span class="nv">$ </span>kubectl get svc <span class="nt">-n</span> gloo-system gloo-proxy-http
<span class="c"># =&gt; NAME              TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE</span>
<span class="c">#    gloo-proxy-http   LoadBalancer   10.96.104.226   &amp;lt;pending&amp;gt;     8080:31461/TCP   101s</span>

<span class="c"># gloo-proxy-http NodePort 30001 설정</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: http
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: gloo-proxy-http
    app.kubernetes.io/version: 1.17.7
    gateway.networking.k8s.io/gateway-name: http
    gloo: kube-gateway
    helm.sh/chart: gloo-gateway-1.17.7
  name: gloo-proxy-http
  namespace: gloo-system
spec:
  ports:
  - name: http
    nodePort: 30001
    port: 8080
  selector:
    app.kubernetes.io/instance: http
    app.kubernetes.io/name: gloo-proxy-http
    gateway.networking.k8s.io/gateway-name: http
  type: LoadBalancer
</span><span class="no">EOF
</span><span class="c"># =&gt; service/gloo-proxy-http configured</span>

<span class="nv">$ </span>kubectl get svc <span class="nt">-n</span> gloo-system gloo-proxy-http
<span class="c"># =&gt; NAME              TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE</span>
<span class="c">#    gloo-proxy-http   LoadBalancer   10.96.104.226   &amp;lt;pending&amp;gt;     8080:30001/TCP   2m17s </span>
<span class="c"># &lt;span style="color: green;"&gt;👉 노드포트가 30001로 변경되었습니다.&lt;/span&gt;</span>
</code></pre></div></div>

<p><strong>Establish External Access to Proxy</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 간편한 테스트를 위해 port-forward를 사용하여 외부로 노출하겠습니다.</span>
<span class="nv">$ </span>kubectl port-forward deployment/gloo-proxy-http <span class="nt">-n</span> gloo-system 8080:8080 &amp;
</code></pre></div></div>

<p><strong>Configure Simple Routing with an HTTPRoute</strong></p>

<p>Let’s begin our routing configuration with the simplest possible <strong>route</strong> to expose the <strong>/get</strong> operation on <strong>httpbin</strong></p>

<p><code class="language-plaintext highlighter-rouge">HTTPRoute</code> is one of the new Kubernetes CRDs introduced by the Gateway API, as documented <a href="https://gateway-api.sigs.k8s.io/api-types/httproute/">here</a>. We’ll start by introducing a simple <code class="language-plaintext highlighter-rouge">HTTPRoute</code> for our service.</p>

<p><strong>HTTPRoute Spec</strong></p>

<ul>
  <li><a href="https://gateway-api.sigs.k8s.io/reference/spec/#gateway.networking.k8s.io/v1.ParentRef">ParentRefs</a>-Define which <strong>Gateways</strong> this <strong>Route</strong> wants to be <strong>attached</strong> to.</li>
  <li><a href="https://gateway-api.sigs.k8s.io/reference/spec/#gateway.networking.k8s.io/v1.Hostname">Hostnames</a> (optional)- Define a list of <strong>hostnames</strong> to use for matching the <strong>Host header</strong> of HTTP requests.</li>
  <li><a href="https://gateway-api.sigs.k8s.io/reference/spec/#gateway.networking.k8s.io/v1.HTTPRouteRule">Rules</a>-Define a list of <strong>rules</strong> to perform <strong>actions</strong> against matching HTTP requests.
    <ul>
      <li>Each rule consists of <a href="https://gateway-api.sigs.k8s.io/reference/spec/#gateway.networking.k8s.io/v1.HTTPRouteMatch">matches</a>, <a href="https://gateway-api.sigs.k8s.io/reference/spec/#gateway.networking.k8s.io/v1.HTTPRouteFilter">filters</a> (optional), <a href="https://gateway-api.sigs.k8s.io/reference/spec/#gateway.networking.k8s.io/v1.HTTPBackendRef">backendRefs</a> (optional) and <a href="https://gateway-api.sigs.k8s.io/reference/spec/#gateway.networking.k8s.io/v1.HTTPRouteTimeouts">timeouts</a> (optional) fields.</li>
    </ul>
  </li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>apiVersion: gateway.networking.k8s.io/v1beta1
kind: HTTPRoute
metadata:
  name: httpbin
  namespace: httpbin
  labels:
    example: httpbin-route
spec:
  parentRefs:
    - name: http
      namespace: gloo-system
  hostnames:
    - <span class="s2">"api.example.com"</span>
  rules:
  - matches:
    - path:
        <span class="nb">type</span>: Exact
        value: /get
    backendRefs:
      - name: httpbin
        port: 8000
</code></pre></div></div>

<p>This example <strong>attaches</strong> to the default <code class="language-plaintext highlighter-rouge">Gateway</code> object created for us when we installed Gloo Gateway earlier.</p>

<p>See the <code class="language-plaintext highlighter-rouge">gloo-system/http</code> reference in the <code class="language-plaintext highlighter-rouge">parentRefs</code> stanza.</p>

<p>The <a href="https://gateway-api.sigs.k8s.io/api-types/gateway/">Gateway</a> object simply represents a host:port listener that the proxy will expose to accept ingress traffic.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Our route watches for HTTP requests directed at the host api.example.com with the request path /get and then forwards the request to the httpbin service on port 8000.</span>
<span class="c"># Let’s establish this route now:</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> https://raw.githubusercontent.com/solo-io/gloo-gateway-use-cases/main/gateway-api-tutorial/03-httpbin-route.yaml
<span class="c"># =&gt; httproute.gateway.networking.k8s.io/httpbin created</span>

<span class="c">#</span>
<span class="nv">$ </span>kubectl get httproute <span class="nt">-n</span> httpbin
<span class="c"># =&gt; NAME      HOSTNAMES             AGE</span>
<span class="c">#    httpbin   [&amp;quot;api.example.com&amp;quot;]   12s</span>

<span class="nv">$ </span>kubectl describe httproute <span class="nt">-n</span> httpbin
<span class="c"># =&gt; ...</span>
<span class="c">#    Spec:</span>
<span class="c">#      Hostnames:</span>
<span class="c">#        api.example.com</span>
<span class="c">#      Parent Refs:</span>
<span class="c">#        Group:      gateway.networking.k8s.io</span>
<span class="c">#        Kind:       Gateway</span>
<span class="c">#        Name:       http</span>
<span class="c">#        Namespace:  gloo-system</span>
<span class="c">#      Rules:</span>
<span class="c">#        Backend Refs:</span>
<span class="c">#          Group:   </span>
<span class="c">#          Kind:    Service</span>
<span class="c">#          Name:    httpbin</span>
<span class="c">#          Port:    8000</span>
<span class="c">#          Weight:  1</span>
<span class="c">#        Matches:</span>
<span class="c">#          Path:</span>
<span class="c">#            Type:   Exact</span>
<span class="c">#            Value:  /get</span>
<span class="c">#    ...</span>
</code></pre></div></div>

<p><strong>Test the Simple Route with Curl</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># let’s use curl to display the response with the -i option to additionally show the HTTP response code and headers.</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"127.0.0.1 api.example.com"</span> | <span class="nb">sudo tee</span> <span class="nt">-a</span> /etc/hosts
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"httproute - http://api.example.com:30001/get"</span> <span class="c"># 웹브라우저</span>
<span class="c"># 혹은</span>
<span class="nv">$ </span>curl <span class="nt">-is</span> <span class="nt">-H</span> <span class="s2">"Host: api.example.com"</span> http://localhost:8080/get <span class="c"># kubectl port-forward 사용 시</span>
<span class="c"># =&gt; HTTP/1.1 200 OK</span>
<span class="c">#    &lt;span style="color: red;"&gt;server: envoy&lt;/span&gt; # &lt;span style="color: green;"&gt;👉 서버가 envoy임을 확인할 수 있습니다.&lt;/span&gt;</span>
<span class="c">#    date: Sat, 1 Oct 2024 16:19:18 GMT</span>
<span class="c">#    content-type: application/json</span>
<span class="c">#    content-length: 239</span>
<span class="c">#    access-control-allow-origin: *</span>
<span class="c">#    access-control-allow-credentials: true</span>
<span class="c">#    x-envoy-upstream-service-time: 13</span>
<span class="c">#    </span>
<span class="c">#    {</span>
<span class="c">#      &amp;quot;args&amp;quot;: {},</span>
<span class="c">#      &amp;quot;headers&amp;quot;: {</span>
<span class="c">#        &amp;quot;Accept&amp;quot;: &amp;quot;*/*&amp;quot;,</span>
<span class="c">#        &amp;quot;Host&amp;quot;: &amp;quot;api.example.com&amp;quot;,</span>
<span class="c">#        &amp;quot;User-Agent&amp;quot;: &amp;quot;curl/8.1.2&amp;quot;,</span>
<span class="c">#        &amp;quot;X-Envoy-Expected-Rq-Timeout-Ms&amp;quot;: &amp;quot;15000&amp;quot;</span>
<span class="c">#      },</span>
<span class="c">#      &amp;quot;origin&amp;quot;: &amp;quot;10.244.0.12&amp;quot;,</span>
<span class="c">#      &amp;quot;url&amp;quot;: &amp;quot;http://api.example.com/get&amp;quot;</span>
<span class="c">#    }</span>
</code></pre></div></div>

<p>Note that if we attempt to invoke another valid endpoint <code class="language-plaintext highlighter-rouge">/delay</code> on the <code class="language-plaintext highlighter-rouge">httpbin</code> service, it will fail with a <code class="language-plaintext highlighter-rouge">404 Not Found</code> error. Why? Because our <code class="language-plaintext highlighter-rouge">HTTPRoute</code> policy is only exposing access to <code class="language-plaintext highlighter-rouge">/get</code>, one of the many endpoints available on the service. If we try to consume an alternative <code class="language-plaintext highlighter-rouge">httpbin</code> endpoint like <code class="language-plaintext highlighter-rouge">/delay</code>:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 호출 응답 왜 그럴까?</span>
<span class="nv">$ </span>curl <span class="nt">-is</span> <span class="nt">-H</span> <span class="s2">"Host: api.example.com"</span> http://localhost:8080/delay/1
<span class="c"># =&gt; Handling connection for 8080</span>
<span class="c">#    HTTP/1.1 404 Not Found</span>
<span class="c">#    date: Sat, 1 Oct 2024 16:20:23 GMT</span>
<span class="c">#    server: envoy</span>
<span class="c">#    content-length: 0</span>
<span class="c"># &lt;span style="color: green;"&gt;👉 gloo를 통했을때는 HTTProute 설정에서 /get 이라는 경로에대해서 정확하게 일치(Exact) 할 경우에만 라우팅하도록 해서 그렇습니다.&lt;/span&gt;</span>

<span class="c"># nodeport 직접 접속 테스트</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"httproute - http://api.example.com:30000/delay/1"</span> <span class="c"># 1초 후 응답</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"httproute - http://api.example.com:30000/delay/5"</span> <span class="c"># 5초 후 응답</span>
<span class="c"># &lt;span style="color: green;"&gt;👉 노드포트로 직접 접속할 경우 gloo HTTPRoute를 거치지 않기 때문에 접속이 가능합니다.&lt;/span&gt;</span>
</code></pre></div></div>

<p>kind 클러스터를 구성할때 30000 포트를 열었기 때문에 NodePort로 직접 접속이 가능합니다.</p>

<p><img src="/assets/2024/kans-3th/w6/20241012_kans_w6_25.png" alt="img.png" class="w-80 image-center" />
<em class="image-caption">http://api.example.com:30000/delay/1 호출 결과 =&gt; 1초후 응답</em></p>

<p><strong>[정규식 패턴 매칭] Explore Routing with Regex Matching Patterns</strong></p>

<p>Let’s assume that now we DO want to expose other <code class="language-plaintext highlighter-rouge">httpbin</code> endpoints like <code class="language-plaintext highlighter-rouge">/delay</code>. Our initial <code class="language-plaintext highlighter-rouge">HTTPRoute</code> is inadequate, because it is looking for an exact path match with <code class="language-plaintext highlighter-rouge">/get</code>.</p>

<p>We’ll <strong>modify</strong> it in a couple of ways. <strong>First</strong>, we’ll modify the matcher to look for <strong>path prefix matches</strong> instead of an <strong>exact match</strong>. <strong>Second</strong>, we’ll add a <strong>new request filter</strong> to <strong>rewrite</strong> the matched <code class="language-plaintext highlighter-rouge">/api/httpbin/</code> prefix with just a <code class="language-plaintext highlighter-rouge">/</code> prefix, which will give us the flexibility to access any endpoint available on the <code class="language-plaintext highlighter-rouge">httpbin</code> service. So a path like <code class="language-plaintext highlighter-rouge">/api/httpbin/delay/1</code> will be sent to <code class="language-plaintext highlighter-rouge">httpbin</code> with the path <code class="language-plaintext highlighter-rouge">/delay/1</code>.</p>

<p>URL에 패턴이 매치가 되면 rewrite해서 실제 접속되는 경로를 변경할 수 있습니다.</p>
<ul>
  <li>예시) <code class="language-plaintext highlighter-rouge">/api/httpbin/delay/1</code> ⇒ <code class="language-plaintext highlighter-rouge">/delay/1</code></li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Here are the modifications we’ll apply to our HTTPRoute:</span>

    - matches:
        <span class="c"># Switch from an Exact Matcher(정확한 매팅) to a PathPrefix (경로 매팅) Matcher</span>
        - path:
            <span class="nb">type</span>: PathPrefix
            value: /api/httpbin/
      filters:
        <span class="c"># Replace(변경) the /api/httpbin matched prefix with /</span>
        - <span class="nb">type</span>: URLRewrite
          urlRewrite:
            path:
              <span class="nb">type</span>: ReplacePrefixMatch
              replacePrefixMatch: /
</code></pre></div></div>

<ul>
  <li>2가지 수정 내용 적용 후 확인</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> https://raw.githubusercontent.com/solo-io/gloo-gateway-use-cases/main/gateway-api-tutorial/04-httpbin-rewrite.yaml
<span class="c"># =&gt; httproute.gateway.networking.k8s.io/httpbin configured</span>

<span class="c"># 확인</span>
<span class="nv">$ </span>kubectl describe httproute <span class="nt">-n</span> httpbin
<span class="c"># =&gt; ...</span>
<span class="c">#    Spec:</span>
<span class="c">#      Hostnames:</span>
<span class="c">#        api.example.com</span>
<span class="c">#      Parent Refs:</span>
<span class="c">#        Group:      gateway.networking.k8s.io</span>
<span class="c">#        Kind:       Gateway</span>
<span class="c">#        Name:       http</span>
<span class="c">#        Namespace:  gloo-system</span>
<span class="c">#      Rules:</span>
<span class="c">#        Backend Refs:</span>
<span class="c">#          Group:</span>
<span class="c">#          Kind:    Service</span>
<span class="c">#          Name:    httpbin</span>
<span class="c">#          Port:    8000</span>
<span class="c">#          Weight:  1</span>
<span class="c">#        Filters:</span>
<span class="c">#          Type:  URLRewrite</span>
<span class="c">#          URL Rewrite:</span>
<span class="c">#            Path:</span>
<span class="c">#              Replace Prefix Match:  /</span>
<span class="c">#              Type:                  ReplacePrefixMatch</span>
<span class="c">#        Matches:</span>
<span class="c">#          Path:</span>
<span class="c">#            Type:   PathPrefix</span>
<span class="c">#            Value:  /api/httpbin/</span>
<span class="c">#    ...</span>
</code></pre></div></div>

<p><strong>Test Routing with Regex Matching Patterns</strong></p>

<p>When we used only a single route with an exact match pattern, we could only exercise the httpbin <code class="language-plaintext highlighter-rouge">/get</code> endpoint. Let’s now use <code class="language-plaintext highlighter-rouge">curl</code> to confirm that both <code class="language-plaintext highlighter-rouge">/get</code> and <code class="language-plaintext highlighter-rouge">/delay</code> work as expected.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"httproute - http://api.example.com:30001/api/httpbin/get"</span> <span class="c"># 웹브라우저</span>
<span class="c"># 혹은</span>
<span class="nv">$ </span>curl <span class="nt">-is</span> <span class="nt">-H</span> <span class="s2">"Host: api.example.com"</span> http://localhost:8080/api/httpbin/get <span class="c"># kubectl port-forward 사용 시</span>
<span class="c"># =&gt; HTTP/1.1 200 OK</span>
<span class="c">#    server: envoy</span>
<span class="c">#    date: Sat, 1 Oct 2024 16:33:20 GMT</span>
<span class="c">#    content-type: application/json</span>
<span class="c">#    content-length: 289</span>
<span class="c">#    access-control-allow-origin: *</span>
<span class="c">#    access-control-allow-credentials: true</span>
<span class="c">#    x-envoy-upstream-service-time: 20</span>
<span class="c">#    </span>
<span class="c">#    {</span>
<span class="c">#      &amp;quot;args&amp;quot;: {},</span>
<span class="c">#      &amp;quot;headers&amp;quot;: {</span>
<span class="c">#        &amp;quot;Accept&amp;quot;: &amp;quot;*/*&amp;quot;,</span>
<span class="c">#        &amp;quot;Host&amp;quot;: &amp;quot;api.example.com&amp;quot;,</span>
<span class="c">#        &amp;quot;User-Agent&amp;quot;: &amp;quot;curl/8.1.2&amp;quot;,</span>
<span class="c">#        &amp;quot;X-Envoy-Expected-Rq-Timeout-Ms&amp;quot;: &amp;quot;15000&amp;quot;,</span>
<span class="c">#        &amp;quot;X-Envoy-Original-Path&amp;quot;: &amp;quot;/api/httpbin/get&amp;quot;</span>
<span class="c">#      },</span>
<span class="c">#      &amp;quot;origin&amp;quot;: &amp;quot;10.244.0.12&amp;quot;,</span>
<span class="c">#      &amp;quot;url&amp;quot;: &amp;quot;http://api.example.com/get&amp;quot;</span>
<span class="c">#    }</span>

<span class="c"># 아래 NodePort 와 GW API 통한 접속 비교</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"httproute - http://api.example.com:30001/api/httpbin/get"</span>
<span class="c"># =&gt; HTTP/1.1 200 OK</span>
<span class="c">#    ...</span>
<span class="c">#      &amp;quot;url&amp;quot;: &amp;quot;&lt;span style="color: red;"&gt;http://api.example.com/get&lt;/span&gt;&amp;quot;</span>
<span class="c">#    }</span>
<span class="c"># &lt;span style="color: green;"&gt;👉 gloo gateway가 /app/httpbin/get =&gt; / 로 변경하여 잘 접속이 되었습니다.&lt;/span&gt;</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"httproute - http://api.example.com:30000/api/httpbin/get"</span> <span class="c"># NodePort 직접 접근</span>
<span class="c"># =&gt; HTTP/1.1 404 NOT FOUND</span>
<span class="c">#    ...</span>
<span class="c"># &lt;span style="color: green;"&gt;👉 NodePort에 직접 접근시에는 /app/httpbin/get이 그대로 파드에 전달되어 없는 경로라서 404 에러가 납니다.&lt;/span&gt;</span>

<span class="nt">---</span>
<span class="c">#</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"httproute - http://api.example.com:30001/api/httpbin/delay/1"</span> <span class="c"># 웹브라우저</span>
<span class="c"># 혹은</span>
<span class="nv">$ </span>curl <span class="nt">-is</span> <span class="nt">-H</span> <span class="s2">"Host: api.example.com"</span> http://localhost:8080/api/httpbin/delay/1 <span class="c"># kubectl port-forward 사용 시</span>
<span class="c"># =&gt; HTTP/1.1 200 OK</span>
<span class="c">#    server: envoy</span>
<span class="c">#    date: Sat, 1 Oct 2024 16:36:49 GMT</span>
<span class="c">#    content-type: application/json</span>
<span class="c">#    content-length: 343</span>
<span class="c">#    access-control-allow-origin: *</span>
<span class="c">#    access-control-allow-credentials: true</span>
<span class="c">#    x-envoy-upstream-service-time: 1049     # envoy 가 업스트림 httpbin 요청 처리에 걸리 시간 1초 이상</span>
<span class="c">#    </span>
<span class="c">#    {</span>
<span class="c">#      &amp;quot;args&amp;quot;: {},</span>
<span class="c">#      &amp;quot;data&amp;quot;: &amp;quot;&amp;quot;,</span>
<span class="c">#      &amp;quot;files&amp;quot;: {},</span>
<span class="c">#      &amp;quot;form&amp;quot;: {},</span>
<span class="c">#      &amp;quot;headers&amp;quot;: {</span>
<span class="c">#        &amp;quot;Accept&amp;quot;: &amp;quot;*/*&amp;quot;,</span>
<span class="c">#        &amp;quot;Host&amp;quot;: &amp;quot;api.example.com&amp;quot;,</span>
<span class="c">#        &amp;quot;User-Agent&amp;quot;: &amp;quot;curl/8.1.2&amp;quot;,</span>
<span class="c">#        &amp;quot;X-Envoy-Expected-Rq-Timeout-Ms&amp;quot;: &amp;quot;15000&amp;quot;,</span>
<span class="c">#        &amp;quot;X-Envoy-Original-Path&amp;quot;: &amp;quot;/api/httpbin/delay/1&amp;quot;</span>
<span class="c">#      },</span>
<span class="c">#      &amp;quot;origin&amp;quot;: &amp;quot;10.244.0.12&amp;quot;,</span>
<span class="c">#      &amp;quot;url&amp;quot;: &amp;quot;http://api.example.com/delay/1&amp;quot;</span>
<span class="c">#    }</span>

<span class="nv">$ </span>curl <span class="nt">-is</span> <span class="nt">-H</span> <span class="s2">"Host: api.example.com"</span> http://localhost:8080/api/httpbin/delay/2
<span class="c"># =&gt; ...</span>
<span class="c">#    x-envoy-upstream-service-time: 2133</span>
<span class="c">#    ...</span>
</code></pre></div></div>

<p>Perfect! It works just as expected! Note that the <code class="language-plaintext highlighter-rouge">/delay</code> operation completed successfully and that the 1-second delay was applied. The response header <code class="language-plaintext highlighter-rouge">x-envoy-upstream-service-time: 1023</code> indicates that Envoy reported that the upstream <code class="language-plaintext highlighter-rouge">httpbin</code> service required just over 1 second (1,023 milliseconds) to process the request. In the initial <code class="language-plaintext highlighter-rouge">/get</code> operation, which doesn’t inject an artificial delay, observe that the same header reported only 14 milliseconds of upstream processing time.</p>

<p><strong>[업스트림 베어러 토큰을 사용한 변환] Test Transformations with Upstream Bearer Tokens</strong></p>

<p><strong>목적</strong> : 요청을 라우팅하는 <strong>백엔드</strong> 시스템 중 하나에서 <strong>인증</strong>해야 하는 <strong>요구</strong> 사항이 있는 경우는 어떻게 할까요? 이 업스트림 시스템에는 권한 부여를 위한 API 키가 필요하고, 이를 소비하는 <strong>클라이언트에 직접 노출하고 싶지 않다</strong>고 가정해 보겠습니다. 즉<strong>, 프록시 계층</strong>에서 <strong>요청</strong>에 <strong>주입</strong>할 간단한 <strong>베어러 토큰</strong>을 구성하고 싶습니다. (정적 API 키 토큰을 직접 주입)</p>

<p>What if we have a requirement to <strong>authenticate</strong> with one of the <strong>backend</strong> systems to which we route our requests?</p>

<p>Let’s assume that this <strong>upstream</strong> system requires an <strong>API key</strong> for authorization, and that we <strong>don’t</strong> want to expose this directly to the <strong>consuming client</strong>. In other words, we’d like to configure a <strong>simple bearer toke</strong>n to be <strong>injected</strong> into the <strong>request</strong> at the <strong>proxy layer.</strong></p>

<p>We can <strong>express</strong> this in the <strong>Gateway API</strong> by adding a <strong>filter</strong> that applies a simple <strong>transformation</strong> to the i<strong>ncoming request</strong>.</p>

<p>This will be applied along with the <strong>URLRewrite</strong> filter we created in the previous step.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># The new filters stanza in our HTTPRoute now looks like this:</span>

      filters:
        - <span class="nb">type</span>: URLRewrite
          urlRewrite:
            path:
              <span class="nb">type</span>: ReplacePrefixMatch
              replacePrefixMatch: /
              
        <span class="c"># Add a Bearer token to supply a static API key when routing to backend system</span>
        - <span class="nb">type</span>: RequestHeaderModifier
          requestHeaderModifier:
            add:
              - name: Authorization
                value: Bearer my-api-key
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> https://raw.githubusercontent.com/solo-io/gloo-gateway-use-cases/main/gateway-api-tutorial/05-httpbin-rewrite-xform.yaml

<span class="c">#</span>
<span class="nv">$ </span>kubectl describe httproute <span class="nt">-n</span> httpbin
<span class="c"># =&gt; ...</span>
<span class="c">#    Spec:</span>
<span class="c">#      ...</span>
<span class="c">#      Rules:</span>
<span class="c">#        Backend Refs:</span>
<span class="c">#          Group:</span>
<span class="c">#          Kind:    Service</span>
<span class="c">#          Name:    httpbin</span>
<span class="c">#          Port:    8000</span>
<span class="c">#          Weight:  1</span>
<span class="c">#        Filters:</span>
<span class="c">#          Type:  URLRewrite</span>
<span class="c">#          URL Rewrite:</span>
<span class="c">#            Path:</span>
<span class="c">#              Replace Prefix Match:  /</span>
<span class="c">#              Type:                  ReplacePrefixMatch</span>
<span class="c">#          Request Header Modifier:</span>
<span class="c">#            Add:</span>
<span class="c">#              Name:   Authorization</span>
<span class="c">#              Value:  Bearer my-api-key</span>
<span class="c">#          Type:       RequestHeaderModifier</span>
<span class="c">#        Matches:</span>
<span class="c">#          Path:</span>
<span class="c">#            Type:   PathPrefix</span>
<span class="c">#            Value:  /api/httpbin/</span>
</code></pre></div></div>

<ul>
  <li>동작 테스트</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"httproute - http://api.example.com:30001/api/httpbin/get"</span> <span class="c"># 웹브라우저</span>
<span class="c"># 혹은</span>
<span class="nv">$ </span>curl <span class="nt">-is</span> <span class="nt">-H</span> <span class="s2">"Host: api.example.com"</span> http://localhost:8080/api/httpbin/get <span class="c"># kubectl port-forward 사용 시</span>
<span class="c"># =&gt; HTTP/1.1 200 OK</span>
<span class="c">#    server: envoy</span>
<span class="c">#    date: Sat, 1 Oct 2024 16:40:59 GMT</span>
<span class="c">#    content-type: application/json</span>
<span class="c">#    content-length: 332</span>
<span class="c">#    access-control-allow-origin: *</span>
<span class="c">#    access-control-allow-credentials: true</span>
<span class="c">#    x-envoy-upstream-service-time: 19</span>
<span class="c">#    </span>
<span class="c">#    {</span>
<span class="c">#      &amp;quot;args&amp;quot;: {},</span>
<span class="c">#      &amp;quot;headers&amp;quot;: {</span>
<span class="c">#        &amp;quot;Accept&amp;quot;: &amp;quot;*/*&amp;quot;,</span>
<span class="c">#        &lt;span style="color: red"&gt;&amp;quot;Authorization&amp;quot;: &amp;quot;Bearer my-api-key&amp;quot;,&lt;/span&gt; </span>
<span class="c">#        &amp;quot;Host&amp;quot;: &amp;quot;api.example.com&amp;quot;,</span>
<span class="c">#        &amp;quot;User-Agent&amp;quot;: &amp;quot;curl/8.1.2&amp;quot;,</span>
<span class="c">#        &amp;quot;X-Envoy-Expected-Rq-Timeout-Ms&amp;quot;: &amp;quot;15000&amp;quot;,</span>
<span class="c">#        &amp;quot;X-Envoy-Original-Path&amp;quot;: &amp;quot;/api/httpbin/get&amp;quot;</span>
<span class="c">#      },</span>
<span class="c">#      &amp;quot;origin&amp;quot;: &amp;quot;10.244.0.12&amp;quot;,</span>
<span class="c">#      &amp;quot;url&amp;quot;: &amp;quot;http://api.example.com/get&amp;quot;</span>
<span class="c">#    }</span>
<span class="c"># &lt;span style="color: green;"&gt;👉 클라이언트에서는 Authorization 헤더를 안 주었지만, Gloo gateway를 통하자&lt;/span&gt; </span>
<span class="c"># &lt;span style="color: green;"&gt;    Authorization 헤더에 Bearer my-api-key 가 추가되어 있습니다.&lt;/span&gt;</span>
</code></pre></div></div>

<h5 id="migrate">Migrate</h5>

<p>In this section, we’ll explore how a couple of common service migration techniques, <strong>dark launches with header-based routing</strong> and <strong>canary releases with percentage-based routing,</strong> are supported by the Gateway API standard.</p>

<p><strong>Configure Two Workloads for Migration Routing</strong></p>

<p>Let’s first establish <strong>two versions</strong> of a <strong>workload</strong> to facilitate our migration example. We’ll use the open-source <a href="https://github.com/nicholasjackson/fake-service">Fake Service</a> to enable this.</p>

<ul>
  <li><strong>Fake service</strong> that can handle both <strong>HTTP</strong> and <strong>gRPC</strong> traffic, for <strong>testing</strong> upstream service communications and testing service mesh and other scenarios.</li>
</ul>

<p>Let’s establish a <code class="language-plaintext highlighter-rouge">v1</code> of our <code class="language-plaintext highlighter-rouge">my-workload</code> service that’s configured to return a response string containing “v1”. We’ll create a corresponding <code class="language-plaintext highlighter-rouge">my-workload-v2</code> service as well.</p>

<ul>
  <li>ingress의 카나리 배포와 유사하게 V1의 일부 트래픽을 V2로 라우팅할 수 있습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># You should see the response below, indicating deployments for both v1 and v2 of my-workload have been created in the my-workload namespace.</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> https://raw.githubusercontent.com/solo-io/gloo-gateway-use-cases/main/gateway-api-tutorial/06-workload-svcs.yaml
<span class="c"># =&gt; namespace/my-workload created</span>
<span class="c">#    serviceaccount/my-workload created</span>
<span class="c">#    deployment.apps/my-workload-v1 created</span>
<span class="c">#    deployment.apps/my-workload-v2 created</span>
<span class="c">#    service/my-workload-v1 created</span>
<span class="c">#    service/my-workload-v2 created</span>

<span class="c"># v1,v2 2가지 버전 워크로드 확인</span>
<span class="nv">$ </span>kubectl get deploy,pod,svc,endpointslices <span class="nt">-n</span> my-workload
<span class="c"># =&gt; NAME                             READY   UP-TO-DATE   AVAILABLE   AGE</span>
<span class="c">#    deployment.apps/my-workload-v1   1/1     1            1           15s</span>
<span class="c">#    deployment.apps/my-workload-v2   1/1     1            1           15s</span>
<span class="c">#    </span>
<span class="c">#    NAME                                  READY   STATUS    RESTARTS   AGE</span>
<span class="c">#    pod/my-workload-v1-644f98bbd9-q6cs5   1/1     Running   0          15s</span>
<span class="c">#    pod/my-workload-v2-5bb5fcfcbc-bq88c   1/1     Running   0          15s</span>
<span class="c">#    </span>
<span class="c">#    NAME                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE</span>
<span class="c">#    service/my-workload-v1   ClusterIP   10.96.203.193   &amp;lt;none&amp;gt;        8080/TCP   15s</span>
<span class="c">#    service/my-workload-v2   ClusterIP   10.96.210.160   &amp;lt;none&amp;gt;        8080/TCP   15s</span>
<span class="c">#    </span>
<span class="c">#    NAME                                                  ADDRESSTYPE   PORTS   ENDPOINTS     AGE</span>
<span class="c">#    endpointslice.discovery.k8s.io/my-workload-v1-d9sqd   IPv4          8080    10.244.0.14   15s</span>
<span class="c">#    endpointslice.discovery.k8s.io/my-workload-v2-mv7fq   IPv4          8080    10.244.0.13   15s</span>
</code></pre></div></div>

<p><strong>Test Simple V1 Routing</strong></p>

<p>Before we dive into routing to multiple services, we’ll start by building a simple <strong><code class="language-plaintext highlighter-rouge">HTTPRoute</code></strong> that sends HTTP requests to host <code class="language-plaintext highlighter-rouge">api.example.com</code> whose paths begin with <strong><code class="language-plaintext highlighter-rouge">/api/my-workload</code></strong> to the <strong><code class="language-plaintext highlighter-rouge">v1</code></strong> workload:</p>

<p><img src="/assets/2024/kans-3th/w6/20241012_kans_w6_26.png" alt="img.png" /></p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">gateway.networking.k8s.io/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">HTTPRoute</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">my-workload</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">my-workload</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">example</span><span class="pi">:</span> <span class="s">my-workload-route</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">parentRefs</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">http</span>
      <span class="na">namespace</span><span class="pi">:</span> <span class="s">gloo-system</span>
  <span class="na">hostnames</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s2">"</span><span class="s">api.example.com"</span>
  <span class="na">rules</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">matches</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">path</span><span class="pi">:</span>
          <span class="na">type</span><span class="pi">:</span> <span class="s">PathPrefix</span>
          <span class="na">value</span><span class="pi">:</span> <span class="s">/api/my-workload</span>
      <span class="na">backendRefs</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">my-workload-v1</span>
          <span class="na">namespace</span><span class="pi">:</span> <span class="s">my-workload</span>
          <span class="na">port</span><span class="pi">:</span> <span class="m">8080</span>
</code></pre></div></div>

<p>Now apply this route:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> https://raw.githubusercontent.com/solo-io/gloo-gateway-use-cases/main/gateway-api-tutorial/07-workload-route.yaml
<span class="c"># =&gt; httproute.gateway.networking.k8s.io/my-workload created</span>

<span class="c">#</span>
<span class="nv">$ </span>kubectl get httproute <span class="nt">-A</span>
<span class="c"># =&gt; NAMESPACE     NAME          HOSTNAMES             AGE</span>
<span class="c">#    httpbin       httpbin       [&amp;quot;api.example.com&amp;quot;]   29m</span>
<span class="c">#    my-workload   my-workload   [&amp;quot;api.example.com&amp;quot;]   29s</span>

<span class="c">#</span>
<span class="nv">$ </span>kubectl describe httproute <span class="nt">-n</span> my-workload
<span class="c"># =&gt; ...</span>
<span class="c">#    Spec:</span>
<span class="c">#      Hostnames:</span>
<span class="c">#        api.example.com</span>
<span class="c">#      Parent Refs:</span>
<span class="c">#        Group:      gateway.networking.k8s.io</span>
<span class="c">#        Kind:       Gateway</span>
<span class="c">#        Name:       http</span>
<span class="c">#        Namespace:  gloo-system</span>
<span class="c">#      Rules:</span>
<span class="c">#        Backend Refs:</span>
<span class="c">#          Group:</span>
<span class="c">#          Kind:       Service</span>
<span class="c">#          Name:       my-workload-v1</span>
<span class="c">#          Namespace:  my-workload</span>
<span class="c">#          Port:       8080</span>
<span class="c">#          Weight:     1</span>
<span class="c">#        Matches:</span>
<span class="c">#          Path:</span>
<span class="c">#            Type:   PathPrefix</span>
<span class="c">#            Value:  /api/my-workload</span>
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> http://api.example.com:8080/api/my-workload | <span class="nb">grep </span>Workload<span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
<span class="c"># =&gt;  100   &amp;quot;body&amp;quot;: &amp;quot;Hello From My Workload (v1)!&amp;quot;,</span>

<span class="c"># &lt;span style="color: green;"&gt;👉 현재는 모든 연결이 v1으로 향합니다.&lt;/span&gt;</span>
</code></pre></div></div>

<p><strong>Simulate a v2 Dark Launch with Header-Based Routing</strong></p>

<p><img src="/assets/2024/kans-3th/w6/20241012_kans_w6_27.png" alt="img.png" /></p>

<p><a href="https://www.cloudbees.com/blog/when-dark-launch-right-release-strategy">Dark Launch</a> is a great cloud migration technique that <strong>releases new feature</strong>s to a select <strong>subset of users</strong> to gather <strong>feedback</strong> and experiment with improvements <strong>before</strong> potentially disrupting a larger user community.</p>

<ul>
  <li>Dark Launch : 일부 사용자에게 새로운 기능을 출시하여 피드백을 수집하고 잠재적으로 더 큰 사용자 커뮤니티를 방해하기 전에 개선 사항을 실험하는 훌륭한 클라우드 마이그레이션 기술</li>
</ul>

<p>We will simulate a dark launch in our example by installing the <strong>new cloud version</strong> of our <strong>service</strong> in our Kubernetes cluster, and then using declarative policy to route only requests containing a <strong>particular heade</strong>r to the new <code class="language-plaintext highlighter-rouge">v2</code> instance. The <strong>vast majority of users</strong> will continue to use the original <strong><code class="language-plaintext highlighter-rouge">v1</code></strong> of the service just as before.</p>

<ul>
  <li>우리는 Kubernetes 클러스터에 서비스의 새로운 클라우드 버전을 설치한 다음 선언적 정책을 사용하여 특정 헤더를 포함하는 요청만 새 인스턴스로 라우팅하여 예제에서 다크 런치를 시뮬레이션할 것입니다 . 대다수의 사용자는 이전과 마찬가지로 서비스의 <code class="language-plaintext highlighter-rouge">v1</code>을 계속 사용할 것 입니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  rules:
    - matches:
      - path:
          <span class="nb">type</span>: PathPrefix
          value: /api/my-workload
        <span class="c"># Add a matcher to route requests with a v2 version header to v2</span>
        <span class="c"># version=v2 헤더값이 있는 사용자만 v2 라우팅</span>
        headers:
        - name: version
          value: v2
      backendRefs:
        - name: my-workload-v2
          namespace: my-workload
          port: 8080      
    - matches:
      <span class="c"># Route requests without the version header to v1 as before</span>
      <span class="c"># 대다수 일반 사용자는 기존 처럼 v1 라우팅</span>
      - path:
          <span class="nb">type</span>: PathPrefix
          value: /api/my-workload
      backendRefs:
        - name: my-workload-v1
          namespace: my-workload
          port: 8080
</code></pre></div></div>

<p>Configure two separate routes, one for <code class="language-plaintext highlighter-rouge">v1</code> that the majority of service consumers will still use, and another route for <code class="language-plaintext highlighter-rouge">v2</code> that will be accessed by specifying a request header with name <code class="language-plaintext highlighter-rouge">version</code> and value <code class="language-plaintext highlighter-rouge">v2</code>. Let’s apply the modified <code class="language-plaintext highlighter-rouge">HTTPRoute</code>:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> https://raw.githubusercontent.com/solo-io/gloo-gateway-use-cases/main/gateway-api-tutorial/08-workload-route-header.yaml
<span class="c"># =&gt; httproute.gateway.networking.k8s.io/my-workload configured</span>

<span class="c"># </span>
<span class="nv">$ </span>kubectl describe httproute <span class="nt">-n</span> my-workload
<span class="c"># =&gt; ...</span>
<span class="c">#    Spec:  </span>
<span class="c">#      Rules:</span>
<span class="c">#        Backend Refs:</span>
<span class="c">#          Group:</span>
<span class="c">#          Kind:       Service</span>
<span class="c">#          Name:       my-workload-v2</span>
<span class="c">#          Namespace:  my-workload</span>
<span class="c">#          Port:       8080</span>
<span class="c">#          Weight:     1</span>
<span class="c">#        Matches:</span>
<span class="c">#          Headers:</span>
<span class="c">#            Name:   version</span>
<span class="c">#            Type:   Exact</span>
<span class="c">#            Value:  v2</span>
<span class="c">#          Path:</span>
<span class="c">#            Type:   PathPrefix</span>
<span class="c">#            Value:  /api/my-workload</span>
<span class="c">#        Backend Refs:</span>
<span class="c">#          Group:</span>
<span class="c">#          Kind:       Service</span>
<span class="c">#          Name:       my-workload-v1</span>
<span class="c">#          Namespace:  my-workload</span>
<span class="c">#          Port:       8080</span>
<span class="c">#          Weight:     1</span>
<span class="c">#        Matches:</span>
<span class="c">#          Path:</span>
<span class="c">#            Type:   PathPrefix</span>
<span class="c">#            Value:  /api/my-workload</span>
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># version: v2 헤더가 없는 경우 v1으로 라우팅됩니다.</span>
<span class="nv">$ </span>curl <span class="nt">-is</span> <span class="nt">-H</span> <span class="s2">"Host: api.example.com"</span> http://localhost:8080/api/my-workload
<span class="nv">$ </span>curl <span class="nt">-is</span> <span class="nt">-H</span> <span class="s2">"Host: api.example.com"</span> http://localhost:8080/api/my-workload | <span class="nb">grep </span>body
<span class="c"># =&gt; "body": "Hello From My Workload (v1)!",</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> http://api.example.com:8080/api/my-workload | <span class="nb">grep </span>Workload<span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
<span class="c"># =&gt;  100   &amp;quot;body&amp;quot;: &amp;quot;Hello From My Workload (v1)!&amp;quot;,</span>

<span class="c"># 하지만 version: v2 헤더가 있는 경우 v2로 라우팅됩니다.</span>
<span class="nv">$ </span>curl <span class="nt">-is</span> <span class="nt">-H</span> <span class="s2">"Host: api.example.com"</span> <span class="nt">-H</span> <span class="s2">"version: v2"</span> http://localhost:8080/api/my-workload
<span class="nv">$ </span>curl <span class="nt">-is</span> <span class="nt">-H</span> <span class="s2">"Host: api.example.com"</span> <span class="nt">-H</span> <span class="s2">"version: v2"</span> http://localhost:8080/api/my-workload | <span class="nb">grep </span>body
<span class="c"># =&gt;   &amp;quot;body&amp;quot;: &amp;quot;Hello From My Workload (v2)!&amp;quot;,</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> http://api.example.com:8080/api/my-workload | <span class="nb">grep </span>Workload<span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
<span class="c"># =&gt;  100   &amp;quot;body&amp;quot;: &amp;quot;Hello From My Workload (v1)!&amp;quot;,</span>
</code></pre></div></div>

<p><strong>Expand V2 Testing with Percentage-Based Routing</strong></p>

<p>After a successful dark-launch, we may want a period where we use a <strong>blue-green strategy</strong> of gradually <strong>shifting</strong> user traffic from the <strong>old</strong> version to the <strong>new</strong> one. Let’s explore this with a routing policy that splits our traffic evenly, sending half our traffic to <strong><code class="language-plaintext highlighter-rouge">v1</code></strong> and the other <strong>half</strong> to <strong><code class="language-plaintext highlighter-rouge">v2</code></strong>.</p>

<ul>
  <li>성공적인 다크 런칭 이후, 우리는 <strong>점진적</strong>으로 이전 버전에서 새 버전으로 사용자 트래픽을 옮기는 <strong>블루-그린 전략</strong>을 사용하는 기간을 원할 수 있습니다. 트래픽을 균등하게 분할하고 트래픽의 절반을 로 보내고 <code class="language-plaintext highlighter-rouge">v1</code>나머지 절반을 로 보내는 라우팅 정책으로 이를 살펴보겠습니다 <code class="language-plaintext highlighter-rouge">v2</code>.</li>
</ul>

<p>We will modify our <strong><code class="language-plaintext highlighter-rouge">HTTPRoute</code></strong> to accomplish this by removing the header-based routing rule that drove our dark launch. Then we will <strong>replace</strong> that with a <strong>50-50 <code class="language-plaintext highlighter-rouge">weight</code></strong> applied to each of the routes, as shown below:</p>

<p><img src="/assets/2024/kans-3th/w6/20241012_kans_w6_28.png" alt="img.png" /></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  rules:
    - matches:
      - path:
          <span class="nb">type</span>: PathPrefix
          value: /api/my-workload
      <span class="c"># Configure a 50-50 traffic split across v1 and v2 : 버전 1,2 50:50 비율</span>
      backendRefs:
        - name: my-workload-v1
          namespace: my-workload
          port: 8080
          weight: 50
        - name: my-workload-v2
          namespace: my-workload
          port: 8080
          weight: 50
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Apply this 50-50 routing policy with kubectl:</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> https://raw.githubusercontent.com/solo-io/gloo-gateway-use-cases/main/gateway-api-tutorial/09-workload-route-split.yaml
<span class="c"># =&gt; httproute.gateway.networking.k8s.io/my-workload configured</span>

<span class="c">#</span>
<span class="nv">$ </span>kubectl describe httproute <span class="nt">-n</span> my-workload
<span class="c"># =&gt; Spec:</span>
<span class="c">#      ...</span>
<span class="c">#      Rules:</span>
<span class="c">#        Backend Refs:</span>
<span class="c">#          Group:</span>
<span class="c">#          Kind:       Service</span>
<span class="c">#          Name:       my-workload-v1</span>
<span class="c">#          Namespace:  my-workload</span>
<span class="c">#          Port:       8080</span>
<span class="c">#          Weight:     50</span>
<span class="c">#          Group:</span>
<span class="c">#          Kind:       Service</span>
<span class="c">#          Name:       my-workload-v2</span>
<span class="c">#          Namespace:  my-workload</span>
<span class="c">#          Port:       8080</span>
<span class="c">#          Weight:     50</span>
<span class="c">#        Matches:</span>
<span class="c">#          Path:</span>
<span class="c">#            Type:   PathPrefix</span>
<span class="c">#            Value:  /api/my-workload</span>
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 반복 접속 후 대략 비률 확인</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nt">-H</span> <span class="s2">"Host: api.example.com"</span> http://localhost:8080/api/my-workload/ | <span class="nb">grep </span>body<span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
<span class="c"># =&gt;   51   &amp;quot;body&amp;quot;: &amp;quot;Hello From My Workload (v1)!&amp;quot;,</span>
<span class="c">#      49   &amp;quot;body&amp;quot;: &amp;quot;Hello From My Workload (v2)!&amp;quot;,</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..200<span class="o">}</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nt">-H</span> <span class="s2">"Host: api.example.com"</span> http://localhost:8080/api/my-workload/ | <span class="nb">grep </span>body<span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
<span class="c"># =&gt;  116   &amp;quot;body&amp;quot;: &amp;quot;Hello From My Workload (v1)!&amp;quot;,</span>
<span class="c">#      84   &amp;quot;body&amp;quot;: &amp;quot;Hello From My Workload (v2)!&amp;quot;,</span>
</code></pre></div></div>

<h5 id="debug">Debug</h5>

<p><strong>Solve a Problem with Glooctl CLI</strong></p>

<p>A common source of Gloo configuration <strong>errors</strong> is <strong>mistyping</strong> an upstream reference, perhaps when copy/pasting it from another source but “missing a spot” when changing the name of the backend service target. In this example, we’ll simulate making an error like that, and then demonstrating how <code class="language-plaintext highlighter-rouge">glooctl</code> can be used to detect it.</p>

<ul>
  <li>Gloo 구성 오류의 일반적인 원인은 <strong>업스트림 참조를 잘못 입력</strong>하는 것입니다. 아마도 다른 소스에서 복사/붙여넣을 때이지만 백엔드 서비스 대상의 이름을 변경할 때 “한 군데를 놓친” 것입니다. 이 예에서 우리는 그런 오류를 만드는 것을 시뮬레이션하고, <code class="language-plaintext highlighter-rouge">glooctl</code>그것을 감지하는 데 어떻게 사용할 수 있는지 보여줍니다.</li>
</ul>

<p><strong>First</strong>, let’s apply a change to simulate the <strong>mistyping</strong> of an upstream config so that it is targeting a <strong>non-existent <code class="language-plaintext highlighter-rouge">my-bad-workload-v2</code></strong> backend service, rather than the correct <strong><code class="language-plaintext highlighter-rouge">my-workload-v2</code></strong>.</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">my-bad-workload-v2</code> 업스트림 구성의 오타를 시뮬레이션하여 올바른 타겟팅하는 대신 존재하지 않는 백엔드 서비스를 타겟팅하도록 변경</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># [신규 터미널] 모니터링</span>
<span class="nv">$ </span>kubectl get httproute <span class="nt">-n</span> my-workload my-workload <span class="nt">-o</span> yaml <span class="nt">-w</span>

<span class="c">#</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> https://raw.githubusercontent.com/solo-io/gloo-gateway-use-cases/main/gateway-api-tutorial/10-workload-route-split-bad-dest.yaml
<span class="c"># =&gt; httproute.gateway.networking.k8s.io/my-workload configured</span>

<span class="c">#</span>
<span class="nv">$ </span>kubectl describe httproute <span class="nt">-n</span> my-workload
<span class="c"># =&gt; ...</span>
<span class="c">#    Spec:</span>
<span class="c">#      Rules:</span>
<span class="c">#        Backend Refs:</span>
<span class="c">#          Group:</span>
<span class="c">#          Kind:       Service</span>
<span class="c">#          Name:       my-workload-v1</span>
<span class="c">#          Namespace:  my-workload</span>
<span class="c">#          Port:       8080</span>
<span class="c">#          Weight:     50</span>
<span class="c">#          Group:</span>
<span class="c">#          Kind:       Service</span>
<span class="c">#          Name:       my-bad-workload-v2</span>
<span class="c">#          Namespace:  my-workload</span>
<span class="c">#          Port:       8080</span>
<span class="c">#          Weight:     50</span>
<span class="c">#        Matches:</span>
<span class="c">#          Path:</span>
<span class="c">#            Type:   PathPrefix</span>
<span class="c">#            Value:  /api/my-workload</span>
<span class="c">#    Status:</span>
<span class="c">#      Parents:</span>
<span class="c">#        Conditions:</span>
<span class="c">#          Last Transition Time:  2024-10-12T16:55:06Z</span>
<span class="c">#          Message:               Service &amp;quot;my-bad-workload-v2&amp;quot; not found</span>
<span class="c">#          Observed Generation:   4</span>
<span class="c">#          Reason:                BackendNotFound</span>
<span class="c">#          Status:                False</span>
<span class="c">#          Type:                  ResolvedRefs</span>
<span class="c">#          Last Transition Time:  2024-10-12T16:45:41Z</span>
<span class="c">#          Message:</span>
<span class="c">#          Observed Generation:   4</span>
<span class="c">#          Reason:                Accepted</span>
<span class="c">#          Status:                True</span>
<span class="c">#          Type:                  Accepted</span>
<span class="c">#        Controller Name:         solo.io/gloo-gateway</span>
<span class="c">#        Parent Ref:</span>
<span class="c">#          Group:      gateway.networking.k8s.io</span>
<span class="c">#          Kind:       Gateway</span>
<span class="c">#          Name:       http</span>
<span class="c">#          Namespace:  gloo-system</span>
<span class="c">#    Events:           &amp;lt;none&amp;gt;</span>
</code></pre></div></div>

<p>When we test this out, note that the 50-50 traffic split is still in place. This means that about half of the requests will be routed to <code class="language-plaintext highlighter-rouge">my-workload-v1</code> and succeed, while the others will attempt to use the non-existent <code class="language-plaintext highlighter-rouge">my-bad-workload-v2</code> and fail like this:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#</span>
<span class="nv">$ </span>curl <span class="nt">-is</span> <span class="nt">-H</span> <span class="s2">"Host: api.example.com"</span> http://localhost:8080/api/my-workload
<span class="c"># =&gt; HTTP/1.1 500 Internal Server Error</span>
<span class="nv">$ </span>curl <span class="nt">-is</span> <span class="nt">-H</span> <span class="s2">"Host: api.example.com"</span> http://localhost:8080/api/my-workload
<span class="c"># =&gt; HTTP/1.1 200 OK</span>
<span class="c">#    vary: Origin</span>
<span class="c">#    date: Sat, 12 Oct 2024 16:56:37 GMT</span>
<span class="c">#    content-length: 292</span>
<span class="c">#    content-type: text/plain; charset=utf-8</span>
<span class="c">#    x-envoy-upstream-service-time: 5</span>
<span class="c">#    server: envoy</span>
<span class="c">#    ...</span>

<span class="c"># </span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nt">-H</span> <span class="s2">"Host: api.example.com"</span> http://localhost:8080/api/my-workload/ | <span class="nb">grep </span>body<span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
<span class="c"># =&gt;   55   &amp;quot;body&amp;quot;: &amp;quot;Hello From My Workload (v1)!&amp;quot;,</span>

<span class="c"># &lt;span style="color: green;"&gt;👉 디버깅 테스트를 위해 일부러 50%의 워크로드에는 오타를 내어서&lt;/span&gt;</span>
<span class="c"># &lt;span style="color: green;"&gt;    50%의 요청은 v1로 라우팅되어 성공하고 나머지 50%는 실패합니다.&lt;/span&gt;</span>
</code></pre></div></div>

<p>So we’ll deploy one of the first weapons from the Gloo debugging arsenal, the <code class="language-plaintext highlighter-rouge">glooctl check</code> utility. It verifies a number of Gloo resources, confirming that they are configured correctly and are interconnected with other resources correctly. For example, in this case, <code class="language-plaintext highlighter-rouge">glooctl</code> will detect the error in the mis-connection between the <code class="language-plaintext highlighter-rouge">HTTPRoute</code> and its backend target:</p>

<ul>
  <li>gloo에서 제공하는 <code class="language-plaintext highlighter-rouge">glooctl check</code> 명령으로 구성 오류를 확인합니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane bash
<span class="c"># -----------------------------------</span>
<span class="nv">$ </span><span class="nb">export </span><span class="nv">PATH</span><span class="o">=</span><span class="nv">$HOME</span>/.gloo/bin:<span class="nv">$PATH</span>
<span class="nv">$ </span>glooctl check
<span class="c"># =&gt; ...</span>
<span class="c">#    Checking Gateways... OK</span>
<span class="c">#    Checking Proxies... 1 Errors!</span>
<span class="c">#    </span>
<span class="c">#    Detected Kubernetes Gateway integration!</span>
<span class="c">#    Checking Kubernetes GatewayClasses... OK</span>
<span class="c">#    Checking Kubernetes Gateways... OK</span>
<span class="c">#    Checking Kubernetes HTTPRoutes... 1 Errors!</span>
<span class="c">#    </span>
<span class="c">#    Skipping Gloo Instance check -- Gloo Federation not detected.</span>
<span class="c">#    Error: 2 errors occurred:</span>
<span class="c">#     * Found proxy with warnings by 'gloo-system': gloo-system gloo-system-http</span>
<span class="c">#    Reason: warning:</span>
<span class="c">#      Route Warning: InvalidDestinationWarning. Reason: invalid destination in weighted destination list: *v1.Upstream { blackhole_ns.kube-svc:blackhole-ns-blackhole-cluster-8080 } not found</span>
<span class="c">#    </span>
<span class="c">#     * HTTPRoute my-workload.my-workload.http status (ResolvedRefs) is not set to expected (True). Reason: BackendNotFound, Message: Service &amp;quot;my-bad-workload-v2&amp;quot; not found</span>

<span class="c"># 원인 관련 정보 확인</span>
<span class="nv">$ </span>kubectl get httproute my-workload <span class="nt">-n</span> my-workload <span class="nt">-o</span> yaml
<span class="c"># =&gt; ...</span>
<span class="c">#    status:</span>
<span class="c">#      parents:</span>
<span class="c">#      - conditions:</span>
<span class="c">#        - lastTransitionTime: &amp;quot;2024-10-12T16:55:06Z&amp;quot;</span>
<span class="c">#          message: Service &amp;quot;my-bad-workload-v2&amp;quot; not found</span>
<span class="c">#          observedGeneration: 4</span>
<span class="c">#          reason: BackendNotFound</span>
<span class="c">#          status: &amp;quot;False&amp;quot;</span>
<span class="c">#          type: ResolvedRefs</span>
<span class="c">#          ...</span>

<span class="c"># 정상 설정으로 해결 configuration is again clean.</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> https://raw.githubusercontent.com/solo-io/gloo-gateway-use-cases/main/gateway-api-tutorial/09-workload-route-split.yaml
<span class="c"># =&gt; httproute.gateway.networking.k8s.io/my-workload configured</span>
<span class="nv">$ </span>kubectl get httproute my-workload <span class="nt">-n</span> my-workload <span class="nt">-o</span> yaml

<span class="c">#</span>
<span class="nv">$ </span>glooctl check
<span class="c"># =&gt; Checking Deployments... OK</span>
<span class="c">#    Checking Pods... OK</span>
<span class="c">#    Checking Upstreams... OK</span>
<span class="c">#    Checking UpstreamGroups... OK</span>
<span class="c">#    Checking AuthConfigs... OK</span>
<span class="c">#    Checking RateLimitConfigs... OK</span>
<span class="c">#    Checking VirtualHostOptions... OK</span>
<span class="c">#    Checking RouteOptions... OK</span>
<span class="c">#    Checking Secrets... OK</span>
<span class="c">#    Checking VirtualServices... OK</span>
<span class="c">#    Checking Gateways... OK</span>
<span class="c">#    Checking Proxies... OK</span>
<span class="c">#    </span>
<span class="c">#    Detected Kubernetes Gateway integration!</span>
<span class="c">#    Checking Kubernetes GatewayClasses... OK</span>
<span class="c">#    Checking Kubernetes Gateways... OK</span>
<span class="c">#    Checking Kubernetes HTTPRoutes... OK</span>
<span class="c">#    </span>
<span class="c">#    Skipping Gloo Instance check -- Gloo Federation not detected.</span>
<span class="c">#    No problems detected.</span>
<span class="c"># &lt;span style="color: green;"&gt;👉 이제 문제가 없다고 합니다. 😀&lt;/span&gt;</span>
</code></pre></div></div>

<h5 id="observe">Observe</h5>

<p><strong>Explore Envoy Metrics</strong></p>

<p><strong>Envoy</strong> publishes a host of <strong>metrics</strong> that may be useful for observing system behavior. In our very modest kind cluster for this exercise, you can count over <strong>3,000 individual metrics</strong>! You can learn more about them in the Envoy documentation <a href="https://www.envoyproxy.io/docs/envoy/latest/configuration/upstream/cluster_manager/cluster_stats">here</a>.</p>

<p>For this 30-minute exercise, let’s take a quick look at a couple of the useful metrics that Envoy produces for every one of our backend targets.</p>

<p>First, we’ll <strong>port-forward</strong> the <strong>Envoy</strong> <strong>administrative</strong> <strong>port</strong> <strong>19000</strong> to our local workstation:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#</span>
<span class="nv">$ </span>kubectl <span class="nt">-n</span> gloo-system port-forward deployment/gloo-proxy-http 19000 &amp;

<span class="c"># 아래 관리 페이지에서 각각 메뉴 링크 클릭 확인</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"Envoy Proxy Admin - http://localhost:19000"</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"Envoy Proxy Admin - http://localhost:19000/stats/prometheus"</span>tZBli7jsXv<span class="s1">'XALZnaKnB2MSBvJNI
</span></code></pre></div></div>

<p>For this exercise, let’s view <strong>two</strong> of the relevant <strong>metrics</strong> from the first part of this exercise: one that counts the <strong>number</strong> of <strong>successful</strong> (HTTP 2xx) requests processed by our <code class="language-plaintext highlighter-rouge">httpbin</code> backend (or <strong><code class="language-plaintext highlighter-rouge">cluster</code></strong>, in Envoy terminology), and another that <strong>counts</strong> the number of requests <strong>returning</strong> server errors (HTTP <strong>5xx</strong>) from that same backend:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 2xx, 5xx 요청 확인</span>
<span class="nv">$ </span>curl <span class="nt">-s</span> http://localhost:19000/stats | <span class="nb">grep</span> <span class="nt">-E</span> <span class="s2">"(^cluster.kube-svc_httpbin-httpbin-8000_httpbin.upstream.*(2xx|5xx))"</span>
<span class="c"># =&gt; cluster.kube-svc_httpbin-httpbin-8000_httpbin.upstream_rq_2xx: 7</span>

<span class="c"># If we apply a curl request that forces a 500 failure from the httpbin backend, using the /status/500 endpoint, I’d expect the number of 2xx requests to remain the same, and the number of 5xx requests to increment by one:</span>
<span class="nv">$ </span>curl <span class="nt">-is</span> <span class="nt">-H</span> <span class="s2">"Host: api.example.com"</span> http://localhost:8080/api/httpbin/status/500
<span class="c"># =&gt; HTTP/1.1 500 Internal Server Error</span>
<span class="c">#    server: envoy</span>
<span class="c">#    date: Sat, 12 Oct 2024 17:02:53 GMT</span>
<span class="c">#    content-type: text/html; charset=utf-8</span>
<span class="c">#    access-control-allow-origin: *</span>
<span class="c">#    access-control-allow-credentials: true</span>
<span class="c">#    content-length: 0</span>
<span class="c">#    x-envoy-upstream-service-time: 38</span>

<span class="c"># 500에러를 발생시키자 500에러가 1개 증가하고 2xx는 변화가 없습니다.</span>
<span class="nv">$ </span>curl <span class="nt">-s</span> http://localhost:19000/stats | <span class="nb">grep</span> <span class="nt">-E</span> <span class="s2">"(^cluster.kube-svc_httpbin-httpbin-8000_httpbin.upstream.*(2xx|5xx))"</span>
<span class="c"># =&gt; cluster.kube-svc_httpbin-httpbin-8000_httpbin.upstream_rq_2xx: 7</span>
<span class="c">#    cluster.kube-svc_httpbin-httpbin-8000_httpbin.upstream_rq_5xx: 1</span>
</code></pre></div></div>

<h5 id="정리">정리</h5>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kind delete cluster <span class="nt">--name</span> myk8s
<span class="c"># =&gt; Deleted nodes: [&amp;quot;myk8s-control-plane&amp;quot;]</span>
</code></pre></div></div>

<h3 id="기타-gateway-api-구현체">기타 Gateway API 구현체</h3>

<ul>
  <li>
    <dl>
      <dt><strong><code class="language-plaintext highlighter-rouge">Cilium</code></strong></dt>
      <dd>Cilium은 CNI로 알려져있지만 Gateway API 역할도 지원합니다.</dd>
    </dl>
    <ul>
      <li><strong>(참고) [OnlineLab] Cilium Gateway API - <a href="https://isovalent.com/labs/cilium-gateway-api/">Link</a></strong></li>
      <li><strong>(참고) [OnlineLab] Advanced Gateway API Use Cases - <a href="https://isovalent.com/labs/cilium-gateway-api-advanced/">Link</a></strong></li>
    </ul>
  </li>
  <li>
    <dl>
      <dt><strong><code class="language-plaintext highlighter-rouge">Istio</code></strong></dt>
      <dd>Istio는 Service Mesh로 알려져있지만 Gateway API 역할도 지원합니다. Gateway API 자체가 Service Mesh인 Istio 등을 참조하였기에 어찌보면 당연한 일입니다.</dd>
    </dl>
    <ul>
      <li>Kubernetes Traffic Management: Combining Gateway API with Service Mesh for North-South and East-West Use Cases - <a href="https://medium.com/@disha.20.10/kubernetes-traffic-management-combining-gateway-api-with-service-mesh-for-north-south-and-63e39ad95dcc">Blog</a></li>
      <li>Istio Gateway API 활용하기 <a href="https://devops-james.tistory.com/317">https://devops-james.tistory.com/317</a></li>
    </ul>
  </li>
  <li><strong><code class="language-plaintext highlighter-rouge">Kong API Gateway</code></strong>
    <ul>
      <li>Kong API Gateway 를 Gateway API 형태 설치 <a href="https://mokpolar.tistory.com/68">https://mokpolar.tistory.com/68</a></li>
    </ul>
  </li>
  <li><strong><code class="language-plaintext highlighter-rouge">Envoy Gateway</code></strong>
    <ul>
      <li>Envoy Gateway 사용하여 + 부하분산 <a href="https://devops-james.tistory.com/320">https://devops-james.tistory.com/320</a></li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="마치며">마치며</h2>

<p>파드 통신에서 부터 CNI, 서비스(ClusterIP, NodePort, LoadBalancer)를 거쳐, ingress, gateway api까지 왔습니다.
나중에 배운 기술이 이전 기술을 필요없게 만드는 부분도 있지만, 기초의 중요성을 알기에 더욱 중요하다고 생각합니다.</p>

<p>그런데 gateway api를 만들면서 ingress를 frozen 하게 된것은 살짝 충격적입니다.
ingress를 없앤다는 얘기는 없지만 결국 gateway api가 더 좋은 기술이고, 
ingress는 점점 점유율을 잃다가 조용히 deprecated 될것 같은 느낌입니다.
ingress가 심심하지 않도록 더 자주 써줘야겠습니다.</p>

<p>이번주는 특히나 실습이 많았던것 같은데, 다른 분들도 다들 잘 생존했으면 좋겠습니다.
(일단 저부터 스터디에서 생존하기를 빕니다.. :smile:)</p>]]></content><author><name></name></author><category term="kans" /><category term="kubernetes," /><category term="network," /><category term="linux" /><summary type="html"><![CDATA[이번주에는 ingress와 gateway api 에대해 알아 보겠습니다.]]></summary></entry><entry><title type="html">[KANS 3기] LoadBalancer(MetalLB), IPVS</title><link href="https://sweetlittlebird.github.io/posts/2024-10-05-KANS-Study-Week5/" rel="alternate" type="text/html" title="[KANS 3기] LoadBalancer(MetalLB), IPVS" /><published>2024-10-05T01:00:18+09:00</published><updated>2024-10-05T01:00:18+09:00</updated><id>https://sweetlittlebird.github.io/posts/KANS%20Study%20-%20Week5</id><content type="html" xml:base="https://sweetlittlebird.github.io/posts/2024-10-05-KANS-Study-Week5/"><![CDATA[<h2 id="들어가며">들어가며</h2>

<p>이번 주에는 LoadBalancer 서비스와 MetalLB, 그리고 kube-proxy의 모드중 하나인 IPVS에 대해 알아보겠습니다.
KANS 3기 5주차 스터디를 시작하겠습니다.</p>

<hr />

<h2 id="loadbalancer-서비스">LoadBalancer 서비스</h2>

<h3 id="loadbalancer란">LoadBalancer란?</h3>

<ul>
  <li>LoadBalancer는 Kubernetes의 Service 유형의 하나로, 클러스터 외부에서 클러스터 내부의 서비스에 접근할 수 있도록 서비스를
노출시키는 역할을 합니다.</li>
  <li>Kubernetes에서는 자체적으로 LoadBalancer를 제공하지 않고, 클라우드 서비스 제공업체의 LoadBalancer(AWS의 ALB, NLB),
LoadBalancer 하드웨어 장비(Citrix, F5 networks), 또는 오픈소스 LoadBalancer (MetalLB 등)를 사용합니다.</li>
  <li>기본적으로 LoadBalancer를 사용하면 NodePort를 먼저 생성한 다음 LoadBalancer와 연결해야 하지만 (NodePort 접근 방식),
구성에 따라 NodePort 없이 바로 LoadBalancer를 생성할 수도 (Pod Direct 접근 방식) 있습니다.</li>
</ul>

<h3 id="환경별-loadbalancer">환경별 LoadBalancer</h3>

<p><img src="/assets/2024/kans-3th/w5/20241005_kans_w5_1.png" alt="환경별 LoadBalancer 비교" class="image-center" />
<em class="image-caption">환경별 LoadBalancer 비교</em></p>

<h4 id="클라우드-서비스-제공업체의-loadbalancer">클라우드 서비스 제공업체의 LoadBalancer</h4>

<ul>
  <li>클라우드 서비스 제공업체의 LoadBalancer는 클라우드 서비스 제공업체가 제공하는 서비스로, 클라우드 서비스 제공업체의
LoadBalancer를 사용하면 클라우드 서비스 제공업체의 LoadBalancer를 통해 클러스터 외부에서 클러스터 내부의 서비스에
접근할 수 있습니다.</li>
  <li>하지만 클라우드 서비스 제공업체마다 동작 방식과 기능이 다르기 때문에 각 클라우드 서비스 제공업체의 LoadBalancer를
사용할 때는 해당 클라우드 서비스 제공업체의 LoadBalancer의 동작 방식과 기능을 확인해야 합니다.</li>
  <li>대표적인 클라우드 서비스 제공업체인 Amazon Web Service는 다음의 LoadBalancer를 제공합니다.
    <ul>
      <li><strong>Classic Load Balancer (CLB)</strong> : 가장 오래된 로드밸런서로 NLB, ALB보다 기능이 적습니다.</li>
      <li><strong>Network Load Balancer (NLB)</strong> : Layer 4 계층의 네트워크 로드밸런서로 TCP/UDP/TLS 트래픽을 지원합니다. CLB/ALB에 비해서 처리속도가 빠릅니다.
(<strong>Application Load Balancer (ALB)</strong>는 Layer 7 계층의 애플리케이션 로드밸런서로 http/https/gRPC 트래픽을 지원합니다. ALB는 Ingress시 생성됩니다.)</li>
    </ul>
  </li>
</ul>

<h5 id="클라우드-서비스-제공업체의-loadbalancer-서비스-동작-방식">클라우드 서비스 제공업체의 LoadBalancer 서비스 동작 방식</h5>

<ol>
  <li>NodePort 접근 방식
<img src="/assets/2024/kans-3th/w5/20241005_kans_w5_3.png" alt="img.png" class="w-80 image-center" />
    <ul>
      <li>외부 클라이언트는 LoadBalancer의 IP 주소로 요청을 보내면 LoadBalancer는 요청을 받아서 노드들의 NodePort로 부하를 분산하여 전달합니다.</li>
      <li>이때 NodePort로 인입 후에 iptables를 통해 파드로 랜덤 부하분산을 통해 전달합니다.</li>
      <li>이 과정에서 DNAT를 통한 부하 분산과정이 두번 수행됩니다. (LoadBalancer에서 NodePort로 전달될때, 노드의 iptables 룰로 파드 IP로 전달될때)</li>
    </ul>
  </li>
  <li>Pod Direct 접근 방식
<img src="/assets/2024/kans-3th/w5/20241005_kans_w5_4.png" alt="img.png" class="w-80 image-center" />
    <ul>
      <li>LoadBalancer에서 파드의 IP로 직접 부하분산해서 전달합니다.</li>
      <li>LoadBalancer가 파드의 IP 정보를 알기 위해서, 별도의 LoadBalancer Controller를 구성하고 LoadBalancer Controller가 
LoadBalancer에게 파드의 IP를 동적으로 전달합니다.</li>
      <li>이 과정에서 부하 분산과정이 한번 수행되며 NodePort 방식 보다 효율 적입니다.</li>
    </ul>
  </li>
</ol>

<p><img src="/assets/2024/kans-3th/w5/20241005_kans_w5_2.png" alt="클라우드의 LoadBalancer 제공 방식 비교" class="image-center" />
<em class="image-caption">클라우드의 LoadBalancer 제공 방식 비교</em></p>

<h4 id="온프레미스-환경에서의-loadbalancer">온프레미스 환경에서의 LoadBalancer</h4>

<h5 id="하드웨어-장비-기반-loadbalancer-서비스-동작-방식">하드웨어 장비 기반 LoadBalancer 서비스 동작 방식</h5>

<p><img src="/assets/2024/kans-3th/w5/20241005_kans_w5_5.png" alt="img.png" class="w-80 image-center" /></p>
<ul>
  <li>하드웨어 장비 기반 LoadBalancer는 AWS LoadBalancer 서비스와 거의 동일하게 별도의 장비로 접속 후 노드에 NodePort 혹은 파드로 직접
전달하여 통신할 수 있습니다.</li>
  <li>대표적으로 Citrix, F5 Networks의 제품 등이 있습니다.</li>
  <li>예시) Citrix ADC for K8S - <a href="https://www.citrix.com/blogs/2019/09/16/citrix-adc-for-kubernetes-service-of-type-loadbalancer/">링크</a> &amp; Citrix ADC(Ingress/Service) with k8s - <a href="https://www.notion.so/e57b6056f1334c9094f444d1c183f378">링크</a>
<img src="/assets/2024/kans-3th/w5/20241005_kans_w5_17.png" alt="img.png" /></li>
</ul>

<h5 id="소프트웨어-기반-loadbalancer-서비스-동작-방식">소프트웨어 기반 LoadBalancer 서비스 동작 방식</h5>

<ul>
  <li>소프트웨어 기반 LoadBalancer는 별도의 네트워크 장비 없이 소프트웨어로 동작합니다.</li>
  <li>대표적으로 MetalLB, OpenELB, PubeLB, kube-vip, LoxiLB 등이 있습니다.</li>
  <li>MetalLB에 대해서는 좀 더 자세히 알아보겠습니다.</li>
</ul>

<h3 id="metallb">MetalLB</h3>

<ul>
  <li>MetalLB는 Bare<strong>MetalL</strong>oad<strong>B</strong>alancer의 약자로, 온프레미스 환경에서 사용할 수 있는 오픈소스 LoadBalancer입니다.</li>
  <li>쿠버네티스는 DaemonSet으로 Speaker 파드를 생성하여 External IP를 전파합니다. External IP는 노드의 IP 대신 외부에서 
접속할 수 있는 IP 입니다.</li>
  <li>이를 통해 노드의 IP를 외부에 노출하지 않을 수 있어서 보안성을 높일 수 있습니다.</li>
  <li>Speaker 파드는 External IP 전파를 위해 표준 프로토콜인 ARP(Address Resolution Protocol) 혹은 BGP(Border Gateway Protocol)를 사용합니다.</li>
  <li>MetalLB는 일부 퍼블릭 클라우드 플랫폼 환경에서 동작하지 않습니다. 이유는 가상서버 IP에 매칭되는 MAC 주소가 아닌 IP에 대한 ARP 요청을 차단하기 때문입니다.</li>
  <li>또한 일부 CNI에서의 동작에 이슈가 있습니다. Calico의 IPIP 모드에서 BGP 사용시 MetalLB의 BGP와 충돌이 생겨 문제가 발생하곤 합니다.</li>
  <li>실무에서 사용시에는 이슈나 제약사항을 확인하고, 사전 테스트 진행후 사용할 필요가 있습니다.</li>
</ul>

<h4 id="layer2-모드">Layer2 모드</h4>

<ul>
  <li>Layer2 모드는 ARP(Address Resolution Protocol)를 통해서 External IP를 전파합니다.</li>
  <li>ARP란?
<img src="/assets/2024/kans-3th/w5/20241005_kans_w5_6.png" alt="img.png" class="w-80 image-center" />
<em class="image-caption">ARP 동작 모식도 (<a href="https://velog.io/@louie/ARPAddress-Resolution-Protocol">출처</a>)</em>
    <ul>
      <li>동일 네트워크 내부에서 통신을 위해서는 상대방의 MAC(Media Access Control) 주소를 알아야 합니다.</li>
      <li>이때 IP 주소를 전송하면서 이 IP의 주인의 MAC 주소를 알려달라는 패킷을 보내면, 해당 IP 주소를 가진 호스트에서 자신의 MAC 주소를 응답합니다.</li>
      <li>이것이 ARP의 동작 방식이며, ARP 테이블에 IP와 MAC 주소를 저장하고, 이후 통신시 ARP 테이블을 참조하여 통신을 합니다.</li>
    </ul>
  </li>
  <li>ARP에 대해서 알아보았으니 Layer2 동작에 대해 다시 알아보겠습니다.
<img src="/assets/2024/kans-3th/w5/20241005_kans_w5_7.png" alt="img.png" class="image-center" />
<em class="image-caption">MetalLB Layer2 동작 (출처: 추가예정)</em></li>
  <li>위의 그림에서 호스트 NS/파드 NS의 NS는 네임스페이스를 의미하며, 여기서의 네임스페이스는 첫주차 컨테이너 격리에서 배웠던
Linux OS 차원의 네임스페이스를 의미합니다.</li>
  <li>흐름을 파악해보면 아래와 같습니다.
    <ol>
      <li>LoadBalancer 서비스 리소스 생성시 MetalLB 스피커 파드중에 리더(Leader) 스피커 파드가 선택됩니다. 리더 스피커 파드는
해당 LoadBalancer 서비스의 External IP를 가지고 ARP 응답을 합니다. 또한 GARP(Gratuitous ARP)를 통해 네트워크 내의 모든 호스트에게
해당 External IP의 MAC 주소를 전파합니다.
        <ul>
          <li>데몬셋으로 배포된 speaker 파드는 <code class="language-plaintext highlighter-rouge">NetworkMode: host</code>로 호스트 네임스페이스를 공유하며, 호스트 네임스페이스에서 ARP 응답을 합니다.</li>
          <li>만약 리더 스피커 파드에 장애가 발생하면, 다른 스피커 파드가 리더 스피커 파드로 선출됩니다.
            <ul>
              <li>멤버 리스터 및 자애 발견은 hashicorp의 memberlist를 사용합니다.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>클라이언트1이 SVC1의 External IP로 접속을 시도하면, 해당 트래픽은 SVC1의 External IP 정보를 전파하는 리더 스피커파드가 
있는 노드1으로 전달됩니다. 또한 클라이언트2는 SVC2의 External IP로 접속을 시도하면, 해당 트래픽은 SVC2의 External IP 정보를 전파하는
리더 스피커파드가 있는 노드3로 전달됩니다.</li>
      <li>노드에 도착한 트래픽은 해당 노드의 iptables를 통해 ClusterIP와 동일하게 해당 서비스에 연동된 엔드포인트 파드들로
(4) 랜덤 부하분산 되어 전달됩니다.</li>
    </ol>
  </li>
  <li>Layer2 모드의 단점
    <ul>
      <li>single-node bottlenecking : 리더 스피커 파드가 있는 노드에만 트래픽이 인입되어 부하가 집중 됩니다.</li>
      <li>potentially slow failover : 리더 스피커 파드에 장애가 발생하면, 나머지 노드 리더가 선출되고, ARP 전파 및 갱신 완료전까지는
장애가 발생됩니다. (대략 10초~20초 소요)</li>
    </ul>
  </li>
</ul>

<h4 id="bgp-모드">BGP 모드</h4>

<p><img src="/assets/2024/kans-3th/w5/20241005_kans_w5_8.png" alt="img.png" /></p>

<ul>
  <li>BGP 모드는 Routing 프로토콜인 BGP(Border Gateway Protocol)를 통해서 External IP를 전파합니다.
    <ul>
      <li>기본은 IP주소(32bit)를 전파하며, 설정으로 축약된 네트워크 정보를 전파할 수 있습니다. (bgp-advertisements에 aggregation-length 설정)</li>
      <li>BGP 커뮤니티, localpref 등 다양한 BGP 속성을 사용할 수 있습니다.</li>
      <li>IP 주소의 마지막이 0과 255로 끝나는 IP를 처리 못하는 라우터 장비가 있는 경우 <code class="language-plaintext highlighter-rouge">avoid-buggy-ips: true</code> 설정을 통해 IP가 0과 255로 끝나는 IP를 사용하지 않도록 설정할 수 있습니다.</li>
    </ul>
  </li>
  <li>외부에서 라우터를 통해 ECMP(Equal Cost Multi Path) 라우팅을 통해 부하 분산을 지원합니다.
    <ul>
      <li>일반적으로 ECMP는 5-tuple(프로토콜, 출발지 IP, 목적지 IP, 출발지 포트, 목적지 포트)을 기반으로 동작합니다.</li>
      <li>라우터 장비에 따라 다양한 라우팅(분산) 처리가 가능합니다.</li>
    </ul>
  </li>
  <li>BGP 모드의 제한사항
    <ul>
      <li>라우터에서 서비스로 인입이 되기 때문에, 라우터 설정이 중요하며 네트워크 팀과 협업이 권장됩니다.</li>
      <li>Speaker 노드 파드 장애시 BGP Timer 설정 등, 구성하고 있는 네트워크 환경에 맞게 최적화 작업이 필요합니다.</li>
      <li>ECMP 부하 분산 접속시 특정 파드에 부하가 집중되거나, 세션 고정, flapping 등 다양한 환경에 대응이 필요합니다.</li>
      <li>BGP 라우팅 설정 및 라우팅 전파 관련 최적화 설정이 필요합니다.</li>
    </ul>
  </li>
</ul>

<h3 id="metallb-실습">MetalLB 실습</h3>

<h4 id="실습환경-준비">실습환경 준비</h4>

<ul>
  <li>이번에도 KIND를 통해 실습을 진행해보겠습니다.</li>
</ul>

<h5 id="kind-클러스터-구성">KIND 클러스터 구성</h5>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># kind 클러스터 설정 파일 작성</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOT</span><span class="sh">&gt; kind-svc-2w.yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
featureGates:
  "InPlacePodVerticalScaling": true  #실행 중인 파드의 리소스 요청 및 제한을 변경할 수 있게 합니다.
  "MultiCIDRServiceAllocator": true  #서비스에 대해 여러 CIDR 블록을 사용할 수 있게 합니다.
nodes:
- role: control-plane
  labels:
    mynode: control-plane
    topology.kubernetes.io/zone: ap-northeast-2a
  extraPortMappings:  #컨테이너 포트를 호스트 포트에 매핑하여 클러스터 외부에서 서비스에 접근할 수 있도록 합니다.
  - containerPort: 30000
    hostPort: 30000
  - containerPort: 30001
    hostPort: 30001
  - containerPort: 30002
    hostPort: 30002
  - containerPort: 30003
    hostPort: 30003
  - containerPort: 30004
    hostPort: 30004
  kubeadmConfigPatches:
  - |
    kind: ClusterConfiguration
    apiServer:
      extraArgs:  #API 서버에 추가 인수를 제공
        runtime-config: api/all=true  #모든 API 버전을 활성화
    controllerManager:
      extraArgs:
        bind-address: 0.0.0.0
    etcd:
      local:
        extraArgs:
          listen-metrics-urls: http://0.0.0.0:2381
    scheduler:
      extraArgs:
        bind-address: 0.0.0.0
  - |
    kind: KubeProxyConfiguration
    metricsBindAddress: 0.0.0.0
- role: worker
  labels:
    mynode: worker1
    topology.kubernetes.io/zone: ap-northeast-2a
- role: worker
  labels:
    mynode: worker2
    topology.kubernetes.io/zone: ap-northeast-2b
- role: worker
  labels:
    mynode: worker3
    topology.kubernetes.io/zone: ap-northeast-2c
networking:
  podSubnet: 10.10.0.0/16  #파드 IP를 위한 CIDR 범위를 정의합니다. 파드는 이 범위에서 IP를 할당받습니다.
  serviceSubnet: 10.200.1.0/24  #서비스 IP를 위한 CIDR 범위를 정의합니다. 서비스는 이 범위에서 IP를 할당받습니다.
</span><span class="no">EOT

</span><span class="c"># k8s 클러스터 설치</span>
<span class="nv">$ </span>kind create cluster <span class="nt">--config</span> kind-svc-2w.yaml <span class="nt">--name</span> myk8s <span class="nt">--image</span> kindest/node:v1.31.0
<span class="nv">$ </span>docker ps
<span class="c"># =&gt; CONTAINER ID   IMAGE                  COMMAND                  CREATED          STATUS          PORTS                                                             NAMES</span>
<span class="c">#    83661e652fb1   kindest/node:v1.31.0   &amp;quot;/usr/local/bin/entr…&amp;quot;   39 seconds ago   Up 34 seconds   0.0.0.0:30000-30004-&amp;gt;30000-30004/tcp, 127.0.0.1:59215-&amp;gt;6443/tcp   myk8s-control-plane</span>
<span class="c">#    242777ad8f3c   kindest/node:v1.31.0   &amp;quot;/usr/local/bin/entr…&amp;quot;   39 seconds ago   Up 34 seconds                                                                     myk8s-worker</span>
<span class="c">#    f8022585c864   kindest/node:v1.31.0   &amp;quot;/usr/local/bin/entr…&amp;quot;   39 seconds ago   Up 34 seconds                                                                     myk8s-worker2</span>
<span class="c">#    80988133cdfc   kindest/node:v1.31.0   &amp;quot;/usr/local/bin/entr…&amp;quot;   39 seconds ago   Up 34 seconds                                                                     myk8s-worker3</span>

<span class="c"># 노드에 기본 툴 설치</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane sh <span class="nt">-c</span> <span class="s1">'apt update &amp;&amp; apt install tree psmisc lsof wget bsdmainutils bridge-utils net-tools dnsutils ipset ipvsadm nfacct tcpdump ngrep iputils-ping arping git vim arp-scan -y'</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> sh <span class="nt">-c</span> <span class="s1">'apt update &amp;&amp; apt install tree psmisc lsof wget bsdmainutils bridge-utils net-tools dnsutils ipset ipvsadm nfacct tcpdump ngrep iputils-ping arping -y'</span><span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>

<span class="c"># k8s v1.31.0 버전 확인</span>
<span class="nv">$ </span>kubectl get node
<span class="c"># =&gt; NAME                  STATUS   ROLES           AGE    VERSION</span>
<span class="c">#    myk8s-control-plane   Ready    control-plane   110s   v1.31.0</span>
<span class="c">#    myk8s-worker          Ready    &amp;lt;none&amp;gt;          100s   v1.31.0</span>
<span class="c">#    myk8s-worker2         Ready    &amp;lt;none&amp;gt;          100s   v1.31.0</span>
<span class="c">#    myk8s-worker3         Ready    &amp;lt;none&amp;gt;          100s   v1.31.0</span>

<span class="c"># 노드 labels 확인</span>
<span class="nv">$ </span>kubectl get nodes <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.items[*].metadata.labels}"</span> | jq
<span class="c"># =&gt; {</span>
<span class="c">#      &amp;quot;kubernetes.io/hostname&amp;quot;: &amp;quot;myk8s-control-plane&amp;quot;,</span>
<span class="c">#      &amp;quot;mynode&amp;quot;: &amp;quot;control-plane&amp;quot;,</span>
<span class="c">#      ...</span>
<span class="c">#    }</span>
<span class="c">#    {</span>
<span class="c">#      &amp;quot;kubernetes.io/hostname&amp;quot;: &amp;quot;myk8s-worker&amp;quot;,</span>
<span class="c">#      &amp;quot;mynode&amp;quot;: &amp;quot;worker1&amp;quot;,</span>
<span class="c">#      ...</span>
<span class="c">#    }</span>
<span class="c">#    {</span>
<span class="c">#      &amp;quot;kubernetes.io/hostname&amp;quot;: &amp;quot;myk8s-worker2&amp;quot;,</span>
<span class="c">#      &amp;quot;mynode&amp;quot;: &amp;quot;worker2&amp;quot;,</span>
<span class="c">#      ...</span>
<span class="c">#    }</span>
<span class="c">#    {</span>
<span class="c">#      &amp;quot;kubernetes.io/hostname&amp;quot;: &amp;quot;myk8s-worker3&amp;quot;,</span>
<span class="c">#      &amp;quot;mynode&amp;quot;: &amp;quot;worker3&amp;quot;,</span>
<span class="c">#      ...</span>
<span class="c">#    }</span>

<span class="c"># kind network 중 컨테이너(노드) IP(대역) 확인</span>
<span class="nv">$ </span>docker ps <span class="nt">-q</span> | xargs docker inspect <span class="nt">--format</span> <span class="s1">' '</span>
<span class="c"># =&gt; /myk8s-control-plane 172.20.0.5</span>
<span class="c">#    /myk8s-worker 172.20.0.4</span>
<span class="c">#    /myk8s-worker2 172.20.0.2</span>
<span class="c">#    /myk8s-worker3 172.20.0.3</span>

<span class="c"># 파드CIDR 과 Service 대역 확인 : CNI는 kindnet 사용</span>
<span class="nv">$ </span>kubectl get cm <span class="nt">-n</span> kube-system kubeadm-config <span class="nt">-oyaml</span> | <span class="nb">grep</span> <span class="nt">-i</span> subnet
<span class="c"># =&gt; podSubnet: 10.10.0.0/16</span>
<span class="c">#    serviceSubnet: 10.200.1.0/24</span>
<span class="nv">$ </span>kubectl cluster-info dump | <span class="nb">grep</span> <span class="nt">-m</span> 2 <span class="nt">-E</span> <span class="s2">"cluster-cidr|service-cluster-ip-range"</span>
<span class="c"># =&gt; &amp;quot;--service-cluster-ip-range=10.200.1.0/24&amp;quot;,</span>
<span class="c">#    &amp;quot;--cluster-cidr=10.10.0.0/16&amp;quot;,</span>

<span class="c"># MultiCIDRServiceAllocator : https://kubernetes.io/docs/tasks/network/extend-service-ip-ranges/</span>
<span class="nv">$ </span>kubectl get servicecidr
<span class="c"># =&gt; NAME         CIDRS           AGE</span>
<span class="c">#    kubernetes   10.200.1.0/24   4m59s</span>

<span class="c"># 노드마다 할당된 dedicated subnet (podCIDR) 확인</span>
<span class="nv">$ </span>kubectl get nodes <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.items[*].spec.podCIDR}"</span>
<span class="c"># =&gt; 10.10.0.0/24 10.10.3.0/24 10.10.2.0/24 10.10.1.0/24</span>

<span class="c"># kube-proxy configmap 확인</span>
<span class="nv">$ </span>kubectl describe cm <span class="nt">-n</span> kube-system kube-proxy
<span class="c"># =&gt; ...</span>
<span class="c">#    mode: iptables</span>
<span class="c">#    iptables:</span>
<span class="c">#      localhostNodePorts: null</span>
<span class="c">#      masqueradeAll: false</span>
<span class="c">#      masqueradeBit: null</span>
<span class="c">#      minSyncPeriod: 1s</span>
<span class="c">#      syncPeriod: 0s</span>
<span class="c">#    ...</span>

<span class="c"># 노드 별 네트워트 정보 확인 : CNI는 kindnet 사용</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> <span class="nb">cat</span> /etc/cni/net.d/10-kindnet.conflist<span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; &amp;gt;&amp;gt; node myk8s-control-plane &amp;lt;&amp;lt;</span>
<span class="c">#    {</span>
<span class="c">#     &amp;quot;cniVersion&amp;quot;: &amp;quot;0.3.1&amp;quot;,</span>
<span class="c">#     &amp;quot;name&amp;quot;: &amp;quot;kindnet&amp;quot;,</span>
<span class="c">#     &amp;quot;plugins&amp;quot;: [</span>
<span class="c">#     {</span>
<span class="c">#       &amp;quot;type&amp;quot;: &amp;quot;ptp&amp;quot;,</span>
<span class="c">#       &amp;quot;ipMasq&amp;quot;: false,</span>
<span class="c">#       &amp;quot;ipam&amp;quot;: {</span>
<span class="c">#         &amp;quot;type&amp;quot;: &amp;quot;host-local&amp;quot;,</span>
<span class="c">#         &amp;quot;dataDir&amp;quot;: &amp;quot;/run/cni-ipam-state&amp;quot;,</span>
<span class="c">#         &amp;quot;routes&amp;quot;: [</span>
<span class="c">#           { &amp;quot;dst&amp;quot;: &amp;quot;0.0.0.0/0&amp;quot; }</span>
<span class="c">#         ],</span>
<span class="c">#         &amp;quot;ranges&amp;quot;: [</span>
<span class="c">#           [ { &amp;quot;subnet&amp;quot;: &amp;quot;10.10.0.0/24&amp;quot; } ]</span>
<span class="c">#         ]</span>
<span class="c">#       },</span>
<span class="c">#       &amp;quot;mtu&amp;quot;: 1500</span>
<span class="c">#     },</span>
<span class="c">#     ...</span>
<span class="c">#     ]</span>
<span class="c">#    }</span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; node myk8s-worker &amp;lt;&amp;lt;</span>
<span class="c">#    ...</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> ip <span class="nt">-c</span> route<span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; &amp;gt;&amp;gt; node myk8s-control-plane &amp;lt;&amp;lt;</span>
<span class="c">#    default via &lt;span style="color:purple;"&gt;172.20.0.1 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:purple;"&gt;10.10.0.2 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;veth545bb56e &lt;/span&gt;scope host </span>
<span class="c">#    &lt;span style="color:purple;"&gt;10.10.0.3 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;veth184fcd53 &lt;/span&gt;scope host </span>
<span class="c">#    &lt;span style="color:purple;"&gt;10.10.0.4 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;vethc5dfe430 &lt;/span&gt;scope host </span>
<span class="c">#    &lt;span style="color:purple;"&gt;10.10.1.0/24 &lt;/span&gt;via &lt;span style="color:purple;"&gt;172.20.0.3 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:purple;"&gt;10.10.2.0/24 &lt;/span&gt;via &lt;span style="color:purple;"&gt;172.20.0.2 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:purple;"&gt;10.10.3.0/24 &lt;/span&gt;via &lt;span style="color:purple;"&gt;172.20.0.4 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.0.0/16 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;proto kernel scope link src &lt;span style="color:purple;"&gt;172.20.0.5 &lt;/span&gt;</span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; node myk8s-worker &amp;lt;&amp;lt;</span>
<span class="c">#    default via &lt;span style="color:purple;"&gt;172.20.0.1 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:purple;"&gt;10.10.0.0/24 &lt;/span&gt;via &lt;span style="color:purple;"&gt;172.20.0.5 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:purple;"&gt;10.10.1.0/24 &lt;/span&gt;via &lt;span style="color:purple;"&gt;172.20.0.3 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:purple;"&gt;10.10.2.0/24 &lt;/span&gt;via &lt;span style="color:purple;"&gt;172.20.0.2 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.0.0/16 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;proto kernel scope link src &lt;span style="color:purple;"&gt;172.20.0.4 &lt;/span&gt;</span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; node myk8s-worker2 &amp;lt;&amp;lt;</span>
<span class="c">#    default via &lt;span style="color:purple;"&gt;172.20.0.1 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:purple;"&gt;10.10.0.0/24 &lt;/span&gt;via &lt;span style="color:purple;"&gt;172.20.0.5 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:purple;"&gt;10.10.1.0/24 &lt;/span&gt;via &lt;span style="color:purple;"&gt;172.20.0.3 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:purple;"&gt;10.10.3.0/24 &lt;/span&gt;via &lt;span style="color:purple;"&gt;172.20.0.4 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.0.0/16 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;proto kernel scope link src &lt;span style="color:purple;"&gt;172.20.0.2 &lt;/span&gt;</span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; node myk8s-worker3 &amp;lt;&amp;lt;</span>
<span class="c">#    default via &lt;span style="color:purple;"&gt;172.20.0.1 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:purple;"&gt;10.10.0.0/24 &lt;/span&gt;via &lt;span style="color:purple;"&gt;172.20.0.5 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:purple;"&gt;10.10.2.0/24 &lt;/span&gt;via &lt;span style="color:purple;"&gt;172.20.0.2 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:purple;"&gt;10.10.3.0/24 &lt;/span&gt;via &lt;span style="color:purple;"&gt;172.20.0.4 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.0.0/16 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;proto kernel scope link src &lt;span style="color:purple;"&gt;172.20.0.3 &lt;/span&gt;</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> ip <span class="nt">-c</span> addr<span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; &amp;gt;&amp;gt; node myk8s-control-plane &amp;lt;&amp;lt;</span>
<span class="c">#    1: &lt;span style="color:teal;"&gt;lo: &lt;/span&gt;&amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</span>
<span class="c">#        link/loopback &lt;span style="color:olive;"&gt;00:00:00:00:00:00&lt;/span&gt; brd &lt;span style="color:olive;"&gt;00:00:00:00:00:00&lt;/span&gt;</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;127.0.0.1&lt;/span&gt;/8 scope host lo</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    2: &lt;span style="color:teal;"&gt;tunl0@NONE: &lt;/span&gt;&amp;lt;NOARP&amp;gt; mtu 1480 qdisc noop state &lt;span style="color:red;"&gt;DOWN &lt;/span&gt;group default qlen 1000</span>
<span class="c">#        link/ipip &lt;span style="color:olive;"&gt;0.0.0.0&lt;/span&gt; brd &lt;span style="color:olive;"&gt;0.0.0.0&lt;/span&gt;</span>
<span class="c">#    4: &lt;span style="color:teal;"&gt;veth545bb56e@if4: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state &lt;span style="color:green;"&gt;UP &lt;/span&gt;group default </span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;0e:8b:3c:4f:43:43&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt; link-netns cni-98b7b37a-bb7a-ea56-47c9-ce3a0b1fb08a</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;10.10.0.1&lt;/span&gt;/32 scope global veth545bb56e</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    5: &lt;span style="color:teal;"&gt;vethc5dfe430@if4: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state &lt;span style="color:green;"&gt;UP &lt;/span&gt;group default </span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;8a:70:dd:42:02:96&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt; link-netns cni-79ddddfd-6177-bbd6-5fdc-3f7f6bf07fdc</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;10.10.0.1&lt;/span&gt;/32 scope global vethc5dfe430</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    6: &lt;span style="color:teal;"&gt;veth184fcd53@if4: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state &lt;span style="color:green;"&gt;UP &lt;/span&gt;group default </span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;8a:92:74:11:f9:d9&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt; link-netns cni-5e4ecb7e-2120-f372-76cc-9a467c85159b</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;10.10.0.1&lt;/span&gt;/32 scope global veth184fcd53</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    28: &lt;span style="color:teal;"&gt;eth0@if29: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state &lt;span style="color:green;"&gt;UP &lt;/span&gt;group default </span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;02:42:ac:14:00:05&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt; link-netnsid 0</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;172.20.0.5&lt;/span&gt;/16 brd &lt;span style="color:purple;"&gt;172.20.255.255 &lt;/span&gt;scope global eth0</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; node myk8s-worker &amp;lt;&amp;lt;</span>
<span class="c">#    1: &lt;span style="color:teal;"&gt;lo: &lt;/span&gt;&amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</span>
<span class="c">#        link/loopback &lt;span style="color:olive;"&gt;00:00:00:00:00:00&lt;/span&gt; brd &lt;span style="color:olive;"&gt;00:00:00:00:00:00&lt;/span&gt;</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;127.0.0.1&lt;/span&gt;/8 scope host lo</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    2: &lt;span style="color:teal;"&gt;tunl0@NONE: &lt;/span&gt;&amp;lt;NOARP&amp;gt; mtu 1480 qdisc noop state &lt;span style="color:red;"&gt;DOWN &lt;/span&gt;group default qlen 1000</span>
<span class="c">#        link/ipip &lt;span style="color:olive;"&gt;0.0.0.0&lt;/span&gt; brd &lt;span style="color:olive;"&gt;0.0.0.0&lt;/span&gt;</span>
<span class="c">#    26: &lt;span style="color:teal;"&gt;eth0@if27: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state &lt;span style="color:green;"&gt;UP &lt;/span&gt;group default </span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;02:42:ac:14:00:04&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt; link-netnsid 0</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;172.20.0.4&lt;/span&gt;/16 brd &lt;span style="color:purple;"&gt;172.20.255.255 &lt;/span&gt;scope global eth0</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    ...</span>

<span class="c"># iptables 정보 확인</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>filter nat mangle raw <span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; IPTables Type : </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane  iptables <span class="nt">-t</span> <span class="nv">$i</span> <span class="nt">-S</span> <span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>filter nat mangle raw <span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; IPTables Type : </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker  iptables <span class="nt">-t</span> <span class="nv">$i</span> <span class="nt">-S</span> <span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>filter nat mangle raw <span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; IPTables Type : </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker2 iptables <span class="nt">-t</span> <span class="nv">$i</span> <span class="nt">-S</span> <span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>filter nat mangle raw <span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; IPTables Type : </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker3 iptables <span class="nt">-t</span> <span class="nv">$i</span> <span class="nt">-S</span> <span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>

<span class="c"># 각 노드 bash 접속</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane bash
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker bash
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker2 bash
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker3 bash
<span class="c"># ----------------------------------------</span>
<span class="nv">$ </span><span class="nb">exit</span>
<span class="c"># ----------------------------------------</span>

<span class="c"># kind 설치 시 kind 이름의 도커 브리지가 생성된다 : 172.20.0.0/16 대역</span>
<span class="nv">$ </span>docker network <span class="nb">ls</span>
<span class="c"># =&gt; NETWORK ID     NAME                     DRIVER    SCOPE</span>
<span class="c">#    a8d530305515   bridge                   bridge    local</span>
<span class="c">#    8204a0851463   host                     host      local</span>
<span class="c">#    3bbcc6aa8f38   kind                     bridge    local</span>

<span class="nv">$ </span>docker inspect kind
<span class="c"># =&gt; [</span>
<span class="c">#        {</span>
<span class="c">#            &amp;quot;Name&amp;quot;: &amp;quot;kind&amp;quot;,</span>
<span class="c">#            &amp;quot;Id&amp;quot;: &amp;quot;3bbcc6aa8f388f86f02478f41de1e4dd917e5812b6cf6257972e4af0bedf5021&amp;quot;,</span>
<span class="c">#            &amp;quot;Created&amp;quot;: &amp;quot;2020-01-01T11:37:09.195259833Z&amp;quot;,</span>
<span class="c">#            &amp;quot;Scope&amp;quot;: &amp;quot;local&amp;quot;,</span>
<span class="c">#            &amp;quot;Driver&amp;quot;: &amp;quot;bridge&amp;quot;,</span>
<span class="c">#            &amp;quot;IPAM&amp;quot;: {</span>
<span class="c">#                &amp;quot;Driver&amp;quot;: &amp;quot;default&amp;quot;,</span>
<span class="c">#                &amp;quot;Options&amp;quot;: {},</span>
<span class="c">#                &amp;quot;Config&amp;quot;: [</span>
<span class="c">#                    {</span>
<span class="c">#                        &amp;quot;Subnet&amp;quot;: &amp;quot;172.20.0.0/16&amp;quot;,</span>
<span class="c">#                        &amp;quot;Gateway&amp;quot;: &amp;quot;172.20.0.1&amp;quot;</span>
<span class="c">#                    }</span>
<span class="c">#                ]</span>
<span class="c">#            },</span>
<span class="c">#            &amp;quot;Internal&amp;quot;: false,</span>
<span class="c">#            &amp;quot;Attachable&amp;quot;: false,</span>
<span class="c">#            &amp;quot;Ingress&amp;quot;: false,</span>
<span class="c">#            &amp;quot;ConfigFrom&amp;quot;: {</span>
<span class="c">#                &amp;quot;Network&amp;quot;: &amp;quot;&amp;quot;</span>
<span class="c">#            },</span>
<span class="c">#            &amp;quot;ConfigOnly&amp;quot;: false,</span>
<span class="c">#            &amp;quot;Containers&amp;quot;: {</span>
<span class="c">#                &amp;quot;242777ad8f3c7009963155c3d7c4551e1407570d6986d9ef6346e6d33990e538&amp;quot;: {</span>
<span class="c">#                    &amp;quot;Name&amp;quot;: &amp;quot;myk8s-worker&amp;quot;,</span>
<span class="c">#                    &amp;quot;EndpointID&amp;quot;: &amp;quot;f6fb304fa38125ed1075d9c71b83559cff5066e71630c272e94311258021144e&amp;quot;,</span>
<span class="c">#                    &amp;quot;MacAddress&amp;quot;: &amp;quot;02:42:ac:14:00:04&amp;quot;,</span>
<span class="c">#                    &amp;quot;IPv4Address&amp;quot;: &amp;quot;172.20.0.4/16&amp;quot;,</span>
<span class="c">#                },</span>
<span class="c">#                &amp;quot;80988133cdfcfaafe520b35cec924b9fa87f26ea474102b833e92d7ca693fb2b&amp;quot;: {</span>
<span class="c">#                    &amp;quot;Name&amp;quot;: &amp;quot;myk8s-worker3&amp;quot;,</span>
<span class="c">#                    &amp;quot;EndpointID&amp;quot;: &amp;quot;42aec973b496fdc7b8ede07c11fd94fe35631216d3ecd54d2ab794849b834787&amp;quot;,</span>
<span class="c">#                    &amp;quot;MacAddress&amp;quot;: &amp;quot;02:42:ac:14:00:03&amp;quot;,</span>
<span class="c">#                    &amp;quot;IPv4Address&amp;quot;: &amp;quot;172.20.0.3/16&amp;quot;,</span>
<span class="c">#                },</span>
<span class="c">#                &amp;quot;83661e652fb1d34542b760209f670f330e25b1c51c8c0404e69d47eb9c79f407&amp;quot;: {</span>
<span class="c">#                    &amp;quot;Name&amp;quot;: &amp;quot;myk8s-control-plane&amp;quot;,</span>
<span class="c">#                    &amp;quot;EndpointID&amp;quot;: &amp;quot;d1e1efb2d90b7d8e9ce16b6274a62e7799d923681739dc8826c36c8b122d09c0&amp;quot;,</span>
<span class="c">#                    &amp;quot;MacAddress&amp;quot;: &amp;quot;02:42:ac:14:00:05&amp;quot;,</span>
<span class="c">#                    &amp;quot;IPv4Address&amp;quot;: &amp;quot;172.20.0.5/16&amp;quot;,</span>
<span class="c">#                },</span>
<span class="c">#                &amp;quot;f8022585c864bd53b31b84e22e2b4381da6c5b7a2ada1583f18136e7f8c6b3b9&amp;quot;: {</span>
<span class="c">#                    &amp;quot;Name&amp;quot;: &amp;quot;myk8s-worker2&amp;quot;,</span>
<span class="c">#                    &amp;quot;EndpointID&amp;quot;: &amp;quot;0866892fb0c2c1c8c2021d92a665a130a20aae5b76fbdc1549138da642a60883&amp;quot;,</span>
<span class="c">#                    &amp;quot;MacAddress&amp;quot;: &amp;quot;02:42:ac:14:00:02&amp;quot;,</span>
<span class="c">#                    &amp;quot;IPv4Address&amp;quot;: &amp;quot;172.20.0.2/16&amp;quot;,</span>
<span class="c">#                }</span>
<span class="c">#            },</span>
<span class="c">#            &amp;quot;Options&amp;quot;: {</span>
<span class="c">#                &amp;quot;com.docker.network.bridge.enable_ip_masquerade&amp;quot;: &amp;quot;true&amp;quot;,</span>
<span class="c">#                &amp;quot;com.docker.network.driver.mtu&amp;quot;: &amp;quot;1500&amp;quot;</span>
<span class="c">#            },</span>
<span class="c">#            &amp;quot;Labels&amp;quot;: {}</span>
<span class="c">#        }</span>
<span class="c">#    ]</span>

<span class="c"># arp scan 해두기</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane arp-scan <span class="nt">--interfac</span><span class="o">=</span>eth0 <span class="nt">--localnet</span>
<span class="c"># =&gt; Interface: eth0, type: EN10MB, MAC: 02:42:ac:14:00:05, IPv4: 172.20.0.5</span>
<span class="c">#    Starting arp-scan 1.10.0 with 65536 hosts (https://github.com/royhills/arp-scan)</span>
<span class="c">#    172.20.0.1      02:42:a0:b9:45:0f       (Unknown: locally administered)</span>
<span class="c">#    172.20.0.2      02:42:ac:14:00:02       (Unknown: locally administered)</span>
<span class="c">#    172.20.0.3      02:42:ac:14:00:03       (Unknown: locally administered)</span>
<span class="c">#    172.20.0.4      02:42:ac:14:00:04       (Unknown: locally administered)</span>

<span class="c"># mypc 컨테이너 기동 : kind 도커 브리지를 사용하고, 컨테이너 IP를 지정 없이 혹은 지정 해서 사용</span>
<span class="nv">$ </span>docker run <span class="nt">-d</span> <span class="nt">--rm</span> <span class="nt">--name</span> mypc <span class="nt">--network</span> kind <span class="nt">--ip</span> 172.20.0.100 nicolaka/netshoot <span class="nb">sleep </span>infinity <span class="c"># IP 지정 실행 시</span>
<span class="c"># =&gt; docker: Error response from daemon: Invalid address 172.20.0.100: It does not belong to any of this network's subnets.</span>
<span class="c"># IP 지정 실행 시 에러 발생 시 아래 처럼 IP 지정 없이 실행</span>
<span class="nv">$ </span>docker run <span class="nt">-d</span> <span class="nt">--rm</span> <span class="nt">--name</span> mypc <span class="nt">--network</span> kind nicolaka/netshoot <span class="nb">sleep </span>infinity <span class="c"># IP 지정 없이 실행 시</span>
<span class="c"># =&gt; 5863ee53a7334a4a524c8c965b2505237c43037ff33f435340b6c167e3484eb6</span>
<span class="nv">$ </span>docker ps
<span class="c"># =&gt; CONTAINER ID   IMAGE                  COMMAND                  CREATED          STATUS          PORTS                                                             NAMES</span>
<span class="c">#    5863ee53a733   nicolaka/netshoot      &amp;quot;sleep infinity&amp;quot;         15 seconds ago   Up 14 seconds                                                                     mypc</span>
<span class="c">#    ...</span>

<span class="c"># mypc2 컨테이너 기동 : kind 도커 브리지를 사용하고, 컨테이너 IP를 지정 없이 혹은 지정 해서 사용</span>
<span class="nv">$ </span>docker run <span class="nt">-d</span> <span class="nt">--rm</span> <span class="nt">--name</span> mypc2 <span class="nt">--network</span> kind <span class="nt">--ip</span> 172.20.0.200 nicolaka/netshoot <span class="nb">sleep </span>infinity <span class="c"># IP 지정 실행 시</span>
<span class="c"># =&gt; docker: Error response from daemon: Invalid address 172.20.0.200: It does not belong to any of this network's subnets.</span>
<span class="c"># IP 지정 실행 시 에러 발생 시 아래 처럼 IP 지정 없이 실행</span>
<span class="nv">$ </span>docker run <span class="nt">-d</span> <span class="nt">--rm</span> <span class="nt">--name</span> mypc2 <span class="nt">--network</span> kind nicolaka/netshoot <span class="nb">sleep </span>infinity <span class="c"># IP 지정 없이 실행 시</span>
<span class="c"># =&gt; 0d1d3bc32161bafcf5e188e4788553c88cd278d0a2e8dac02d42216e80a9985c</span>
<span class="nv">$ </span>docker ps
<span class="c"># =&gt; CONTAINER ID   IMAGE                  COMMAND                  CREATED              STATUS              PORTS                                                             NAMES</span>
<span class="c">#    0d1d3bc32161   nicolaka/netshoot      &amp;quot;sleep infinity&amp;quot;         9 seconds ago        Up 7 seconds                                                                          mypc2</span>
<span class="c">#    5863ee53a733   nicolaka/netshoot      &amp;quot;sleep infinity&amp;quot;         About a minute ago   Up About a minute                                                                     mypc</span>
<span class="c">#    ...</span>

<span class="c"># kind network 중 컨테이너(노드) IP(대역) 확인</span>
<span class="nv">$ </span>docker ps <span class="nt">-q</span> | xargs docker inspect <span class="nt">--format</span> <span class="s1">' '</span>
<span class="c"># =&gt; /myk8s-control-plane 172.20.0.5</span>
<span class="c">#    /myk8s-worker 172.20.0.4</span>
<span class="c">#    /myk8s-worker2 172.20.0.2</span>
<span class="c">#    /myk8s-worker3 172.20.0.3</span>
<span class="c">#    /mypc 172.20.0.6</span>
<span class="c">#    /mypc2 172.20.0.7</span>

<span class="c"># kube-ops-view 설치</span>
<span class="nv">$ </span>helm repo add geek-cookbook https://geek-cookbook.github.io/charts/
<span class="nv">$ </span>helm <span class="nb">install </span>kube-ops-view geek-cookbook/kube-ops-view <span class="nt">--version</span> 1.2.2 <span class="nt">--set</span> service.main.type<span class="o">=</span>NodePort,service.main.ports.http.nodePort<span class="o">=</span>30000 <span class="nt">--set</span> env.TZ<span class="o">=</span><span class="s2">"Asia/Seoul"</span> <span class="nt">--namespace</span> kube-system
<span class="c"># =&gt; NAME: kube-ops-view</span>
<span class="c">#    LAST DEPLOYED: Sun Jan  1 15:57:45 2020</span>
<span class="c">#    NAMESPACE: kube-system</span>
<span class="c">#    STATUS: deployed</span>
<span class="c">#    REVISION: 1</span>
<span class="c">#    TEST SUITE: None</span>
<span class="c">#    NOTES:</span>
<span class="c">#    1. Get the application URL by running these commands:</span>
<span class="c">#      export NODE_PORT=$(kubectl get --namespace kube-system -o jsonpath="{.spec.ports[0].nodePort}" services kube-ops-view)</span>
<span class="c">#      export NODE_IP=$(kubectl get nodes --namespace kube-system -o jsonpath="{.items[0].status.addresses[0].address}")</span>
<span class="c">#      echo http://$NODE_IP:$NODE_PORT</span>

<span class="c"># myk8s-control-plane 배치하기 위해서 nodeSelector, tolerations 설정</span>
<span class="nv">$ </span>kubectl <span class="nt">-n</span> kube-system edit deploy kube-ops-view
<span class="c"># =&gt; ---</span>
<span class="c">#    spec:</span>
<span class="c">#      ...</span>
<span class="c">#      template:</span>
<span class="c">#        ...</span>
<span class="c">#        spec:</span>
<span class="c">#          nodeSelector:</span>
<span class="c">#            mynode: control-plane</span>
<span class="c">#          tolerations:</span>
<span class="c">#          - key: "node-role.kubernetes.io/control-plane"</span>
<span class="c">#            operator: "Equal"</span>
<span class="c">#            effect: "NoSchedule"</span>
<span class="c">#    ---</span>

<span class="c"># 설치 확인</span>
<span class="nv">$ </span>kubectl <span class="nt">-n</span> kube-system get pod <span class="nt">-o</span> wide <span class="nt">-l</span> app.kubernetes.io/instance<span class="o">=</span>kube-ops-view
<span class="c"># =&gt; NAME                             READY   STATUS              RESTARTS   AGE   IP       NODE                  NOMINATED NODE   READINESS GATES</span>
<span class="c">#    kube-ops-view-58f96c464d-kp8l8   0/1     ContainerCreating   0          5s    &amp;lt;none&amp;gt;   &lt;span style="color: red;"&gt;myk8s-control-plane&lt;/span&gt;   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>

<span class="c"># kube-ops-view 접속 URL 확인 (1.5 , 2 배율) : macOS 사용자</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"KUBE-OPS-VIEW URL = http://localhost:30000/#scale=1.5"</span>
<span class="c"># =&gt; KUBE-OPS-VIEW URL = http://localhost:30000/#scale=1.5</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"KUBE-OPS-VIEW URL = http://localhost:30000/#scale=2"</span>

<span class="c"># kube-ops-view 접속 URL 확인 (1.5 , 2 배율) : Windows 사용자</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"KUBE-OPS-VIEW URL = http://192.168.50.10:30000/#scale=1.5"</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"KUBE-OPS-VIEW URL = http://192.168.50.10:30000/#scale=2"</span>

<span class="c"># kube-ops-view 접속 URL 확인 (1.5 , 2 배율) : AWS_EC2 사용자</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"KUBE-OPS-VIEW URL = http://</span><span class="si">$(</span>curl <span class="nt">-s</span> ipinfo.io/ip<span class="si">)</span><span class="s2">:30000/#scale=1.5"</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"KUBE-OPS-VIEW URL = http://</span><span class="si">$(</span>curl <span class="nt">-s</span> ipinfo.io/ip<span class="si">)</span><span class="s2">:30000/#scale=2"</span>
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w5/20241005_kans_w5_9.png" alt="img.png" class="image-center" />
<em class="image-caption">실습환경이 구축 완료된 kube-ops-view 화면</em></p>

<h5 id="프로메테우스-스택-설치">프로메테우스 스택 설치</h5>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#</span>
<span class="nv">$ </span>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
<span class="c"># =&gt; "prometheus-community" has been added to your repositories</span>

<span class="c"># 파라미터 파일 생성</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOT</span><span class="sh"> &gt; monitor-values.yaml
prometheus:
  service:
    type: NodePort
    nodePort: 30001

  prometheusSpec:
    podMonitorSelectorNilUsesHelmValues: false
    serviceMonitorSelectorNilUsesHelmValues: false
    nodeSelector:
      mynode: control-plane
    tolerations:
    - key: "node-role.kubernetes.io/control-plane"
      operator: "Equal"
      effect: "NoSchedule"


grafana:
  defaultDashboardsTimezone: Asia/Seoul
  adminPassword: kans1234

  service:
    type: NodePort
    nodePort: 30002
  nodeSelector:
    mynode: control-plane
  tolerations:
  - key: "node-role.kubernetes.io/control-plane"
    operator: "Equal"
    effect: "NoSchedule"

  #  sidecar:
  #    dashboards:
  #      enabled: true
  #  dashboards:
  #    default:
  #      custom-dashboard:
  #        gnetId: 20162  # MetalLB 대시보드 ID
  #        datasource: Prometheus  # 사용할 데이터소스 이름을 명시
  #        revision: 1    # 대시보드의 버전

defaultRules:
  create: false
alertmanager:
  enabled: false
</span><span class="no">EOT

</span><span class="c"># 배포</span>
<span class="nv">$ </span>kubectl create ns monitoring
<span class="c"># =&gt; namespace/monitoring created</span>
<span class="nv">$ </span>helm <span class="nb">install </span>kube-prometheus-stack prometheus-community/kube-prometheus-stack <span class="nt">--version</span> 62.3.0 <span class="nt">-f</span> monitor-values.yaml <span class="nt">--namespace</span> monitoring
<span class="c"># =&gt; NAME: kube-prometheus-stack</span>
<span class="c">#    LAST DEPLOYED: Sun Jan  1 16:16:32 2020</span>
<span class="c">#    NAMESPACE: monitoring</span>
<span class="c">#    STATUS: deployed</span>
<span class="c">#    REVISION: 1</span>
<span class="c">#    NOTES:</span>
<span class="c">#    kube-prometheus-stack has been installed. Check its status by running:</span>
<span class="c">#      kubectl --namespace monitoring get pods -l &amp;quot;release=kube-prometheus-stack&amp;quot;</span>
<span class="c">#    </span>
<span class="c">#    Visit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create &amp;amp; configure Alertmanager and Prometheus instances using the Operator.</span>

<span class="c"># 확인</span>
<span class="nv">$ </span>helm list <span class="nt">-n</span> monitoring
<span class="c"># =&gt; NAME                   NAMESPACE   REVISION  UPDATED                               STATUS    CHART                         APP VERSION</span>
<span class="c">#    kube-prometheus-stack  monitoring  1         2020-01-01 16:16:32.988771 +0900 KST  deployed  kube-prometheus-stack-62.3.0  v0.76.0    </span>

<span class="c"># Grafana 접속 계정 : admin / kans1234 : macOS 사용자</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"Prometheus URL = http://localhost:30001"</span>
<span class="c"># =&gt; Prometheus URL = http://localhost:30001</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"Grafana URL = http://localhost:30002"</span>
<span class="c"># =&gt; Grafana URL = http://localhost:30002</span>

<span class="c"># Grafana 접속 계정 : admin / kans1234 : Windows 사용자</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"Prometheus URL = http://192.168.50.10:30001"</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"Grafana URL = http://192.168.50.10:30002"</span>

<span class="c"># Grafana 접속 계정 : admin / kans1234 : AWS_EC2 사용자</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"Prometheus URL = http://</span><span class="si">$(</span>curl <span class="nt">-s</span> ipinfo.io/ip<span class="si">)</span><span class="s2">:30001"</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"Grafana URL = http://</span><span class="si">$(</span>curl <span class="nt">-s</span> ipinfo.io/ip<span class="si">)</span><span class="s2">:30002"</span>

<span class="c"># (참고) helm 삭제</span>
<span class="nv">$ </span>helm uninstall <span class="nt">-n</span> monitoring kube-prometheus-stack
</code></pre></div></div>

<ul>
  <li>그라파나 접속 후 MetalLB 대시보드 import
    <ul>
      <li>Dashboards &gt; Manage &gt; Import</li>
      <li>GnetId : 20162</li>
      <li>Datasource : Prometheus</li>
      <li>Import 버튼 클릭</li>
    </ul>
  </li>
  <li>그라파나 대시보드 확인
    <ul>
      <li>Home &gt; MetalLB 대시보드 선택
<img src="/assets/2024/kans-3th/w5/20241005_kans_w5_10.png" alt="img.png" class="image-center" /></li>
    </ul>
  </li>
</ul>

<h5 id="파드-생성">파드 생성</h5>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: webpod1
  labels:
    app: webpod
spec:
  nodeName: myk8s-worker
  containers:
  - name: container
    image: traefik/whoami
  terminationGracePeriodSeconds: 0
---
apiVersion: v1
kind: Pod
metadata:
  name: webpod2
  labels:
    app: webpod
spec:
  nodeName: myk8s-worker2
  containers:
  - name: container
    image: traefik/whoami
  terminationGracePeriodSeconds: 0
</span><span class="no">EOF
</span><span class="c"># =&gt; pod/webpod1 created</span>
<span class="c">#    pod/webpod2 created</span>

<span class="c"># 파드 정보 확인</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-owide</span>
<span class="c"># =&gt; NAME      READY   STATUS    RESTARTS   AGE   IP          NODE            NOMINATED NODE   READINESS GATES</span>
<span class="c">#    webpod1   1/1     Running   0          38s   10.10.3.2   myk8s-worker    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    webpod2   1/1     Running   0          38s   10.10.2.3   myk8s-worker2   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>

<span class="c"># 파드 IP주소를 변수에 지정</span>
<span class="nv">$ WPOD1</span><span class="o">=</span><span class="si">$(</span>kubectl get pod webpod1 <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.status.podIP}"</span><span class="si">)</span>
<span class="nv">$ WPOD2</span><span class="o">=</span><span class="si">$(</span>kubectl get pod webpod2 <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.status.podIP}"</span><span class="si">)</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$WPOD1</span> <span class="nv">$WPOD2</span>
<span class="c"># =&gt; 10.10.3.2 10.10.2.3</span>

<span class="c"># 접속 확인</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane  ping <span class="nt">-i</span> 1 <span class="nt">-W</span> 1 <span class="nt">-c</span> 1 <span class="nv">$WPOD1</span>
<span class="c"># =&gt; PING 10.10.3.2 (10.10.3.2) 56(84) bytes of data.</span>
<span class="c">#    64 bytes from 10.10.3.2: icmp_seq=1 ttl=63 time=0.082 ms</span>
<span class="c">#    </span>
<span class="c">#    --- 10.10.3.2 ping statistics ---</span>
<span class="c">#    1 packets transmitted, 1 received, 0% packet loss, time 0ms</span>
<span class="c">#    rtt min/avg/max/mdev = 0.082/0.082/0.082/0.000 ms</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane  ping <span class="nt">-i</span> 1 <span class="nt">-W</span> 1 <span class="nt">-c</span> 1 <span class="nv">$WPOD2</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane  curl <span class="nt">-s</span> <span class="nt">--connect-timeout</span> 1 <span class="nv">$WPOD1</span> | <span class="nb">grep </span>Hostname
<span class="c"># =&gt; Hostname: webpod1</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane  curl <span class="nt">-s</span> <span class="nt">--connect-timeout</span> 1 <span class="nv">$WPOD2</span> | <span class="nb">grep </span>Hostname
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane  curl <span class="nt">-s</span> <span class="nt">--connect-timeout</span> 1 <span class="nv">$WPOD1</span> | egrep <span class="s1">'Hostname|RemoteAddr|Host:'</span>
<span class="c"># =&gt; Hostname: webpod1</span>
<span class="c">#    RemoteAddr: 172.20.0.5:41896</span>
<span class="c">#    Host: 10.10.3.2</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane  curl <span class="nt">-s</span> <span class="nt">--connect-timeout</span> 1 <span class="nv">$WPOD2</span> | egrep <span class="s1">'Hostname|RemoteAddr|Host:'</span>
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w5/20241005_kans_w5_11.png" alt="img.png" /></p>

<h4 id="metallb---layer2-모드-실습">MetalLB - Layer2 모드 실습</h4>

<h5 id="metallb-설치">MetalLB 설치</h5>

<ul>
  <li>링크 : <a href="https://metallb.universe.tf/installation/">https://metallb.universe.tf/installation/</a></li>
  <li>설치 방법 : Kubernetes manifests, Kustomize, using Helm
    <ul>
      <li>kube-proxy가 ipvs 모드 사용시 <code class="language-plaintext highlighter-rouge">strictARP: true</code> 설정 필요</li>
    </ul>
  </li>
  <li>간단하게 manifests로 설치하겠습니다. - <a href="https://github.com/metallb/metallb/tree/main/config/manifests">링크</a></li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Kubernetes manifests 로 설치</span>
<span class="c"># kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.14.8/config/manifests/metallb-native.yaml</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> https://raw.githubusercontent.com/metallb/metallb/refs/heads/main/config/manifests/metallb-native-prometheus.yaml
<span class="c"># =&gt; namespace/metallb-system created</span>
<span class="c">#    ...</span>
<span class="c">#    serviceaccount/speaker created</span>
<span class="c">#    ...</span>
<span class="c">#    daemonset.apps/speaker created</span>
<span class="c">#    servicemonitor.monitoring.coreos.com/controller-monitor created</span>
<span class="c">#    servicemonitor.monitoring.coreos.com/speaker-monitor created</span>
<span class="c">#    validatingwebhookconfiguration.admissionregistration.k8s.io/metallb-webhook-configuration created</span>

<span class="c"># metallb crd 확인</span>
<span class="nv">$ </span>kubectl get crd | <span class="nb">grep </span>metallb
<span class="c"># =&gt; bfdprofiles.metallb.io                      2020-01-01T07:31:16Z</span>
<span class="c">#    bgpadvertisements.metallb.io                2020-01-01T07:31:16Z</span>
<span class="c">#    bgppeers.metallb.io                         2020-01-01T07:31:16Z</span>
<span class="c">#    communities.metallb.io                      2020-01-01T07:31:16Z</span>
<span class="c">#    ipaddresspools.metallb.io                   2020-01-01T07:31:17Z</span>
<span class="c">#    l2advertisements.metallb.io                 2020-01-01T07:31:17Z</span>
<span class="c">#    servicel2statuses.metallb.io                2020-01-01T07:31:17Z</span>

<span class="c"># 생성된 리소스 확인 : metallb-system 네임스페이스 생성, 파드(컨트롤러, 스피커) 생성, RBAC(서비스/파드/컨피그맵 조회 등등 권한들), SA 등</span>
<span class="nv">$ </span>kubectl get-all <span class="nt">-n</span> metallb-system <span class="c"># kubectl krew 플러그인 get-all 설치 후 사용 가능</span>
<span class="nv">$ </span>kubectl get all,configmap,secret,ep <span class="nt">-n</span> metallb-system
<span class="c"># =&gt; NAME                              READY   STATUS    RESTARTS      AGE</span>
<span class="c">#    pod/controller-679855f7d7-m8spp   2/2     Running   0             5m36s</span>
<span class="c">#    pod/speaker-dm26z                 2/2     Running   4 (91s ago)   5m36s</span>
<span class="c">#    pod/speaker-dr8kh                 2/2     Running   0             5m36s</span>
<span class="c">#    pod/speaker-pctt7                 2/2     Running   3 (90s ago)   5m36s</span>
<span class="c">#    pod/speaker-w69v6                 2/2     Running   4 (91s ago)   5m36s</span>
<span class="c">#    </span>
<span class="c">#    NAME                                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE</span>
<span class="c">#    service/controller-monitor-service   ClusterIP   None           &amp;lt;none&amp;gt;        9120/TCP   5m36s</span>
<span class="c">#    service/metallb-webhook-service      ClusterIP   10.200.1.191   &amp;lt;none&amp;gt;        443/TCP    5m36s</span>
<span class="c">#    service/speaker-monitor-service      ClusterIP   None           &amp;lt;none&amp;gt;        9120/TCP   5m36s</span>
<span class="c">#    </span>
<span class="c">#    NAME                     DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE</span>
<span class="c">#    daemonset.apps/speaker   4         4         4       4            4           kubernetes.io/os=linux   5m36s</span>
<span class="c">#    </span>
<span class="c">#    NAME                         READY   UP-TO-DATE   AVAILABLE   AGE</span>
<span class="c">#    deployment.apps/controller   1/1     1            1           5m36s</span>
<span class="c">#    </span>
<span class="c">#    NAME                                    DESIRED   CURRENT   READY   AGE</span>
<span class="c">#    replicaset.apps/controller-679855f7d7   1         1         1       5m36s</span>
<span class="c">#    </span>
<span class="c">#    NAME                          DATA   AGE</span>
<span class="c">#    configmap/kube-root-ca.crt    1      5m37s</span>
<span class="c">#    configmap/metallb-excludel2   1      5m36s</span>
<span class="c">#    </span>
<span class="c">#    NAME                          TYPE     DATA   AGE</span>
<span class="c">#    secret/memberlist             Opaque   1      5m18s</span>
<span class="c">#    secret/metallb-webhook-cert   Opaque   4      5m36s</span>
<span class="c">#    </span>
<span class="c">#    NAME                                   ENDPOINTS                                                     AGE</span>
<span class="c">#    endpoints/controller-monitor-service   10.10.1.3:9120                                                5m36s</span>
<span class="c">#    endpoints/metallb-webhook-service      10.10.1.3:9443                                                5m36s</span>
<span class="c">#    endpoints/speaker-monitor-service      172.20.0.2:9120,172.20.0.3:9120,172.20.0.4:9120 + 1 more...   5m36s</span>

<span class="c"># 파드 내에 kube-rbac-proxy 컨테이너는 프로메테우스 익스포터 역할 제공</span>
<span class="nv">$ </span>kubectl get pods <span class="nt">-n</span> metallb-system <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>metallb <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{range .items[*]}{.metadata.name}{':</span><span class="se">\n</span><span class="s2">'}{range .spec.containers[*]}{'  '}{.name}{' -&gt; '}{.image}{'</span><span class="se">\n</span><span class="s2">'}{end}{end}"</span>
<span class="c"># =&gt; controller-679855f7d7-m8spp:</span>
<span class="c">#      kube-rbac-proxy -&amp;gt; gcr.io/kubebuilder/kube-rbac-proxy:v0.12.0</span>
<span class="c">#      controller -&amp;gt; quay.io/metallb/controller:main</span>
<span class="c">#    speaker-dm26z:</span>
<span class="c">#      kube-rbac-proxy -&amp;gt; gcr.io/kubebuilder/kube-rbac-proxy:v0.12.0</span>
<span class="c">#      speaker -&amp;gt; quay.io/metallb/speaker:main</span>
<span class="c">#    speaker-dr8kh:</span>
<span class="c">#      kube-rbac-proxy -&amp;gt; gcr.io/kubebuilder/kube-rbac-proxy:v0.12.0</span>
<span class="c">#      speaker -&amp;gt; quay.io/metallb/speaker:main</span>
<span class="c">#    speaker-pctt7:</span>
<span class="c">#      kube-rbac-proxy -&amp;gt; gcr.io/kubebuilder/kube-rbac-proxy:v0.12.0</span>
<span class="c">#      speaker -&amp;gt; quay.io/metallb/speaker:main</span>
<span class="c">#    speaker-w69v6:</span>
<span class="c">#      kube-rbac-proxy -&amp;gt; gcr.io/kubebuilder/kube-rbac-proxy:v0.12.0</span>
<span class="c">#      speaker -&amp;gt; quay.io/metallb/speaker:main</span>

<span class="c">## metallb 컨트롤러는 디플로이먼트로 배포됨</span>
<span class="nv">$ </span>kubectl get ds,deploy <span class="nt">-n</span> metallb-system
<span class="c"># =&gt; NAME                     DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE</span>
<span class="c">#    daemonset.apps/speaker   4         4         4       4            4           kubernetes.io/os=linux   6m26s</span>
<span class="c">#    </span>
<span class="c">#    NAME                         READY   UP-TO-DATE   AVAILABLE   AGE</span>
<span class="c">#    deployment.apps/controller   1/1     1            1           6m26s</span>

<span class="c">## 데몬셋으로 배포되는 metallb 스피커 파드의 IP는 네트워크가 host 모드이므로 노드의 IP를 그대로 사용</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-n</span> metallb-system <span class="nt">-o</span> wide
<span class="c"># =&gt; NAME                          READY   STATUS    RESTARTS   AGE   IP           NODE                  NOMINATED NODE   READINESS GATES</span>
<span class="c">#    controller-679855f7d7-rg9pw   2/2     Running   0          22m   10.10.1.3    myk8s-worker3         &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    speaker-9njww                 2/2     Running   0          22m   172.20.0.3   myk8s-worker3         &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    speaker-lk9wt                 2/2     Running   0          22m   172.20.0.2   myk8s-worker2         &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    speaker-wz9w5                 2/2     Running   0          22m   172.20.0.5   myk8s-control-plane   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    speaker-zbwdq                 2/2     Running   0          22m   172.20.0.4   myk8s-worker          &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>

<span class="c"># (참고) 상세 정보 확인</span>
<span class="nv">$ </span>kubectl get sa,cm,secret <span class="nt">-n</span> metallb-system
<span class="nv">$ </span>kubectl describe role <span class="nt">-n</span> metallb-system
<span class="nv">$ </span>kubectl describe deploy controller <span class="nt">-n</span> metallb-system
<span class="nv">$ </span>kubectl describe ds speaker <span class="nt">-n</span> metallb-system
</code></pre></div></div>

<ul>
  <li>컨피그맵 생성 : 모드 및 서비스 대역 지정
    <ul>
      <li>서비스(External-IP) 대역을 노드가 속한 eth0의 대역이 아니여도 상관없습니다.
다만, 이 경우 GW 역할의 라우터에서 노드들로 라우팅 경로 지정 필요합니다.</li>
    </ul>
  </li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># kind 설치 시 kind 이름의 도커 브리지가 생성됩니다 : 172.20.0.0/16 대역</span>
<span class="nv">$ </span>docker network <span class="nb">ls</span>
<span class="c"># =&gt; NETWORK ID     NAME                     DRIVER    SCOPE</span>
<span class="c">#    a8d530305515   bridge                   bridge    local</span>
<span class="c">#    8204a0851463   host                     host      local</span>
<span class="c">#    3bbcc6aa8f38   kind                     bridge    local</span>
<span class="nv">$ </span>docker inspect kind
<span class="c"># kind network 중 컨테이너(노드) IP(대역) 확인 : 172.20.0.2~ 부터 할당되며, control-plane 이 꼭 172.20.0.2가 안될 수 도 있음</span>
<span class="nv">$ </span>docker ps <span class="nt">-q</span> | xargs docker inspect <span class="nt">--format</span> <span class="s1">' '</span>
<span class="c"># =&gt; /mypc2 172.20.0.7</span>
<span class="c">#    /mypc 172.20.0.6</span>
<span class="c">#    /myk8s-worker 172.20.0.4</span>
<span class="c">#    /myk8s-control-plane 172.20.0.5</span>
<span class="c">#    /myk8s-worker2 172.20.0.2</span>
<span class="c">#    /myk8s-worker3 172.20.0.3</span>

<span class="c"># IPAddressPool 생성 : LoadBalancer External IP로 사용할 IP 대역</span>
<span class="c">## MetalLB는 서비스를 위한 외부 IP 주소를 관리하고, 서비스가 생성될 때 해당 IP 주소를 동적으로 할당할 수 있습니다.</span>
<span class="nv">$ </span>kubectl explain ipaddresspools.metallb.io

<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: my-ippool
  namespace: metallb-system
spec:
  addresses:
  - 172.20.255.200-172.20.255.250
</span><span class="no">EOF
</span><span class="c"># =&gt; ipaddresspool.metallb.io/my-ippool unchanged</span>

<span class="nv">$ </span>kubectl get ipaddresspools <span class="nt">-n</span> metallb-system
<span class="c"># =&gt; NAME        AUTO ASSIGN   AVOID BUGGY IPS   ADDRESSES</span>
<span class="c">#    my-ippool   true          false             [&amp;quot;172.20.255.200-172.20.255.250&amp;quot;]</span>

<span class="c"># L2Advertisement 생성 : 설정한 IPpool을 기반으로 Layer2 모드로 LoadBalancer IP 사용 허용</span>
<span class="c">## Kubernetes 클러스터 내의 서비스가 외부 네트워크에 IP 주소를 광고하는 방식을 정의</span>

<span class="nv">$ </span>kubectl explain l2advertisements.metallb.io

<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: my-l2-advertise
  namespace: metallb-system
spec:
  ipAddressPools:
  - my-ippool
</span><span class="no">EOF
</span><span class="c"># =&gt; l2advertisement.metallb.io/my-l2-advertise created</span>

<span class="nv">$ </span>kubectl get l2advertisements <span class="nt">-n</span> metallb-system
<span class="c"># =&gt; NAME              IPADDRESSPOOLS   IPADDRESSPOOL SELECTORS   INTERFACES</span>
<span class="c">#    my-l2-advertise   [&amp;quot;my-ippool&amp;quot;]                              </span>
</code></pre></div></div>

<ul>
  <li>로그 확인</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># (옵션) metallb-speaker 파드 로그 확인</span>
<span class="nv">$ </span>kubectl logs <span class="nt">-n</span> metallb-system <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>metallb <span class="nt">-f</span>
<span class="nv">$ </span>kubectl logs <span class="nt">-n</span> metallb-system <span class="nt">-l</span> <span class="nv">component</span><span class="o">=</span>speaker <span class="nt">--since</span> 1h
<span class="nv">$ </span>kubectl logs <span class="nt">-n</span> metallb-system <span class="nt">-l</span> <span class="nv">component</span><span class="o">=</span>speaker <span class="nt">-f</span>

<span class="c"># (옵션) kubectl krew 플러그인 stern 설치 후 아래 명령 사용 가능</span>
<span class="nv">$ </span>kubectl stern <span class="nt">-n</span> metallb-system <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>metallb
<span class="nv">$ </span>kubectl stern <span class="nt">-n</span> metallb-system <span class="nt">-l</span> <span class="nv">component</span><span class="o">=</span>speaker <span class="nt">--since</span> 1h
<span class="nv">$ </span>kubectl stern <span class="nt">-n</span> metallb-system <span class="nt">-l</span> <span class="nv">component</span><span class="o">=</span>speaker <span class="c"># 기본 설정이 follow</span>
<span class="nv">$ </span>kubectl stern <span class="nt">-n</span> metallb-system speaker  <span class="c"># 매칭 사용 가능</span>
</code></pre></div></div>

<h5 id="서비스-생성-및-확인">서비스 생성 및 확인</h5>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: svc1
spec:
  ports:
    - name: svc1-webport
      port: 80
      targetPort: 80
  selector:
    app: webpod
  type: LoadBalancer  # 서비스 타입이 LoadBalancer
---
apiVersion: v1
kind: Service
metadata:
  name: svc2
spec:
  ports:
    - name: svc2-webport
      port: 80
      targetPort: 80
  selector:
    app: webpod
  type: LoadBalancer
---
apiVersion: v1
kind: Service
metadata:
  name: svc3
spec:
  ports:
    - name: svc3-webport
      port: 80
      targetPort: 80
  selector:
    app: webpod
  type: LoadBalancer
</span><span class="no">EOF
</span><span class="c"># =&gt; service/svc1 created</span>
<span class="c">#    service/svc2 created</span>
<span class="c">#    service/svc3 created</span>
</code></pre></div></div>

<h5 id="서비스-확인-및-리더-speaker-파드-확인">서비스 확인 및 리더 Speaker 파드 확인</h5>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># arp scan 해두기</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane arp-scan <span class="nt">--interfac</span><span class="o">=</span>eth0 <span class="nt">--localnet</span>
<span class="c"># =&gt; Interface: eth0, type: EN10MB, MAC: 02:42:ac:14:00:05, IPv4: 172.20.0.5</span>
<span class="c">#    Starting arp-scan 1.10.0 with 65536 hosts (https://github.com/royhills/arp-scan)</span>
<span class="c">#    172.20.0.1      02:42:1f:41:79:66       (Unknown: locally administered)</span>
<span class="c">#    172.20.0.2      02:42:ac:14:00:02       (Unknown: locally administered)</span>
<span class="c">#    172.20.0.3      02:42:ac:14:00:03       (Unknown: locally administered)</span>
<span class="c">#    172.20.0.4      02:42:ac:14:00:04       (Unknown: locally administered)</span>
<span class="c">#    172.20.0.6      02:42:ac:14:00:06       (Unknown: locally administered)</span>
<span class="c">#    172.20.0.7      02:42:ac:14:00:07       (Unknown: locally administered)</span>

<span class="c"># LoadBalancer 타입의 서비스 생성 확인 : EXTERNAL-IP가 서비스 마다 할당되며, 실습 환경에 따라 다를 수 있음</span>
<span class="c">## LoadBalancer 타입의 서비스는 NodePort 와 ClusterIP 를 포함함 - 'allocateLoadBalancerNodePorts : true' 기본값</span>
<span class="c">## ExternalIP 로 접속 시 사용하는 포트는 PORT(S) 의 앞에 있는 값을 사용 (아래의 경우는 TCP 80 임)</span>
<span class="c">## 만약 노드의 IP에 NodePort 로 접속 시 사용하는 포트는 PORT(S) 의 뒤에 있는 값을 사용 (아래는 30485 임)</span>
<span class="nv">$ </span>kubectl get service,ep
<span class="c"># =&gt; NAME                 TYPE           CLUSTER-IP     EXTERNAL-IP      PORT(S)        AGE</span>
<span class="c">#    service/kubernetes   ClusterIP      10.200.1.1     &amp;lt;none&amp;gt;           443/TCP        3h55m</span>
<span class="c">#    service/svc1         LoadBalancer   10.200.1.213   172.20.255.200   80:32145/TCP   128m</span>
<span class="c">#    service/svc2         LoadBalancer   10.200.1.59    172.20.255.201   80:32238/TCP   128m</span>
<span class="c">#    service/svc3         LoadBalancer   10.200.1.201   172.20.255.202   80:31593/TCP   128m</span>
<span class="c">#    </span>
<span class="c">#    NAME                   ENDPOINTS                   AGE</span>
<span class="c">#    endpoints/kubernetes   172.20.0.5:6443             3h55m</span>
<span class="c">#    endpoints/svc1         10.10.2.3:80,10.10.3.2:80   128m</span>
<span class="c">#    endpoints/svc2         10.10.2.3:80,10.10.3.2:80   128m</span>
<span class="c">#    endpoints/svc3         10.10.2.3:80,10.10.3.2:80   128m</span>

<span class="c"># LoadBalancer 타입은 기본적으로 NodePort를 포함 사용. NodePort는 ClusterIP를 포함 사용.</span>
<span class="c">## 클라우드사업자 LB Type이나 온프레미스환경 HW LB Type 경우 LB 사용 시 NodePort 미사용 설정 가능</span>
<span class="nv">$ </span>kubectl describe svc svc1
<span class="c"># =&gt; Name:                     svc1</span>
<span class="c">#    ...</span>
<span class="c">#    Annotations:              metallb.io/ip-allocated-from-pool: my-ippool</span>
<span class="c">#    Selector:                 app=webpod</span>
<span class="c">#    Type:                     LoadBalancer</span>
<span class="c">#    IP Family Policy:         SingleStack</span>
<span class="c">#    IP Families:              IPv4</span>
<span class="c">#    IP:                       10.200.1.213</span>
<span class="c">#    IPs:                      10.200.1.213</span>
<span class="c">#    LoadBalancer Ingress:     172.20.255.200 (VIP)</span>
<span class="c">#    Port:                     svc1-webport  80/TCP</span>
<span class="c">#    TargetPort:               80/TCP</span>
<span class="c">#    NodePort:                 svc1-webport  32145/TCP</span>
<span class="c">#    Endpoints:                10.10.3.2:80,10.10.2.3:80</span>
<span class="c">#    Session Affinity:         None</span>
<span class="c">#    External Traffic Policy:  Cluster</span>
<span class="c">#    Internal Traffic Policy:  Cluster</span>
<span class="c">#    Events:</span>
<span class="c">#      Type    Reason       Age    From                Message</span>
<span class="c">#      ----    ------       ----   ----                -------</span>
<span class="c">#      Normal  IPAllocated  3m19s  metallb-controller  Assigned IP [&amp;quot;172.20.255.200&amp;quot;]</span>
<span class="c">#      Normal  nodeAssigned  5m55s (x2 over 5m55s)  metallb-speaker  announcing from node &amp;quot;myk8s-worker&amp;quot; with protocol &amp;quot;layer2&amp;quot;</span>

<span class="c">## 아래 처럼 LB VIP 별로 이던 speaker 배포된 노드가 리더 역할을 하는지 확인 가능</span>
<span class="nv">$ </span>kubectl describe svc | <span class="nb">grep </span>Events: <span class="nt">-A5</span>
<span class="c"># =&gt; Events:                   &amp;lt;none&amp;gt;</span>
<span class="c">#</span>
<span class="c">#    Name:                     svc1</span>
<span class="c">#    Namespace:                default</span>
<span class="c">#    Labels:                   &amp;lt;none&amp;gt;</span>
<span class="c">#    --</span>
<span class="c">#    Events:</span>
<span class="c">#      Type    Reason       Age    From                Message</span>
<span class="c">#      ----    ------       ----   ----                -------</span>
<span class="c">#      Normal  IPAllocated  4m24s  metallb-controller  Assigned IP [&amp;quot;172.20.255.200&amp;quot;]</span>
<span class="c">#      Normal  nodeAssigned 6m42s (x2 over 6m42s)  metallb-speaker  announcing from node "myk8s-worker" with protocol "layer2"</span>
<span class="c">#    ...</span>
<span class="nv">$ </span>kubectl get svc svc1 <span class="nt">-o</span> json | jq
<span class="c"># =&gt; ...</span>
<span class="c">#      &amp;quot;spec&amp;quot;: {</span>
<span class="c">#        &amp;quot;allocateLoadBalancerNodePorts&amp;quot;: true,</span>
<span class="c">#      ...</span>
<span class="c">#      &amp;quot;status&amp;quot;: {</span>
<span class="c">#        &amp;quot;loadBalancer&amp;quot;: {</span>
<span class="c">#          &amp;quot;ingress&amp;quot;: [</span>
<span class="c">#            {</span>
<span class="c">#              &amp;quot;ip&amp;quot;: &amp;quot;172.20.255.200&amp;quot;,</span>
<span class="c">#              &amp;quot;ipMode&amp;quot;: &amp;quot;VIP&amp;quot;</span>
<span class="c">#    ...</span>

<span class="c"># metallb CRD인 servicel2status 로 상태 정보 확인</span>
<span class="nv">$ </span>kubectl explain servicel2status
<span class="nv">$ </span>kubectl get servicel2status <span class="nt">-n</span> metallb-system
<span class="c"># =&gt; NAME       ALLOCATED NODE   SERVICE NAME   SERVICE NAMESPACE</span>
<span class="c">#    l2-cm8sw   myk8s-worker     svc2           default</span>
<span class="c">#    l2-j6w4k   myk8s-worker     svc1           default</span>
<span class="c">#    l2-k5cdm   myk8s-worker3    svc3           default</span>
<span class="nv">$ </span>kubectl describe servicel2status <span class="nt">-n</span> metallb-system
<span class="nv">$ </span>kubectl get servicel2status <span class="nt">-n</span> metallb-system <span class="nt">-o</span> json <span class="nt">--watch</span> <span class="c"># watch 모드</span>

<span class="c"># 현재 SVC EXTERNAL-IP를 변수에 지정</span>
<span class="nv">$ SVC1EXIP</span><span class="o">=</span><span class="si">$(</span>kubectl get svc svc1 <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.status.loadBalancer.ingress[0].ip}'</span><span class="si">)</span>
<span class="nv">$ SVC2EXIP</span><span class="o">=</span><span class="si">$(</span>kubectl get svc svc2 <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.status.loadBalancer.ingress[0].ip}'</span><span class="si">)</span>
<span class="nv">$ SVC3EXIP</span><span class="o">=</span><span class="si">$(</span>kubectl get svc svc3 <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.status.loadBalancer.ingress[0].ip}'</span><span class="si">)</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$SVC1EXIP</span> <span class="nv">$SVC2EXIP</span> <span class="nv">$SVC3EXIP</span>
<span class="c"># =&gt; 172.20.255.200 172.20.255.201 172.20.255.202</span>

<span class="c"># mypc/mypc2 에서 현재 SVC EXTERNAL-IP를 담당하는 리더 Speaker 파드 찾는법 : arping 툴 사용</span>
<span class="c">## Unicast reply from 172.20.255.200: 해당 IP 주소에서 응답을 받았음을 의미합니다. </span>
<span class="c">## Sent 1 probes (1 broadcast(s)): 하나의 ARP 요청을 보냈고, 브로드캐스트 방식으로 요청을 전송했음을 나타냅니다.</span>
<span class="c">## Received 1 response(s): 하나의 응답을 수신했음을 나타냅니다.</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc arping <span class="nt">-I</span> eth0 <span class="nt">-f</span> <span class="nt">-c</span> 1 <span class="nv">$SVC1EXIP</span>
<span class="c"># =&gt; ARPING 172.20.255.200 from 172.20.0.6 eth0</span>
<span class="c">#    Unicast reply from 172.20.255.200 [02:42:AC:14:00:03]  1.139ms</span>
<span class="c">#    Sent 1 probes (1 broadcast(s))</span>
<span class="c">#    Received 1 response(s)</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc arping <span class="nt">-I</span> eth0 <span class="nt">-f</span> <span class="nt">-c</span> 1 <span class="nv">$SVC2EXIP</span>
<span class="c"># =&gt; ARPING 172.20.255.201 from 172.20.0.6 eth0</span>
<span class="c">#    Unicast reply from 172.20.255.201 [02:42:AC:14:00:02]  1.827ms</span>
<span class="c">#    ...</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc arping <span class="nt">-I</span> eth0 <span class="nt">-f</span> <span class="nt">-c</span> 1 <span class="nv">$SVC3EXIP</span>
<span class="c"># =&gt; ARPING 172.20.255.202 from 172.20.0.6 eth0</span>
<span class="c">#    Unicast reply from 172.20.255.202 [02:42:AC:14:00:04]  0.982ms</span>
<span class="c">#    ...</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="nv">$SVC1EXIP</span> <span class="nv">$SVC2EXIP</span> <span class="nv">$SVC3EXIP</span><span class="p">;</span> <span class="k">do </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc arping <span class="nt">-I</span> eth0 <span class="nt">-f</span> <span class="nt">-c</span> 1 <span class="nv">$i</span><span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; ARPING 172.20.255.200 from 172.20.0.6 eth0</span>
<span class="c">#    Unicast reply from 172.20.255.200 [02:42:AC:14:00:03]  1.016ms</span>
<span class="c">#    Sent 1 probes (1 broadcast(s))</span>
<span class="c">#    Received 1 response(s)</span>
<span class="c">#    ARPING 172.20.255.201 from 172.20.0.6 eth0</span>
<span class="c">#    Unicast reply from 172.20.255.201 [02:42:AC:14:00:02]  1.965ms</span>
<span class="c">#    ...</span>
<span class="c">#    ARPING 172.20.255.202 from 172.20.0.6 eth0</span>
<span class="c">#    Unicast reply from 172.20.255.202 [02:42:AC:14:00:04]  1.789ms</span>
<span class="c">#    ...</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc ip <span class="nt">-c</span> neigh
<span class="c"># =&gt; &lt;span style="color:purple;"&gt;172.20.0.1 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:1f:41:79:66 &lt;/span&gt;STALE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.0.4 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:04 &lt;/span&gt;STALE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.0.5 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:05 &lt;/span&gt;STALE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.255.200 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:03 &lt;/span&gt;REACHABLE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.255.201 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:02 &lt;/span&gt;REACHABLE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.255.202 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:04 &lt;/span&gt;REACHABLE </span>

<span class="c"># &lt;span style="color: green;"&gt;ping은 모두 패킷 100% 로스되면서 실패합니다.&lt;/span&gt;</span>
<span class="c"># &lt;span style="color: green;"&gt;서비스 port로만 열려있기때문에 ping은 실패하는것입니다.&lt;/span&gt;</span>
<span class="c"># &lt;span style="color: green;"&gt;여기서 ping을 하는 이유는 arp table을 생성하기 위함입니다.&lt;/span&gt;</span>
  
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc ping <span class="nt">-c</span> 1 <span class="nt">-w</span> 1 <span class="nt">-W</span> 1 <span class="nv">$SVC1EXIP</span>
<span class="c"># =&gt; PING 172.20.255.200 (172.20.255.200) 56(84) bytes of data.</span>
<span class="c">#    </span>
<span class="c">#    --- 172.20.255.200 ping statistics ---</span>
<span class="c">#    1 packets transmitted, 0 received, 100% packet loss, time 0ms</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc ping <span class="nt">-c</span> 1 <span class="nt">-w</span> 1 <span class="nt">-W</span> 1 <span class="nv">$SVC2EXIP</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc ping <span class="nt">-c</span> 1 <span class="nt">-w</span> 1 <span class="nt">-W</span> 1 <span class="nv">$SVC3EXIP</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="nv">$SVC1EXIP</span> <span class="nv">$SVC2EXIP</span> <span class="nv">$SVC3EXIP</span><span class="p">;</span> <span class="k">do </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc ping <span class="nt">-c</span> 1 <span class="nt">-w</span> 1 <span class="nt">-W</span> 1 <span class="nv">$i</span><span class="p">;</span> <span class="k">done</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>172.20.0.2 172.20.0.3 172.20.0.4 172.20.0.5<span class="p">;</span> <span class="k">do </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc ping <span class="nt">-c</span> 1 <span class="nt">-w</span> 1 <span class="nt">-W</span> 1 <span class="nv">$i</span><span class="p">;</span> <span class="k">done</span>

<span class="c"># mypc/mypc2 에서 arp 테이블 정보 확인 &gt;&gt; SVC IP별로 리더 파드(스피커) 역할의 노드를 확인!</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc ip <span class="nt">-c</span> neigh | <span class="nb">sort</span>
<span class="c"># =&gt; &lt;span style="color:purple;"&gt;172.20.0.1 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:1f:41:79:66 &lt;/span&gt;STALE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.0.2 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:02 &lt;/span&gt;REACHABLE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.0.3 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:03 &lt;/span&gt;REACHABLE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.0.4 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:04 &lt;/span&gt;REACHABLE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.0.5 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:05 &lt;/span&gt;REACHABLE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.255.200 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:03 &lt;/span&gt;REACHABLE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.255.201 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:02 &lt;/span&gt;REACHABLE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.255.202 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:04 &lt;/span&gt;REACHABLE </span>

<span class="nv">$ </span>kubectl get node <span class="nt">-owide</span> <span class="c"># mac 주소에 매칭되는 IP(노드) 찾기</span>
<span class="c"># =&gt; NAME                  STATUS   ROLES           AGE     VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                         KERNEL-VERSION     CONTAINER-RUNTIME</span>
<span class="c">#    myk8s-control-plane   Ready    control-plane   4h35m   v1.31.0   172.20.0.5    &amp;lt;none&amp;gt;        Debian GNU/Linux 12 (bookworm)   5.10.76-linuxkit   containerd://1.7.18</span>
<span class="c">#    myk8s-worker          Ready    &amp;lt;none&amp;gt;          4h34m   v1.31.0   172.20.0.4    &amp;lt;none&amp;gt;        Debian GNU/Linux 12 (bookworm)   5.10.76-linuxkit   containerd://1.7.18</span>
<span class="c">#    myk8s-worker2         Ready    &amp;lt;none&amp;gt;          4h34m   v1.31.0   172.20.0.2    &amp;lt;none&amp;gt;        Debian GNU/Linux 12 (bookworm)   5.10.76-linuxkit   containerd://1.7.18</span>
<span class="c">#    myk8s-worker3         Ready    &amp;lt;none&amp;gt;          4h34m   v1.31.0   172.20.0.3    &amp;lt;none&amp;gt;        Debian GNU/Linux 12 (bookworm)   5.10.76-linuxkit   containerd://1.7.18</span>

<span class="c"># (옵션) 노드에서 ARP 패킷 캡쳐 확인</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane tcpdump <span class="nt">-i</span> eth0 <span class="nt">-nn</span> arp
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker        tcpdump <span class="nt">-i</span> eth0 <span class="nt">-nn</span> arp
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker2       tcpdump <span class="nt">-i</span> eth0 <span class="nt">-nn</span> arp
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker3       tcpdump <span class="nt">-i</span> eth0 <span class="nt">-nn</span> arp

<span class="c"># (옵션) metallb-speaker 파드 로그 확인</span>
<span class="nv">$ </span>kubectl logs <span class="nt">-n</span> metallb-system <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>metallb <span class="nt">-f</span>
<span class="nv">$ </span>kubectl logs <span class="nt">-n</span> metallb-system <span class="nt">-l</span> <span class="nv">component</span><span class="o">=</span>speaker <span class="nt">--since</span> 1h
<span class="nv">$ </span>kubectl logs <span class="nt">-n</span> metallb-system <span class="nt">-l</span> <span class="nv">component</span><span class="o">=</span>speaker <span class="nt">-f</span>

<span class="c"># (옵션) kubectl krew 플러그인 stern 설치 후 아래 명령 사용 가능</span>
<span class="nv">$ </span>kubectl stern <span class="nt">-n</span> metallb-system <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>metallb
<span class="nv">$ </span>kubectl stern <span class="nt">-n</span> metallb-system <span class="nt">-l</span> <span class="nv">component</span><span class="o">=</span>speaker <span class="nt">--since</span> 1h
<span class="nv">$ </span>kubectl stern <span class="nt">-n</span> metallb-system <span class="nt">-l</span> <span class="nv">component</span><span class="o">=</span>speaker <span class="c"># 기본 설정이 follow</span>
<span class="nv">$ </span>kubectl stern <span class="nt">-n</span> metallb-system speaker  <span class="c"># 매칭 사용 가능</span>
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w5/20241005_kans_w5_12.png" alt="20241005_kans_w5_12.png" /></p>

<h5 id="서비스-접속-테스트">서비스 접속 테스트</h5>

<ul>
  <li>클러스터 외부에서 external ip와 port를 통해 k8s 클러스터 내부의 서비스에 접속해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 현재 SVC EXTERNAL-IP를 변수에 지정</span>
<span class="nv">$ SVC1EXIP</span><span class="o">=</span><span class="si">$(</span>kubectl get svc svc1 <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.status.loadBalancer.ingress[0].ip}'</span><span class="si">)</span>
<span class="nv">$ SVC2EXIP</span><span class="o">=</span><span class="si">$(</span>kubectl get svc svc2 <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.status.loadBalancer.ingress[0].ip}'</span><span class="si">)</span>
<span class="nv">$ SVC3EXIP</span><span class="o">=</span><span class="si">$(</span>kubectl get svc svc3 <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.status.loadBalancer.ingress[0].ip}'</span><span class="si">)</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$SVC1EXIP</span> <span class="nv">$SVC2EXIP</span> <span class="nv">$SVC3EXIP</span>
<span class="c"># =&gt; 172.20.255.200 172.20.255.201 172.20.255.202</span>

<span class="c"># mypc/mypc2 에서 접속 테스트</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc curl <span class="nt">-s</span> <span class="nv">$SVC1EXIP</span>
<span class="c"># =&gt; Hostname: webpod2</span>
<span class="c">#    IP: 127.0.0.1</span>
<span class="c">#    IP: 10.10.2.3</span>
<span class="c">#    RemoteAddr: 172.20.0.3:40816</span>
<span class="c">#    GET / HTTP/1.1</span>
<span class="c">#    Host: 172.20.255.200</span>
<span class="c">#    User-Agent: curl/8.7.1</span>
<span class="c">#    Accept: */*</span>

<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc curl <span class="nt">-s</span> <span class="nv">$SVC1EXIP</span> | <span class="nb">grep </span>Hostname
<span class="c"># =&gt; Hostname: webpod2</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="nv">$SVC1EXIP</span> <span class="nv">$SVC2EXIP</span> <span class="nv">$SVC3EXIP</span><span class="p">;</span> <span class="k">do </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc curl <span class="nt">-s</span> <span class="nv">$i</span> | <span class="nb">grep </span>Hostname <span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; Hostname: webpod1</span>
<span class="c">#    Hostname: webpod2</span>
<span class="c">#    Hostname: webpod2</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="nv">$SVC1EXIP</span> <span class="nv">$SVC2EXIP</span> <span class="nv">$SVC3EXIP</span><span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; Access Service External-IP : </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span> <span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> mypc curl <span class="nt">-s</span> <span class="nv">$i</span> | <span class="nb">grep </span>Hostname <span class="p">;</span> <span class="nb">echo</span> <span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; &amp;gt;&amp;gt; Access Service External-IP : 172.20.255.200 &amp;lt;&amp;lt;</span>
<span class="c">#    Hostname: webpod1</span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; Access Service External-IP : 172.20.255.201 &amp;lt;&amp;lt;</span>
<span class="c">#    Hostname: webpod2</span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; Access Service External-IP : 172.20.255.202 &amp;lt;&amp;lt;</span>
<span class="c">#    Hostname: webpod1    </span>

<span class="c">## RemoteAddr 주소는 어떻게 나오나요? 왜 그럴까요?</span>
<span class="c">##  NodePort 기본 동작과 동일하게 인입한 노드의 인터페이스로 SNAT 되어서 최종 파드로 전달되기 때문입니다.</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="nv">$SVC1EXIP</span> <span class="nv">$SVC2EXIP</span> <span class="nv">$SVC3EXIP</span><span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; Access Service External-IP : </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span> <span class="p">;</span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc curl <span class="nt">-s</span> <span class="nv">$i</span> | egrep <span class="s1">'Hostname|RemoteAddr|Host:'</span> <span class="p">;</span> <span class="nb">echo</span> <span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; &amp;gt;&amp;gt; Access Service External-IP : 172.20.255.200 &amp;lt;&amp;lt;</span>
<span class="c">#    Hostname: webpod2</span>
<span class="c">#    RemoteAddr: 172.20.0.3:23163</span>
<span class="c">#    Host: 172.20.255.200</span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; Access Service External-IP : 172.20.255.201 &amp;lt;&amp;lt;</span>
<span class="c">#    Hostname: webpod2</span>
<span class="c">#    RemoteAddr: 10.10.2.1:15401</span>
<span class="c">#    Host: 172.20.255.201</span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; Access Service External-IP : 172.20.255.202 &amp;lt;&amp;lt;</span>
<span class="c">#    Hostname: webpod2</span>
<span class="c">#    RemoteAddr: 172.20.0.4:12711</span>
<span class="c">#    Host: 172.20.255.202</span>

<span class="c"># 부하분산 접속됨</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc zsh <span class="nt">-c</span> <span class="s2">"for i in {1..100}; do curl -s </span><span class="nv">$SVC1EXIP</span><span class="s2"> | grep Hostname; done | sort | uniq -c | sort -nr"</span>
<span class="c"># =&gt;      54 Hostname: webpod2</span>
<span class="c">#         46 Hostname: webpod1</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc zsh <span class="nt">-c</span> <span class="s2">"for i in {1..100}; do curl -s </span><span class="nv">$SVC2EXIP</span><span class="s2"> | grep Hostname; done | sort | uniq -c | sort -nr"</span>
<span class="c"># =&gt;      56 Hostname: webpod1</span>
<span class="c">#         44 Hostname: webpod2</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc zsh <span class="nt">-c</span> <span class="s2">"for i in {1..100}; do curl -s </span><span class="nv">$SVC3EXIP</span><span class="s2"> | grep Hostname; done | sort | uniq -c | sort -nr"</span>
<span class="c"># =&gt;      53 Hostname: webpod1</span>
<span class="c">#         47 Hostname: webpod2</span>

<span class="c"># 지속적으로 반복 접속</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc zsh <span class="nt">-c</span> <span class="s2">"while true; do curl -s --connect-timeout 1 </span><span class="nv">$SVC1EXIP</span><span class="s2"> | egrep 'Hostname|RemoteAddr'; date '+%Y-%m-%d %H:%M:%S' ; echo ;  sleep 1; done"</span>
<span class="c"># =&gt; Hostname: webpod1</span>
<span class="c">#    RemoteAddr: 172.20.0.3:39516</span>
<span class="c">#    2024-01-01 11:22:10</span>
<span class="c">#    </span>
<span class="c">#    Hostname: webpod1</span>
<span class="c">#    RemoteAddr: 172.20.0.3:20966</span>
<span class="c">#    2024-01-01 11:22:11</span>
<span class="c">#    </span>
<span class="c">#    Hostname: webpod2</span>
<span class="c">#    RemoteAddr: 172.20.0.3:3638</span>
<span class="c">#    2024-01-01 11:22:12</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc zsh <span class="nt">-c</span> <span class="s2">"while true; do curl -s --connect-timeout 1 </span><span class="nv">$SVC2EXIP</span><span class="s2"> | egrep 'Hostname|RemoteAddr'; date '+%Y-%m-%d %H:%M:%S' ; echo ;  sleep 1; done"</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc zsh <span class="nt">-c</span> <span class="s2">"while true; do curl -s --connect-timeout 1 </span><span class="nv">$SVC3EXIP</span><span class="s2"> | egrep 'Hostname|RemoteAddr'; date '+%Y-%m-%d %H:%M:%S' ; echo ;  sleep 1; done"</span>

<span class="c"># LoadBalancer Type은 기본값으로 NodePort 포함. NodePort 서비스는 ClusterIP 를 포함</span>
<span class="c"># NodePort:PORT 및 CLUSTER-IP:PORT 로 접속 가능!</span>
<span class="nv">$ </span>kubectl get svc svc1
<span class="c"># =&gt; NAME   TYPE           CLUSTER-IP    EXTERNAL-IP      PORT(S)        AGE</span>
<span class="c">#    svc1   LoadBalancer   10.200.1.89   172.20.255.200   80:30613/TCP   22m</span>

<span class="c"># 컨트롤노드에서 각각 접속 확인 실행 해보자</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane curl <span class="nt">-s</span> 127.0.0.1:30613 <span class="c"># NodePort Type</span>
<span class="c"># =&gt; Hostname: webpod2</span>
<span class="c">#    IP: 127.0.0.1</span>
<span class="c">#    IP: 10.10.2.3</span>
<span class="c">#    RemoteAddr: 172.20.0.5:44387</span>
<span class="c">#    GET / HTTP/1.1</span>
<span class="c">#    Host: 127.0.0.1:30613</span>
<span class="c">#    User-Agent: curl/7.88.1</span>
<span class="c">#    Accept: */*    </span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane curl <span class="nt">-s</span> 10.200.1.89     <span class="c"># ClusterIP Tpye</span>
<span class="c"># =&gt; Hostname: webpod2</span>
<span class="c">#    IP: 127.0.0.1</span>
<span class="c">#    IP: 10.10.2.3</span>
<span class="c">#    RemoteAddr: 172.20.0.5:28647</span>
<span class="c">#    GET / HTTP/1.1</span>
<span class="c">#    Host: 10.200.1.89</span>
<span class="c">#    User-Agent: curl/7.88.1</span>
<span class="c">#    Accept: */*</span>
</code></pre></div></div>

<h5 id="failover-테스트">Failover 테스트</h5>

<p><img src="/assets/2024/kans-3th/w5/20241005_kans_w5_13.png" alt="img.png" /></p>

<ul>
  <li>위의 그림처럼 장애 발생전에 워커노드 1의 스피커 파드가 SVC1, SVC2 서비스의 리더 역할을 하고 있는 상태에서,
워커노드 1에 장애가 발생하면, 남아있는 스피커 파드들이 워커노드 1의 장애 상황을 인지하게 됩니다.</li>
  <li>이후 장애가 발생한 스피커 파드가 소유한 ExternalIP에 대해 리더파드를 다시 선출하고 GARP로 새로 선출된 리더파드의 MAC 주소를 전파합니다.</li>
  <li>다만 장애 발생으로 문제를 인식하는 시간과 ARP 정보가 전파되는 시간, 그리고 클라이언트의 ARP 캐시 갱신 시간 등을 
고려하면 20초~1분 이내의 장애 지속시간이 발생할 수 있습니다.</li>
  <li>현재 실습에서 SVC1 =&gt; worker node 3, SVC2 =&gt; worker node 2, SVC3 =&gt; worker node 1 에 배포되어 있는 상태에서
워커노드 중 1대를 중지하여 장애를 발생시키고, 장애시간을 확인해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 사전 준비</span>
<span class="c">## 지속적으로 반복 접속</span>
<span class="nv">$ SVC1EXIP</span><span class="o">=</span><span class="si">$(</span>kubectl get svc svc1 <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.status.loadBalancer.ingress[0].ip}'</span><span class="si">)</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc zsh <span class="nt">-c</span> <span class="s2">"while true; do curl -s --connect-timeout 1 </span><span class="nv">$SVC1EXIP</span><span class="s2"> | egrep 'Hostname|RemoteAddr'; date '+%Y-%m-%d %H:%M:%S' ; echo ;  sleep 1; done"</span>

<span class="c">## 상태 모니터링</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> kubectl get pod,svc,ep

<span class="c">## 실시간 로그 확인</span>
<span class="nv">$ </span>kubectl logs <span class="nt">-n</span> metallb-system <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>metallb <span class="nt">-f</span>
<span class="c"># 혹은</span>
<span class="nv">$ </span>kubectl stern <span class="nt">-n</span> metallb-system <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>metallb


<span class="c"># 장애 재연</span>
<span class="c">## 리더 Speaker 파드가 존재하는 노드(실제는 컨테이너)를 중지</span>
<span class="c"># $ docker stop &lt;svc1 번 리더 Speaker 파드가 존재하는 노드(실제는 컨테이너)&gt; --signal 9</span>
<span class="nv">$ </span>docker stop myk8s-worker <span class="nt">--signal</span> 9
<span class="c"># 혹은</span>
<span class="c"># $ docker stop &lt;svc1 번 리더 Speaker 파드가 존재하는 노드(실제는 컨테이너)&gt; --signal 15</span>
<span class="nv">$ </span>docker stop myk8s-worker <span class="nt">--signal</span> 15

<span class="nv">$ </span>docker ps <span class="nt">-a</span>
<span class="nv">$ </span>docker ps <span class="nt">-a</span> | <span class="nb">grep </span>worker<span class="err">$</span>
<span class="c"># =&gt; 242777ad8f3c   kindest/node:v1.31.0                 "/usr/local/bin/entr…"   6 hours ago     Exited (130) 7 minutes ago    myk8s-worker</span>

<span class="c">## 지속적으로 반복 접속 상태 모니터링</span>
<span class="c">### curl 연속 접속 시도 &gt;&gt; 대략 10초 이내에 정상 접근 되었지만, 20초까지는 불안정하게 접속이 되었다</span>
<span class="c">### 실제로는 다른 노드의 speaker 파드가 리더가 되고, 이후 다시 노드(컨테이너)가 정상화되면, 다시 리더 speaker 가 됨</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc zsh <span class="nt">-c</span> <span class="s2">"while true; do curl -s --connect-timeout 1 </span><span class="nv">$SVC1EXIP</span><span class="s2"> | egrep 'Hostname|RemoteAddr'; date '+%Y-%m-%d %H:%M:%S' ; echo ;  sleep 1; done"</span>
<span class="c"># =&gt; Hostname: webpod2</span>
<span class="c">#    RemoteAddr: 172.20.0.3:25432</span>
<span class="c">#    2024-10-05 12:04:30</span>
<span class="c">#    </span>
<span class="c">#    2024-10-05 12:04:32</span>
<span class="c">#    </span>
<span class="c">#    2024-10-05 12:04:34</span>
<span class="c">#    </span>
<span class="c">#    Hostname: webpod2</span>
<span class="c">#    RemoteAddr: 172.20.0.3:18511</span>
<span class="c">#    2024-10-05 12:04:35</span>
<span class="c">#    </span>
<span class="c">#    2024-10-05 12:04:37</span>
<span class="c">#    </span>
<span class="c">#    2024-10-05 12:04:39</span>
<span class="c">#    ...</span>

<span class="c"># 변경된 리더 Speaker 파드 확인</span>
<span class="c"># mypc/mypc2 에서 현재 SVC EXTERNAL-IP를 담당하는 리더 Speaker 파드 찾기</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="nv">$SVC1EXIP</span> <span class="nv">$SVC2EXIP</span> <span class="nv">$SVC3EXIP</span><span class="p">;</span> <span class="k">do </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc ping <span class="nt">-c</span> 1 <span class="nt">-w</span> 1 <span class="nt">-W</span> 1 <span class="nv">$i</span><span class="p">;</span> <span class="k">done</span>

<span class="c"># mypc/mypc2 에서 arp 테이블 정보 확인 &gt;&gt; SVC IP별로 리더 파드(스피커) 역할의 노드를 확인!</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc ip <span class="nt">-c</span> neigh | <span class="nb">sort</span>
<span class="c"># =&gt; &lt;span style="color:purple;"&gt;172.20.0.1 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:1f:41:79:66 &lt;/span&gt;STALE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.0.2 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:02 &lt;/span&gt;STALE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.0.3 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:03 &lt;/span&gt;STALE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.0.4 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:04 &lt;/span&gt;STALE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.0.5 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:05 &lt;/span&gt;STALE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.255.200 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:03 &lt;/span&gt;REACHABLE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.255.201 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:02 &lt;/span&gt;REACHABLE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.255.202 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:03 &lt;/span&gt;REACHABLE </span>
<span class="nv">$ </span>kubectl get node <span class="nt">-owide</span> <span class="c"># mac 주소에 매칭되는 IP(노드) 찾기</span>
<span class="c"># =&gt; NAME                  STATUS     ROLES           AGE     VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                         KERNEL-VERSION     CONTAINER-RUNTIME</span>
<span class="c">#    myk8s-control-plane   Ready      control-plane   5h47m   v1.31.0   172.20.0.5    &amp;lt;none&amp;gt;        Debian GNU/Linux 12 (bookworm)   5.10.76-linuxkit   containerd://1.7.18</span>
<span class="c">#    myk8s-worker          NotReady   &amp;lt;none&amp;gt;          5h47m   v1.31.0   172.20.0.4    &amp;lt;none&amp;gt;        Debian GNU/Linux 12 (bookworm)   5.10.76-linuxkit   containerd://1.7.18</span>
<span class="c">#    myk8s-worker2         Ready      &amp;lt;none&amp;gt;          5h47m   v1.31.0   172.20.0.2    &amp;lt;none&amp;gt;        Debian GNU/Linux 12 (bookworm)   5.10.76-linuxkit   containerd://1.7.18</span>
<span class="c">#    myk8s-worker3         Ready      &amp;lt;none&amp;gt;          5h47m   v1.31.0   172.20.0.3    &amp;lt;none&amp;gt;        Debian GNU/Linux 12 (bookworm)   5.10.76-linuxkit   containerd://1.7.18</span>

<span class="c"># &lt;span style="color: green;"&gt;원래 리더 Speaker 파드가 존재했던 myk8s-worker 노드가 아닌&lt;/span&gt; </span>
<span class="c"># &lt;span style="color: green;"&gt;myk8s-worker3 노드가 리더 Speaker 파드를 가지고 있음을 확인할 수 있습니다.&lt;/span&gt;</span>

<span class="c"># 장애 원복(노드 정상화)</span>
<span class="c">## 노드(실제 컨테이너) 정상화 </span>
<span class="c"># $ docker start &lt;svc1 번 리더 Speaker 파드가 존재하는 노드(실제는 컨테이너)&gt;</span>
<span class="nv">$ </span>docker start myk8s-worker

<span class="c"># 변경된 리더 Speaker 파드 확인</span>
<span class="c"># mypc/mypc2 에서 현재 SVC EXTERNAL-IP를 담당하는 리더 Speaker 파드 찾기</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="nv">$SVC1EXIP</span> <span class="nv">$SVC2EXIP</span> <span class="nv">$SVC3EXIP</span><span class="p">;</span> <span class="k">do </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc ping <span class="nt">-c</span> 1 <span class="nt">-w</span> 1 <span class="nt">-W</span> 1 <span class="nv">$i</span><span class="p">;</span> <span class="k">done</span>

<span class="c"># mypc/mypc2 에서 arp 테이블 정보 확인 &gt;&gt; SVC IP별로 리더 파드(스피커) 역할의 노드를 확인!</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc ip <span class="nt">-c</span> neigh | <span class="nb">sort</span>
<span class="c"># =&gt; &lt;span style="color:purple;"&gt;172.20.0.1 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:1f:41:79:66 &lt;/span&gt;STALE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.0.2 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:02 &lt;/span&gt;STALE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.0.3 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:03 &lt;/span&gt;STALE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.0.4 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:04 &lt;/span&gt;STALE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.0.5 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:05 &lt;/span&gt;STALE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.255.200 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:03 &lt;/span&gt;REACHABLE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.255.201 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:02 &lt;/span&gt;REACHABLE </span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.255.202 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;lladdr &lt;span style="color:olive;"&gt;02:42:ac:14:00:04 &lt;/span&gt;REACHABLE </span>
<span class="nv">$ </span>kubectl get node <span class="nt">-owide</span> <span class="c"># mac 주소에 매칭되는 IP(노드) 찾기</span>

<span class="c"># &lt;span style="color: green;"&gt;myk8s-worker를 복구하니 SVC3의 리더 스피커 파드가 다시 myk8s-worker가 되었습니다.&lt;/span&gt; </span>
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w5/20241005_kans_w5_14.png" alt="20241005_kans_w5_14.png" class="image-center" />
<em class="image-caption">장애발생후 복구 되기까지 실습 화면</em></p>

<h5 id="옵션-externaltrafficpolicy-local">(옵션) externalTrafficPolicy: Local</h5>

<ul>
  <li>LoadBalancer도 NodePort와 마찬가지로 externalTrafficPolicy 옵션을 사용할 수 있습니다.</li>
  <li>설정 방법</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl patch svc svc1 <span class="nt">-p</span> <span class="s1">'{"spec":{"externalTrafficPolicy": "Local"}}'</span>
<span class="nv">$ </span>kubectl patch svc svc2 <span class="nt">-p</span> <span class="s1">'{"spec":{"externalTrafficPolicy": "Local"}}'</span>
<span class="nv">$ </span>kubectl patch svc svc3 <span class="nt">-p</span> <span class="s1">'{"spec":{"externalTrafficPolicy": "Local"}}'</span>
</code></pre></div></div>

<ul>
  <li>클라이언트에서 서비스의 External IP로 접속시, 리더 스피커 노드에 위치한 애플리케이션 파드로만 접속이 되며, 클라이언트 IP가 보존됩니다.</li>
  <li>단점
    <ul>
      <li>부하분산이 되지 않아 비효율적입니다.</li>
      <li>리더 노드에 애플리케이션 파드가 없을 경우 서비스 접속이 불가능합니다.</li>
    </ul>
  </li>
  <li>따라서 MetalLB에서는 externalTrafficPolicy: Local 옵션을 사용하지 않는 것을 권장합니다.</li>
</ul>

<h4 id="metallb---bgp-모드">MetalLB - BGP 모드</h4>

<ul>
  <li>현재 실습환경이 KIND 여서 BGP 모드는 실습을 못해보는것 같습니다.</li>
  <li>향후에 baremetal이나 VM으로 구성된 클러스터에서 BGP 모드를 실습해 보고 이번에는 이론만 살펴보겠습니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w5/20241005_kans_w5_15.png" alt="img.png" /></p>

<ul>
  <li>BGP 모드에서는 ARP를 사용하지 않고, BGP 데몬을 사용하여, 클러스터 외부의 라우터에 External IP를 전파합니다.</li>
  <li>ARP 모드는 스피커 리더가 있는 노드로만 트래픽이 전달되었지만, BGP 모드에서는 ECMP를 지원하여 여러 노드에 서비스를 분산시킬 수 있습니다.</li>
  <li>BGP 패킷을 캡쳐해보면 아래와 같이 Service의 External IP를 전파하는 것을 확인할 수 있습니다.
<img src="/assets/2024/kans-3th/w5/20241005_kans_w5_16.png" alt="img.png" /></li>
  <li>이때는 <code class="language-plaintext highlighter-rouge">externalTrafficPolicy: Local</code> 의 사용을 적극 권장합니다.</li>
  <li>또한 Failover가 매우 빠르며 거의 무중단으로 서비스가 가능합니다.</li>
</ul>

<h5 id="bgp-모드-설정">BGP 모드 설정</h5>

<ul>
  <li>
    <p>MetalLB의 BGP 모드 설정은 ConfigMap을 통해 설정합니다.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl replace --force -f -
apiVersion: v1
kind: ConfigMap
metadata:
  namespace: metallb-system
  name: config
data:
  config: |
    peers:
    - peer-address: 192.168.10.254
      peer-asn: 64513
      my-asn: 64512
    address-pools:
    - name: default
      protocol: bgp
      avoid-buggy-ips: true
      addresses:
      - 172.20.1.0/24
</span><span class="no">EOF
</span></code></pre></div>    </div>
  </li>
  <li>
    <p>리눅스 라우터에 BGP 설정 예시</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>router bgp 64513
  bgp router-id 192.168.10.254
  maximum-paths 4
  network 10.1.1.0/24
  neighbor 192.168.10.10  remote-as 64512
  neighbor 192.168.10.101 remote-as 64512
  neighbor 192.168.10.102 remote-as 64512
  ...
</code></pre></div>    </div>
  </li>
</ul>

<hr />

<h2 id="externalip-서비스">ExternalIP 서비스</h2>

<ul>
  <li>ExternalIP 서비스는 NodePort 서비스와 유사하게 외부 IP를 제공하는 서비스입니다.</li>
  <li>ExternalIP 서비스는 특정 노드IP로 인입한 트래픽을 해당 노드의 파드로 전달해서 외부에서 접속할 수 있게 합니다.</li>
  <li>단 사용을 권장하고 있지는 않으며, 특별한 이유가 없다면 NodePort를 사용하는것이 좋습니다.</li>
</ul>

<table>
  <thead>
    <tr>
      <th>설정 항목</th>
      <th>설명</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>spec.externalIPs</td>
      <td>노드 IP 주소(ExternalIP)</td>
    </tr>
    <tr>
      <td>spec.ports[].port</td>
      <td>ExternalIP 와 ClusterIP 에서 수신할 포트 번호</td>
    </tr>
    <tr>
      <td>spec.ports[].targetPort</td>
      <td>목적지 컨테이너 포트 번호</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>실습</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl create -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deploy-echo
spec:
  replicas: 2
  selector:
    matchLabels:
      app: deploy-websrv
  template:
    metadata:
      labels:
        app: deploy-websrv
    spec:
      terminationGracePeriodSeconds: 0
      containers:
      - name: ndks-websrv
        image: k8s.gcr.io/echoserver:1.5
        ports:
        - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: svc-externalip
spec:
  type: ClusterIP
  externalIPs:
    - 192.168.10.101
    - 192.168.10.102
  ports:
    - name: svc-webport
      port: 9000
      targetPort: 8080
  selector:
    app: deploy-websrv
</span><span class="no">EOF
</span><span class="c"># =&gt; deployment.apps/deploy-echo created</span>
<span class="c">#    service/svc-externalip created</span>

<span class="c"># 확인 : ExternalIP 도 결국 ClusterIP를 사용(포함)</span>
<span class="nv">$ </span>kubectl get svc svc-externalip
<span class="c"># =&gt; NAME             TYPE        CLUSTER-IP    EXTERNAL-IP                     PORT(S)    AGE</span>
<span class="c">#    svc-externalip   ClusterIP   10.200.1.42   192.168.10.101,192.168.10.102   9000/TCP   14s</span>

<span class="nv">$ </span>kubectl describe svc svc-externalip
<span class="c"># =&gt; Name:                     svc-externalip</span>
<span class="c">#    Namespace:                default</span>
<span class="c">#    Labels:                   &amp;lt;none&amp;gt;</span>
<span class="c">#    Annotations:              &amp;lt;none&amp;gt;</span>
<span class="c">#    Selector:                 app=deploy-websrv</span>
<span class="c">#    Type:                     ClusterIP</span>
<span class="c">#    IP Family Policy:         SingleStack</span>
<span class="c">#    IP Families:              IPv4</span>
<span class="c">#    IP:                       10.200.1.42</span>
<span class="c">#    IPs:                      10.200.1.42</span>
<span class="c">#    External IPs:             192.168.10.101,192.168.10.102</span>
<span class="c">#    Port:                     svc-webport  9000/TCP</span>
<span class="c">#    TargetPort:               8080/TCP</span>
<span class="c">#    Endpoints:                10.10.1.3:8080,10.10.3.2:8080</span>
<span class="c">#    Session Affinity:         None</span>
<span class="c">#    External Traffic Policy:  Cluster</span>
<span class="c">#    Internal Traffic Policy:  Cluster</span>
<span class="c">#    Events:                   &amp;lt;none&amp;gt;</span>

<span class="c"># ExternalTrafficPolicy 설정이 없음</span>
<span class="nv">$ </span>kubectl get svc svc-externalip <span class="nt">-o</span> yaml
<span class="c"># =&gt; apiVersion: v1</span>
<span class="c">#    kind: Service</span>
<span class="c">#    metadata:</span>
<span class="c">#      creationTimestamp: &amp;quot;2024-10-05T12:45:06Z&amp;quot;</span>
<span class="c">#      name: svc-externalip</span>
<span class="c">#      namespace: default</span>
<span class="c">#      resourceVersion: &amp;quot;38088&amp;quot;</span>
<span class="c">#      uid: b64588f5-1589-4b9b-9652-b5979f8872a1</span>
<span class="c">#    spec:</span>
<span class="c">#      clusterIP: 10.200.1.42</span>
<span class="c">#      clusterIPs:</span>
<span class="c">#      - 10.200.1.42</span>
<span class="c">#      externalIPs:</span>
<span class="c">#      - 192.168.10.101</span>
<span class="c">#      - 192.168.10.102</span>
<span class="c">#      externalTrafficPolicy: Cluster</span>
<span class="c">#      internalTrafficPolicy: Cluster</span>
<span class="c">#      ipFamilies:</span>
<span class="c">#      - IPv4</span>
<span class="c">#      ipFamilyPolicy: SingleStack</span>
<span class="c">#      ports:</span>
<span class="c">#      - name: svc-webport</span>
<span class="c">#        port: 9000</span>
<span class="c">#        protocol: TCP</span>
<span class="c">#        targetPort: 8080</span>
<span class="c">#      selector:</span>
<span class="c">#        app: deploy-websrv</span>
<span class="c">#      sessionAffinity: None</span>
<span class="c">#      type: ClusterIP</span>
<span class="c">#    status:</span>
<span class="c">#      loadBalancer: {}</span>
</code></pre></div></div>

<hr />

<h2 id="ipvs-proxy-모드">IPVS Proxy 모드</h2>

<h3 id="ipvs-proxy-모드-소개">IPVS Proxy 모드 소개</h3>

<ul>
  <li>IPVS Proxy 모드는 지난주에 살펴보았던 <strong>kube-proxy의 모드중 하나</strong>로, 리눅스 <strong>커널의 IPVS 기능을 사용하여 로드밸런싱을 수행</strong>합니다.</li>
  <li>IPVS는 L4 레이어에서 동작하며, kube-proxy의 iptables 모드보다 <strong>성능이 우수</strong>하고, 대규모 클러스터에서 더 <strong>효율적으로 동작</strong>합니다.</li>
  <li>iptables이 비해 좀 더 높은 성능을 보여주며, 규칙 갯수도 줄일 수 있습니다.</li>
  <li>부하분산 알고리즘도 다음과 같이 다양하게 지원합니다.
    <ul>
      <li>라운드 로빈 (Round Robin) : 우선순위를 두지 않고 요청을 순차적으로 전달합니다.
        <ul>
          <li>가중치 라운드 로빈 (Weighted Round Robin) : 서버에 가중치를 부여하여 요청을 전달합니다.</li>
        </ul>
      </li>
      <li>최소 연결 (Least Connection) : 현재 연결 수가 가장 적은 서버로 요청을 전달합니다.
        <ul>
          <li>가중치 최소 연결 (Weighted Least Connection) : 서버에 가중치를 부여하여 연결 수가 가장 적은 서버로 요청을 전달합니다.</li>
          <li>지역성 기반 최소 연결 (Locality-Based Least Connection) : 클라이언트와 가까우면서 요청이 적은 서버로 요청을 전달합니다.</li>
        </ul>
      </li>
      <li>목적지 해싱 (Destination Hashing) : 요청의 목적지 IP 주소를 해싱하여 서버를 선택합니다.</li>
      <li>출발지 해싱 (Source Hashing) : 요청의 출발지 IP 주소를 해싱하여 서버를 선택합니다.</li>
      <li>최단 지연 (Shortest Expected Delay) : 서버의 응답 지연 시간을 고려하여 서버를 선택합니다.</li>
      <li>큐잉 방지 (Never Queue) : 연결이 없는 서버에 우선적으로 트래픽을 보내고, 모든 서버에 트래픽이 있으면 최단 지연 방식으로 트래픽을 보냅니다.</li>
    </ul>
  </li>
</ul>

<h3 id="ipvs-proxy-모드-실습">IPVS Proxy 모드 실습</h3>

<h4 id="실습환경-설정">실습환경 설정</h4>

<ul>
  <li>먼저 기존 실습 환경을 삭제합니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kind delete cluster <span class="nt">--name</span> myk8s
<span class="c"># =&gt; Deleting cluster "myk8s" ...</span>
<span class="c">#    Deleted nodes: ["myk8s-worker" "myk8s-control-plane" "myk8s-worker2" "myk8s-worker3"]</span>
</code></pre></div></div>

<ul>
  <li>실습환경은 KIND를 사용하며, KIND 클러스터에 IPVS Proxy 모드를 적용해보겠습니다.</li>
  <li>실습 환경 : K8S v1.31.0, CNI(Kindnet / Direct Routing mode),  IPVS proxy mode
    <ul>
      <li>노드(실제로는 컨테이너) 네트워크 대역 : 172.20.0.0/16</li>
      <li>파드 사용 네트워크 대역 : 10.10.0.0/16 ⇒ 각각 10.10.1.0/24, 10.10.2.0/24, 10.10.3.0/24, 10.10.4.0/24</li>
      <li>서비스 사용 네트워크 대역 : 10.200.1.0/24</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/2024/kans-3th/w5/20241005_kans_w5_18.png" alt="img.png" class="image-center" /></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 파일 작성</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOT</span><span class="sh">&gt; kind-svc-2w-ipvs.yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
featureGates:
  "InPlacePodVerticalScaling": true
  "MultiCIDRServiceAllocator": true
nodes:
- role: control-plane
  labels:
    mynode: control-plane
    topology.kubernetes.io/zone: ap-northeast-2a
  extraPortMappings:
  - containerPort: 30000
    hostPort: 30000
  - containerPort: 30001
    hostPort: 30001
  - containerPort: 30002
    hostPort: 30002
  - containerPort: 30003
    hostPort: 30003
  - containerPort: 30004
    hostPort: 30004
  kubeadmConfigPatches:
  - |
    kind: ClusterConfiguration
    apiServer:
      extraArgs:
        runtime-config: api/all=true
    controllerManager:
      extraArgs:
        bind-address: 0.0.0.0
    etcd:
      local:
        extraArgs:
          listen-metrics-urls: http://0.0.0.0:2381
    scheduler:
      extraArgs:
        bind-address: 0.0.0.0
  - |
    kind: KubeProxyConfiguration
    metricsBindAddress: 0.0.0.0
    ipvs:
      strictARP: true
- role: worker
  labels:
    mynode: worker1
    topology.kubernetes.io/zone: ap-northeast-2a
- role: worker
  labels:
    mynode: worker2
    topology.kubernetes.io/zone: ap-northeast-2b
- role: worker
  labels:
    mynode: worker3
    topology.kubernetes.io/zone: ap-northeast-2c
networking:
  podSubnet: 10.10.0.0/16
  serviceSubnet: 10.200.1.0/24
  kubeProxyMode: "ipvs"        
</span><span class="no">EOT

</span><span class="c"># k8s 클러스터 설치</span>
<span class="nv">$ </span>kind create cluster <span class="nt">--config</span> kind-svc-2w-ipvs.yaml <span class="nt">--name</span> myk8s <span class="nt">--image</span> kindest/node:v1.31.0
<span class="nv">$ </span>docker ps
<span class="c"># =&gt; CONTAINER ID   IMAGE                  COMMAND                  CREATED         STATUS         PORTS                                                             NAMES</span>
<span class="c">#    aa2f031f0959   kindest/node:v1.31.0   &amp;quot;/usr/local/bin/entr…&amp;quot;   2 minutes ago   Up 2 minutes                                                                     myk8s-worker3</span>
<span class="c">#    0a67b245f7ee   kindest/node:v1.31.0   &amp;quot;/usr/local/bin/entr…&amp;quot;   2 minutes ago   Up 2 minutes   0.0.0.0:30000-30004-&amp;gt;30000-30004/tcp, 127.0.0.1:49623-&amp;gt;6443/tcp   myk8s-control-plane</span>
<span class="c">#    4525aac3b06f   kindest/node:v1.31.0   &amp;quot;/usr/local/bin/entr…&amp;quot;   2 minutes ago   Up 2 minutes                                                                     myk8s-worker2</span>
<span class="c">#    0087fe52e3d2   kindest/node:v1.31.0   &amp;quot;/usr/local/bin/entr…&amp;quot;   2 minutes ago   Up 2 minutes                                                                     myk8s-worker</span>

<span class="c"># 노드에 기본 툴 설치</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane sh <span class="nt">-c</span> <span class="s1">'apt update &amp;&amp; apt install tree psmisc lsof wget bsdmainutils bridge-utils net-tools dnsutils ipset ipvsadm nfacct tcpdump ngrep iputils-ping arping git vim arp-scan -y'</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> sh <span class="nt">-c</span> <span class="s1">'apt update &amp;&amp; apt install tree psmisc lsof wget bsdmainutils bridge-utils net-tools dnsutils ipset ipvsadm nfacct tcpdump ngrep iputils-ping arping -y'</span><span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>

<span class="c"># kube-proxy configmap 확인</span>
<span class="nv">$ </span>kubectl describe cm <span class="nt">-n</span> kube-system kube-proxy
<span class="c"># =&gt; ...</span>
<span class="c">#    mode: ipvs</span>
<span class="c">#    ipvs: # 아래 각각 옵션 의미 조사해봅시다.</span>
<span class="c">#      excludeCIDRs: null   # IPVS에서 제외할 CIDR을 지정합니다. IPVS 룰을 정리할때 제외할 대역을 지정합니다.</span>
<span class="c">#      minSyncPeriod: 0s    # IPVS 룰을 동기화할 최소 주기를 지정합니다. 이 옵션을 사용하면 IPVS 룰을 동기화하는 주기를 제한할 수 있습니다.</span>
<span class="c">#      scheduler: ""        # IPVS 스케줄러는 IPVS가 사용할 로드밸런싱 알고리즘을 지정합니다.</span>
<span class="c">#      strictARP: true      # MetalLB 동작을 위해서 true 설정 변경 필요</span>
<span class="c">#      syncPeriod: 0s       # IPVS 룰을 동기화할 주기를 지정합니다. 이 옵션을 사용하면 IPVS 룰을 동기화하는 주기를 지정할 수 있습니다.</span>
<span class="c">#      tcpFinTimeout: 0s    # IPVS에서 TCP 연결이 종료된 후 FIN 상태를 유지하는 시간을 지정합니다.</span>
<span class="c">#      tcpTimeout: 0s       # IPVS에서 TCP 패킷을 처리하는 시간을 지정합니다.</span>
<span class="c">#      udpTimeout: 0s       # IPVS에서 UDP 패킷을 처리하는 시간을 지정합니다.</span>
<span class="c">#    ...</span>

<span class="c"># strictARP: true는 ARP 패킷을 보다 엄격하게 처리하겠다는 설정입니다.</span>
<span class="c">## IPVS 모드에서 strict ARP가 활성화되면, 노드의 인터페이스는 자신에게 할당된 IP 주소에 대해서만 ARP 응답을 보내게 됩니다. </span>
<span class="c">## 이는 IPVS로 로드밸런싱할 때 ARP 패킷이 잘못된 인터페이스로 전달되는 문제를 방지합니다.</span>
<span class="c">## 이 설정은 특히 클러스터 내에서 여러 노드가 동일한 IP를 갖는 VIP(Virtual IP)를 사용하는 경우 중요합니다.</span>

<span class="c"># 노드 별 네트워트 정보 확인 : kube-ipvs0 네트워크 인터페이스 확인</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> ip <span class="nt">-c</span> route<span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; &amp;gt;&amp;gt; node myk8s-control-plane &amp;lt;&amp;lt;</span>
<span class="c">#    default via &lt;span style="color:purple;"&gt;172.20.0.1 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:purple;"&gt;10.10.0.2 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;vethc61550c2 &lt;/span&gt;scope host </span>
<span class="c">#    &lt;span style="color:purple;"&gt;10.10.0.3 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;veth85b53091 &lt;/span&gt;scope host </span>
<span class="c">#    &lt;span style="color:purple;"&gt;10.10.0.4 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;veth8ad445fd &lt;/span&gt;scope host </span>
<span class="c">#    &lt;span style="color:purple;"&gt;10.10.1.0/24 &lt;/span&gt;via &lt;span style="color:purple;"&gt;172.20.0.3 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:purple;"&gt;10.10.2.0/24 &lt;/span&gt;via &lt;span style="color:purple;"&gt;172.20.0.2 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:purple;"&gt;10.10.3.0/24 &lt;/span&gt;via &lt;span style="color:purple;"&gt;172.20.0.4 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.0.0/16 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;proto kernel scope link src &lt;span style="color:purple;"&gt;172.20.0.5 &lt;/span&gt;</span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; node myk8s-worker &amp;lt;&amp;lt;</span>
<span class="c">#    default via &lt;span style="color:purple;"&gt;172.20.0.1 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:purple;"&gt;10.10.0.0/24 &lt;/span&gt;via &lt;span style="color:purple;"&gt;172.20.0.5 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:purple;"&gt;10.10.1.0/24 &lt;/span&gt;via &lt;span style="color:purple;"&gt;172.20.0.3 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:purple;"&gt;10.10.2.0/24 &lt;/span&gt;via &lt;span style="color:purple;"&gt;172.20.0.2 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:purple;"&gt;172.20.0.0/16 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;eth0 &lt;/span&gt;proto kernel scope link src &lt;span style="color:purple;"&gt;172.20.0.4 &lt;/span&gt;</span>
<span class="c">#    ...</span>

<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> ip <span class="nt">-c</span> addr<span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; &amp;gt;&amp;gt; node myk8s-control-plane &amp;lt;&amp;lt;</span>
<span class="c">#    1: &lt;span style="color:teal;"&gt;lo: &lt;/span&gt;&amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</span>
<span class="c">#        link/loopback &lt;span style="color:olive;"&gt;00:00:00:00:00:00&lt;/span&gt; brd &lt;span style="color:olive;"&gt;00:00:00:00:00:00&lt;/span&gt;</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;127.0.0.1&lt;/span&gt;/8 scope host lo</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    2: &lt;span style="color:teal;"&gt;tunl0@NONE: &lt;/span&gt;&amp;lt;NOARP&amp;gt; mtu 1480 qdisc noop state &lt;span style="color:red;"&gt;DOWN &lt;/span&gt;group default qlen 1000</span>
<span class="c">#        link/ipip &lt;span style="color:olive;"&gt;0.0.0.0&lt;/span&gt; brd &lt;span style="color:olive;"&gt;0.0.0.0&lt;/span&gt;</span>
<span class="c">#    4: &lt;span style="color:teal;"&gt;kube-ipvs0: &lt;/span&gt;&amp;lt;BROADCAST,NOARP&amp;gt; mtu 1500 qdisc noop state &lt;span style="color:red;"&gt;DOWN &lt;/span&gt;group default </span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;0a:91:66:a1:e6:bb&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt;</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;10.200.1.1&lt;/span&gt;/32 scope global kube-ipvs0</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;10.200.1.10&lt;/span&gt;/32 scope global kube-ipvs0</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    5: &lt;span style="color:teal;"&gt;vethc61550c2@if4: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state &lt;span style="color:green;"&gt;UP &lt;/span&gt;group default </span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;de:a6:86:25:bb:08&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt; link-netns cni-4751a95b-cc8c-ff23-9dd5-35e7f1a2223b</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;10.10.0.1&lt;/span&gt;/32 scope global vethc61550c2</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    6: &lt;span style="color:teal;"&gt;veth85b53091@if4: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state &lt;span style="color:green;"&gt;UP &lt;/span&gt;group default </span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;ea:36:d9:10:fc:2f&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt; link-netns cni-db21fe8b-f48c-f3fe-6c6a-b2f204eea0e5</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;10.10.0.1&lt;/span&gt;/32 scope global veth85b53091</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    7: &lt;span style="color:teal;"&gt;veth8ad445fd@if4: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state &lt;span style="color:green;"&gt;UP &lt;/span&gt;group default </span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;b6:bb:1b:44:13:eb&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt; link-netns cni-26749ed0-4863-eef5-640b-96f804e871ae</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;10.10.0.1&lt;/span&gt;/32 scope global veth8ad445fd</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    46: &lt;span style="color:teal;"&gt;eth0@if47: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state &lt;span style="color:green;"&gt;UP &lt;/span&gt;group default </span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;02:42:ac:14:00:05&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt; link-netnsid 0</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;172.20.0.5&lt;/span&gt;/16 brd &lt;span style="color:purple;"&gt;172.20.255.255 &lt;/span&gt;scope global eth0</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; node myk8s-worker &amp;lt;&amp;lt;</span>
<span class="c">#    1: &lt;span style="color:teal;"&gt;lo: &lt;/span&gt;&amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</span>
<span class="c">#        link/loopback &lt;span style="color:olive;"&gt;00:00:00:00:00:00&lt;/span&gt; brd &lt;span style="color:olive;"&gt;00:00:00:00:00:00&lt;/span&gt;</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;127.0.0.1&lt;/span&gt;/8 scope host lo</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    2: &lt;span style="color:teal;"&gt;tunl0@NONE: &lt;/span&gt;&amp;lt;NOARP&amp;gt; mtu 1480 qdisc noop state &lt;span style="color:red;"&gt;DOWN &lt;/span&gt;group default qlen 1000</span>
<span class="c">#        link/ipip &lt;span style="color:olive;"&gt;0.0.0.0&lt;/span&gt; brd &lt;span style="color:olive;"&gt;0.0.0.0&lt;/span&gt;</span>
<span class="c">#    4: &lt;span style="color:teal;"&gt;kube-ipvs0: &lt;/span&gt;&amp;lt;BROADCAST,NOARP&amp;gt; mtu 1500 qdisc noop state &lt;span style="color:red;"&gt;DOWN &lt;/span&gt;group default </span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;fe:2c:45:76:c8:8d&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt;</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;10.200.1.1&lt;/span&gt;/32 scope global kube-ipvs0</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;10.200.1.10&lt;/span&gt;/32 scope global kube-ipvs0</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    44: &lt;span style="color:teal;"&gt;eth0@if45: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state &lt;span style="color:green;"&gt;UP &lt;/span&gt;group default </span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;02:42:ac:14:00:04&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt; link-netnsid 0</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;172.20.0.4&lt;/span&gt;/16 brd &lt;span style="color:purple;"&gt;172.20.255.255 &lt;/span&gt;scope global eth0</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    ...</span>

<span class="c"># &lt;span style="color: green;"&gt;👉 노드별로 kube-ipvs0 인터페이스가 생성되었으며, IP 주소가 할당되어 있습니다.&lt;/span&gt;</span>

<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> ip <span class="nt">-br</span> <span class="nt">-c</span> addr show kube-ipvs0<span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; &amp;gt;&amp;gt; node myk8s-control-plane &amp;lt;&amp;lt;</span>
<span class="c">#    &lt;span style="color:teal;"&gt;kube-ipvs0       &lt;/span&gt;&lt;span style="color:red;"&gt;DOWN           &lt;/span&gt;&lt;span style="color:purple;"&gt;10.200.1.1&lt;/span&gt;/32 &lt;span style="color:purple;"&gt;10.200.1.10&lt;/span&gt;/32 </span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; node myk8s-worker &amp;lt;&amp;lt;</span>
<span class="c">#    &lt;span style="color:teal;"&gt;kube-ipvs0       &lt;/span&gt;&lt;span style="color:red;"&gt;DOWN           &lt;/span&gt;&lt;span style="color:purple;"&gt;10.200.1.1&lt;/span&gt;/32 &lt;span style="color:purple;"&gt;10.200.1.10&lt;/span&gt;/32 </span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; node myk8s-worker2 &amp;lt;&amp;lt;</span>
<span class="c">#    &lt;span style="color:teal;"&gt;kube-ipvs0       &lt;/span&gt;&lt;span style="color:red;"&gt;DOWN           &lt;/span&gt;&lt;span style="color:purple;"&gt;10.200.1.1&lt;/span&gt;/32 &lt;span style="color:purple;"&gt;10.200.1.10&lt;/span&gt;/32 </span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; node myk8s-worker3 &amp;lt;&amp;lt;</span>
<span class="c">#    &lt;span style="color:teal;"&gt;kube-ipvs0       &lt;/span&gt;&lt;span style="color:red;"&gt;DOWN           &lt;/span&gt;&lt;span style="color:purple;"&gt;10.200.1.1&lt;/span&gt;/32 &lt;span style="color:purple;"&gt;10.200.1.10&lt;/span&gt;/32 </span>

<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> ip <span class="nt">-d</span> <span class="nt">-c</span> addr show kube-ipvs0<span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; &amp;gt;&amp;gt; node myk8s-control-plane &amp;lt;&amp;lt;</span>
<span class="c">#    4: &lt;span style="color:teal;"&gt;kube-ipvs0: &lt;/span&gt;&amp;lt;BROADCAST,NOARP&amp;gt; mtu 1500 qdisc noop state &lt;span style="color:red;"&gt;DOWN &lt;/span&gt;group default </span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;0a:91:66:a1:e6:bb&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt; promiscuity 0 minmtu 0 maxmtu 0 </span>
<span class="c">#        dummy numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535 </span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;10.200.1.1&lt;/span&gt;/32 scope global kube-ipvs0</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;10.200.1.10&lt;/span&gt;/32 scope global kube-ipvs0</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; node myk8s-worker &amp;lt;&amp;lt;</span>
<span class="c">#    4: &lt;span style="color:teal;"&gt;kube-ipvs0: &lt;/span&gt;&amp;lt;BROADCAST,NOARP&amp;gt; mtu 1500 qdisc noop state &lt;span style="color:red;"&gt;DOWN &lt;/span&gt;group default </span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;fe:2c:45:76:c8:8d&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt; promiscuity 0 minmtu 0 maxmtu 0 </span>
<span class="c">#        dummy numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535 </span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;10.200.1.1&lt;/span&gt;/32 scope global kube-ipvs0</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;10.200.1.10&lt;/span&gt;/32 scope global kube-ipvs0</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; node myk8s-worker2 &amp;lt;&amp;lt;</span>
<span class="c">#    4: &lt;span style="color:teal;"&gt;kube-ipvs0: &lt;/span&gt;&amp;lt;BROADCAST,NOARP&amp;gt; mtu 1500 qdisc noop state &lt;span style="color:red;"&gt;DOWN &lt;/span&gt;group default </span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;5a:26:49:54:18:21&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt; promiscuity 0 minmtu 0 maxmtu 0 </span>
<span class="c">#        dummy numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535 </span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;10.200.1.1&lt;/span&gt;/32 scope global kube-ipvs0</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;10.200.1.10&lt;/span&gt;/32 scope global kube-ipvs0</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; node myk8s-worker3 &amp;lt;&amp;lt;</span>
<span class="c">#    4: &lt;span style="color:teal;"&gt;kube-ipvs0: &lt;/span&gt;&amp;lt;BROADCAST,NOARP&amp;gt; mtu 1500 qdisc noop state &lt;span style="color:red;"&gt;DOWN &lt;/span&gt;group default </span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;fa:0c:2d:44:52:23&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt; promiscuity 0 minmtu 0 maxmtu 0 </span>
<span class="c">#        dummy numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535 </span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;10.200.1.1&lt;/span&gt;/32 scope global kube-ipvs0</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;10.200.1.10&lt;/span&gt;/32 scope global kube-ipvs0</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>

<span class="c"># kube-ipvs0 에 할당된 IP(기본 IP + 보조 IP들) 정보 확인 </span>
<span class="nv">$ </span>kubectl get svc,ep <span class="nt">-A</span>
<span class="c"># =&gt; NAMESPACE     NAME                 TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)                  AGE</span>
<span class="c">#    default       service/kubernetes   ClusterIP   10.200.1.1    &amp;lt;none&amp;gt;        443/TCP                  31m</span>
<span class="c">#    kube-system   service/kube-dns     ClusterIP   10.200.1.10   &amp;lt;none&amp;gt;        53/UDP,53/TCP,9153/TCP   31m</span>
<span class="c">#    </span>
<span class="c">#    NAMESPACE     NAME                   ENDPOINTS                                            AGE</span>
<span class="c">#    default       endpoints/kubernetes   172.20.0.5:6443                                      31m</span>
<span class="c">#    kube-system   endpoints/kube-dns     10.10.0.3:53,10.10.0.4:53,10.10.0.3:53 + 3 more...   31m</span>

<span class="c"># ipvsadm 툴로 부하분산 되는 정보 확인 : 서비스의 IP와 서비스에 연동되어 있는 파드의 IP 를 확인</span>
<span class="c">## Service IP(VIP) 처리를 ipvs 에서 담당 -&gt; 이를 통해 iptables 에 체인/정책이 상당 수준 줄어듬</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> ipvsadm <span class="nt">-Ln</span> <span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; &amp;gt;&amp;gt; node myk8s-control-plane &amp;lt;&amp;lt;</span>
<span class="c">#    IP Virtual Server version 1.2.1 (size=4096)</span>
<span class="c">#    Prot LocalAddress:Port Scheduler Flags</span>
<span class="c">#      -&amp;gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn</span>
<span class="c">#    TCP  10.200.1.1:443 rr</span>
<span class="c">#      -&amp;gt; 172.20.0.5:6443              Masq    1      3          0         </span>
<span class="c">#    TCP  10.200.1.10:53 rr</span>
<span class="c">#      -&amp;gt; 10.10.0.3:53                 Masq    1      0          0         </span>
<span class="c">#      -&amp;gt; 10.10.0.4:53                 Masq    1      0          0         </span>
<span class="c">#    TCP  10.200.1.10:9153 rr</span>
<span class="c">#      -&amp;gt; 10.10.0.3:9153               Masq    1      0          0         </span>
<span class="c">#      -&amp;gt; 10.10.0.4:9153               Masq    1      0          0         </span>
<span class="c">#    UDP  10.200.1.10:53 rr</span>
<span class="c">#      -&amp;gt; 10.10.0.3:53                 Masq    1      0          0         </span>
<span class="c">#      -&amp;gt; 10.10.0.4:53                 Masq    1      0          0         </span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; node myk8s-worker &amp;lt;&amp;lt;</span>
<span class="c">#    IP Virtual Server version 1.2.1 (size=4096)</span>
<span class="c">#    Prot LocalAddress:Port Scheduler Flags</span>
<span class="c">#      -&amp;gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn</span>
<span class="c">#    TCP  10.200.1.1:443 rr</span>
<span class="c">#      -&amp;gt; 172.20.0.5:6443              Masq    1      0          0         </span>
<span class="c">#    TCP  10.200.1.10:53 rr</span>
<span class="c">#      -&amp;gt; 10.10.0.3:53                 Masq    1      0          0         </span>
<span class="c">#      -&amp;gt; 10.10.0.4:53                 Masq    1      0          0         </span>
<span class="c">#    TCP  10.200.1.10:9153 rr</span>
<span class="c">#      -&amp;gt; 10.10.0.3:9153               Masq    1      0          0         </span>
<span class="c">#      -&amp;gt; 10.10.0.4:9153               Masq    1      0          0         </span>
<span class="c">#    UDP  10.200.1.10:53 rr</span>
<span class="c">#      -&amp;gt; 10.10.0.3:53                 Masq    1      0          0         </span>
<span class="c">#      -&amp;gt; 10.10.0.4:53                 Masq    1      0          0         </span>
<span class="c">#    ...</span>

<span class="c">## IPSET 확인</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker ipset <span class="nt">-h</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker ipset <span class="nt">-L</span>

<span class="c"># iptables 정보 확인 : 정책 갯수를 iptables proxy 모드와 비교해보자</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>filter nat mangle raw <span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; IPTables Type : </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane  iptables <span class="nt">-t</span> <span class="nv">$i</span> <span class="nt">-S</span> <span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>filter nat mangle raw <span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; IPTables Type : </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker  iptables <span class="nt">-t</span> <span class="nv">$i</span> <span class="nt">-S</span> <span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>filter nat mangle raw <span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; IPTables Type : </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker2 iptables <span class="nt">-t</span> <span class="nv">$i</span> <span class="nt">-S</span> <span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>filter nat mangle raw <span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; IPTables Type : </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker3 iptables <span class="nt">-t</span> <span class="nv">$i</span> <span class="nt">-S</span> <span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>

<span class="c"># 각 노드 bash 접속</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane bash
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker bash
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker2 bash
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker3 bash
<span class="nt">----------------------------------------</span>
<span class="nv">$ </span><span class="nb">exit</span>
<span class="nt">----------------------------------------</span>

<span class="c"># mypc 컨테이너 기동 : kind 도커 브리지를 사용하고, 컨테이너 IP를 직접 지정 혹은 IP 지정 없이 배포</span>
<span class="nv">$ </span>docker run <span class="nt">-d</span> <span class="nt">--rm</span> <span class="nt">--name</span> mypc <span class="nt">--network</span> kind <span class="nt">--ip</span> 172.20.0.100 nicolaka/netshoot <span class="nb">sleep </span>infinity
<span class="c"># 혹은</span>
<span class="nv">$ </span>docker run <span class="nt">-d</span> <span class="nt">--rm</span> <span class="nt">--name</span> mypc <span class="nt">--network</span> kind nicolaka/netshoot <span class="nb">sleep </span>infinity

<span class="nv">$ </span>docker ps
<span class="c"># =&gt; CONTAINER ID   IMAGE                  COMMAND                  CREATED          STATUS          PORTS                                                             NAMES</span>
<span class="c">#    16b541ee953e   nicolaka/netshoot      &amp;quot;sleep infinity&amp;quot;         31 seconds ago   Up 31 seconds                                                                     mypc</span>
<span class="c">#    aa2f031f0959   kindest/node:v1.31.0   &amp;quot;/usr/local/bin/entr…&amp;quot;   34 minutes ago   Up 34 minutes                                                                     myk8s-worker3</span>
<span class="c">#    0a67b245f7ee   kindest/node:v1.31.0   &amp;quot;/usr/local/bin/entr…&amp;quot;   34 minutes ago   Up 34 minutes   0.0.0.0:30000-30004-&amp;gt;30000-30004/tcp, 127.0.0.1:49623-&amp;gt;6443/tcp   myk8s-control-plane</span>
<span class="c">#    4525aac3b06f   kindest/node:v1.31.0   &amp;quot;/usr/local/bin/entr…&amp;quot;   34 minutes ago   Up 34 minutes                                                                     myk8s-worker2</span>
<span class="c">#    0087fe52e3d2   kindest/node:v1.31.0   &amp;quot;/usr/local/bin/entr…&amp;quot;   34 minutes ago   Up 34 minutes                                                                     myk8s-worker</span>
</code></pre></div></div>

<h5 id="ipvs-정보-확인">IPVS 정보 확인</h5>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># kube-proxy 로그 확인 :  기본값 부하분산 스케줄러(RoundRobin = RR)</span>
<span class="nv">$ </span>kubectl stern <span class="nt">-n</span> kube-system <span class="nt">-l</span> k8s-app<span class="o">=</span>kube-proxy <span class="nt">--since</span> 2h | egrep <span class="s1">'(ipvs|IPVS)'</span>
<span class="c"># =&gt; ...</span>
<span class="c">#    kube-proxy-24z49 kube-proxy I1005 15:10:04.041490       1 server_linux.go:230] "Using ipvs Proxier"</span>
<span class="c">#    kube-proxy-24z49 kube-proxy I1005 15:10:04.048394       1 proxier.go:364] "IPVS scheduler not specified, use rr by default" ipFamily="IPv4"</span>
<span class="c">#    kube-proxy-24z49 kube-proxy I1005 15:10:04.048529       1 proxier.go:364] "IPVS scheduler not specified, use rr by default" ipFamily="IPv6"</span>

<span class="c"># 기본 모드 정보 확인</span>
<span class="nv">$ </span>kubectl get cm <span class="nt">-n</span> kube-system kube-proxy <span class="nt">-o</span> yaml | egrep <span class="s1">'mode|strictARP|scheduler'</span>
<span class="c"># =&gt;       scheduler: &amp;quot;&amp;quot;</span>
<span class="c">#          strictARP: true</span>
<span class="c">#        mode: ipvs</span>

<span class="c"># ipvsadm 툴로 부하분산 되는 정보 확인 : RR 부하분산 스케줄러 확인</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> ipvsadm <span class="nt">-Ln</span> <span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; &amp;gt;&amp;gt; node myk8s-control-plane &amp;lt;&amp;lt;</span>
<span class="c">#    IP Virtual Server version 1.2.1 (size=4096)</span>
<span class="c">#    Prot LocalAddress:Port Scheduler Flags</span>
<span class="c">#      -&amp;gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn</span>
<span class="c">#    TCP  10.200.1.1:443 rr</span>
<span class="c">#      -&amp;gt; 172.20.0.5:6443              Masq    1      3          0         </span>
<span class="c">#    ...</span>

<span class="c"># 커널 파라미터 확인</span>
<span class="c"># (심화 옵션) strictARP - 링크 설정(유사한)이유</span>
<span class="c"># --ipvs-strict-arp : Enable strict ARP by setting arp_ignore to 1 and arp_announce to 2</span>
<span class="c"># arp_ignore : ARP request 를 받았을때 응답 여부 - 0(ARP 요청 도착시, any Interface 있으면 응답), 1(ARP 요청을 받은 Interface 가 해당 IP일때만 응답)</span>
<span class="c"># arp_announce : ARP request 를 보낼 때 'ARP Sender IP 주소'에 지정 값 - 0(sender IP로 시스템의 any IP 가능), 2(sender IP로 실제 전송하는 Interface 에 IP를 사용)</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker tree /proc/sys/net/ipv4/conf/kube-ipvs0
<span class="c"># =&gt; /proc/sys/net/ipv4/conf/kube-ipvs0</span>
<span class="c">#    |-- ...</span>
<span class="c">#    |-- arp_accept</span>
<span class="c">#    |-- arp_announce</span>
<span class="c">#    `-- ...</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker <span class="nb">cat</span> /proc/sys/net/ipv4/conf/kube-ipvs0/arp_ignore
<span class="c"># =&gt; 0</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker <span class="nb">cat</span> /proc/sys/net/ipv4/conf/kube-ipvs0/arp_announce
<span class="c"># =&gt; 0</span>

<span class="c"># all 은 모든 인터페이스에 영항을 줌, 단 all 과 interface 값이 다를때 우선순위는 커널 파라미터 별로 다르다 - 링크</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker sysctl net.ipv4.conf.all.arp_ignore
<span class="c"># =&gt; net.ipv4.conf.all.arp_ignore = 1</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker sysctl net.ipv4.conf.all.arp_announce
<span class="c"># =&gt; net.ipv4.conf.all.arp_announce = 2</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker sysctl net.ipv4.conf.kube-ipvs0.arp_ignore
<span class="c"># =&gt; net.ipv4.conf.kube-ipvs0.arp_ignore = 0</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker sysctl net.ipv4.conf.kube-ipvs0.arp_announce
<span class="c"># =&gt; net.ipv4.conf.kube-ipvs0.arp_announce = 0</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker sysctl <span class="nt">-a</span> | <span class="nb">grep </span>arp_ignore
<span class="c"># =&gt; net.ipv4.conf.all.arp_ignore = 1</span>
<span class="c">#    net.ipv4.conf.default.arp_ignore = 0</span>
<span class="c">#    net.ipv4.conf.eth0.arp_ignore = 0</span>
<span class="c">#    net.ipv4.conf.ip6tnl0.arp_ignore = 0</span>
<span class="c">#    net.ipv4.conf.kube-ipvs0.arp_ignore = 0</span>
<span class="c">#    net.ipv4.conf.lo.arp_ignore = 0</span>
<span class="c">#    net.ipv4.conf.tunl0.arp_ignore = 0</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker sysctl <span class="nt">-a</span> | <span class="nb">grep </span>arp_announce
<span class="c"># =&gt; net.ipv4.conf.all.arp_announce = 2</span>
<span class="c">#    net.ipv4.conf.default.arp_announce = 0</span>
<span class="c">#    net.ipv4.conf.eth0.arp_announce = 0</span>
<span class="c">#    net.ipv4.conf.ip6tnl0.arp_announce = 0</span>
<span class="c">#    net.ipv4.conf.kube-ipvs0.arp_announce = 0</span>
<span class="c">#    net.ipv4.conf.lo.arp_announce = 0</span>
<span class="c">#    net.ipv4.conf.tunl0.arp_announce = 0</span>

<span class="c"># IPSET 확인</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker ipset <span class="nt">-h</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker ipset <span class="nt">-L</span>
</code></pre></div></div>

<h5 id="목적지backend-파드pod-생성--3podyaml">목적지(backend) 파드(Pod) 생성 : 3pod.yaml</h5>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOT</span><span class="sh">&gt; 3pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: webpod1
  labels:
    app: webpod
spec:
  nodeName: myk8s-worker
  containers:
  - name: container
    image: traefik/whoami
  terminationGracePeriodSeconds: 0
---
apiVersion: v1
kind: Pod
metadata:
  name: webpod2
  labels:
    app: webpod
spec:
  nodeName: myk8s-worker2
  containers:
  - name: container
    image: traefik/whoami
  terminationGracePeriodSeconds: 0
---
apiVersion: v1
kind: Pod
metadata:
  name: webpod3
  labels:
    app: webpod
spec:
  nodeName: myk8s-worker3
  containers:
  - name: container
    image: traefik/whoami
  terminationGracePeriodSeconds: 0
</span><span class="no">EOT
</span></code></pre></div></div>

<h5 id="클라이언트testpod-생성--netpodyaml">클라이언트(TestPod) 생성 : netpod.yaml</h5>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOT</span><span class="sh">&gt; netpod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: net-pod
spec:
  nodeName: myk8s-control-plane
  containers:
  - name: netshoot-pod
    image: nicolaka/netshoot
    command: ["tail"]
    args: ["-f", "/dev/null"]
  terminationGracePeriodSeconds: 0
</span><span class="no">EOT
</span></code></pre></div></div>

<h5 id="서비스clusterip-생성--svc-clusteripyaml">서비스(ClusterIP) 생성 : svc-clusterip.yaml</h5>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOT</span><span class="sh">&gt; svc-clusterip.yaml
apiVersion: v1
kind: Service
metadata:
  name: svc-clusterip
spec:
  ports:
    - name: svc-webport
      port: 9000        # 서비스 IP 에 접속 시 사용하는 포트 port 를 의미
      targetPort: 80    # 타킷 targetPort 는 서비스를 통해서 목적지 파드로 접속 시 해당 파드로 접속하는 포트를 의미
  selector:
    app: webpod         # 셀렉터 아래 app:webpod 레이블이 설정되어 있는 파드들은 해당 서비스에 연동됨
  type: ClusterIP       # 서비스 타입
</span><span class="no">EOT
</span></code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w5/20241005_kans_w5_19.png" alt="img.png" /></p>

<h5 id="생성-및-확인--ipvs-proxy-모드">생성 및 확인 : IPVS Proxy 모드</h5>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 생성</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> 3pod.yaml,netpod.yaml,svc-clusterip.yaml
<span class="c"># =&gt; pod/webpod1 created</span>
<span class="c">#    pod/webpod2 created</span>
<span class="c">#    pod/webpod3 created</span>
<span class="c">#    pod/net-pod created</span>
<span class="c">#    service/svc-clusterip created</span>

<span class="c"># 파드와 서비스 사용 네트워크 대역 정보 확인 </span>
<span class="nv">$ </span>kubectl cluster-info dump | <span class="nb">grep</span> <span class="nt">-m</span> 2 <span class="nt">-E</span> <span class="s2">"cluster-cidr|service-cluster-ip-range"</span>
<span class="c"># =&gt;                             &amp;quot;--service-cluster-ip-range=10.200.1.0/24&amp;quot;,</span>
<span class="c">#                                &amp;quot;--cluster-cidr=10.10.0.0/16&amp;quot;,</span>

<span class="c"># 확인</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-owide</span>
<span class="c"># =&gt; NAME      READY   STATUS    RESTARTS   AGE   IP          NODE                  NOMINATED NODE   READINESS GATES</span>
<span class="c">#    net-pod   1/1     Running   0          36s   10.10.0.5   myk8s-control-plane   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    webpod1   1/1     Running   0          36s   10.10.3.2   myk8s-worker          &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    webpod2   1/1     Running   0          36s   10.10.1.2   myk8s-worker2         &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    webpod3   1/1     Running   0          36s   10.10.2.2   myk8s-worker3         &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="nv">$ </span>kubectl get svc svc-clusterip
<span class="c"># =&gt; NAME            TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)    AGE</span>
<span class="c">#    svc-clusterip   ClusterIP   10.200.1.17   &amp;lt;none&amp;gt;        9000/TCP   44s</span>
<span class="nv">$ </span>kubectl describe svc svc-clusterip
<span class="nv">$ </span>kubectl get endpoints svc-clusterip
<span class="c"># =&gt; NAME            ENDPOINTS                                AGE</span>
<span class="c">#    svc-clusterip   10.10.1.2:80,10.10.2.2:80,10.10.3.2:80   55s</span>
<span class="nv">$ </span>kubectl get endpointslices <span class="nt">-l</span> kubernetes.io/service-name<span class="o">=</span>svc-clusterip
<span class="c"># =&gt; NAME                  ADDRESSTYPE   PORTS   ENDPOINTS                       AGE</span>
<span class="c">#    svc-clusterip-scf9k   IPv4          80      10.10.2.2,10.10.1.2,10.10.3.2   63s</span>

<span class="c"># 노드 별 네트워트 정보 확인 : kube-ipvs0 네트워크 인터페이스 확인</span>
<span class="c">## ClusterIP 생성 시 kube-ipvs0 인터페이스에 ClusterIP 가 할당되는 것을 확인</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> ip <span class="nt">-c</span> addr<span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> ip <span class="nt">-br</span> <span class="nt">-c</span> addr show kube-ipvs0<span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> ip <span class="nt">-d</span> <span class="nt">-c</span> addr show kube-ipvs0<span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>

<span class="c"># 변수 지정</span>
<span class="nv">$ CIP</span><span class="o">=</span><span class="si">$(</span>kubectl get svc svc-clusterip <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.spec.clusterIP}"</span><span class="si">)</span>
<span class="nv">$ CPORT</span><span class="o">=</span><span class="si">$(</span>kubectl get svc svc-clusterip <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.spec.ports[0].port}"</span><span class="si">)</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$CIP</span> <span class="nv">$CPORT</span>
<span class="c"># =&gt; 10.200.1.17 9000</span>

<span class="c"># ipvsadm 툴로 부하분산 되는 정보 확인</span>
<span class="c">## 10.200.1.216(TCP 9000) 인입 시 3곳의 목적지로 라운드로빈(rr)로 부하분산하여 전달됨을 확인 : 모든 노드에서 동일한 IPVS 분산 설정 정보 확인</span>
<span class="c">## 3곳의 목적지는 각각 서비스에 연동된 목적지 파드 3개이며, 전달 시 출발지 IP는 마스커레이딩 변환 처리</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane ipvsadm <span class="nt">-Ln</span> <span class="nt">-t</span> <span class="nv">$CIP</span>:<span class="nv">$CPORT</span>
<span class="c"># =&gt; Prot LocalAddress:Port Scheduler Flags</span>
<span class="c">#      -&amp;gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn</span>
<span class="c">#    TCP  10.200.1.17:9000 rr</span>
<span class="c">#      -&amp;gt; 10.10.1.2:80                 Masq    1      0          0         </span>
<span class="c">#      -&amp;gt; 10.10.2.2:80                 Masq    1      0          0         </span>
<span class="c">#      -&amp;gt; 10.10.3.2:80                 Masq    1      0          0         </span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> ipvsadm <span class="nt">-Ln</span> <span class="nt">-t</span> <span class="nv">$CIP</span>:<span class="nv">$CPORT</span> <span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; &amp;gt;&amp;gt; node myk8s-control-plane &amp;lt;&amp;lt;</span>
<span class="c">#    Prot LocalAddress:Port Scheduler Flags</span>
<span class="c">#      -&amp;gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn</span>
<span class="c">#    TCP  10.200.1.17:9000 rr</span>
<span class="c">#      -&amp;gt; 10.10.1.2:80                 Masq    1      0          0         </span>
<span class="c">#      -&amp;gt; 10.10.2.2:80                 Masq    1      0          0         </span>
<span class="c">#      -&amp;gt; 10.10.3.2:80                 Masq    1      0          0         </span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; node myk8s-worker &amp;lt;&amp;lt;</span>
<span class="c">#    Prot LocalAddress:Port Scheduler Flags</span>
<span class="c">#      -&amp;gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn</span>
<span class="c">#    TCP  10.200.1.17:9000 rr</span>
<span class="c">#      -&amp;gt; 10.10.1.2:80                 Masq    1      0          0         </span>
<span class="c">#      -&amp;gt; 10.10.2.2:80                 Masq    1      0          0         </span>
<span class="c">#      -&amp;gt; 10.10.3.2:80                 Masq    1      0          0         </span>
<span class="c">#    ...</span>

<span class="c"># ipvsadm 툴로 부하분산 되는 현재 연결 정보 확인 : 추가로 --rate 도 있음</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane ipvsadm <span class="nt">-Ln</span> <span class="nt">-t</span> <span class="nv">$CIP</span>:<span class="nv">$CPORT</span> <span class="nt">--stats</span>
<span class="c"># =&gt; Prot LocalAddress:Port               Conns   InPkts  OutPkts  InBytes OutBytes</span>
<span class="c">#      -&amp;gt; RemoteAddress:Port</span>
<span class="c">#    TCP  10.200.1.17:9000                    0        0        0        0        0</span>
<span class="c">#      -&amp;gt; 10.10.1.2:80                        0        0        0        0        0</span>
<span class="c">#      -&amp;gt; 10.10.2.2:80                        0        0        0        0        0</span>
<span class="c">#      -&amp;gt; 10.10.3.2:80                        0        0        0        0        0</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> ipvsadm <span class="nt">-Ln</span> <span class="nt">-t</span> <span class="nv">$CIP</span>:<span class="nv">$CPORT</span> <span class="nt">--stats</span> <span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; &amp;gt;&amp;gt; node myk8s-control-plane &amp;lt;&amp;lt;</span>
<span class="c">#    Prot LocalAddress:Port               Conns   InPkts  OutPkts  InBytes OutBytes</span>
<span class="c">#      -&amp;gt; RemoteAddress:Port</span>
<span class="c">#    TCP  10.200.1.17:9000                    0        0        0        0        0</span>
<span class="c">#      -&amp;gt; 10.10.1.2:80                        0        0        0        0        0</span>
<span class="c">#      -&amp;gt; 10.10.2.2:80                        0        0        0        0        0</span>
<span class="c">#      -&amp;gt; 10.10.3.2:80                        0        0        0        0        0</span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; node myk8s-worker &amp;lt;&amp;lt;</span>
<span class="c">#    Prot LocalAddress:Port               Conns   InPkts  OutPkts  InBytes OutBytes</span>
<span class="c">#      -&amp;gt; RemoteAddress:Port</span>
<span class="c">#    TCP  10.200.1.17:9000                    0        0        0        0        0</span>
<span class="c">#      -&amp;gt; 10.10.1.2:80                        0        0        0        0        0</span>
<span class="c">#      -&amp;gt; 10.10.2.2:80                        0        0        0        0        0</span>
<span class="c">#      -&amp;gt; 10.10.3.2:80                        0        0        0        0        0</span>
<span class="c">#    ...</span>

<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane ipvsadm <span class="nt">-Ln</span> <span class="nt">-t</span> <span class="nv">$CIP</span>:<span class="nv">$CPORT</span> <span class="nt">--rate</span>
<span class="c"># =&gt; Prot LocalAddress:Port                 CPS    InPPS   OutPPS    InBPS   OutBPS</span>
<span class="c">#      -&amp;gt; RemoteAddress:Port</span>
<span class="c">#    TCP  10.200.1.17:9000                    0        0        0        0        0</span>
<span class="c">#      -&amp;gt; 10.10.1.2:80                        0        0        0        0        0</span>
<span class="c">#      -&amp;gt; 10.10.2.2:80                        0        0        0        0        0</span>
<span class="c">#      -&amp;gt; 10.10.3.2:80                        0        0        0        0        0</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> ipvsadm <span class="nt">-Ln</span> <span class="nt">-t</span> <span class="nv">$CIP</span>:<span class="nv">$CPORT</span> <span class="nt">--rate</span> <span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; &amp;gt;&amp;gt; node myk8s-control-plane &amp;lt;&amp;lt;</span>
<span class="c">#    Prot LocalAddress:Port                 CPS    InPPS   OutPPS    InBPS   OutBPS</span>
<span class="c">#      -&amp;gt; RemoteAddress:Port</span>
<span class="c">#    TCP  10.200.1.17:9000                    0        0        0        0        0</span>
<span class="c">#      -&amp;gt; 10.10.1.2:80                        0        0        0        0        0</span>
<span class="c">#      -&amp;gt; 10.10.2.2:80                        0        0        0        0        0</span>
<span class="c">#      -&amp;gt; 10.10.3.2:80                        0        0        0        0        0</span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; node myk8s-worker &amp;lt;&amp;lt;</span>
<span class="c">#    Prot LocalAddress:Port                 CPS    InPPS   OutPPS    InBPS   OutBPS</span>
<span class="c">#      -&amp;gt; RemoteAddress:Port</span>
<span class="c">#    TCP  10.200.1.17:9000                    0        0        0        0        0</span>
<span class="c">#      -&amp;gt; 10.10.1.2:80                        0        0        0        0        0</span>
<span class="c">#      -&amp;gt; 10.10.2.2:80                        0        0        0        0        0</span>
<span class="c">#      -&amp;gt; 10.10.3.2:80                        0        0        0        0        0</span>
<span class="c">#    ...</span>

<span class="c"># iptables 규칙 확인 : ipset list 를 활용</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane iptables <span class="nt">-t</span> nat <span class="nt">-S</span> | <span class="nb">grep </span>KUBE-CLUSTER-IP
<span class="c"># =&gt; -A KUBE-SERVICES ! -s 10.10.0.0/16 -m comment --comment &amp;quot;Kubernetes service cluster ip + port for masquerade purpose&amp;quot; -m set --match-set KUBE-CLUSTER-IP dst,dst -j KUBE-MARK-MASQ</span>
<span class="c">#    -A KUBE-SERVICES -m set --match-set KUBE-CLUSTER-IP dst,dst -j ACCEPT</span>

<span class="c"># ipset list 정보를 확인 : KUBE-CLUSTER-IP 이름은 아래 6개의 IP:Port 조합을 지칭</span>
<span class="c"># 예를 들면 ipset list 를 사용하지 않을 경우 6개의 iptables 규칙이 필요하지만, ipset 사용 시 1개의 규칙으로 가능</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane ipset list KUBE-CLUSTER-IP
<span class="c"># =&gt; Name: KUBE-CLUSTER-IP</span>
<span class="c">#    Type: hash:ip,port</span>
<span class="c">#    Revision: 5</span>
<span class="c">#    Header: family inet hashsize 1024 maxelem 65536</span>
<span class="c">#    Size in memory: 512</span>
<span class="c">#    References: 3</span>
<span class="c">#    Number of entries: 5</span>
<span class="c">#    Members:</span>
<span class="c">#    10.200.1.1,tcp:443</span>
<span class="c">#    10.200.1.10,tcp:9153</span>
<span class="c">#    10.200.1.10,tcp:53</span>
<span class="c">#    10.200.1.17,tcp:9000</span>
<span class="c">#    10.200.1.10,udp:53</span>
</code></pre></div></div>

<h4 id="ipvs-정보-확인-및-서비스-접속-확인">IPVS 정보 확인 및 서비스 접속 확인</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> ipvsadm <span class="nt">-Ln</span> <span class="nt">-t</span> <span class="nv">$CIP</span>:<span class="nv">$CPORT</span> <span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; &amp;gt;&amp;gt; node myk8s-control-plane &amp;lt;&amp;lt;</span>
<span class="c">#    Prot LocalAddress:Port Scheduler Flags</span>
<span class="c">#      -&amp;gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn</span>
<span class="c">#    TCP  10.200.1.17:9000 rr</span>
<span class="c">#      -&amp;gt; 10.10.1.2:80                 Masq    1      0          0         </span>
<span class="c">#      -&amp;gt; 10.10.2.2:80                 Masq    1      0          0         </span>
<span class="c">#      -&amp;gt; 10.10.3.2:80                 Masq    1      0          0         </span>
<span class="c">#    </span>
<span class="c">#    &amp;gt;&amp;gt; node myk8s-worker &amp;lt;&amp;lt;</span>
<span class="c">#    Prot LocalAddress:Port Scheduler Flags</span>
<span class="c">#      -&amp;gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn</span>
<span class="c">#    TCP  10.200.1.17:9000 rr</span>
<span class="c">#      -&amp;gt; 10.10.1.2:80                 Masq    1      0          0         </span>
<span class="c">#      -&amp;gt; 10.10.2.2:80                 Masq    1      0          0         </span>
<span class="c">#      -&amp;gt; 10.10.3.2:80                 Masq    1      0          0         </span>
<span class="c">#    ...</span>

<span class="c"># 변수 지정</span>
<span class="nv">$ CIP</span><span class="o">=</span><span class="si">$(</span>kubectl get svc svc-clusterip <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.spec.clusterIP}"</span><span class="si">)</span>
<span class="nv">$ CPORT</span><span class="o">=</span><span class="si">$(</span>kubectl get svc svc-clusterip <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.spec.ports[0].port}"</span><span class="si">)</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$CIP</span> <span class="nv">$CPORT</span>
<span class="c"># =&gt; 10.200.1.17 9000</span>

<span class="c"># 컨트롤플레인 노드에서 ipvsadm 모니터링 실행 : ClusterIP 접속 시 아래 처럼 연결 정보 확인됨</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s2">"docker exec -it myk8s-control-plane ipvsadm -Ln -t </span><span class="nv">$CIP</span><span class="s2">:</span><span class="nv">$CPORT</span><span class="s2"> --stats; echo; docker exec -it myk8s-control-plane ipvsadm -Ln -t </span><span class="nv">$CIP</span><span class="s2">:</span><span class="nv">$CPORT</span><span class="s2"> --rate"</span>

<span class="c"># --------------------------</span>

<span class="c"># 서비스 IP 변수 지정 : svc-clusterip 의 ClusterIP주소</span>
<span class="nv">$ SVC1</span><span class="o">=</span><span class="si">$(</span>kubectl get svc svc-clusterip <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.spec.clusterIP<span class="o">}</span><span class="si">)</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$SVC1</span>
<span class="c"># =&gt; 10.200.1.17</span>

<span class="c"># TCP 80,9000 포트별 접속 확인 : 출력 정보 의미 확인</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> curl <span class="nt">-s</span> <span class="nt">--connect-timeout</span> 1 <span class="nv">$SVC1</span>:9000
<span class="c"># =&gt; Hostname: webpod2</span>
<span class="c">#    IP: 127.0.0.1</span>
<span class="c">#    IP: ::1</span>
<span class="c">#    IP: 10.10.1.2</span>
<span class="c">#    IP: fe80::3009:36ff:fe8f:d5a</span>
<span class="c">#    RemoteAddr: 10.10.0.5:58980</span>
<span class="c">#    GET / HTTP/1.1</span>
<span class="c">#    Host: 10.200.1.17:9000</span>
<span class="c">#    User-Agent: curl/8.7.1</span>
<span class="c">#    Accept: */*</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> curl <span class="nt">-s</span> <span class="nt">--connect-timeout</span> 1 <span class="nv">$SVC1</span>:9000 | <span class="nb">grep </span>Hostname
<span class="c"># =&gt; Hostname: webpod3</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> curl <span class="nt">-s</span> <span class="nt">--connect-timeout</span> 1 <span class="nv">$SVC1</span>:9000 | <span class="nb">grep </span>Hostname
<span class="c"># =&gt; Hostname: webpod1</span>

<span class="c"># 서비스(ClusterIP) 부하분산 접속 확인 : 부하분산 비률 확인</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> zsh <span class="nt">-c</span> <span class="s2">"for i in {1..10};   do curl -s </span><span class="nv">$SVC1</span><span class="s2">:9000 | grep Hostname; done | sort | uniq -c | sort -nr"</span>
<span class="c"># =&gt;       4 Hostname: webpod3</span>
<span class="c">#          3 Hostname: webpod2</span>
<span class="c">#          3 Hostname: webpod1</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> zsh <span class="nt">-c</span> <span class="s2">"for i in {1..100};  do curl -s </span><span class="nv">$SVC1</span><span class="s2">:9000 | grep Hostname; done | sort | uniq -c | sort -nr"</span>
<span class="c"># =&gt;      34 Hostname: webpod1</span>
<span class="c">#         33 Hostname: webpod3</span>
<span class="c">#         33 Hostname: webpod2</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> zsh <span class="nt">-c</span> <span class="s2">"for i in {1..1000}; do curl -s </span><span class="nv">$SVC1</span><span class="s2">:9000 | grep Hostname; done | sort | uniq -c | sort -nr"</span>
<span class="c"># =&gt;     334 Hostname: webpod2</span>
<span class="c">#        333 Hostname: webpod3</span>
<span class="c">#        333 Hostname: webpod1</span>
<span class="c"># 혹은</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> zsh <span class="nt">-c</span> <span class="s2">"for i in {1..100};   do curl -s </span><span class="nv">$SVC1</span><span class="s2">:9000 | grep Hostname; sleep 1; done"</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> zsh <span class="nt">-c</span> <span class="s2">"for i in {1..100};   do curl -s </span><span class="nv">$SVC1</span><span class="s2">:9000 | grep Hostname; sleep 0.1; done"</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> zsh <span class="nt">-c</span> <span class="s2">"for i in {1..10000}; do curl -s </span><span class="nv">$SVC1</span><span class="s2">:9000 | grep Hostname; sleep 0.01; done"</span>

<span class="c"># 반복 접속</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> zsh <span class="nt">-c</span> <span class="s2">"while true; do curl -s --connect-timeout 1 </span><span class="nv">$SVC1</span><span class="s2">:9000 | egrep 'Hostname|RemoteAddr|Host:'; date '+%Y-%m-%d %H:%M:%S' ; echo '--------------' ;  sleep 1; done"</span>
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w5/20241005_kans_w5_20.png" alt="img.png" class="image-center" />
<em class="image-caption">IPVS Proxy 모드 : 부하분산 확인</em></p>

<ul>
  <li>IPVS는 기존의 iptables의 부하분산보다 더 균등하게 부하분산을 수행함을 확인 할 수 있었습니다.</li>
</ul>

<hr />

<h2 id="마치며">마치며</h2>

<p>이번 주에는 LoadBalancer, LoadBalancer를 온프레미스에서 사용하기 위한 MetalLB, ClusterIP, IPVS Proxy 모드에 대해 알아보았습니다.
온프레미스 K8S에서 서비스 유형을 LoadBalancer로 했을때 ExternalIP가 할당 되지 않은 이유를 이제야 알았습니다. 
단순히 쓰기만 해왔던 기술의 원리와 이유를 알게되니 뿌듯합니다. 
아직 알아야 할 것이 산더미이고 지금 이순간에도 새로운 기술들이 개발된다니 또다시 첩첩산중이라는것을 느낍니다.</p>

<p>IPVS는 아직 모르는 부분이 많지만, 실무에 적용해보고 싶은 기술입니다. 네트워크 부하때문에 CPU가 높아지는 경우가 많은데, 
이를 해결할 수 있는 방법인것 같아 유용할것 같습니다.</p>

<p>정말 매운맛의 스터디이지만 많은 것을 배우고 있습니다.
다음 주에는 드디어 기다리던 GatewayAPI를 스터디 합니다. 기대가 됩니다. :)</p>]]></content><author><name></name></author><category term="kans" /><category term="kubernetes," /><category term="network," /><category term="linux" /><summary type="html"><![CDATA[이번 주에는 LoadBalancer 서비스와 MetalLB, 그리고 kube-proxy의 모드중 하나인 IPVS에 대해 알아보겠습니다.]]></summary></entry><entry><title type="html">[KANS 3기] K8S Service : ClusterIP, NodePort</title><link href="https://sweetlittlebird.github.io/posts/2024-09-27-KANS-Study-Week4/" rel="alternate" type="text/html" title="[KANS 3기] K8S Service : ClusterIP, NodePort" /><published>2024-09-27T01:00:18+09:00</published><updated>2024-09-27T01:00:18+09:00</updated><id>https://sweetlittlebird.github.io/posts/KANS%20Study%20-%20Week4</id><content type="html" xml:base="https://sweetlittlebird.github.io/posts/2024-09-27-KANS-Study-Week4/"><![CDATA[<h2 id="들어가며">들어가며</h2>

<p>지난주에 이어 이번주에는 Kubernetes의 Service, 그 중에 ClusterIP, NodePort에 대해 알아보겠습니다.
KANS 3기 4주차 스터디를 시작하겠습니다.</p>

<hr />

<h2 id="k8s-service">K8S Service</h2>

<p>Kubernetes의 Service는 개별 Pod에 접근하기 위한 추상화된 방법을 제공합니다.
Pod는 생성될 때마다 IP가 동적으로 할당되기 때문에 Pod의 IP를 직접 사용하는 것은 좋은 방법이 아닙니다.
Service는 Pod의 IP를 추상화하여 Pod에 접근할 수 있도록 해줍니다.</p>

<h3 id="service의-탄생-배경">Service의 탄생 배경</h3>

<p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_4.png" alt="img.png" /></p>

<p>위의 그림과 같이 하나의 파드의 엔드포인트를 다른 파드 (또는 외부)에서 사용할때, 해당 파드의 IP로 지정을 하면, 파드가 재실행 될 때 IP가 변경되어 접속이 안 되서 장애가 발생하는 현상이 생깁니다.</p>

<p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_5.png" alt="img.png" /></p>

<p>그래서 고정된 IP의 서비스를 만들고 서비스의 IP로 접속시 파드가 재실행되어도 안정적으로 접속할 수 있도록 하기위해서 만들어졌습니다.</p>

<p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_6.png" alt="img.png" /></p>

<p>서비스는 또한 부하분산의 기능도 할 수 있습니다. 위의 그림과 같이 파드가 여러개일때 서비스 IP로 접속시 각 파드들에 부하를 분산시킬 수 있게 됩니다.</p>

<h3 id="k8s-service-종류">K8S Service 종류</h3>

<h4 id="clusterip">ClusterIP</h4>

<p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_1.png" alt="img.png" class="w-80 image-center" /></p>

<ul>
  <li>동일한 애플리케이션을 실행하는 여러 Pod에 접속을 용이하기 위해 사용합니다.</li>
  <li>ClusterIP는 Cluster 내부에서만 접근이 가능하며 외부에서는 접근이 불가능합니다.</li>
  <li>iptables 의 NAT 기능을 이용하여 Pod에 접근하며, 동일한 iptables 분산룰을 각 노드에 적용합니다.</li>
</ul>

<h4 id="nodeport">NodePort</h4>

<p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_2.png" alt="img.png" class="w-80 image-center" /></p>

<ul>
  <li>NodePort는 ClusterIP와 같이 Cluster 내부에서 접근이 가능하며, 외부에서도 접근이 가능합니다.</li>
  <li>NodePort도 ClusterIP와 같이 iptables의 NAT 기능을 이용하여 Pod에 접근하며, 각 노드에 NodePort를 할당합니다.</li>
  <li>외부에서는 NodePort를 통해 각 노드에 접근 할 수 있습니다.</li>
</ul>

<h4 id="loadbalancer">LoadBalancer</h4>

<p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_3.png" alt="img.png" class="w-80 image-center" /></p>

<ul>
  <li>LoadBalancer도 외부에서 접근이 가능하며, 클라우드 서비스에서 제공하는 LoadBalancer를 사용합니다. (AWS의 경우 ELB(Elastic Load Balancer)가 사용됩니다.)</li>
  <li>온프레미스 환경에서도 MetalLB와 같은 LoadBalancer를 사용할 수 있습니다.</li>
</ul>

<h3 id="서비스의-구조">서비스의 구조</h3>

<p>서비스를 선언시 <code class="language-plaintext highlighter-rouge">port</code>와 <code class="language-plaintext highlighter-rouge">targetPort</code>, 그리고 <code class="language-plaintext highlighter-rouge">label</code> <code class="language-plaintext highlighter-rouge">selector</code> 를 사용합니다. 각각의 역할은 다음과 같습니다.</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">port</code> : 서비스가 listen 할 포트를 지정합니다.</li>
  <li><code class="language-plaintext highlighter-rouge">targetPort</code> : 대상 파드의 port를 지정합니다.</li>
  <li><code class="language-plaintext highlighter-rouge">label selector</code>  : 대상 파드를 특정합니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_7.png" alt="img.png" /></p>

<h3 id="kube-proxy-모드">kube-proxy 모드</h3>

<ul>
  <li>kube-proxy는 서비스 통신 동작에 대한 설정을 관리합니다. 데몬셋으로 배포되어 모든 노드에 파드가 생성됩니다.</li>
  <li>kube-proxy 모드의 종류는 userspace proxy 모드, iptables proxy 모드, ipvs proxy 모드, nftables proxy 모드 등이 있습니다.</li>
</ul>

<h4 id="userspace-proxy-모드">userspace proxy 모드</h4>

<p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_8.png" alt="img.png" /></p>

<ul>
  <li>기초적인 모드이며 사용자 영역의 kube-proxy를 통해 NIC1으로 들어온 패킷을 NIC2로 전달하여 목적 파드로 전달합니다.</li>
  <li>이렇게 하는 과정에서 커널영역(netfilter)과 사용자영역(kube-proxy)를 오가는 과정에서 스위칭에 의한 오버헤드가 발생하는 단점이 있습니다.</li>
</ul>

<h4 id="iptables-proxy-모드">iptables proxy 모드</h4>

<p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_9.png" alt="img.png" /></p>

<ul>
  <li>쿠버네티스 설치시 기본 모드이며, kube-proxy는  트래픽 전달에 직접 관여하지는 않고, iptables 규칙을 관리하는 역할을 합니다.</li>
  <li>iptables proxy 모드는 트래픽 전달 과정에서 kube-proxy를 경유하지 않고, 커널 영역과 사용자 영역 전환이 필요하지 않아서, 유저스페이스 proxy 모드에 비해 오버헤드가 줄어듭니다.</li>
  <li>단점으로는 iptables 규칙이 많아 질 경우 모든 규칙 평가 하는데 지연이 발생할 수 있습니다.</li>
  <li>또한 장애시 모든 규칙을 확인하기 어려워 장애 처리에 불리합니다.</li>
</ul>

<h4 id="ipvs-proxy-모드">ipvs proxy 모드</h4>

<p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_10.png" alt="img.png" /></p>

<ul>
  <li>ipvs proxy 모드는 지금까지의 모드 중 가장 효율적인 모드입니다. IPVS(IP Virtual Server)는 넷필터에서 동작하는 Layer 4 로드밸런서입니다. iptables 보다 더 높은 성능 처리를 보여주고, 규칙 갯수를 줄일 수 있습니다. 또한 다양한 부하분산 알고리즘을 제공합니다.</li>
</ul>

<h4 id="nftables-proxy-모드">nftables proxy 모드</h4>
<ul>
  <li>nftables 는 iptables를 대체하기 위해 개발된 패킷 필터링 프레임워크로, iptables 보다 더 유연하고 강력한 규칙 설정을 제공합니다.</li>
  <li>하지만 아직  실험적으로 개발중인 단계로 실무에서는 ipvs proxy 모드를 권장합니다.</li>
</ul>

<h4 id="ebpf-모드--xdp">eBPF 모드 + XDP</h4>

<p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_11.png" alt="img.png" class="w-90 image-center" /></p>

<ul>
  <li>앞에서 알아보았던 모든 모드들이 netfilter 기반인데 반해, eBPF 모드 +  XDP 는 netfilter 전 단계에서 트래픽 라우팅을 처리하여 훨씬 효율 적입니다. calico나 cilium을 사용하여서 eBPF 모드를 사용할 수 있습니다.</li>
</ul>

<h3 id="실습">실습</h3>

<h4 id="실습환경-구축">실습환경 구축</h4>

<ul>
  <li>이번 실습은 실습환경 구축의 용이성을 위해서 kind를 이용하여 실습하였습니다.</li>
  <li>실습 환경 구축은 다음과 같이 진행 하였습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># kind 클러스터 정의 파일 생성</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOT</span><span class="sh">&gt; kind-svc-w3.yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
featureGates:
  "InPlacePodVerticalScaling": true
  "MultiCIDRServiceAllocator": true
nodes:
- role: control-plane
  labels:
    mynode: control-plane
  extraPortMappings:
  - containerPort: 30000
    hostPort: 30000
  - containerPort: 30001
    hostPort: 30001
  - containerPort: 30002
    hostPort: 30002
  kubeadmConfigPatches:
  - |
    kind: ClusterConfiguration
    apiServer:
      extraArgs:
        runtime-config: api/all=true
- role: worker
  labels:
    mynode: worker1
- role: worker
  labels:
    mynode: worker2
- role: worker
  labels:
    mynode: worker3
networking:
  podSubnet: 10.10.0.0/16
  serviceSubnet: 10.200.1.0/24
</span><span class="no">EOT

</span><span class="c"># k8s 클러스터 설치</span>
<span class="nv">$ </span>kind create cluster <span class="nt">--config</span> kind-svc-w3.yaml <span class="nt">--name</span> myk8s <span class="nt">--image</span> kindest/node:v1.31.0
<span class="c"># =&gt; Creating cluster "myk8s" ...</span>
<span class="c">#     ✓ Ensuring node image (kindest/node:v1.31.0) 🖼</span>
<span class="c">#     ✓ Preparing nodes 📦 📦 📦 📦</span>
<span class="c">#     ✓ Writing configuration 📜</span>
<span class="c">#     ✓ Starting control-plane 🕹️</span>
<span class="c">#     ✓ Installing CNI 🔌</span>
<span class="c">#     ✓ Installing StorageClass 💾</span>
<span class="c">#     ✓ Joining worker nodes 🚜</span>
<span class="c">#    Set kubectl context to "kind-myk8s"</span>
<span class="c">#    You can now use your cluster with:</span>
<span class="c">#    </span>
<span class="c">#    kubectl cluster-info --context kind-myk8s</span>

<span class="nv">$ </span>docker ps
<span class="c"># =&gt; CONTAINER ID   IMAGE                                COMMAND                  CREATED          STATUS                 PORTS                                                            NAMES</span>
<span class="c">#    1b7e6b646e48   kindest/node:v1.31.0                 "/usr/local/bin/entr…"   18 minutes ago   Up 18 minutes                                                                           myk8s-worker</span>
<span class="c">#    5406c013a571   kindest/node:v1.31.0                 "/usr/local/bin/entr…"   18 minutes ago   Up 18 minutes          0.0.0.0:30000-30002-&gt;30000-30002/tcp, 127.0.0.1:43315-&gt;6443/tcp  myk8s-control-plane</span>
<span class="c">#    4134657c5a70   kindest/node:v1.31.0                 "/usr/local/bin/entr…"   18 minutes ago   Up 18 minutes                                                                           myk8s-worker3</span>
<span class="c">#    6caf2b177502   kindest/node:v1.31.0                 "/usr/local/bin/entr…"   18 minutes ago   Up 18 minutes                                                                           myk8s-worker2</span>

<span class="c"># 노드에 기본 툴 설치</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane sh <span class="nt">-c</span> <span class="s1">'apt update &amp;&amp; apt install tree psmisc lsof wget bridge-utils net-tools ipset ipvsadm nfacct tcpdump ngrep iputils-ping arping git vim arp-scan -y'</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> sh <span class="nt">-c</span> <span class="s1">'apt update &amp;&amp; apt install tree psmisc lsof wget bridge-utils net-tools ipset ipvsadm nfacct tcpdump ngrep iputils-ping arping -y'</span><span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>

<span class="c"># k8s v1.31.0 버전 확인</span>
<span class="nv">$ </span>kubectl get node
<span class="c"># =&gt; NAME                  STATUS   ROLES           AGE   VERSION</span>
<span class="c">#    myk8s-control-plane   Ready    control-plane   40m   v1.31.0</span>
<span class="c">#    myk8s-worker          Ready    &lt;none&gt;          40m   v1.31.0</span>
<span class="c">#    myk8s-worker2         Ready    &lt;none&gt;          40m   v1.31.0</span>
<span class="c">#    myk8s-worker3         Ready    &lt;none&gt;          40m   v1.31.0</span>

<span class="c"># 노드 labels 확인</span>
<span class="nv">$ </span>kubectl get nodes <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.items[*].metadata.labels}"</span> | <span class="nb">grep </span>mynode
<span class="nv">$ </span>kubectl get nodes <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.items[*].metadata.labels}"</span> | jq | <span class="nb">grep </span>mynode
<span class="c"># =&gt;   "mynode": "control-plane",</span>
<span class="c">#      "mynode": "worker1"</span>
<span class="c">#      "mynode": "worker2"</span>
<span class="c">#      "mynode": "worker3"</span>

<span class="c"># kind network 중 컨테이너(노드) IP(대역) 확인 : 172.18.0.2~ 부터 할당되며, control-plane 이 꼭 172.18.0.2가 안될 수 도 있음</span>
<span class="nv">$ </span>docker ps <span class="nt">-q</span> | xargs docker inspect <span class="nt">--format</span> <span class="s1">' '</span>
<span class="c"># =&gt; /myk8s-control-plane 172.23.0.2</span>
<span class="c">#    /myk8s-worker 172.23.0.4</span>
<span class="c">#    /myk8s-worker2 172.23.0.5</span>
<span class="c">#    /myk8s-worker3 172.23.0.3</span>
    
<span class="c"># 파드CIDR 과 Service 대역 확인 : CNI는 kindnet 사용</span>
<span class="nv">$ </span>kubectl get cm <span class="nt">-n</span> kube-system kubeadm-config <span class="nt">-oyaml</span> | <span class="nb">grep</span> <span class="nt">-i</span> subnet
<span class="c"># =&gt;       podSubnet: 10.10.0.0/16</span>
<span class="c">#          serviceSubnet: 10.200.1.0/24</span>
<span class="nv">$ </span>kubectl cluster-info dump | <span class="nb">grep</span> <span class="nt">-m</span> 2 <span class="nt">-E</span> <span class="s2">"cluster-cidr|service-cluster-ip-range"</span>
<span class="c"># =&gt;                             "--service-cluster-ip-range=10.200.1.0/24",</span>
<span class="c">#                                "--cluster-cidr=10.10.0.0/16",</span>

<span class="c"># feature-gates 확인 : https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/</span>
<span class="nv">$ </span>kubectl describe pod <span class="nt">-n</span> kube-system | <span class="nb">grep </span>feature-gates
<span class="c"># =&gt;       --feature-gates=InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true</span>
<span class="nv">$ </span>kubectl describe pod <span class="nt">-n</span> kube-system | <span class="nb">grep </span>runtime-config
<span class="c"># =&gt;       --runtime-config=api/all=true</span>

<span class="c"># MultiCIDRServiceAllocator : https://kubernetes.io/docs/tasks/network/extend-service-ip-ranges/</span>
<span class="nv">$ </span>kubectl get servicecidr
<span class="c"># =&gt; NAME         CIDRS           AGE</span>
<span class="c">#    kubernetes   10.200.1.0/24   62m</span>

<span class="c"># 노드마다 할당된 dedicated subnet (podCIDR) 확인</span>
<span class="nv">$ </span>kubectl get nodes <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.items[*].spec.podCIDR}"</span>
<span class="c"># =&gt; 10.10.0.0/24 10.10.4.0/24 10.10.1.0/24 10.10.2.0/24</span>

<span class="c"># kube-proxy configmap 확인</span>
<span class="nv">$ </span>kubectl describe cm <span class="nt">-n</span> kube-system kube-proxy
<span class="c"># =&gt; ...</span>
<span class="c">#    iptables:</span>
<span class="c">#      localhostNodePorts: null</span>
<span class="c">#      masqueradeAll: false</span>
<span class="c">#      masqueradeBit: null</span>
<span class="c">#      minSyncPeriod: 1s</span>
<span class="c">#      syncPeriod: 0s</span>
<span class="c">#    mode: iptables</span>
<span class="c">#    ...</span>

<span class="c"># kube-proxy가 iptables 모드로 동작중임을 확인할 수 있습니다.</span>

<span class="c"># 노드 별 네트워트 정보 확인 : CNI는 kindnet 사용</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> <span class="nb">ls</span> /opt/cni/bin/<span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> <span class="nb">cat</span> /etc/cni/net.d/10-kindnet.conflist<span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> ip <span class="nt">-c</span> route<span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> ip <span class="nt">-c</span> addr<span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> ip <span class="nt">-c</span> <span class="nt">-4</span> addr show dev eth0<span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>

<span class="c"># iptables 정보 확인</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>filter nat mangle raw <span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; IPTables Type : </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane  iptables <span class="nt">-t</span> <span class="nv">$i</span> <span class="nt">-S</span> <span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>filter nat mangle raw <span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; IPTables Type : </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker  iptables <span class="nt">-t</span> <span class="nv">$i</span> <span class="nt">-S</span> <span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>filter nat mangle raw <span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; IPTables Type : </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker2 iptables <span class="nt">-t</span> <span class="nv">$i</span> <span class="nt">-S</span> <span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>filter nat mangle raw <span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; IPTables Type : </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker3 iptables <span class="nt">-t</span> <span class="nv">$i</span> <span class="nt">-S</span> <span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>

<span class="c"># 각 노드 bash 접속</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane bash
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker bash
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker2 bash
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker3 bash
<span class="nt">----------------------------------------</span>
<span class="nv">$ </span><span class="nb">exit</span>
<span class="nt">----------------------------------------</span>

<span class="c"># kind 설치 시 kind 이름의 도커 브리지가 생성됩니다. : 172.18.0.0/16 대역</span>
<span class="nv">$ </span>docker network <span class="nb">ls</span>
<span class="c"># =&gt; NETWORK ID     NAME                           DRIVER    SCOPE</span>
<span class="c">#    ...</span>
<span class="c">#    1c5d73657215   kind                           bridge    local</span>

<span class="nv">$ </span>docker inspect kind
<span class="c"># =&gt; [</span>
<span class="c">#        {</span>
<span class="c">#            "Name": "kind",</span>
<span class="c">#            ...</span>
<span class="c">#            "IPAM": {</span>
<span class="c">#                ...</span>
<span class="c">#                "Config": [</span>
<span class="c">#                    {</span>
<span class="c">#                        "Subnet": "172.23.0.0/16",</span>
<span class="c">#                        "Gateway": "172.23.0.1"</span>
<span class="c">#                    }</span>
<span class="c">#                ]</span>
<span class="c">#            },</span>
<span class="c">#            ...</span>
<span class="c">#            "Containers": {</span>
<span class="c">#                "1b7e6b646e4867591b5dd2a3bb4fcd2223dfcfd36dc08d86c8efc8fdc2112462": {</span>
<span class="c">#                    "Name": "myk8s-worker",</span>
<span class="c">#                    "IPv4Address": "172.23.0.4/16",</span>
<span class="c">#                },</span>
<span class="c">#                "4134657c5a7049d20944c2f80d3a3183a91a70107a47be72888e5c5fa972312a": {</span>
<span class="c">#                    "Name": "myk8s-worker3",</span>
<span class="c">#                    "IPv4Address": "172.23.0.3/16",</span>
<span class="c">#                },</span>
<span class="c">#                "5406c013a57167caf9a94ee9e89e550899a6efed9386f35548f03d2f670e8196": {</span>
<span class="c">#                    "Name": "myk8s-control-plane",</span>
<span class="c">#                    "IPv4Address": "172.23.0.2/16",</span>
<span class="c">#                },</span>
<span class="c">#                "6caf2b177502b92eccd4353ae3f4b3ac2da2949fc840225a02c9e83e1d24b09a": {</span>
<span class="c">#                    "Name": "myk8s-worker2",</span>
<span class="c">#                    "IPv4Address": "172.23.0.5/16",</span>
<span class="c">#                }</span>
<span class="c">#            },</span>
<span class="c">#            ...</span>
<span class="c">#        }</span>
<span class="c">#    ]</span>

<span class="c"># arp scan 해두기</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane arp-scan <span class="nt">--interfac</span><span class="o">=</span>eth0 <span class="nt">--localnet</span>
<span class="c"># =&gt; Interface: eth0, type: EN10MB, MAC: 02:42:ac:17:00:02, IPv4: 172.23.0.2</span>
<span class="c">#    Starting arp-scan 1.10.0 with 65536 hosts (https://github.com/royhills/arp-scan)</span>
<span class="c">#    172.23.0.1	02:42:a4:3f:b3:d9	(Unknown: locally administered)</span>
<span class="c">#    172.23.0.3	02:42:ac:17:00:03	(Unknown: locally administered)</span>
<span class="c">#    172.23.0.4	02:42:ac:17:00:04	(Unknown: locally administered)</span>
<span class="c">#    172.23.0.5	02:42:ac:17:00:05	(Unknown: locally administered)</span>

<span class="c"># mypc 컨테이너 기동 : kind 도커 브리지를 사용하고, 컨테이너 IP를 직접 지정</span>
<span class="nv">$ </span>docker run <span class="nt">-d</span> <span class="nt">--rm</span> <span class="nt">--name</span> mypc <span class="nt">--network</span> kind <span class="nt">--ip</span> 172.23.0.100 nicolaka/netshoot <span class="nb">sleep </span>infinity
<span class="nv">$ </span>docker ps
<span class="c">## 만약 kind 네트워크 대역이 다를 경우 위 IP 지정이 실패할 수 있으니, 그냥 IP 지정 없이 mypc 컨테이너 기동 할 것</span>
<span class="c">## docker run -d --rm --name mypc --network kind nicolaka/netshoot sleep infinity</span>

<span class="c"># 통신 확인</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc ping <span class="nt">-c</span> 1 172.23.0.1
<span class="c"># =&gt; PING 172.23.0.1 (172.23.0.1) 56(84) bytes of data.</span>
<span class="c">#    64 bytes from 172.23.0.1: icmp_seq=1 ttl=64 time=0.154 ms</span>
<span class="c">#    </span>
<span class="c">#    --- 172.23.0.1 ping statistics ---</span>
<span class="c">#    1 packets transmitted, 1 received, 0% packet loss, time 0ms</span>
<span class="c">#    rtt min/avg/max/mdev = 0.154/0.154/0.154/0.000 ms</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..5<span class="o">}</span> <span class="p">;</span> <span class="k">do </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc ping <span class="nt">-c</span> 1 172.23.0.<span class="nv">$i</span><span class="p">;</span> <span class="k">done</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc zsh
<span class="nt">-------------</span>
<span class="nv">$ </span>ifconfig
<span class="c"># =&gt; eth0      Link encap:Ethernet  HWaddr 02:42:AC:17:00:06  </span>
<span class="c">#              inet addr:172.23.0.6  Bcast:172.23.255.255  Mask:255.255.0.0</span>
<span class="c">#    ...</span>
<span class="nv">$ </span>ping <span class="nt">-c</span> 1 172.23.0.2
<span class="c"># =&gt; PING 172.23.0.2 (172.23.0.2) 56(84) bytes of data.</span>
<span class="c">#    64 bytes from 172.23.0.2: icmp_seq=1 ttl=64 time=0.258 ms</span>
<span class="c">#    </span>
<span class="c">#    --- 172.23.0.2 ping statistics ---</span>
<span class="c">#    1 packets transmitted, 1 received, 0% packet loss, time 0ms</span>
<span class="c">#    rtt min/avg/max/mdev = 0.258/0.258/0.258/0.000 ms</span>
<span class="nv">$ </span><span class="nb">exit</span>
<span class="nt">-------------</span>

<span class="c"># kube-ops-view 설치</span>
<span class="nv">$ </span>helm repo add geek-cookbook https://geek-cookbook.github.io/charts/
<span class="c"># =&gt; "geek-cookbook" has been added to your repositories</span>
<span class="nv">$ </span>helm <span class="nb">install </span>kube-ops-view geek-cookbook/kube-ops-view <span class="nt">--version</span> 1.2.2 <span class="nt">--set</span> service.main.type<span class="o">=</span>NodePort,service.main.ports.http.nodePort<span class="o">=</span>30000 <span class="nt">--set</span> env.TZ<span class="o">=</span><span class="s2">"Asia/Seoul"</span> <span class="nt">--namespace</span> kube-system
<span class="c"># =&gt; NAME: kube-ops-view</span>
<span class="c">#    ...</span>
<span class="c">#    1. Get the application URL by running these commands:</span>
<span class="c">#      export NODE_PORT=$(kubectl get --namespace kube-system -o jsonpath="{.spec.ports[0].nodePort}" services kube-ops-view)</span>
<span class="c">#      export NODE_IP=$(kubectl get nodes --namespace kube-system -o jsonpath="{.items[0].status.addresses[0].address}")</span>
<span class="c">#      echo http://$NODE_IP:$NODE_PORT</span>

<span class="c"># myk8s-control-plane 배치</span>
<span class="nv">$ </span>kubectl <span class="nt">-n</span> kube-system edit deploy kube-ops-view
<span class="nt">---</span>
spec:
  ...
  template:
    ...
    spec:
      nodeSelector:
        mynode: control-plane
      tolerations:
      - key: <span class="s2">"node-role.kubernetes.io/control-plane"</span>
        operator: <span class="s2">"Equal"</span>
        effect: <span class="s2">"NoSchedule"</span>
<span class="nt">---</span>

<span class="c"># 설치 확인</span>
<span class="nv">$ </span>kubectl <span class="nt">-n</span> kube-system get pod <span class="nt">-o</span> wide <span class="nt">-l</span> app.kubernetes.io/instance<span class="o">=</span>kube-ops-view
<span class="c"># =&gt; NAME                             READY   STATUS    RESTARTS   AGE   IP          NODE                  NOMINATED NODE   READINESS GATES</span>
<span class="c">#    kube-ops-view-58f96c464d-t5t68   1/1     Running   0          30s   10.10.0.5   myk8s-control-plane   &lt;none&gt;           &lt;none&gt;</span>

<span class="c"># kube-ops-view 접속 URL 확인 (1.5 , 2 배율) : macOS 사용자</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"KUBE-OPS-VIEW URL = http://localhost:30000/#scale=1.5"</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"KUBE-OPS-VIEW URL = http://localhost:30000/#scale=2"</span>

<span class="c"># kube-ops-view 접속 URL 확인 (1.5 , 2 배율) : Windows 사용자</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"KUBE-OPS-VIEW URL = http://192.168.50.10:30000/#scale=1.5"</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"KUBE-OPS-VIEW URL = http://192.168.50.10:30000/#scale=2"</span>

</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_12.png" alt="img.png" /></p>

<h4 id="clusterip-실습">ClusterIP 실습</h4>

<ul>
  <li>앞에서 알아본 ClusterIP 타입에 대해 실습해 보겠습니다.</li>
  <li>다음의 사항들을 살펴볼 것입니다.
    <ul>
      <li>ClusterIP의 서비스의 경우 클러스터 내부에서만 접근이 가능한 특성이 있습니다.</li>
      <li>IP로도 접속할 수 있지만 도메인 명으로도 접속이 가능합니다.</li>
      <li>서비스 타입(ClusterIP)을 생성하면 apiserver ⇒ (kubelet) ⇒ kube-proxy ⇒ iptables 에 rule 이 생성 됩니다.</li>
      <li>모든 노드(컨트롤 플레인 포함) 에 iptables rule이 설정 되므로, 파드에서 접속 시 해당 노드에 존재하는 iptables rule 에 의해 분산 접속됩니다.</li>
    </ul>
  </li>
  <li>실습 구성
    <ul>
      <li>
        <p>목적지(backend) 파드 (pod) 생성 : 3pod.yml</p>

        <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c1"># 3pod.yml</span>
  <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
  <span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
  <span class="na">metadata</span><span class="pi">:</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">webpod1</span>
    <span class="na">labels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">webpod</span>
  <span class="na">spec</span><span class="pi">:</span>
    <span class="na">nodeName</span><span class="pi">:</span> <span class="s">myk8s-worker</span>
    <span class="na">containers</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">container</span>
      <span class="na">image</span><span class="pi">:</span> <span class="s">traefik/whoami</span>
    <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
  <span class="s">---</span>
  <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
  <span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
  <span class="na">metadata</span><span class="pi">:</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">webpod2</span>
    <span class="na">labels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">webpod</span>
  <span class="na">spec</span><span class="pi">:</span>
    <span class="na">nodeName</span><span class="pi">:</span> <span class="s">myk8s-worker2</span>
    <span class="na">containers</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">container</span>
      <span class="na">image</span><span class="pi">:</span> <span class="s">traefik/whoami</span>
    <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
  <span class="s">---</span>
  <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
  <span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
  <span class="na">metadata</span><span class="pi">:</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">webpod3</span>
    <span class="na">labels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">webpod</span>
  <span class="na">spec</span><span class="pi">:</span>
    <span class="na">nodeName</span><span class="pi">:</span> <span class="s">myk8s-worker3</span>
    <span class="na">containers</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">container</span>
      <span class="na">image</span><span class="pi">:</span> <span class="s">traefik/whoami</span>
    <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>클라이언트 생성 : netpod.yml</p>

        <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c1"># netpod.yml</span>
  <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
  <span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
  <span class="na">metadata</span><span class="pi">:</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">net-pod</span>
  <span class="na">spec</span><span class="pi">:</span>
    <span class="na">nodeName</span><span class="pi">:</span> <span class="s">myk8s-control-plane</span>
    <span class="na">containers</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">netshoot-pod</span>
      <span class="na">image</span><span class="pi">:</span> <span class="s">nicolaka/netshoot</span>
      <span class="na">command</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">tail"</span><span class="pi">]</span>
      <span class="na">args</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">-f"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">/dev/null"</span><span class="pi">]</span>
    <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>서비스(ClusterIP) 생성 : svc-clusterip.yml</p>

        <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c1"># svc-clusterip.yml</span>
  <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
  <span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
  <span class="na">metadata</span><span class="pi">:</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">svc-clusterip</span>
  <span class="na">spec</span><span class="pi">:</span>
    <span class="na">ports</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">svc-webport</span>
        <span class="na">port</span><span class="pi">:</span> <span class="m">9000</span>        <span class="c1"># 서비스 IP 에 접속 시 사용하는 포트 port 를 의미</span>
        <span class="na">targetPort</span><span class="pi">:</span> <span class="m">80</span>    <span class="c1"># 타킷 targetPort 는 서비스를 통해서 목적지 파드로 접속 시 해당 파드로 접속하는 포트를 의미</span>
    <span class="na">selector</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">webpod</span>         <span class="c1"># 셀렉터 아래 app:webpod 레이블이 설정되어 있는 파드들은 해당 서비스에 연동됨</span>
    <span class="na">type</span><span class="pi">:</span> <span class="s">ClusterIP</span>       <span class="c1"># 서비스 타입</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>생성 및 확인</p>

        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c"># 모니터링</span>
  <span class="nv">$ </span><span class="k">**</span>watch <span class="nt">-d</span> <span class="s1">'kubectl get pod -owide ;echo; kubectl get svc,ep svc-clusterip'</span><span class="k">**</span>
      
  <span class="c"># 생성</span>
  <span class="nv">$ </span>kubectl apply <span class="nt">-f</span> 3pod.yml,netpod.yml,svc-clusterip.yml
  <span class="c"># =&gt; pod/webpod1 created</span>
  <span class="c">#    pod/webpod2 created</span>
  <span class="c">#    pod/webpod3 created</span>
  <span class="c">#    pod/net-pod created</span>
  <span class="c">#    service/svc-clusterip created</span>
      
  <span class="c"># 파드와 서비스 사용 네트워크 대역 정보 확인 </span>
  <span class="nv">$ </span>kubectl cluster-info dump | <span class="nb">grep</span> <span class="nt">-m</span> 2 <span class="nt">-E</span> <span class="s2">"cluster-cidr|service-cluster-ip-range"</span>
  <span class="c"># =&gt; "--service-cluster-ip-range=10.200.1.0/24",</span>
  <span class="c">#    "--cluster-cidr=10.10.0.0/16",</span>
      
  <span class="c"># 확인</span>
  <span class="nv">$ </span>kubectl get pod <span class="nt">-owide</span>
  <span class="c"># =&gt; NAME      READY   STATUS    RESTARTS   AGE    IP          NODE                  NOMINATED NODE   READINESS GATES</span>
  <span class="c">#    net-pod   1/1     Running   0          2m8s   10.10.0.7   myk8s-control-plane   &lt;none&gt;           &lt;none&gt;</span>
  <span class="c">#    webpod1   1/1     Running   0          2m8s   10.10.4.3   myk8s-worker          &lt;none&gt;           &lt;none&gt;</span>
  <span class="c">#    webpod2   1/1     Running   0          2m8s   10.10.1.4   myk8s-worker2         &lt;none&gt;           &lt;none&gt;</span>
  <span class="c">#    webpod3   1/1     Running   0          2m8s   10.10.2.3   myk8s-worker3         &lt;none&gt;           &lt;none&gt;</span>
  <span class="nv">$ </span>kubectl get svc svc-clusterip
  <span class="c"># =&gt; NAME            TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)    AGE</span>
  <span class="c">#    svc-clusterip   ClusterIP   10.200.1.96   &lt;none&gt;        9000/TCP   2m15s</span>
      
  <span class="c"># spec.ports.port 와 spec.ports.targetPort 가 어떤 의미인지 꼭 이해하자!</span>
  <span class="nv">$ </span>kubectl describe svc svc-clusterip
  <span class="c"># =&gt; Name:              svc-clusterip</span>
  <span class="c">#    Namespace:         default</span>
  <span class="c">#    Labels:            &lt;none&gt;</span>
  <span class="c">#    Annotations:       &lt;none&gt;</span>
  <span class="c">#    Selector:          app=webpod</span>
  <span class="c">#    Type:              ClusterIP</span>
  <span class="c">#    IP Family Policy:  SingleStack</span>
  <span class="c">#    IP Families:       IPv4</span>
  <span class="c">#    IP:                10.200.1.96</span>
  <span class="c">#    IPs:               10.200.1.96</span>
  <span class="c">#    Port:              svc-webport  9000/TCP                    # service의 listening port</span>
  <span class="c">#    TargetPort:        80/TCP                                   # pod의 실제 port</span>
  <span class="c">#    Endpoints:         10.10.1.4:80,10.10.2.3:80,10.10.4.3:80   # pod의 ip:port 목록</span>
  <span class="c">#    Session Affinity:  None</span>
  <span class="c">#    Events:            &lt;none&gt;</span>
      
  <span class="c"># 서비스 생성 시 엔드포인트를 자동으로 생성, 물론 수동으로 설정 생성도 가능</span>
  <span class="nv">$ </span>kubectl get endpoints svc-clusterip
  <span class="c"># =&gt; NAME            ENDPOINTS                                AGE</span>
  <span class="c">#    svc-clusterip   10.10.1.4:80,10.10.2.3:80,10.10.4.3:80   3m32s</span>
  <span class="nv">$ </span>kubectl get endpointslices <span class="nt">-l</span> kubernetes.io/service-name<span class="o">=</span>svc-clusterip
  <span class="c"># =&gt; NAME                  ADDRESSTYPE   PORTS   ENDPOINTS                       AGE</span>
  <span class="c">#    svc-clusterip-xxvws   IPv4          80      10.10.4.3,10.10.1.4,10.10.2.3   3m39s</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
</ul>

<p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_13.png" alt="img.png" /></p>

<ul>
  <li>서비스 (ClusterIP) 접속 확인
    <ul>
      <li>
        <p>클라이언트 (TestPod)의 Shell 에 접속하여 서비스(ClusterIP) 부하분산 접속 확인</p>

        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c"># webpod 파드의 IP 를 출력</span>
  <span class="nv">$ </span>kubectl get pod <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>webpod <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.items[*].status.podIP}"</span>
  <span class="c"># =&gt; 10.10.4.3 10.10.1.4 10.10.2.3</span>
      
  <span class="c"># webpod 파드의 IP를 변수에 지정</span>
  <span class="nv">$ WEBPOD1</span><span class="o">=</span><span class="si">$(</span>kubectl get pod webpod1 <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.status.podIP<span class="o">}</span><span class="si">)</span>
  <span class="nv">$ WEBPOD2</span><span class="o">=</span><span class="si">$(</span>kubectl get pod webpod2 <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.status.podIP<span class="o">}</span><span class="si">)</span>
  <span class="nv">$ WEBPOD3</span><span class="o">=</span><span class="si">$(</span>kubectl get pod webpod3 <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.status.podIP<span class="o">}</span><span class="si">)</span>
  <span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$WEBPOD1</span> <span class="nv">$WEBPOD2</span> <span class="nv">$WEBPOD3</span>
  <span class="c"># =&gt; 10.10.4.3 10.10.1.4 10.10.2.3</span>
      
  <span class="c"># net-pod 파드에서 webpod 파드의 IP로 직접 curl 로 반복 접속</span>
  <span class="nv">$ </span><span class="k">for </span>pod <span class="k">in</span> <span class="nv">$WEBPOD1</span> <span class="nv">$WEBPOD2</span> <span class="nv">$WEBPOD3</span><span class="p">;</span> <span class="k">do </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> curl <span class="nt">-s</span> <span class="nv">$pod</span><span class="p">;</span> <span class="k">done</span>
  <span class="c"># =&gt; Hostname: webpod1</span>
  <span class="c">#    IP: 10.10.4.3</span>
  <span class="c">#    RemoteAddr: 10.10.0.7:56374</span>
  <span class="c">#    GET / HTTP/1.1</span>
  <span class="c">#    Host: 10.10.4.3</span>
  <span class="c">#    User-Agent: curl/8.7.1</span>
  <span class="c">#    Accept: */*</span>
  <span class="c">#    </span>
  <span class="c">#    Hostname: webpod2</span>
  <span class="c">#    ...</span>
  <span class="c">#    Hostname: webpod3</span>
  <span class="c">#    ...</span>
  <span class="nv">$ </span><span class="k">for </span>pod <span class="k">in</span> <span class="nv">$WEBPOD1</span> <span class="nv">$WEBPOD2</span> <span class="nv">$WEBPOD3</span><span class="p">;</span> <span class="k">do </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> curl <span class="nt">-s</span> <span class="nv">$pod</span> | <span class="nb">grep </span>Hostname<span class="p">;</span> <span class="k">done</span>
  <span class="c"># =&gt; Hostname: webpod1</span>
  <span class="c">#    Hostname: webpod2</span>
  <span class="c">#    Hostname: webpod3</span>
  <span class="nv">$ </span><span class="k">for </span>pod <span class="k">in</span> <span class="nv">$WEBPOD1</span> <span class="nv">$WEBPOD2</span> <span class="nv">$WEBPOD3</span><span class="p">;</span> <span class="k">do </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> curl <span class="nt">-s</span> <span class="nv">$pod</span> | <span class="nb">grep </span>Host<span class="p">;</span> <span class="k">done</span>
  <span class="c"># =&gt; Hostname: webpod1</span>
  <span class="c">#    Host: 10.10.4.3</span>
  <span class="c">#    Hostname: webpod2</span>
  <span class="c">#    Host: 10.10.1.4</span>
  <span class="c">#    Hostname: webpod3</span>
  <span class="c">#    Host: 10.10.2.3</span>
  <span class="nv">$ </span><span class="k">for </span>pod <span class="k">in</span> <span class="nv">$WEBPOD1</span> <span class="nv">$WEBPOD2</span> <span class="nv">$WEBPOD3</span><span class="p">;</span> <span class="k">do </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> curl <span class="nt">-s</span> <span class="nv">$pod</span> | egrep <span class="s1">'Host|RemoteAddr'</span><span class="p">;</span> <span class="k">done</span>
  <span class="c"># =&gt; Hostname: webpod1</span>
  <span class="c">#    RemoteAddr: 10.10.0.7:36382</span>
  <span class="c">#    Host: 10.10.4.3</span>
  <span class="c">#    Hostname: webpod2</span>
  <span class="c">#    RemoteAddr: 10.10.0.7:52122</span>
  <span class="c">#    Host: 10.10.1.4</span>
  <span class="c">#    Hostname: webpod3</span>
  <span class="c">#    RemoteAddr: 10.10.0.7:55962</span>
  <span class="c">#    Host: 10.10.2.3</span>
      
  <span class="c"># 서비스 IP 변수 지정 : svc-clusterip 의 ClusterIP주소</span>
  <span class="nv">$ SVC1</span><span class="o">=</span><span class="si">$(</span>kubectl get svc svc-clusterip <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.spec.clusterIP<span class="o">}</span><span class="si">)</span>
  <span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$SVC1</span>
  <span class="c"># =&gt; 10.200.1.96</span>
      
  <span class="c"># 위 서비스 생성 시 kube-proxy 에 의해서 iptables 규칙이 모든 노드에 추가됨 </span>
  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane iptables <span class="nt">-t</span> nat <span class="nt">-S</span> | <span class="nb">grep</span> <span class="nv">$SVC1</span>
  <span class="c"># =&gt; -A KUBE-SERVICES -d 10.200.1.96/32 -p tcp -m comment --comment "default/svc-clusterip:svc-webport cluster IP" -m tcp --dport 9000 -j KUBE-SVC-KBDEBIL6IU6WL7RF</span>
  <span class="c">#    -A KUBE-SVC-KBDEBIL6IU6WL7RF ! -s 10.10.0.0/16 -d 10.200.1.96/32 -p tcp -m comment --comment "default/svc-clusterip:svc-webport cluster IP" -m tcp --dport 9000 -j KUBE-MARK-MASQ</span>
  <span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> iptables <span class="nt">-t</span> nat <span class="nt">-S</span> | <span class="nb">grep</span> <span class="nv">$SVC1</span><span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
  <span class="c"># =&gt; &gt;&gt; node myk8s-control-plane &lt;&lt;</span>
  <span class="c">#    -A KUBE-SERVICES -d 10.200.1.96/32 -p tcp -m comment --comment "default/svc-clusterip:svc-webport cluster IP" -m tcp --dport 9000 -j KUBE-SVC-KBDEBIL6IU6WL7RF</span>
  <span class="c">#    -A KUBE-SVC-KBDEBIL6IU6WL7RF ! -s 10.10.0.0/16 -d 10.200.1.96/32 -p tcp -m comment --comment "default/svc-clusterip:svc-webport cluster IP" -m tcp --dport 9000 -j KUBE-MARK-MASQ</span>
  <span class="c">#    </span>
  <span class="c">#    &gt;&gt; node myk8s-worker &lt;&lt;</span>
  <span class="c">#    -A KUBE-SERVICES -d 10.200.1.96/32 -p tcp -m comment --comment "default/svc-clusterip:svc-webport cluster IP" -m tcp --dport 9000 -j KUBE-SVC-KBDEBIL6IU6WL7RF</span>
  <span class="c">#    -A KUBE-SVC-KBDEBIL6IU6WL7RF ! -s 10.10.0.0/16 -d 10.200.1.96/32 -p tcp -m comment --comment "default/svc-clusterip:svc-webport cluster IP" -m tcp --dport 9000 -j KUBE-MARK-MASQ</span>
  <span class="c">#    </span>
  <span class="c">#    &gt;&gt; node myk8s-worker2 &lt;&lt;</span>
  <span class="c">#    -A KUBE-SERVICES -d 10.200.1.96/32 -p tcp -m comment --comment "default/svc-clusterip:svc-webport cluster IP" -m tcp --dport 9000 -j KUBE-SVC-KBDEBIL6IU6WL7RF</span>
  <span class="c">#    -A KUBE-SVC-KBDEBIL6IU6WL7RF ! -s 10.10.0.0/16 -d 10.200.1.96/32 -p tcp -m comment --comment "default/svc-clusterip:svc-webport cluster IP" -m tcp --dport 9000 -j KUBE-MARK-MASQ</span>
  <span class="c">#    </span>
  <span class="c">#    &gt;&gt; node myk8s-worker3 &lt;&lt;</span>
  <span class="c">#    -A KUBE-SERVICES -d 10.200.1.96/32 -p tcp -m comment --comment "default/svc-clusterip:svc-webport cluster IP" -m tcp --dport 9000 -j KUBE-SVC-KBDEBIL6IU6WL7RF</span>
  <span class="c">#    -A KUBE-SVC-KBDEBIL6IU6WL7RF ! -s 10.10.0.0/16 -d 10.200.1.96/32 -p tcp -m comment --comment "default/svc-clusterip:svc-webport cluster IP" -m tcp --dport 9000 -j KUBE-MARK-MASQ</span>
      
  <span class="c">## (참고) ss 툴로 tcp listen 정보에는 없음 , 별도 /32 host 라우팅 추가 없음 -&gt; 즉, iptables rule 에 의해서 처리됨을 확인</span>
  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane ss <span class="nt">-tnlp</span>
  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane ip <span class="nt">-c</span> route
      
  <span class="c"># TCP 80,9000 포트별 접속 확인 : 출력 정보 의미 확인</span>
  <span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> curl <span class="nt">-s</span> <span class="nt">--connect-timeout</span> 1 <span class="nv">$SVC1</span>:80
  <span class="c"># =&gt; (공백)</span>
  <span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> curl <span class="nt">-s</span> <span class="nt">--connect-timeout</span> 1 <span class="nv">$SVC1</span>:9000
  <span class="c"># =&gt; Hostname: webpod2</span>
  <span class="c">#    ...</span>
  <span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> curl <span class="nt">-s</span> <span class="nt">--connect-timeout</span> 1 <span class="nv">$SVC1</span>:9000 | <span class="nb">grep </span>Hostname
  <span class="c"># =&gt; Hostname: webpod3</span>
  <span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> curl <span class="nt">-s</span> <span class="nt">--connect-timeout</span> 1 <span class="nv">$SVC1</span>:9000 | <span class="nb">grep </span>Hostname
  <span class="c"># =&gt; Hostname: webpod1 </span>
      
  <span class="c"># curl로 접속했을때 컨테이너의 포트인 targetPort 80으로는 접속이 안 되고 port 9000로는 접속이 됩니다.</span>
  <span class="c"># 또한 접속시마다 각 pod에 부하가 분산되어 HostName: 이 변경됨을 확인할 수 있습니다.</span>
      
  <span class="c"># 서비스(ClusterIP) 부하분산 접속 확인</span>
  <span class="c">## for 문을 이용하여 SVC1 IP 로 100번 접속을 시도 후 출력되는 내용 중 반복되는 내용의 갯수 출력</span>
  <span class="c">## 반복해서 실행을 해보면, SVC1 IP로 curl 접속 시 3개의 파드로 대략 33% 정도로 부하분산 접속됨을 확인</span>
  <span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> zsh <span class="nt">-c</span> <span class="s2">"for i in {1..10};   do curl -s </span><span class="nv">$SVC1</span><span class="s2">:9000 | grep Hostname; done | sort | uniq -c | sort -nr"</span>
  <span class="c"># =&gt;       4 Hostname: webpod3</span>
  <span class="c">#          4 Hostname: webpod2</span>
  <span class="c">#          2 Hostname: webpod1</span>
  <span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> zsh <span class="nt">-c</span> <span class="s2">"for i in {1..100};  do curl -s </span><span class="nv">$SVC1</span><span class="s2">:9000 | grep Hostname; done | sort | uniq -c | sort -nr"</span>
  <span class="c"># =&gt;      38 Hostname: webpod3</span>
  <span class="c">#         35 Hostname: webpod1</span>
  <span class="c">#         27 Hostname: webpod2</span>
  <span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> zsh <span class="nt">-c</span> <span class="s2">"for i in {1..1000}; do curl -s </span><span class="nv">$SVC1</span><span class="s2">:9000 | grep Hostname; done | sort | uniq -c | sort -nr"</span>
  <span class="c"># =&gt;     346 Hostname: webpod2</span>
  <span class="c">#        336 Hostname: webpod1</span>
  <span class="c">#        318 Hostname: webpod3</span>
  <span class="c"># 혹은</span>
  <span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> zsh <span class="nt">-c</span> <span class="s2">"for i in {1..100};   do curl -s </span><span class="nv">$SVC1</span><span class="s2">:9000 | grep Hostname; sleep 1; done"</span>
  <span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> zsh <span class="nt">-c</span> <span class="s2">"for i in {1..100};   do curl -s </span><span class="nv">$SVC1</span><span class="s2">:9000 | grep Hostname; sleep 0.1; done"</span>
  <span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> zsh <span class="nt">-c</span> <span class="s2">"for i in {1..10000}; do curl -s </span><span class="nv">$SVC1</span><span class="s2">:9000 | grep Hostname; sleep 0.01; done"</span>
      
  <span class="c"># conntrack 확인</span>
  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane bash
  <span class="nt">----------------------------------------</span>
  <span class="nv">$ </span>conntrack <span class="nt">-h</span>
  <span class="nv">$ </span>conntrack <span class="nt">-E</span>
  <span class="c"># =&gt;     [NEW] tcp      6 120 SYN_SENT src=172.23.0.2 dst=172.23.0.2 sport=45466 dport=6443 [UNREPLIED] src=172.23.0.2 dst=172.23.0.2 sport=6443 dport=45466</span>
  <span class="c">#     [UPDATE] tcp      6 60 SYN_RECV src=172.23.0.2 dst=172.23.0.2 sport=45466 dport=6443 src=172.23.0.2 dst=172.23.0.2 sport=6443 dport=45466</span>
  <span class="nv">$ </span>conntrack <span class="nt">-C</span>
  <span class="c"># =&gt; 2763</span>
  <span class="nv">$ </span>conntrack <span class="nt">-S</span>
  <span class="c"># =&gt; cpu=0           found=0 invalid=0 insert=0 insert_failed=0 drop=0 early_drop=0 error=0 search_restart=3 clash_resolve=0 chaintoolong=0</span>
  <span class="c">#    cpu=1           found=0 invalid=0 insert=0 insert_failed=0 drop=0 early_drop=0 error=0 search_restart=0 clash_resolve=0 chaintoolong=0</span>
  <span class="nv">$ </span>conntrack <span class="nt">-L</span> <span class="nt">--src</span> 10.200.0.7 <span class="c"># net-pod IP</span>
  <span class="c"># =&gt; tcp      6 93 TIME_WAIT src=10.10.0.7 dst=10.200.1.96 sport=37008 dport=9000 src=10.10.2.3 dst=10.10.0.7 sport=80 dport=37008 [ASSURED] mark=0 use=1</span>
  <span class="c">#    tcp      6 93 TIME_WAIT src=10.10.0.7 dst=10.200.1.96 sport=36584 dport=9000 src=10.10.2.3 dst=10.10.0.7 sport=80 dport=36584 [ASSURED] mark=0 use=1</span>
  <span class="nv">$ SVC1</span><span class="o">=</span><span class="si">$(</span>kubectl get svc svc-clusterip <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.spec.clusterIP<span class="o">}</span><span class="si">)</span>
  <span class="nv">$ </span>conntrack <span class="nt">-L</span> <span class="nt">--dst</span> <span class="nv">$SVC1</span>     <span class="c"># service ClusterIP</span>
  <span class="c"># =&gt; tcp      6 31 TIME_WAIT src=10.10.0.7 dst=10.200.1.96 sport=37008 dport=9000 src=10.10.2.3 dst=10.10.0.7 sport=80 dport=37008 [ASSURED] mark=0 use=1</span>
  <span class="c">#    tcp      6 31 TIME_WAIT src=10.10.0.7 dst=10.200.1.96 sport=36584 dport=9000 src=10.10.2.3 dst=10.10.0.7 sport=80 dport=36584 [ASSURED] mark=0 use=1</span>
  <span class="nv">$ </span><span class="nb">exit</span>
  <span class="nt">----------------------------------------</span>
      
  <span class="c"># (참고) Link layer 에서 동작하는 ebtables</span>
  <span class="nv">$ </span>ebtables <span class="nt">-L</span>
  <span class="c"># =&gt; Bridge table: filter</span>
  <span class="c">#    Bridge chain: INPUT, entries: 0, policy: ACCEPT</span>
  <span class="c">#    Bridge chain: FORWARD, entries: 0, policy: ACCEPT</span>
  <span class="c">#    Bridge chain: OUTPUT, entries: 0, policy: ACCEPT</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>각 워커 노드에서 패킷  덤프 확인</p>

        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c"># 방안1 : 1대 혹은 3대 bash 진입 후 tcpdump 해둘 것</span>
  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker bash
  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker2 bash
  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker3 bash
  <span class="nt">----------------------------------</span>
  <span class="c"># nic 정보 확인</span>
  <span class="nv">$ </span>ip <span class="nt">-c</span> <span class="nb">link</span>
  <span class="c"># =&gt; &lt;&lt;myk8s-worker&gt;&gt;</span>
  <span class="c">#    3: veth9a888981@if2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default</span>
  <span class="c">#        link/ether 26:a5:d2:44:f4:b4 brd ff:ff:ff:ff:ff:ff link-netns cni-0b7e59c3-3920-ce1f-874a-9e228adf3b72</span>
  <span class="c">#    134: eth0@if135: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default</span>
  <span class="c">#        link/ether 02:42:ac:17:00:04 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span>
  <span class="c">#    </span>
  <span class="c">#    &lt;&lt;myk8s-worker2&gt;&gt;</span>
  <span class="c">#    4: veth570fce87@if2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default</span>
  <span class="c">#        link/ether a6:8a:d8:a3:f4:ac brd ff:ff:ff:ff:ff:ff link-netns cni-8dcb2d16-f339-2571-03d2-d6b0f850878d</span>
  <span class="c">#    136: eth0@if137: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default</span>
  <span class="c">#        link/ether 02:42:ac:17:00:05 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span>
  <span class="c">#    </span>
  <span class="c">#    &lt;&lt;myk8s-worker3&gt;&gt;</span>
  <span class="c">#    3: veth2e19df47@if2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default</span>
  <span class="c">#        link/ether da:5e:a5:14:62:7b brd ff:ff:ff:ff:ff:ff link-netns cni-1f44bebd-9ebf-6af4-7888-945649a2b5c8</span>
  <span class="c">#    132: eth0@if133: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default</span>
  <span class="c">#        link/ether 02:42:ac:17:00:03 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span>
      
  <span class="nv">$ </span>ip <span class="nt">-c</span> route
  <span class="c"># =&gt; &lt;&lt;myk8s-worker&gt;&gt;</span>
  <span class="c">#    default via 172.23.0.1 dev eth0</span>
  <span class="c">#    10.10.0.0/24 via 172.23.0.2 dev eth0</span>
  <span class="c">#    10.10.1.0/24 via 172.23.0.5 dev eth0</span>
  <span class="c">#    10.10.2.0/24 via 172.23.0.3 dev eth0</span>
  <span class="c">#    10.10.4.3 dev veth9a888981 scope host</span>
  <span class="c">#    172.23.0.0/16 dev eth0 proto kernel scope link src 172.23.0.4</span>
  <span class="c">#</span>
  <span class="c">#    &lt;&lt;myk8s-worker2&gt;&gt;</span>
  <span class="c">#    default via 172.23.0.1 dev eth0</span>
  <span class="c">#    10.10.0.0/24 via 172.23.0.2 dev eth0</span>
  <span class="c">#    10.10.1.4 dev veth570fce87 scope host</span>
  <span class="c">#    10.10.2.0/24 via 172.23.0.3 dev eth0</span>
  <span class="c">#    10.10.4.0/24 via 172.23.0.4 dev eth0</span>
  <span class="c">#    172.23.0.0/16 dev eth0 proto kernel scope link src 172.23.0.5</span>
  <span class="c">#    </span>
  <span class="c">#    &lt;&lt;myk8s-worker3&gt;&gt;</span>
  <span class="c">#    default via 172.23.0.1 dev eth0</span>
  <span class="c">#    10.10.0.0/24 via 172.23.0.2 dev eth0</span>
  <span class="c">#    10.10.1.0/24 via 172.23.0.5 dev eth0</span>
  <span class="c">#    10.10.2.3 dev veth2e19df47 scope host</span>
  <span class="c">#    10.10.4.0/24 via 172.23.0.4 dev eth0</span>
  <span class="c">#    172.23.0.0/16 dev eth0 proto kernel scope link src 172.23.0.3</span>
      
  <span class="nv">$ </span>ip <span class="nt">-c</span> addr
  <span class="c"># =&gt; &lt;&lt;myk8s-worker&gt;&gt;</span>
  <span class="c">#    3: veth9a888981@if2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span>
  <span class="c">#        link/ether 26:a5:d2:44:f4:b4 brd ff:ff:ff:ff:ff:ff link-netns cni-0b7e59c3-3920-ce1f-874a-9e228adf3b72</span>
  <span class="c">#        inet 10.10.4.1/32 scope global veth9a888981</span>
  <span class="c">#           valid_lft forever preferred_lft forever</span>
  <span class="c">#    134: eth0@if135: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span>
  <span class="c">#        link/ether 02:42:ac:17:00:04 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span>
  <span class="c">#        inet 172.23.0.4/16 brd 172.23.255.255 scope global eth0</span>
  <span class="c">#           valid_lft forever preferred_lft forever</span>
  <span class="c">#    </span>
  <span class="c">#    &lt;&lt;myk8s-worker2&gt;&gt;</span>
  <span class="c">#    4: veth570fce87@if2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span>
  <span class="c">#        link/ether a6:8a:d8:a3:f4:ac brd ff:ff:ff:ff:ff:ff link-netns cni-8dcb2d16-f339-2571-03d2-d6b0f850878d</span>
  <span class="c">#        inet 10.10.1.1/32 scope global veth570fce87</span>
  <span class="c">#           valid_lft forever preferred_lft forever</span>
  <span class="c">#    136: eth0@if137: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span>
  <span class="c">#        link/ether 02:42:ac:17:00:05 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span>
  <span class="c">#        inet 172.23.0.5/16 brd 172.23.255.255 scope global eth0</span>
  <span class="c">#           valid_lft forever preferred_lft forever</span>
  <span class="c">#    </span>
  <span class="c">#    &lt;&lt;myk8s-worker3&gt;&gt;</span>
  <span class="c">#    3: veth2e19df47@if2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span>
  <span class="c">#        link/ether da:5e:a5:14:62:7b brd ff:ff:ff:ff:ff:ff link-netns cni-1f44bebd-9ebf-6af4-7888-945649a2b5c8</span>
  <span class="c">#        inet 10.10.2.1/32 scope global veth2e19df47</span>
  <span class="c">#           valid_lft forever preferred_lft forever</span>
  <span class="c">#    132: eth0@if133: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span>
  <span class="c">#        link/ether 02:42:ac:17:00:03 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span>
  <span class="c">#        inet 172.23.0.3/16 brd 172.23.255.255 scope global eth0</span>
  <span class="c">#           valid_lft forever preferred_lft forever</span>
      
  <span class="c"># tcpdump/ngrep : eth0 &gt;&gt; tcp 9000 포트 트래픽은 왜 없을까? iptables rule 동작 그림을 한번 더 확인하고 이해해보자</span>
  <span class="c">## ngrep 네트워크 패킷 분석기 활용해보기 : 특정 url 호출에 대해서만 필터 등 깔끔하게 볼 수 있음 - 링크</span>
  <span class="nv">$ </span>tcpdump <span class="nt">-i</span> eth0 tcp port 80 <span class="nt">-nnq</span>
  <span class="nv">$ </span>tcpdump <span class="nt">-i</span> eth0 tcp port 80 <span class="nt">-w</span> /root/svc1-1.pcap
  <span class="nv">$ </span>tcpdump <span class="nt">-i</span> eth0 tcp port 9000 <span class="nt">-nnq</span>
  <span class="nv">$ </span>ngrep <span class="nt">-tW</span> byline <span class="nt">-d</span> eth0 <span class="s1">''</span> <span class="s1">'tcp port 80'</span>
      
  <span class="c"># tcpdump/ngrep : vethX</span>
  <span class="c"># $ VETH1=&lt;각자 자신의 veth 이름&gt;</span>
  <span class="nv">$ VETH1</span><span class="o">=</span>veth9a888981
  <span class="nv">$ </span>tcpdump <span class="nt">-i</span> <span class="nv">$VETH1</span> tcp port 80 <span class="nt">-nn</span>
  <span class="nv">$ </span>tcpdump <span class="nt">-i</span> <span class="nv">$VETH1</span> tcp port 80 <span class="nt">-w</span> /root/svc1-2.pcap
  <span class="nv">$ </span>tcpdump <span class="nt">-i</span> <span class="nv">$VETH1</span> tcp port 9000 <span class="nt">-nn</span>
  <span class="nv">$ </span>ngrep <span class="nt">-tW</span> byline <span class="nt">-d</span> <span class="nv">$VETH1</span> <span class="s1">''</span> <span class="s1">'tcp port 80'</span>
      
  <span class="nv">$ </span><span class="nb">exit</span>
  <span class="nt">----------------------------------</span>
      
  <span class="c"># 방안2 : kind 노드 컨테이너 bash 직접 접속하지 않고 호스트에서 tcpdump 하기</span>
  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker tcpdump <span class="nt">-i</span> eth0 tcp port 80 <span class="nt">-nnq</span>
  <span class="nv">$ VETH1</span><span class="o">=</span>&lt;각자 자신의 veth 이름&gt; docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker ip <span class="nt">-c</span> route
  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker tcpdump <span class="nt">-i</span> <span class="nv">$VETH1</span> tcp port 80 <span class="nt">-nnq</span>
      
  <span class="c"># 호스트PC에 pcap 파일 복사 &gt;&gt; wireshark 에서 분석</span>
  <span class="nv">$ </span>docker <span class="nb">cp </span>myk8s-worker:/root/svc1-1.pcap <span class="nb">.</span>
  <span class="nv">$ </span>docker <span class="nb">cp </span>myk8s-worker:/root/svc1-2.pcap <span class="nb">.</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>net-pod 포드에 접속 후 10개 curl 요청</p>

        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> zsh
<span class="nt">----------------------------------</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..10<span class="o">}</span><span class="p">;</span>   <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$SVC1</span>:9000 | <span class="nb">grep </span>Hostname<span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
<span class="c"># =&gt;       4 Hostname: webpod3</span>
<span class="c">#          3 Hostname: webpod2</span>
<span class="c">#          3 Hostname: webpod1</span>
    
<span class="nv">$ </span><span class="nb">exit</span>
<span class="nt">----------------------------------</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>각각 net-pod와 워커 노드들의 패킷캡쳐파일(*.pcap)를 받아서 와이어샤크로 확인해보겠습니다.</p>
        <ul>
          <li>
            <p>net-pod(10.10.0.7)에서 서비스:9000 (IP:10.200.1.96)으로 요청된 패킷이 DNAT 되어 k8s-worker의 webpod1:80 (IP:10.10.4.3)으로 전달되고, 응답은 그 반대로 전달 되는 것을 확인 할 수 있습니다.</p>

            <p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_14.png" alt="img.png" /></p>
          </li>
          <li>
            <p>또한 Stastics 메뉴의→ Flow Graph 기능을 통해 패킷의 흐름을 확인할 수 있었습니다.</p>

            <p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_15.png" alt="img.png" /></p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>iptables 정책 확인
    <ul>
      <li>kubernetes에서 service는 다음의 iptables 과정을 거칩니다.
        <ul>
          <li>(1) PREROUTING ⇒ (2) KUBE-SERVICES ⇒ (3) KUBE-SVC-YYY ⇒ (4) KUBE-SEP-#파드1, KUBE-SEP-#파드2, KUBE-SEP-#파드3</li>
          <li>그림으로 나타내면 다음과 같습니다.</li>
        </ul>

        <p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_16.png" alt="img.png" /></p>

        <ul>
          <li>
            <p>각각에 대하여 iptables 룰을 확인해보겠습니다.</p>

            <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c"># 컨트롤플레인에서 확인하겠습니다.</span>
  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane bash
  <span class="nt">----------------------------------------</span>
        
  <span class="c"># iptables 확인</span>
  <span class="nv">$ </span>iptables <span class="nt">-t</span> filter <span class="nt">-S</span>
  <span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span>
  <span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span> | <span class="nb">wc</span> <span class="nt">-l</span>
  <span class="c"># =&gt; 97</span>
  <span class="nv">$ </span>iptables <span class="nt">-t</span> mangle <span class="nt">-S</span>
        
  <span class="c"># iptables 상세 확인 - 매칭 패킷 카운트, 인터페이스 정보 등 포함</span>
  <span class="nv">$ </span>iptables <span class="nt">-nvL</span> <span class="nt">-t</span> filter
  <span class="nv">$ </span>iptables <span class="nt">-nvL</span> <span class="nt">-t</span> nat
  <span class="nv">$ </span>iptables <span class="nt">-nvL</span> <span class="nt">-t</span> mangle
        
  <span class="c"># rule 갯수 확인</span>
  <span class="nv">$ </span>iptables <span class="nt">-nvL</span> <span class="nt">-t</span> filter | <span class="nb">wc</span> <span class="nt">-l</span>
  <span class="c"># =&gt; 47</span>
  <span class="nv">$ </span>iptables <span class="nt">-nvL</span> <span class="nt">-t</span> nat | <span class="nb">wc</span> <span class="nt">-l</span>
  <span class="c"># =&gt; 158</span>
        
  <span class="c"># 규칙 패킷 바이트 카운트 초기화</span>
  <span class="nv">$ </span>iptables <span class="nt">-t</span> filter <span class="nt">--zero</span><span class="p">;</span> iptables <span class="nt">-t</span> nat <span class="nt">--zero</span><span class="p">;</span> iptables <span class="nt">-t</span> mangle <span class="nt">--zero</span>
        
  <span class="c"># 정책 확인 : 아래 정책 내용은 핵심적인 룰(rule)만 표시했습니다!</span>
  <span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-nvL</span>
  <span class="c"># =&gt; Chain PREROUTING (policy ACCEPT 121 packets, 7260 bytes) &lt;&lt;1. PREROUTING&gt;&gt;</span>
  <span class="c">#     pkts bytes target     prot opt in     out     source               destination         </span>
  <span class="c">#      121  7260 KUBE-SERVICES  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service portals */</span>
  <span class="c">#    ...</span>
  <span class="c">#    </span>
  <span class="c">#    Chain INPUT (policy ACCEPT 121 packets, 7260 bytes)</span>
  <span class="c">#    </span>
  <span class="c">#    Chain OUTPUT (policy ACCEPT 392 packets, 23520 bytes)</span>
  <span class="c">#     pkts bytes target     prot opt in     out     source               destination         </span>
  <span class="c">#      392 23520 KUBE-SERVICES  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service portals */</span>
  <span class="c">#    ...</span>
  <span class="c">#    </span>
  <span class="c">#    Chain KUBE-MARK-MASQ (18 references)</span>
  <span class="c">#     pkts bytes target     prot opt in     out     source               destination         </span>
  <span class="c">#        0     0 MARK       0    --  *      *       0.0.0.0/0            0.0.0.0/0            MARK or 0x4000</span>
  <span class="c">#    </span>
  <span class="c">#    Chain KUBE-SERVICES (2 references) &lt;&lt;2. SERVICES&gt;&gt;</span>
  <span class="c">#     pkts bytes target     prot opt in     out     source               destination         </span>
  <span class="c">#        0     0 KUBE-SVC-KBDEBIL6IU6WL7RF  6    --  *      *       0.0.0.0/0            10.200.1.96          /* default/svc-clusterip:svc-webport cluster IP */ tcp dpt:9000</span>
  <span class="c">#    ...</span>
  <span class="c">#    </span>
  <span class="c">#    Chain KUBE-SVC-KBDEBIL6IU6WL7RF (1 references) &lt;&lt;3. KUBE-SVC-YYY&gt;&gt;</span>
  <span class="c">#     pkts bytes target     prot opt in     out     source               destination         </span>
  <span class="c">#        0     0 KUBE-MARK-MASQ  6    --  *      *      !10.10.0.0/16         10.200.1.96          /* default/svc-clusterip:svc-webport cluster IP */ tcp dpt:9000</span>
  <span class="c">#        0     0 KUBE-SEP-T7YVH2JOMUTQFUDU  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/svc-clusterip:svc-webport -&gt; 10.10.1.4:80 */ statistic mode random probability 0.33333333349</span>
  <span class="c">#        0     0 KUBE-SEP-SZHENXPAXVOCHRDA  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/svc-clusterip:svc-webport -&gt; 10.10.2.3:80 */ statistic mode random probability 0.50000000000</span>
  <span class="c">#        0     0 KUBE-SEP-X47GKN7LA32LZ4H7  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/svc-clusterip:svc-webport -&gt; 10.10.4.3:80 */</span>
  <span class="c">#    </span>
  <span class="c">#    Chain KUBE-SEP-X47GKN7LA32LZ4H7 (1 references) &lt;&lt;4. KUBE-SEP-#WEBPOD1&gt;&gt;</span>
  <span class="c">#     pkts bytes target     prot opt in     out     source               destination         </span>
  <span class="c">#        0     0 KUBE-MARK-MASQ  0    --  *      *       10.10.4.3            0.0.0.0/0            /* default/svc-clusterip:svc-webport */</span>
  <span class="c">#        0     0 DNAT       6    --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/svc-clusterip:svc-webport */ tcp to:10.10.4.3:80</span>
  <span class="c">#    </span>
  <span class="c">#    Chain KUBE-SEP-T7YVH2JOMUTQFUDU (1 references) &lt;&lt;4. KUBE-SEP-#WEBPOD2&gt;&gt;</span>
  <span class="c">#     pkts bytes target     prot opt in     out     source               destination         </span>
  <span class="c">#        0     0 KUBE-MARK-MASQ  0    --  *      *       10.10.1.4            0.0.0.0/0            /* default/svc-clusterip:svc-webport */</span>
  <span class="c">#        0     0 DNAT       6    --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/svc-clusterip:svc-webport */ tcp to:10.10.1.4:80</span>
  <span class="c">#</span>
  <span class="c">#    Chain KUBE-SEP-SZHENXPAXVOCHRDA (1 references) &lt;&lt;4. KUBE-SEP-#WEBPOD3&gt;&gt;</span>
  <span class="c">#     pkts bytes target     prot opt in     out     source               destination         </span>
  <span class="c">#        0     0 KUBE-MARK-MASQ  0    --  *      *       10.10.2.3            0.0.0.0/0            /* default/svc-clusterip:svc-webport */</span>
  <span class="c">#        0     0 DNAT       6    --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/svc-clusterip:svc-webport */ tcp to:10.10.2.3:80</span>
        
  <span class="nv">$ </span>iptables <span class="nt">-v</span> <span class="nt">--numeric</span> <span class="nt">--table</span> nat <span class="nt">--list</span> PREROUTING | column <span class="nt">-t</span>
  <span class="c"># =&gt; Chain  PREROUTING  (policy        ACCEPT  777  packets,  46620  bytes)</span>
  <span class="c">#    pkts   bytes       target         prot    opt  in        out    source     destination</span>
  <span class="c">#    777    46620       KUBE-SERVICES  0       --   *         *      0.0.0.0/0  0.0.0.0/0    /*  kubernetes  service  portals  */</span>
  <span class="c">#    0      0           DOCKER_OUTPUT  0       --   *         *      0.0.0.0/0  172.23.0.1</span>
        
  <span class="nv">$ </span>iptables <span class="nt">-v</span> <span class="nt">--numeric</span> <span class="nt">--table</span> nat <span class="nt">--list</span> KUBE-SERVICES | column
  <span class="c"># 바로 아래 룰(rule)에 의해서 서비스(ClusterIP)를 인지하고 처리를 합니다</span>
  <span class="c"># =&gt; Chain  KUBE-SERVICES  (2                         references)</span>
  <span class="c">#    pkts   bytes          target                     prot         opt  in  out  source     destination</span>
  <span class="c">#    0      0              KUBE-SVC-KBDEBIL6IU6WL7RF  6            --   *   *    0.0.0.0/0  10.200.1.96   /*  default/svc-clusterip:svc-webport  cluster  IP          */     tcp   dpt:9000</span>
        
  <span class="nv">$ </span>iptables <span class="nt">-v</span> <span class="nt">--numeric</span> <span class="nt">--table</span> nat <span class="nt">--list</span> KUBE-SVC-KBDEBIL6IU6WL7RF | column
  <span class="c"># =&gt; Chain  KUBE-SVC-KBDEBIL6IU6WL7RF  (1                         references)</span>
  <span class="c">#    pkts   bytes                      target                     prot         opt  in  out  source         destination</span>
  <span class="c">#    0      0                          KUBE-SEP-T7YVH2JOMUTQFUDU  0            --   *   *    0.0.0.0/0      0.0.0.0/0    /*  default/svc-clusterip:svc-webport  -&gt;       10.10.1.4:80  */  statistic  mode      random  probability  0.33333333349</span>
  <span class="c">#    0      0                          KUBE-SEP-SZHENXPAXVOCHRDA  0            --   *   *    0.0.0.0/0      0.0.0.0/0    /*  default/svc-clusterip:svc-webport  -&gt;       10.10.2.3:80  */  statistic  mode      random  probability  0.50000000000</span>
  <span class="c">#    0      0                          KUBE-SEP-X47GKN7LA32LZ4H7  0            --   *   *    0.0.0.0/0      0.0.0.0/0    /*  default/svc-clusterip:svc-webport  -&gt;       10.10.4.3:80  */</span>
        
  <span class="c"># 패킷 전달 수를 확인 하기 위해 watch를 겁니다.</span>
  <span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s1">'iptables -v --numeric --table nat --list KUBE-SVC-KBDEBIL6IU6WL7RF'</span>
        
  <span class="c"># control-plane 에서 테스트 패킷을 보냅니다.</span>
  <span class="nv">$ SVC1</span><span class="o">=</span><span class="si">$(</span>kubectl get svc svc-clusterip <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.spec.clusterIP<span class="o">}</span><span class="si">)</span>
  <span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> zsh <span class="nt">-c</span> <span class="s2">"for i in {1..100};   do curl -s </span><span class="nv">$SVC1</span><span class="s2">:9000 | grep Hostname; sleep 1; done"</span>
</code></pre></div>            </div>

            <p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_17.png" alt="img.png" /></p>
          </li>
          <li>
            <p>iptables에서  카운트가 증가함을 확인 할 수 있습니다.</p>

            <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># SVC-### 에서 랜덤 확률(대략 33%)로 SEP(Service EndPoint)인 각각 파드 IP로 DNAT 됩니다!</span>
<span class="c">## 첫번째 룰에 일치 확률은 33% 이고, 매칭되지 않을 경우 아래 2개 남을때는 룰 일치 확률은 50%가 됩니다. 이것도 매칭되지 않으면 마지막 룰로 100% 일치됩니다</span>
<span class="c">#    Chain KUBE-SVC-KBDEBIL6IU6WL7RF (1 references)</span>
<span class="c">#     pkts bytes target     prot opt in     out     source               destination</span>
<span class="c">#        0     0 KUBE-MARK-MASQ  6    --  *      *      !10.10.0.0/16         10.200.1.96          /* default/svc-clusterip:svc-webport cluster IP */ tcp dpt:9000</span>
<span class="c">#       41  2460 KUBE-SEP-T7YVH2JOMUTQFUDU  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/svc-clusterip:svc-webport -&gt; 10.10.1.4:80 */ statistic mode random probability 0.33333333349</span>
<span class="c">#       47  2820 KUBE-SEP-SZHENXPAXVOCHRDA  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/svc-clusterip:svc-webport -&gt; 10.10.2.3:80 */ statistic mode random probability 0.50000000000</span>
<span class="c">#       45  2700 KUBE-SEP-X47GKN7LA32LZ4H7  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/svc-clusterip:svc-webport -&gt; 10.10.4.3:80 */</span>
      
<span class="c"># $ iptables -v --numeric --table nat --list KUBE-SEP-&lt;각자 값 입력&gt;</span>
<span class="nv">$ </span>iptables <span class="nt">-v</span> <span class="nt">--numeric</span> <span class="nt">--table</span> nat <span class="nt">--list</span> KUBE-SEP-T7YVH2JOMUTQFUDU
<span class="c"># =&gt; Chain  KUBE-SEP-T7YVH2JOMUTQFUDU  (1              references)</span>
<span class="c">#    pkts   bytes                      target          prot         opt  in  out  source     destination</span>
<span class="c">#    49     2940                       DNAT            6            --   *   *    0.0.0.0/0  0.0.0.0/0    /*  default/svc-clusterip:svc-webport  */  tcp  to:10.10.1.4:80</span>
      
<span class="nv">$ </span>iptables <span class="nt">-v</span> <span class="nt">--numeric</span> <span class="nt">--table</span> nat <span class="nt">--list</span> KUBE-SEP-SZHENXPAXVOCHRDA  | column <span class="nt">-t</span>
<span class="c"># =&gt; Chain  KUBE-SEP-SZHENXPAXVOCHRDA  (1              references)</span>
<span class="c">#    pkts   bytes                      target          prot         opt  in  out  source     destination</span>
<span class="c">#    0      0                          KUBE-MARK-MASQ  0            --   *   *    10.10.2.3  0.0.0.0/0    /*  default/svc-clusterip:svc-webport  */</span>
<span class="c">#    56     3360                       DNAT            6            --   *   *    0.0.0.0/0  0.0.0.0/0    /*  default/svc-clusterip:svc-webport  */  tcp  to:10.10.2.3:80</span>
      
<span class="nv">$ </span>iptables <span class="nt">-v</span> <span class="nt">--numeric</span> <span class="nt">--table</span> nat <span class="nt">--list</span> KUBE-SEP-X47GKN7LA32LZ4H7  | column <span class="nt">-t</span>
<span class="c"># =&gt; Chain  KUBE-SEP-X47GKN7LA32LZ4H7  (1              references)</span>
<span class="c">#    pkts   bytes                      target          prot         opt  in  out  source     destination</span>
<span class="c">#    0      0                          KUBE-MARK-MASQ  0            --   *   *    10.10.4.3  0.0.0.0/0    /*  default/svc-clusterip:svc-webport  */</span>
<span class="c">#    48     2880                       DNAT            6            --   *   *    0.0.0.0/0  0.0.0.0/0    /*  default/svc-clusterip:svc-webport  */  tcp  to:10.10.4.3:80</span>
      
<span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">--zero</span>
<span class="nv">$ </span>iptables <span class="nt">-v</span> <span class="nt">--numeric</span> <span class="nt">--table</span> nat <span class="nt">--list</span> POSTROUTING | column<span class="p">;</span> <span class="nb">echo</span> <span class="p">;</span> iptables <span class="nt">-v</span> <span class="nt">--numeric</span> <span class="nt">--table</span> nat <span class="nt">--list</span> KUBE-POSTROUTING | column
<span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s1">'iptables -v --numeric --table nat --list POSTROUTING; echo ; iptables -v --numeric --table nat --list KUBE-POSTROUTING'</span>
<span class="c"># POSTROUTE(nat) : 0x4000 마킹 되어 있지 않으니 RETURN 되고 그냥 빠져나가서 SNAT 되지 않는다!</span>
<span class="c"># =&gt; Chain  POSTROUTING  (policy             ACCEPT  0    packets,  0    bytes)</span>
<span class="c">#    pkts   bytes        target              prot    opt  in        out  source     destination</span>
<span class="c">#    0      0            KUBE-POSTROUTING    0       --   *         *    0.0.0.0/0  0.0.0.0/0    /*        kubernetes  postrouting  rules   */</span>
<span class="c">#    0      0            DOCKER_POSTROUTING  0       --   *         *    0.0.0.0/0  172.23.0.1</span>
<span class="c">#    0      0            KIND-MASQ-AGENT     0       --   *         *    0.0.0.0/0  0.0.0.0/0    ADDRTYPE  match       dst-type     !LOCAL  /*  kind-masq-agent:  ensure  nat  POSTROUTING  directs  all  non-LOCAL  destination  traffic  to  our  custom  KIND-MASQ-AGENT  chain  */</span>
<span class="c"># =&gt; Chain  KUBE-POSTROUTING  (1          references)</span>
<span class="c">#    pkts   bytes             target      prot         opt  in  out  source     destination</span>
<span class="c">#    0      0                 RETURN      0            --   *   *    0.0.0.0/0  0.0.0.0/0    mark  match       !        0x4000/0x4000</span>
<span class="c">#    0      0                 MARK        0            --   *   *    0.0.0.0/0  0.0.0.0/0    MARK  xor         0x4000</span>
<span class="c">#    0      0                 MASQUERADE  0            --   *   *    0.0.0.0/0  0.0.0.0/0    /*    kubernetes  service  traffic        requiring  SNAT  */  random-fully</span>
      
<span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span> | <span class="nb">grep </span>KUBE-POSTROUTING
<span class="c"># =&gt; -N KUBE-POSTROUTING</span>
<span class="c">#    -A POSTROUTING -m comment --comment "kubernetes postrouting rules" -j KUBE-POSTROUTING</span>
<span class="c">#    -A KUBE-POSTROUTING -m mark ! --mark 0x4000/0x4000 -j RETURN</span>
<span class="c">#    -A KUBE-POSTROUTING -j MARK --set-xmark 0x4000/0x0</span>
<span class="c">#    -A KUBE-POSTROUTING -m comment --comment "kubernetes service traffic requiring SNAT" -j MASQUERADE --random-fully</span>
      
<span class="nv">$ </span><span class="nb">exit</span>
<span class="nt">----------------------------------------</span>
      
<span class="c"># 위 서비스 생성 시 kube-proxy 에 의해서 iptables 규칙이 모든 노드에 추가됨을 한번 더 확인</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane iptables <span class="nt">-v</span> <span class="nt">--numeric</span> <span class="nt">--table</span> nat <span class="nt">--list</span> KUBE-SVC-KBDEBIL6IU6WL7RF
<span class="c"># =&gt; Chain KUBE-SVC-KBDEBIL6IU6WL7RF (1 references)</span>
<span class="c">#     pkts bytes target     prot opt in     out     source               destination         </span>
<span class="c">#        0     0 KUBE-MARK-MASQ  6    --  *      *      !10.10.0.0/16         10.200.1.96          /* default/svc-clusterip:svc-webport cluster IP */ tcp dpt:9000</span>
<span class="c">#        0     0 KUBE-SEP-T7YVH2JOMUTQFUDU  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/svc-clusterip:svc-webport -&gt; 10.10.1.4:80 */ statistic mode random probability 0.33333333349</span>
<span class="c">#        0     0 KUBE-SEP-SZHENXPAXVOCHRDA  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/svc-clusterip:svc-webport -&gt; 10.10.2.3:80 */ statistic mode random probability 0.50000000000</span>
<span class="c">#        0     0 KUBE-SEP-X47GKN7LA32LZ4H7  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/svc-clusterip:svc-webport -&gt; 10.10.4.3:80 */</span>
      
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> iptables <span class="nt">-v</span> <span class="nt">--numeric</span> <span class="nt">--table</span> nat <span class="nt">--list</span> KUBE-SVC-KBDEBIL6IU6WL7RF<span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; &gt;&gt; node myk8s-control-plane &lt;&lt;</span>
<span class="c">#    Chain KUBE-SVC-KBDEBIL6IU6WL7RF (1 references)</span>
<span class="c">#     pkts bytes target     prot opt in     out     source               destination         </span>
<span class="c">#        0     0 KUBE-MARK-MASQ  6    --  *      *      !10.10.0.0/16         10.200.1.96          /* default/svc-clusterip:svc-webport cluster IP */ tcp dpt:9000</span>
<span class="c">#        0     0 KUBE-SEP-T7YVH2JOMUTQFUDU  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/svc-clusterip:svc-webport -&gt; 10.10.1.4:80 */ statistic mode random probability 0.33333333349</span>
<span class="c">#        0     0 KUBE-SEP-SZHENXPAXVOCHRDA  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/svc-clusterip:svc-webport -&gt; 10.10.2.3:80 */ statistic mode random probability 0.50000000000</span>
<span class="c">#        0     0 KUBE-SEP-X47GKN7LA32LZ4H7  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/svc-clusterip:svc-webport -&gt; 10.10.4.3:80 */</span>
<span class="c">#    </span>
<span class="c">#    &gt;&gt; node myk8s-worker &lt;&lt;</span>
<span class="c">#    Chain KUBE-SVC-KBDEBIL6IU6WL7RF (1 references)</span>
<span class="c">#     pkts bytes target     prot opt in     out     source               destination         </span>
<span class="c">#        0     0 KUBE-MARK-MASQ  6    --  *      *      !10.10.0.0/16         10.200.1.96          /* default/svc-clusterip:svc-webport cluster IP */ tcp dpt:9000</span>
<span class="c">#        0     0 KUBE-SEP-T7YVH2JOMUTQFUDU  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/svc-clusterip:svc-webport -&gt; 10.10.1.4:80 */ statistic mode random probability 0.33333333349</span>
<span class="c">#        0     0 KUBE-SEP-SZHENXPAXVOCHRDA  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/svc-clusterip:svc-webport -&gt; 10.10.2.3:80 */ statistic mode random probability 0.50000000000</span>
<span class="c">#        0     0 KUBE-SEP-X47GKN7LA32LZ4H7  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/svc-clusterip:svc-webport -&gt; 10.10.4.3:80 */</span>
<span class="c">#    </span>
<span class="c">#    &gt;&gt; node myk8s-worker2 &lt;&lt;</span>
<span class="c">#    Chain KUBE-SVC-KBDEBIL6IU6WL7RF (1 references)</span>
<span class="c">#     pkts bytes target     prot opt in     out     source               destination         </span>
<span class="c">#        0     0 KUBE-MARK-MASQ  6    --  *      *      !10.10.0.0/16         10.200.1.96          /* default/svc-clusterip:svc-webport cluster IP */ tcp dpt:9000</span>
<span class="c">#        0     0 KUBE-SEP-T7YVH2JOMUTQFUDU  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/svc-clusterip:svc-webport -&gt; 10.10.1.4:80 */ statistic mode random probability 0.33333333349</span>
<span class="c">#        0     0 KUBE-SEP-SZHENXPAXVOCHRDA  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/svc-clusterip:svc-webport -&gt; 10.10.2.3:80 */ statistic mode random probability 0.50000000000</span>
<span class="c">#        0     0 KUBE-SEP-X47GKN7LA32LZ4H7  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/svc-clusterip:svc-webport -&gt; 10.10.4.3:80 */</span>
<span class="c">#    </span>
<span class="c">#    &gt;&gt; node myk8s-worker3 &lt;&lt;</span>
<span class="c">#    Chain KUBE-SVC-KBDEBIL6IU6WL7RF (1 references)</span>
<span class="c">#     pkts bytes target     prot opt in     out     source               destination         </span>
<span class="c">#        0     0 KUBE-MARK-MASQ  6    --  *      *      !10.10.0.0/16         10.200.1.96          /* default/svc-clusterip:svc-webport cluster IP */ tcp dpt:9000</span>
<span class="c">#        0     0 KUBE-SEP-T7YVH2JOMUTQFUDU  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/svc-clusterip:svc-webport -&gt; 10.10.1.4:80 */ statistic mode random probability 0.33333333349</span>
<span class="c">#        0     0 KUBE-SEP-SZHENXPAXVOCHRDA  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/svc-clusterip:svc-webport -&gt; 10.10.2.3:80 */ statistic mode random probability 0.50000000000</span>
<span class="c">#        0     0 KUBE-SEP-X47GKN7LA32LZ4H7  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/svc-clusterip:svc-webport -&gt; 10.10.4.3:80 */</span>
</code></pre></div>            </div>
          </li>
          <li>
            <p>동일한 iptables 룰이 각 노드에 있는 것을 확인할 수 있습니다.</p>
          </li>
        </ul>
      </li>
      <li>파드 1개에 장애를 발생시켜서 장애시 동작을 확인해보겠습니다.
        <ul>
          <li>
            <p>동작 확인을 위한 모니터링</p>

            <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c"># 터미널1 &gt;&gt; ENDPOINTS 변화를 잘 확인해보자!</span>
  <span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s1">'kubectl get pod -owide;echo; kubectl get svc,ep svc-clusterip;echo; kubectl get endpointslices -l kubernetes.io/service-name=svc-clusterip'</span>
        
  <span class="c"># 터미널2</span>
  <span class="nv">$ SVC1</span><span class="o">=</span><span class="si">$(</span>kubectl get svc svc-clusterip <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.spec.clusterIP<span class="o">}</span><span class="si">)</span>
  <span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> zsh <span class="nt">-c</span> <span class="s2">"while true; do curl -s --connect-timeout 1 </span><span class="nv">$SVC1</span><span class="s2">:9000 | egrep 'Hostname|IP: 10'; date '+%Y-%m-%d %H:%M:%S' ; echo ;  sleep 1; done"</span>
  <span class="c"># 혹은</span>
  <span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> zsh <span class="nt">-c</span> <span class="s2">"for i in {1..100};  do curl -s </span><span class="nv">$SVC1</span><span class="s2">:9000 | grep Hostname; done | sort | uniq -c | sort -nr"</span>
</code></pre></div>            </div>
          </li>
          <li>
            <p>파드 1개 삭제 후 확인</p>

            <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c"># (방안1) 파드3번 삭제 &gt;&gt; 서비스의 엔드포인트가 어떻게 변경되는지 확인 하자!, 지속적인 curl 접속 결과 확인!, for 문 실행 시 결과 확인!, 절체 시간(순단) 확인!</span>
  <span class="nv">$ </span>kubectl delete pod webpod3
        
  <span class="c"># (방안1) 결과 확인 후 다시 파드 3번 생성 &gt;&gt; 서비스 디스커버리!</span>
  <span class="nv">$ </span>kubectl apply <span class="nt">-f</span> 3pod.yaml
        
  <span class="nt">---------------------------------</span>
  <span class="c"># (방안2) 파드3번에 레이블 삭제</span>
  <span class="nv">$ </span>kubectl get pod <span class="nt">--show-labels</span>
        
  <span class="c">## 레이블(라벨)의 키값 바로 뒤에 하이픈(-) 입력 시 해당 레이블 삭제됨! &gt;&gt; 레이블과 셀렉터는 쿠버네티스 환경에서 매우 많이 활용된다!</span>
  <span class="nv">$ </span>kubectl label pod webpod3 app-
  <span class="nv">$ </span>kubectl get pod <span class="nt">--show-labels</span>
        
  <span class="c"># (방안2) 결과 확인 후 파드3번에 다시 레이블 생성</span>
  <span class="nv">$ </span>kubectl label pod webpod3 <span class="nv">app</span><span class="o">=</span>webpod
</code></pre></div>            </div>

            <ul>
              <li>
                <p>파드 삭제 전</p>

                <p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_18.png" alt="img.png" /></p>
              </li>
              <li>
                <p>파드 삭제 후</p>

                <p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_19.png" alt="img.png" /></p>
              </li>
              <li>
                <p>파드 다시 생성 후</p>

                <p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_20.png" alt="img.png" /></p>
              </li>
              <li>
                <p>레이블 삭제 후</p>

                <p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_21.png" alt="img.png" /></p>
              </li>
              <li>
                <p>레이블 복구 후</p>

                <p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_22.png" alt="img.png" /></p>
              </li>
            </ul>
          </li>
          <li>
            <p>파드가 삭제되고 복구 됨에 따라 서비스 엔드포인트에서 삭제되고, label selector 에 따라서도 엔드포인트에서 삭제되고 복구됨을 확인할 수 있었습니다.</p>
          </li>
        </ul>
      </li>
      <li>sessionAffinity: ClientIP
        <ul>
          <li><code class="language-plaintext highlighter-rouge">sessionAffinity: ClientIP</code> : 클라이언트가 접속한 목적지(파드)에 고정적인 접속을 지원하게 할 수 있습니다.</li>
          <li>
            <p>기본적으로 서비스는 파드에 랜덤으로 부하를 분산하지만 <code class="language-plaintext highlighter-rouge">sessionAffinity: ClientIP</code>를 통해 동일한 파드에 접속하도록 강제 할 수 있습니다.</p>

            <p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_23.png" alt="img.png" /></p>
          </li>
          <li>
            <p>설정 및 파드 접속 확인</p>

            <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c"># 기본 정보 확인</span>
  <span class="nv">$ </span>kubectl get svc svc-clusterip <span class="nt">-o</span> yaml
  <span class="nv">$ </span>kubectl get svc svc-clusterip <span class="nt">-o</span> yaml | <span class="nb">grep </span>sessionAffinity
  <span class="c"># =&gt;   sessionAffinity: None</span>
        
  <span class="c"># 반복 접속</span>
  <span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> zsh <span class="nt">-c</span> <span class="s2">"while true; do curl -s --connect-timeout 1 </span><span class="nv">$SVC1</span><span class="s2">:9000 | egrep 'Hostname|IP: 10|Remote'; date '+%Y-%m-%d %H:%M:%S' ; echo ;  sleep 1; done"</span>
  <span class="c"># =&gt; Hostname: webpod2</span>
  <span class="c">#    IP: 10.10.1.4</span>
  <span class="c">#    RemoteAddr: 10.10.0.7:57246</span>
  <span class="c">#    2024-09-01 12:25:49</span>
  <span class="c">#    </span>
  <span class="c">#    Hostname: webpod1</span>
  <span class="c">#    IP: 10.10.4.3</span>
  <span class="c">#    RemoteAddr: 10.10.0.7:57250</span>
  <span class="c">#    2024-09-01 12:25:50</span>
  <span class="c">#    </span>
  <span class="c">#    Hostname: webpod3</span>
  <span class="c">#    IP: 10.10.2.6</span>
  <span class="c">#    RemoteAddr: 10.10.0.7:57252</span>
  <span class="c">#    2024-09-01 12:25:51</span>
        
  <span class="c"># 현재는 랜덤으로 접속 됩니다.</span>
        
  <span class="c"># sessionAffinity: ClientIP 설정 변경</span>
  <span class="nv">$ </span>kubectl patch svc svc-clusterip <span class="nt">-p</span> <span class="s1">'{"spec":{"sessionAffinity":"ClientIP"}}'</span>
  <span class="c"># =&gt; service/svc-clusterip patched</span>
  <span class="c"># 혹은</span>
  <span class="c">## $ kubectl get svc svc-clusterip -o yaml | sed -e "s/sessionAffinity: None/sessionAffinity: ClientIP/" | kubectl apply -f -</span>
        
  <span class="c">#</span>
  <span class="nv">$ </span>kubectl get svc svc-clusterip <span class="nt">-o</span> yaml
  <span class="c"># =&gt; ...</span>
  <span class="c">#      sessionAffinity: ClientIP</span>
  <span class="c">#      sessionAffinityConfig:</span>
  <span class="c">#        clientIP:</span>
  <span class="c">#          timeoutSeconds: 10800</span>
  <span class="c">#    ...</span>
        
  <span class="c"># 클라이언트(TestPod) Shell 실행</span>
  <span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> zsh <span class="nt">-c</span> <span class="s2">"for i in {1..100};  do curl -s </span><span class="nv">$SVC1</span><span class="s2">:9000 | grep Hostname; done | sort | uniq -c | sort -nr"</span>
  <span class="c"># =&gt; 100 Hostname: webpod2</span>
  <span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> net-pod <span class="nt">--</span> zsh <span class="nt">-c</span> <span class="s2">"for i in {1..1000}; do curl -s </span><span class="nv">$SVC1</span><span class="s2">:9000 | grep Hostname; done | sort | uniq -c | sort -nr"</span>
  <span class="c"># =&gt; 1000 Hostname: webpod2</span>
</code></pre></div>            </div>

            <ul>
              <li>sessionAffinity: ClientIP를 하면 spec.sessionAffinityConfig.clientIP.timeoutSeconds 시간동안 서비스를 통해 접속 되는 파드가 고정됨을 확인할 수 있었습니다.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>이상과 같이 ClusterIP 타입의 서비스를 확인해보았습니다.</li>
  <li>ClusterIP 타입의 서비스는 다음과 같은 단점이 있다고 합니다.
    <ul>
      <li>클러스터 외부에서는 서비스(ClusterIP)로 접속이 불가능합니다. ⇒ <strong>NodePort</strong> 타입으로 외부에서 접속 가능</li>
      <li>IPtables 는 파드에 대한 헬스체크 기능이 없어서 문제 있는 파드에 연결이 되는 경우가 있습니다. ⇒ 서비스 사용, 파드에 Readiness Probe 설정으로 파드 문제 시 서비스의 엔드포인트에서 제거되게 하자! ← 이 정도면 충분한가? 혹시 부족한 점이 없을까?</li>
      <li>서비스에 연동된 파드 갯수 퍼센트(%)로 <strong>랜덤 분산</strong> 방식, <strong>세션어피니티</strong> 이외에 <strong>다른 분산 방식 불가능합니다.</strong> ⇒ <strong>IPVS</strong> 경우 다양한 분산 방식(알고리즘) 가능
        <ul>
          <li>목적지 파드 다수가 있는 환경에서, 출발지 파드와 목적지 파드가 동일한 노드에 배치되어 있어도, 랜덤 분산으로 다른 노드에 목적지 파드로 연결 가능</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h4 id="nodeport-실습">NodePort 실습</h4>

<ul>
  <li>NodePort는 ClusterIP와 다르게 클러스터 외부에서도 접속 할 수 있습니다.</li>
  <li>컨트롤플레인을 포함한 모든 노드에 iptables rule이 적용되므로, 모든 노드에 NodePort로 접속시 iptables rule에 의해서 분산 접속이 됩니다.</li>
  <li>Node의 모든 Local IP (loopback을 포함한 각 호스트의 interface의 IP) 사용 가능하고 Local IP 지정도 가능합니다.</li>
  <li>쿠버네티스의 NodePort는 기본 30000~32767 포트에서 랜덤으로 지정됩니다.
    <ul>
      <li>
        <p>랜덤 포트 범위를 바꾸려면 다음과 같이  <code class="language-plaintext highlighter-rouge">/etc/kubernetes/manifests/kube-apiserver.yaml</code> 파일을 수정하여 kube-apiserver 의 파라메터에 <code class="language-plaintext highlighter-rouge">--service-node-port-range=시작포트-종료포트</code>를 변경하면됩니다. <a href="https://blog.frec.kr/cloud/modify_nodeport_range/">참고</a></p>

        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c"># /etc/kubernetes/manifests/kube-apiserver.yaml</span>
      
  ...
  spec:
    containers:
    - <span class="nb">command</span>:
      - kube-apiserver
      - <span class="nt">--authorization-mode</span><span class="o">=</span>Node,RBAC
      ...
      - <span class="nt">--service-node-port-range</span><span class="o">=</span>30000-50000
  ...
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>실습 구성
    <ul>
      <li>
        <p>목적지(backend) 디플로이먼트 파일 생성 : echo-deploy.yml</p>

        <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
  <span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
  <span class="na">metadata</span><span class="pi">:</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">deploy-echo</span>
  <span class="na">spec</span><span class="pi">:</span>
    <span class="na">replicas</span><span class="pi">:</span> <span class="m">3</span>
    <span class="na">selector</span><span class="pi">:</span>
      <span class="na">matchLabels</span><span class="pi">:</span>
        <span class="na">app</span><span class="pi">:</span> <span class="s">deploy-websrv</span>
    <span class="na">template</span><span class="pi">:</span>
      <span class="na">metadata</span><span class="pi">:</span>
        <span class="na">labels</span><span class="pi">:</span>
          <span class="na">app</span><span class="pi">:</span> <span class="s">deploy-websrv</span>
      <span class="na">spec</span><span class="pi">:</span>
        <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
        <span class="na">containers</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">kans-websrv</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">mendhak/http-https-echo</span>
          <span class="na">ports</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">8080</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>서비스(NodePort) 파일 생성 : svc-nodeport.yml</p>

        <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
  <span class="na">kind</span><span class="pi">:</span> <span class="err">*</span><span class="nv">*Service</span><span class="err">**</span>
  <span class="na">metadata</span><span class="pi">:</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">svc-nodeport</span>
  <span class="na">spec</span><span class="pi">:</span>
    <span class="na">ports</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">svc-webport</span>
        <span class="na">port</span><span class="pi">:</span> <span class="m">9000</span>        <span class="c1"># 서비스 ClusterIP 에 접속 시 사용하는 포트 port 를 의미</span>
        <span class="na">targetPort</span><span class="pi">:</span> <span class="m">8080</span>  <span class="c1"># 타킷 targetPort 는 서비스를 통해서 목적지 파드로 접속 시 해당 파드로 접속하는 포트를 의미</span>
    <span class="na">selector</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">deploy-websrv</span>
    <span class="na">**type</span><span class="pi">:</span> <span class="s">NodePort**</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>생성 및 확인</p>

        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c"># 생성</span>
  <span class="nv">$ </span>kubectl apply <span class="nt">-f</span> echo-deploy.yml,svc-nodeport.yml
  <span class="c"># =&gt; deployment.apps/deploy-echo created</span>
  <span class="c">#    service/svc-nodeport created</span>
      
  <span class="c"># 모니터링</span>
  <span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s1">'kubectl get pod -owide;echo; kubectl get svc,ep svc-nodeport'</span>
      
  <span class="c"># 확인</span>
  <span class="nv">$ </span>kubectl get deploy,pod <span class="nt">-o</span> wide
  <span class="c"># =&gt; NAME                          READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS    IMAGES                    SELECTOR</span>
  <span class="c">#    deployment.apps/deploy-echo   3/3     3            3           49s   kans-websrv   mendhak/http-https-echo   app=deploy-websrv</span>
  <span class="c">#    </span>
  <span class="c">#    NAME                               READY   STATUS    RESTARTS   AGE    IP          NODE                  NOMINATED NODE   READINESS GATES</span>
  <span class="c">#    pod/deploy-echo-5c689d5454-dxf2t   1/1     Running   0          49s    10.10.4.4   myk8s-worker          &lt;none&gt;           &lt;none&gt;</span>
  <span class="c">#    pod/deploy-echo-5c689d5454-rbgcp   1/1     Running   0          49s    10.10.1.5   myk8s-worker2         &lt;none&gt;           &lt;none&gt;</span>
  <span class="c">#    pod/deploy-echo-5c689d5454-wppr8   1/1     Running   0          49s    10.10.2.7   myk8s-worker3         &lt;none&gt;           &lt;none&gt;</span>
      
  <span class="c"># 아래 31791은 서비스(NodePort) 정보!</span>
  <span class="nv">$ </span>kubectl get svc svc-nodeport
  <span class="c"># =&gt; NAME           TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE</span>
  <span class="c">#    svc-nodeport   NodePort   10.200.1.169   &lt;none&gt;        9000:31791/TCP   69s</span>
      
  <span class="nv">$ </span>kubectl get endpoints svc-nodeport
  <span class="c"># =&gt; NAME           ENDPOINTS                                      AGE</span>
  <span class="c">#    svc-nodeport   10.10.1.5:8080,10.10.2.7:8080,10.10.4.4:8080   85s</span>
      
  <span class="c"># Port , TargetPort , NodePort 각각의 차이점의 의미를 알자!</span>
  <span class="nv">$ </span>kubectl describe svc svc-nodeport
  <span class="c"># =&gt; Name:                     svc-nodeport</span>
  <span class="c">#    Namespace:                default</span>
  <span class="c">#    Labels:                   &lt;none&gt;</span>
  <span class="c">#    Annotations:              &lt;none&gt;</span>
  <span class="c">#    Selector:                 app=deploy-websrv</span>
  <span class="c">#    Type:                     NodePort</span>
  <span class="c">#    IP Family Policy:         SingleStack</span>
  <span class="c">#    IP Families:              IPv4</span>
  <span class="c">#    IP:                       10.200.1.169</span>
  <span class="c">#    IPs:                      10.200.1.169</span>
  <span class="c">#    Port:                     svc-webport  9000/TCP     &lt;&lt;ClusterIP와 동일하게 동작하는 클러스터 내부에서 사용하는 포트&gt;&gt;</span>
  <span class="c">#    TargetPort:               8080/TCP                  &lt;&lt;파드의 컨테이너의 포트&gt;&gt;</span>
  <span class="c">#    NodePort:                 svc-webport  31791/TCP    &lt;&lt;각 Node에서 Listening 하는 nodePort&gt;&gt;</span>
  <span class="c">#    Endpoints:                10.10.1.5:8080,10.10.2.7:8080,10.10.4.4:8080    &lt;&lt;Port Forwarding 대상이 되는 파드의 엔드포인트 파드IP:파드Port&gt;&gt;</span>
  <span class="c">#    Session Affinity:         None</span>
  <span class="c">#    External Traffic Policy:  Cluster &lt;&lt;부하 분산방식&gt;&gt;</span>
  <span class="c">#    Events:                   &lt;none&gt;</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>2.3. 서비스 접속 확인
    <ul>
      <li>
        <p>NodePort의 서비스 접속을 통한 통신의 흐름</p>

        <p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_24.png" alt="img.png" /></p>

        <ul>
          <li>Client 가상 머신(192.168.10.200)에서 컨트롤 플레인 IP(192.168.10.10)의 nodePort 접속을 시도합니다.</li>
          <li>nodePort는 서비스(NodePort) 생성시에 할당된 랜덤포트가 사용 됩니다.</li>
          <li>컨트롤 플레인의 iptables의 NAT 테이블의 규칙과 매칭되어 목적지 IP와 목적지 Port는 변환 됩니다. 목적지 IP는 app=deploy-websrv 레이블을 가지고 있는 파드 3개가 대상이 되며, 랜덤 부하분산이 선택됩니다.</li>
        </ul>
      </li>
      <li>
        <p>실습을 통해 위의 과정을 확인해보겠습니다. 단 현재 실습환경에서는 컨트롤 플레인에는 파드가 없으므로 위의 설명과는 다르게 워커노드의 파드를 접속하는것으로 실습하겠습니다.</p>

        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c"># NodePort 확인 : 아래 NodePort 는 범위내 랜덤 할당으로 실습 환경마다 다릅니다</span>
  <span class="nv">$ </span>kubectl get service svc-nodeport <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.spec.ports[0].nodePort}'</span>
  <span class="c"># =&gt; 31791</span>
      
  <span class="c"># NodePort 를 변수에 지정</span>
  <span class="nv">$ NPORT</span><span class="o">=</span><span class="si">$(</span>kubectl get service svc-nodeport <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.spec.ports[0].nodePort}'</span><span class="si">)</span>
  <span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$NPORT</span>
  <span class="c"># =&gt; 31791</span>
      
  <span class="c"># 현재 k8s 버전에서는 포트 Listen 되지 않고, iptables rules 처리됨</span>
  <span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> ss <span class="nt">-tlnp</span><span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
  <span class="c"># =&gt; &gt;&gt; node myk8s-control-plane &lt;&lt;</span>
  <span class="c">#    State  Recv-Q Send-Q Local Address:Port  Peer Address:PortProcess                                  </span>
  <span class="c">#    LISTEN 0      4096       127.0.0.1:2381       0.0.0.0:*    users:(("etcd",pid=710,fd=15))          </span>
  <span class="c">#    ... nodePort인 31791를 LISTEN하는 건이 없음</span>
  <span class="c">#</span>
  <span class="c">#    &gt;&gt; node myk8s-worker &lt;&lt;</span>
  <span class="c">#    State  Recv-Q Send-Q Local Address:Port  Peer Address:PortProcess                              </span>
  <span class="c">#    LISTEN 0      4096      127.0.0.11:35033      0.0.0.0:*                                        </span>
  <span class="c">#    ... nodePort인 31791를 LISTEN하는 건이 없음</span>
  <span class="c">#    </span>
  <span class="c">#    &gt;&gt; node myk8s-worker2 &lt;&lt;</span>
  <span class="c">#    State  Recv-Q Send-Q Local Address:Port  Peer Address:PortProcess                              </span>
  <span class="c">#    LISTEN 0      4096      127.0.0.11:45927      0.0.0.0:*                                        </span>
  <span class="c">#    ... nodePort인 31791를 LISTEN하는 건이 없음</span>
  <span class="c">#    </span>
  <span class="c">#    &gt;&gt; node myk8s-worker3 &lt;&lt;</span>
  <span class="c">#    State  Recv-Q Send-Q Local Address:Port  Peer Address:PortProcess                              </span>
  <span class="c">#    LISTEN 0      4096       127.0.0.1:10248      0.0.0.0:*    users:(("kubelet",pid=262,fd=19))   </span>
  <span class="c">#    ... nodePort인 31791를 LISTEN하는 건이 없음</span>
      
  <span class="c">## (참고) 아래처럼 예전 k8s 환경에서 Service(NodePort) 생성 시, TCP Port Listen 되었었음</span>
  <span class="c"># $ root@k8s-m:~# ss -4tlnp | egrep "(Process|$NPORT)"</span>
  <span class="c"># State     Recv-Q    Send-Q        Local Address:Port        Peer Address:Port   Process</span>
  <span class="c"># LISTEN    0         4096                0.0.0.0:30466            0.0.0.0:*       users:(("kube-proxy",pid=8661,fd=10))</span>
      
  <span class="c"># 파드 로그 실시간 확인 (웹 파드에 접속자의 IP가 출력)</span>
  <span class="nv">$ </span>kubectl logs <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>deploy-websrv <span class="nt">-f</span>
  <span class="c"># =&gt; Listening on ports 8080 for http, and 8443 for https.</span>
  <span class="c">#    ...</span>
  <span class="c">#    ::ffff:172.23.0.2 - - [26/Sep/2024:04:35:00 +0000] "GET / HTTP/1.1" 200 396 "-" "curl/7.88.1"</span>
  <span class="c">#    ...</span>
      
  <span class="c"># 외부 클라이언트(mypc 컨테이너)에서 접속 시도를 해보자</span>
      
  <span class="c"># 노드의 IP와 NodePort를 변수에 지정</span>
  <span class="c">## CNODE=&lt;컨트롤플레인노드의 IP주소&gt;</span>
  <span class="c">## NODE1=&lt;노드1의 IP주소&gt;</span>
  <span class="c">## NODE2=&lt;노드2의 IP주소&gt;</span>
  <span class="c">## NODE3=&lt;노드3의 IP주소&gt;</span>
  <span class="nv">$ CNODE</span><span class="o">=</span>172.23.0.2
  <span class="nv">$ NODE1</span><span class="o">=</span>172.23.0.4
  <span class="nv">$ NODE2</span><span class="o">=</span>172.23.0.5
  <span class="nv">$ NODE3</span><span class="o">=</span>172.23.0.3
      
  <span class="nv">$ NPORT</span><span class="o">=</span><span class="si">$(</span>kubectl get service svc-nodeport <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.spec.ports[0].nodePort}'</span><span class="si">)</span>
  <span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$NPORT</span>
      
  <span class="c"># 서비스(NodePort) 부하분산 접속 확인</span>
  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc curl <span class="nt">-s</span> <span class="nv">$CNODE</span>:<span class="nv">$NPORT</span> | jq <span class="c"># headers.host 주소는 왜 그런거죠?</span>
  <span class="c"># =&gt; {</span>
  <span class="c">#      "path": "/",</span>
  <span class="c">#      "headers": {</span>
  <span class="c">#        "host": "172.23.0.2:31791",  &lt;&lt;여기의 headers.host는 요청하는 url의 주소인데, 우리가 $CNODE(컨트롤플레인의 IP)의 url로 접속했기 때문입니다.&gt;&gt;</span>
  <span class="c">#        "user-agent": "curl/8.7.1",</span>
  <span class="c">#        "accept": "*/*"</span>
  <span class="c">#      },</span>
  <span class="c">#      "method": "GET",</span>
  <span class="c">#      "body": "",</span>
  <span class="c">#      "fresh": false,</span>
  <span class="c">#      "hostname": "172.23.0.2",   &lt;&lt;이 hostname과&gt;&gt;</span>
  <span class="c">#      "ip": "::ffff:172.23.0.2",  &lt;&lt;이 ip는 접속하는 클라이언트의 ip인데 부하분산 과정에서 목적지가 Local Pod가 아닌 경우 Node IP로 POSTROUTING(SNAT) 되기 때문입니다.&gt;&gt;</span>
  <span class="c">#      "ips": [],</span>
  <span class="c">#      "protocol": "http",</span>
  <span class="c">#      "query": {},</span>
  <span class="c">#      "subdomains": [],</span>
  <span class="c">#      "xhr": false,</span>
  <span class="c">#      "os": {</span>
  <span class="c">#        "hostname": "deploy-echo-5c689d5454-dxf2t"</span>
  <span class="c">#      },</span>
  <span class="c">#      "connection": {}</span>
  <span class="c">#    }</span>
      
  <span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="nv">$CNODE</span> <span class="nv">$NODE1</span> <span class="nv">$NODE2</span> <span class="nv">$NODE3</span> <span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> mypc curl <span class="nt">-s</span> <span class="nv">$i</span>:<span class="nv">$NPORT</span><span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
  <span class="c"># =&gt; &gt;&gt; node 172.23.0.2 &lt;&lt;</span>
  <span class="c">#    {</span>
  <span class="c">#      "headers": {</span>
  <span class="c">#        "host": "172.23.0.2:31791",</span>
  <span class="c">#        ...</span>
  <span class="c">#      },</span>
  <span class="c">#      ...</span>
  <span class="c">#      "hostname": "172.23.0.2",</span>
  <span class="c">#      "ip": "::ffff:172.23.0.2",</span>
  <span class="c">#      ...</span>
  <span class="c">#      "os": {</span>
  <span class="c">#        "hostname": "deploy-echo-5c689d5454-dxf2t" </span>
  <span class="c">#      }</span>
  <span class="c">#    }</span>
  <span class="c">#    &gt;&gt; node 172.23.0.4 &lt;&lt;</span>
  <span class="c">#    {</span>
  <span class="c">#      "headers": {</span>
  <span class="c">#        "host": "172.23.0.4:31791",</span>
  <span class="c">#        ...</span>
  <span class="c">#      },</span>
  <span class="c">#      ...</span>
  <span class="c">#      "hostname": "172.23.0.4",</span>
  <span class="c">#      "ip": "::ffff:10.10.4.1",</span>
  <span class="c">#      ...</span>
  <span class="c">#      "os": {</span>
  <span class="c">#        "hostname": "deploy-echo-5c689d5454-dxf2t"</span>
  <span class="c">#      }</span>
  <span class="c">#    }</span>
  <span class="c">#    &gt;&gt; node 172.23.0.5 &lt;&lt;</span>
  <span class="c">#    {</span>
  <span class="c">#      "headers": {</span>
  <span class="c">#        "host": "172.23.0.5:31791",</span>
  <span class="c">#        ...</span>
  <span class="c">#      },</span>
  <span class="c">#      ...</span>
  <span class="c">#      "hostname": "172.23.0.5",</span>
  <span class="c">#      "ip": "::ffff:10.10.1.1",</span>
  <span class="c">#      ...</span>
  <span class="c">#      "os": {</span>
  <span class="c">#        "hostname": "deploy-echo-5c689d5454-rbgcp"</span>
  <span class="c">#      }</span>
  <span class="c">#    }</span>
  <span class="c">#    &gt;&gt; node 172.23.0.3 &lt;&lt;</span>
  <span class="c">#    {</span>
  <span class="c">#      "headers": {</span>
  <span class="c">#        "host": "172.23.0.3:31791",</span>
  <span class="c">#        ...</span>
  <span class="c">#      },</span>
  <span class="c">#      ...</span>
  <span class="c">#      "hostname": "172.23.0.3",</span>
  <span class="c">#      "ip": "::ffff:10.10.2.1",</span>
  <span class="c">#      ...</span>
  <span class="c">#      "os": {</span>
  <span class="c">#        "hostname": "deploy-echo-5c689d5454-wppr8"</span>
  <span class="c">#      }</span>
  <span class="c">#    }</span>
      
  <span class="c"># 컨트롤플레인 노드에는 목적지 파드가 없는데도, 접속을 받아줍니다! 이유는 서비스(nodePort)의 endpoint로 로드밸런싱 되기 때문입니다.</span>
  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc zsh <span class="nt">-c</span> <span class="s2">"for i in {1..100}; do curl -s </span><span class="nv">$CNODE</span><span class="s2">:</span><span class="nv">$NPORT</span><span class="s2"> | grep hostname; done | sort | uniq -c | sort -nr"</span>
  <span class="c"># =&gt;     100   "hostname": "172.23.0.2",</span>
  <span class="c">#         37     "hostname": "deploy-echo-5c689d5454-wppr8"</span>
  <span class="c">#         33     "hostname": "deploy-echo-5c689d5454-rbgcp"</span>
  <span class="c">#         30     "hostname": "deploy-echo-5c689d5454-dxf2t"</span>
  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc zsh <span class="nt">-c</span> <span class="s2">"for i in {1..100}; do curl -s </span><span class="nv">$NODE1</span><span class="s2">:</span><span class="nv">$NPORT</span><span class="s2"> | grep hostname; done | sort | uniq -c | sort -nr"</span>
  <span class="c"># =&gt;     100   "hostname": "172.23.0.4",</span>
  <span class="c">#         40     "hostname": "deploy-echo-5c689d5454-wppr8"</span>
  <span class="c">#         32     "hostname": "deploy-echo-5c689d5454-dxf2t"</span>
  <span class="c">#         28     "hostname": "deploy-echo-5c689d5454-rbgcp"$ docker exec -it mypc zsh -c "for i in {1..100}; do curl -s $NODE2:$NPORT | grep hostname; done | sort | uniq -c | sort -nr"</span>
  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc zsh <span class="nt">-c</span> <span class="s2">"for i in {1..100}; do curl -s </span><span class="nv">$NODE3</span><span class="s2">:</span><span class="nv">$NPORT</span><span class="s2"> | grep hostname; done | sort | uniq -c | sort -nr"</span>
  <span class="c"># =&gt;     100   "hostname": "172.23.0.3",</span>
  <span class="c">#         43     "hostname": "deploy-echo-5c689d5454-wppr8"</span>
  <span class="c">#         34     "hostname": "deploy-echo-5c689d5454-dxf2t"</span>
  <span class="c">#         23     "hostname": "deploy-echo-5c689d5454-rbgcp"</span>
  <span class="c"># 아래 반복 접속 실행 해두자</span>
  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc zsh <span class="nt">-c</span> <span class="s2">"while true; do curl -s --connect-timeout 1 </span><span class="nv">$CNODE</span><span class="s2">:</span><span class="nv">$NPORT</span><span class="s2"> | grep hostname; date '+%Y-%m-%d %H:%M:%S' ; echo ;  sleep 1; done"</span>
      
  <span class="c"># NodePort 서비스는 ClusterIP 를 포함</span>
  <span class="c"># CLUSTER-IP:PORT 로 접속 가능! &lt;- 컨트롤노드에서 아래 실행 해보자</span>
  <span class="nv">$ </span>kubectl get svc svc-nodeport
  <span class="c"># =&gt; NAME           TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE</span>
  <span class="c">#    svc-nodeport   NodePort   10.200.1.169   &lt;none&gt;        9000:31791/TCP   51m</span>
      
  <span class="nv">$ CIP</span><span class="o">=</span><span class="si">$(</span>kubectl get service svc-nodeport <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.spec.clusterIP}"</span><span class="si">)</span>
  <span class="nv">$ CIPPORT</span><span class="o">=</span><span class="si">$(</span>kubectl get service svc-nodeport <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.spec.ports[0].port}"</span><span class="si">)</span>
  <span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$CIP</span> <span class="nv">$CIPPORT</span>
  <span class="c"># =&gt; 10.200.1.169 9000</span>
  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane curl <span class="nt">-s</span> <span class="nv">$CIP</span>:<span class="nv">$CIPPORT</span> | jq
  <span class="c"># =&gt; {</span>
  <span class="c">#      "path": "/",</span>
  <span class="c">#      "headers": {</span>
  <span class="c">#        "host": "10.200.1.169:9000",</span>
  <span class="c">#        "user-agent": "curl/7.88.1",</span>
  <span class="c">#        "accept": "*/*"</span>
  <span class="c">#      },</span>
  <span class="c">#      "method": "GET",</span>
  <span class="c">#      "body": "",</span>
  <span class="c">#      "fresh": false,</span>
  <span class="c">#      "hostname": "10.200.1.169",</span>
  <span class="c">#      "ip": "::ffff:172.23.0.2",</span>
  <span class="c">#      "ips": [],</span>
  <span class="c">#      "protocol": "http",</span>
  <span class="c">#      "query": {},</span>
  <span class="c">#      "subdomains": [],</span>
  <span class="c">#      "xhr": false,</span>
  <span class="c">#      "os": {</span>
  <span class="c">#        "hostname": "deploy-echo-5c689d5454-rbgcp"</span>
  <span class="c">#      },</span>
  <span class="c">#      "connection": {}</span>
  <span class="c">#    }</span>
      
  <span class="c"># mypc에서 CLUSTER-IP:PORT 로 접속 가능할까?</span>
  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc curl <span class="nt">-s</span> <span class="nv">$CIP</span>:<span class="nv">$CIPPORT</span>
  <span class="c"># =&gt; (에러)</span>
      
  <span class="c"># mypc에서 cluster ip port로의 접속은 불가능합니다. mypc는 kubernetes 클러스터 내부에 있지 않기 때문입니다.</span>
      
  <span class="c"># (옵션) 노드에서 Network Connection</span>
  <span class="nv">$ </span>conntrack <span class="nt">-E</span>
  <span class="c"># =&gt;     [NEW] tcp      6 120 SYN_SENT src=172.23.0.2 dst=10.10.4.4 sport=36907 dport=8080 [UNREPLIED] src=10.10.4.4 dst=172.23.0.2 sport=8080 dport=36907</span>
  <span class="c">#     [UPDATE] tcp      6 60 SYN_RECV src=172.23.0.2 dst=10.10.4.4 sport=36907 dport=8080 src=10.10.4.4 dst=172.23.0.2 sport=8080 dport=36907</span>
  <span class="c">#     [UPDATE] tcp      6 86400 ESTABLISHED src=172.23.0.2 dst=10.10.4.4 sport=36907 dport=8080 src=10.10.4.4 dst=172.23.0.2 sport=8080 dport=36907 [ASSURED]</span>
  <span class="c">#     [UPDATE] tcp      6 120 FIN_WAIT src=172.23.0.2 dst=10.10.4.4 sport=36907 dport=8080 src=10.10.4.4 dst=172.23.0.2 sport=8080 dport=36907 [ASSURED]</span>
  <span class="c">#     [UPDATE] tcp      6 30 LAST_ACK src=172.23.0.2 dst=10.10.4.4 sport=36907 dport=8080 src=10.10.4.4 dst=172.23.0.2 sport=8080 dport=36907 [ASSURED]</span>
  <span class="c">#     [UPDATE] tcp      6 120 TIME_WAIT src=172.23.0.2 dst=10.10.4.4 sport=36907 dport=8080 src=10.10.4.4 dst=172.23.0.2 sport=8080 dport=36907 [ASSURED]</span>
  <span class="c">#      ...</span>
  <span class="c"># SNAT나 빠른 iptables 룰 처리등을 위해 접속 정보가 추적됨을 알 수 있습니다.</span>
      
  <span class="nv">$ </span>conntrack <span class="nt">-L</span> <span class="nt">--any-nat</span>
</code></pre></div>        </div>
      </li>
      <li>파드에서 바라본 클라이언트의 주소가 실제 클라이언트가 아닌 node의 ip로 표시되는데 그 이유를 살펴보겠습니다.
        <ul>
          <li>
            <p>컨트롤 플레인에서 iptables의 nat 테이블의 KUBE-POSTROUTING 룰을 확인하면 다음과 같습니다.</p>

            <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="nv">$ </span>iptables <span class="nt">-v</span> <span class="nt">--numeric</span> <span class="nt">--table</span> nat <span class="nt">--list</span> 
  <span class="c"># =&gt; Chain POSTROUTING (policy ACCEPT 5813 packets, 349K bytes)</span>
  <span class="c">#     pkts bytes target     prot opt in     out     source               destination         </span>
  <span class="c">#    37925 2276K KUBE-POSTROUTING  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes postrouting rules */</span>
  <span class="c">#    ...</span>
  <span class="c">#    Chain KUBE-POSTROUTING (1 references)</span>
  <span class="c">#     pkts bytes target     prot opt in     out     source               destination</span>
  <span class="c">#     5343  321K RETURN     0    --  *      *       0.0.0.0/0            0.0.0.0/0            mark match ! 0x4000/0x4000</span>
  <span class="c">#     1265 75900 MARK       0    --  *      *       0.0.0.0/0            0.0.0.0/0            MARK xor 0x4000</span>
  <span class="c">#     1265 75900 MASQUERADE  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service traffic requiring SNAT */ random-fully</span>
</code></pre></div>            </div>

            <p>확인 결과 POSTROUTING시 KUBE-POSTROUTING을 통해 SNAT 되고 있음을 알 수 있습니다.</p>
          </li>
        </ul>
      </li>
      <li>
        <p>외부 클라이언트 → 서비스(NodePort) 접속 시 : 3개의 목적지(backend) 파드로 <strong>랜덤 부하 분산</strong> 접속됨을 확인해보겠습니다.</p>

        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$CNODE</span>:<span class="nv">$NPORT</span> | <span class="nb">grep hostname</span><span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
  <span class="c"># =&gt;    100   "hostname": "172.23.0.2",</span>
  <span class="c">#        42     "hostname": "deploy-echo-5c689d5454-wppr8"</span>
  <span class="c">#        31     "hostname": "deploy-echo-5c689d5454-dxf2t"</span>
  <span class="c">#        27     "hostname": "deploy-echo-5c689d5454-rbgcp"</span>
      
  <span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$NODE1</span>:<span class="nv">$NPORT</span> | <span class="nb">grep hostname</span><span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
  <span class="c"># =&gt;     100   "hostname": "172.23.0.4",</span>
  <span class="c">#         37     "hostname": "deploy-echo-5c689d5454-dxf2t"</span>
  <span class="c">#         34     "hostname": "deploy-echo-5c689d5454-rbgcp"</span>
  <span class="c">#         29     "hostname": "deploy-echo-5c689d5454-wppr8"</span>
      
  <span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$NODE2</span>:<span class="nv">$NPORT</span> | <span class="nb">grep hostname</span><span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
  <span class="c"># =&gt;     100   "hostname": "172.23.0.5",</span>
  <span class="c">#         41     "hostname": "deploy-echo-5c689d5454-wppr8"</span>
  <span class="c">#         32     "hostname": "deploy-echo-5c689d5454-rbgcp"</span>
  <span class="c">#         27     "hostname": "deploy-echo-5c689d5454-dxf2t"</span>
      
  <span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..100<span class="o">}</span><span class="p">;</span> <span class="k">do </span>curl <span class="nt">-s</span> <span class="nv">$NODE3</span>:<span class="nv">$NPORT</span> | <span class="nb">grep hostname</span><span class="p">;</span> <span class="k">done</span> | <span class="nb">sort</span> | <span class="nb">uniq</span> <span class="nt">-c</span> | <span class="nb">sort</span> <span class="nt">-nr</span>
  <span class="c"># =&gt;     100   "hostname": "172.23.0.3",</span>
  <span class="c">#         39     "hostname": "deploy-echo-5c689d5454-dxf2t"</span>
  <span class="c">#         34     "hostname": "deploy-echo-5c689d5454-wppr8"</span>
  <span class="c">#         27     "hostname": "deploy-echo-5c689d5454-rbgcp"</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>웹 파드에서  log를 통해 접속자의 IP 확인시 외부 클라이언트 IP가 아닌, 노드의 IP로 SNAT 되어서 접속됨을 확인할 수 있습니다.</p>

        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="nv">$ </span>kubectl logs <span class="nt">-f</span> deploy-echo-5c689d5454-dxf2t | <span class="nb">grep </span>HTTP
  <span class="c"># =&gt; ::ffff:172.23.0.3 - - [01/Sep/2024:05:28:36 +0000] "GET / HTTP/1.1" 200 398 "-" "curl/7.88.1"</span>
  <span class="c">#    ::ffff:172.23.0.3 - - [01/Sep/2024:05:28:36 +0000] "GET / HTTP/1.1" 200 398 "-" "curl/7.88.1"</span>
  <span class="c">#    ::ffff:172.23.0.3 - - [01/Sep/2024:05:28:36 +0000] "GET / HTTP/1.1" 200 398 "-" "curl/7.88.1"</span>
  <span class="c">#    ::ffff:172.23.0.3 - - [01/Sep/2024:05:28:36 +0000] "GET / HTTP/1.1" 200 398 "-" "curl/7.88.1"</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>2.4.  IPTABLES 정책 확인
    <ul>
      <li>
        <p>iptables 정책 적용 순서는 다음과 같습니다.</p>

        <p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_25.png" alt="img.png" /></p>

        <ul>
          <li>PREROUTING → KUBE-SERVICES → KUBE-NODEPORTS → <strong>KUBE-EXT-#(MARK)</strong> → KUBE-SVC-# → KUBE-SEP-#  ⇒ KUBE-POSTROUTING (MASQUERADE) <strong>**</strong></li>
          <li><code class="language-plaintext highlighter-rouge">KUBE-EXT-#(MARK)</code> 규칙 과정이 추가됨을 확인할 수 있습니다.</li>
        </ul>
      </li>
      <li>기본 규칙은 ClusterIP 서비스 동작 규칙과 거의 같으며 차이점은 KUBE-NODEPORTS, KUBE-MARK-MASK, KUBE-POSTROUTING 체인이  다릅니다. 핵심 내용은 NodePort에 매칭시 마킹 후 출발지 IP를 해당 노드에 있는 네트워크 IP로 변환(MASQUERADE : SNAT)하여 목적지 파드로 전달합니다.</li>
      <li>
        <p>실습을 통해 iptables 정책에 대해 확인해보겠습니다.  컨트롤플레인에서 실습을 진행하겠습니다.</p>

        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane bash
  <span class="nt">----------------------------------------</span>
      
  <span class="c"># 패킷 카운트 초기화</span>
  <span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">--zero</span>
      
  <span class="c"># PREROUTING 정보 확인</span>
  <span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span> | <span class="nb">grep </span>PREROUTING
  <span class="c"># =&gt; -P PREROUTING ACCEPT</span>
  <span class="c">#    -A PREROUTING -m comment --comment "kubernetes service portals" -j KUBE-SERVICES</span>
  <span class="c">#    -A PREROUTING -d 172.23.0.1/32 -j DOCKER_OUTPUT</span>
      
  <span class="c"># 외부 클라이언트가 노드IP:NodePort 로 접속하기 때문에 --dst-type LOCAL 에 매칭되어서 -j KUBE-NODEPORTS 로 점프!</span>
  <span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span> | <span class="nb">grep </span>KUBE-SERVICES
  <span class="c"># =&gt; ...</span>
  <span class="c">#    -A KUBE-SERVICES -m comment --comment "kubernetes service nodeports; NOTE: this must be the last rule in this chain" -m addrtype --dst-type LOCAL -j KUBE-NODEPORTS</span>
      
  <span class="c"># KUBE-NODEPORTS 에서 KUBE-EXT-# 로 점프!</span>
  <span class="c">## -m nfacct --nfacct-name localhost_nps_accepted_pkts 추가됨 : 패킷 flow 카운팅 - 카운트 이름 지정 </span>
  <span class="nv">$ NPORT</span><span class="o">=</span><span class="si">$(</span>kubectl get service svc-nodeport <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.spec.ports[0].nodePort}'</span><span class="si">)</span>
  <span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$NPORT</span>
  <span class="c"># =&gt; 31791</span>
      
  <span class="c"># $ iptables -t nat -S | grep KUBE-NODEPORTS | grep &lt;NodePort&gt;</span>
  <span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span> | <span class="nb">grep </span>KUBE-NODEPORTS | <span class="nb">grep</span> <span class="nv">$NPORT</span>
  <span class="c"># =&gt; -A KUBE-NODEPORTS -p tcp -m comment --comment "default/svc-nodeport:svc-webport" -m tcp --dport 31791 -j KUBE-EXT-VTR7MTHHNMFZ3OFS</span>
      
  <span class="c"># (참고) nfacct 확인</span>
  <span class="nv">$ </span>nfacct list
  <span class="c">## nfacct flush # 초기화</span>
      
  <span class="c">## KUBE-EXT-# 에서 'KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000' 마킹 및 KUBE-SVC-# 로 점프!</span>
  <span class="c"># docker exec -it mypc zsh -c "while true; do curl -s --connect-timeout 1 $CNODE:$NPORT | grep hostname; date '+%Y-%m-%d %H:%M:%S' ; echo ;  sleep 1; done" 반복 접속 후 아래 확인</span>
  <span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s1">'iptables -v --numeric --table nat --list KUBE-EXT-VTR7MTHHNMFZ3OFS'</span>
  <span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span> | <span class="nb">grep</span> <span class="s2">"A KUBE-EXT-VTR7MTHHNMFZ3OFS"</span>
  <span class="c"># =&gt; -A KUBE-EXT-VTR7MTHHNMFZ3OFS -m comment --comment "masquerade traffic for default/svc-nodeport:svc-webport external destinations" -j KUBE-MARK-MASQ</span>
  <span class="c">#    -A KUBE-EXT-VTR7MTHHNMFZ3OFS -j KUBE-SVC-VTR7MTHHNMFZ3OFS</span>
      
  <span class="c"># iptables -t nat -S | grep "A KUBE-MARK-MASQ" | sed -e 's/^/#    /' -e '1s/^#    /# =&gt; /'</span>
  <span class="c"># =&gt; -A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000        # 0x4000/0x4000으로 마킹하는 룰</span>
      
  <span class="c"># KUBE-SVC-# 이후 과정은 Cluster-IP 와 동일! : 3개의 파드로 DNAT 되어서 전달</span>
  <span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span> | <span class="nb">grep</span> <span class="s2">"A KUBE-SVC-VTR7MTHHNMFZ3OFS -"</span>
  <span class="c"># =&gt; -A KUBE-SVC-VTR7MTHHNMFZ3OFS -m comment --comment "default/svc-nodeport:svc-webport -&gt; 10.10.1.5:8080" -m statistic --mode random --probability 0.33333333349 -j KUBE-SEP-SESYGQFRQSLJQZ6Q</span>
  <span class="c">#    -A KUBE-SVC-VTR7MTHHNMFZ3OFS -m comment --comment "default/svc-nodeport:svc-webport -&gt; 10.10.2.7:8080" -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-FBJG45W6XHLV2NA6</span>
  <span class="c">#    -A KUBE-SVC-VTR7MTHHNMFZ3OFS -m comment --comment "default/svc-nodeport:svc-webport -&gt; 10.10.4.4:8080" -j KUBE-SEP-GEQNJ6BO5AOHB6LH</span>
      
  <span class="c"># POSTROUTING 정보 확인</span>
  <span class="c"># 마킹되어 있어서 출발지IP를 접속한 노드의 IP 로 SNAT(MASQUERADE) 처리함! , 최초 출발지Port는 랜덤Port 로 변경</span>
  <span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span> | <span class="nb">grep</span> <span class="s2">"A KUBE-POSTROUTING"</span>
  <span class="c"># =&gt; -A KUBE-POSTROUTING -m mark ! --mark 0x4000/0x4000 -j RETURN   # 0x4000/0x4000 되어 있으니 여기에 매칭되지 않고 아래 Rule로 내려감</span>
  <span class="c">#    -A KUBE-POSTROUTING -j MARK --set-xmark 0x4000/0x0</span>
  <span class="c">#    -A KUBE-POSTROUTING -m comment --comment "kubernetes service traffic requiring SNAT" -j MASQUERADE --random-fully</span>
      
  <span class="c"># docker exec -it mypc zsh -c "while true; do curl -s --connect-timeout 1 $CNODE:$NPORT | grep hostname; date '+%Y-%m-%d %H:%M:%S' ; echo ;  sleep 1; done" 반복 접속 후 아래 확인</span>
  <span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s1">'iptables -v --numeric --table nat --list KUBE-POSTROUTING;echo;iptables -v --numeric --table nat --list POSTROUTING'</span>
      
  <span class="nv">$ </span><span class="nb">exit</span>
  <span class="nt">----------------------------------------</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>서비스 (NodePort) 생성 시 kube-proxy에 의해서 iptables 규칙이 모든 노드에 추가되는지 확인해보겠습니다.</p>

        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c">#</span>
  <span class="nv">$ NPORT</span><span class="o">=</span><span class="si">$(</span>kubectl get service svc-nodeport <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.spec.ports[0].nodePort}'</span><span class="si">)</span>
  <span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$NPORT</span> 
  <span class="c"># =&gt; 31791</span>
  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane iptables <span class="nt">-t</span> nat <span class="nt">-S</span> | <span class="nb">grep </span>KUBE-NODEPORTS | <span class="nb">grep</span> <span class="nv">$NPORT</span>
  <span class="c"># =&gt; -A KUBE-NODEPORTS -p tcp -m comment --comment "default/svc-nodeport:svc-webport" -m tcp --dport 31791 -j KUBE-EXT-VTR7MTHHNMFZ3OFS</span>
      
  <span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> iptables <span class="nt">-t</span> nat <span class="nt">-S</span> | <span class="nb">grep </span>KUBE-NODEPORTS | <span class="nb">grep</span> <span class="nv">$NPORT</span><span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
  <span class="c"># =&gt; &gt;&gt; node myk8s-control-plane &lt;&lt;</span>
  <span class="c">#    -A KUBE-NODEPORTS -p tcp -m comment --comment "default/svc-nodeport:svc-webport" -m tcp --dport 31791 -j KUBE-EXT-VTR7MTHHNMFZ3OFS</span>
  <span class="c">#    </span>
  <span class="c">#    &gt;&gt; node myk8s-worker &lt;&lt;</span>
  <span class="c">#    -A KUBE-NODEPORTS -p tcp -m comment --comment "default/svc-nodeport:svc-webport" -m tcp --dport 31791 -j KUBE-EXT-VTR7MTHHNMFZ3OFS</span>
  <span class="c">#    </span>
  <span class="c">#    &gt;&gt; node myk8s-worker2 &lt;&lt;</span>
  <span class="c">#    -A KUBE-NODEPORTS -p tcp -m comment --comment "default/svc-nodeport:svc-webport" -m tcp --dport 31791 -j KUBE-EXT-VTR7MTHHNMFZ3OFS</span>
  <span class="c">#    </span>
  <span class="c">#    &gt;&gt; node myk8s-worker3 &lt;&lt;</span>
  <span class="c">#    -A KUBE-NODEPORTS -p tcp -m comment --comment "default/svc-nodeport:svc-webport" -m tcp --dport 31791 -j KUBE-EXT-VTR7MTHHNMFZ3OFS</span>
</code></pre></div>        </div>
      </li>
      <li>iptables 룰이 모든 노드에 추가되어있음을 확인 할 수 있습니다.</li>
    </ul>
  </li>
  <li>2.5. externalTrafficPolicy  설정
    <ul>
      <li>
        <p><code class="language-plaintext highlighter-rouge">externalTrafficPolicy: Local</code> : 앞에서 실습한 바와같이 서비스가 바라보는 파드에 접속시 클라이언트 IP가 node의 IP로 접속됩니다. 이때 <code class="language-plaintext highlighter-rouge">externalTrafficPolicy: Local</code> 를 하면 <strong>해당 노드에 배치된 파드로만 접속되면서</strong>, SNAT가 되지않아 <strong>외부 클라이언트 IP가 보존</strong>됩니다.</p>

        <p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_26.png" alt="img.png" /></p>

        <ul>
          <li>이전까지는 같은 iptables 룰이 모든 노드에 적용 되었지만, 노드 자신의 파드로만 가는 룰만 있어서 각각 조금씩 다른 룰이 적용되게 됩니다.</li>
        </ul>

        <p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_27.png" alt="img.png" /></p>

        <ul>
          <li>만약 노드에 해당하는 파드가 없으면 위의 그림과 같이 연결이 실패하게되니 사용에 주의가 필요합니다.</li>
        </ul>
      </li>
      <li>
        <p><code class="language-plaintext highlighter-rouge">externalTrafficPolicy: Local</code> 설정 시의 통신 흐름을 좀 더 자세히 알아보겠습니다.</p>

        <p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_28.png" alt="img.png" /></p>

        <ul>
          <li>클라이언트에서 파드가 배포되어있는 워커노드1에 NodePort로 접속합니다.</li>
          <li>워커노드1의 IPTABLES의 nat 테이블 규칙과 매칭되어 목적지 IP와 목적지 Port는 변환 되지만, SNAT 되지 않고 바로 파드로 전달되므로 클라이언트의 IP가 파드에 그대로 전달 됩니다.</li>
        </ul>
      </li>
      <li>
        <p>설정 및 파드 접속 확인</p>

        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c"># 기본 정보 확인</span>
  <span class="nv">$ </span>kubectl get svc svc-nodeport <span class="nt">-o</span> json | <span class="nb">grep</span> <span class="s1">'TrafficPolicy"'</span>
  <span class="c"># =&gt;         "externalTrafficPolicy": "Cluster",</span>
  <span class="c">#            "internalTrafficPolicy": "Cluster",</span>
      
  <span class="c"># 기존 통신 연결 정보(conntrack) 제거 후 아래 실습 진행하자! : (모든 노드에서) conntrack -F</span>
  <span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> conntrack <span class="nt">-F</span><span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
  <span class="c"># =&gt; &gt;&gt; node myk8s-control-plane &lt;&lt;</span>
  <span class="c">#    conntrack v1.4.7 (conntrack-tools): connection tracking table has been emptied.</span>
  <span class="c">#    </span>
  <span class="c">#    &gt;&gt; node myk8s-worker &lt;&lt;</span>
  <span class="c">#    conntrack v1.4.7 (conntrack-tools): connection tracking table has been emptied.</span>
  <span class="c">#    </span>
  <span class="c">#    &gt;&gt; node myk8s-worker2 &lt;&lt;</span>
  <span class="c">#    conntrack v1.4.7 (conntrack-tools): connection tracking table has been emptied.</span>
  <span class="c">#    </span>
  <span class="c">#    &gt;&gt; node myk8s-worker3 &lt;&lt;</span>
  <span class="c">#    conntrack v1.4.7 (conntrack-tools): connection tracking table has been emptied.</span>
  <span class="c">#    </span>
  <span class="nv">$ </span>kubectl delete <span class="nt">-f</span> svc-nodeport.yml
  <span class="c"># =&gt; service "svc-nodeport" deleted</span>
  <span class="nv">$ </span>kubectl apply <span class="nt">-f</span> svc-nodeport.yml
  <span class="c"># =&gt; service/svc-nodeport created</span>
      
  <span class="c"># externalTrafficPolicy: local 설정 변경</span>
  <span class="nv">$ </span>kubectl patch svc svc-nodeport <span class="nt">-p</span> <span class="s1">'{"spec":{"externalTrafficPolicy": "Local"}}'</span>
  <span class="c"># =&gt; service/svc-nodeport patched</span>
  <span class="nv">$ </span>kubectl get svc svc-nodeport <span class="nt">-o</span> json | <span class="nb">grep</span> <span class="s1">'TrafficPolicy"'</span>
  <span class="c"># =&gt;         "externalTrafficPolicy": "Local",</span>
  <span class="c">#            "internalTrafficPolicy": "Cluster",</span>
      
  <span class="c"># 파드 3개를 2개로 줄입니다.</span>
  <span class="nv">$ </span>kubectl scale deployment deploy-echo <span class="nt">--replicas</span><span class="o">=</span>2
  <span class="c"># =&gt; deployment.apps/deploy-echo scaled</span>
</code></pre></div>        </div>

        <p><img src="/assets/2024/kans-3th/w4/20240928_kans_w4_29.png" alt="img.png" /></p>

        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c"># 파드 존재하는 노드 정보 확인</span>
  <span class="nv">$ </span>kubectl get pod <span class="nt">-owide</span>
  <span class="c"># =&gt; NAME                           READY   STATUS    RESTARTS   AGE   IP          NODE                  NOMINATED NODE   READINESS GATES</span>
  <span class="c">#    deploy-echo-5c689d5454-24cql   1/1     Running   0          30s   10.10.4.5   myk8s-worker          &lt;none&gt;           &lt;none&gt;</span>
  <span class="c">#    deploy-echo-5c689d5454-2kgfj   1/1     Running   0          30s   10.10.1.6   myk8s-worker2         &lt;none&gt;           &lt;none&gt;</span>
  <span class="c">#    net-pod                        1/1     Running   0          46h   10.10.0.7   myk8s-control-plane   &lt;none&gt;           &lt;none&gt;</span>
      
  <span class="c"># 파드 로그 실시간 확인 (웹 파드에 접속자의 IP가 출력)</span>
  <span class="nv">$ </span>kubectl logs <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>deploy-websrv <span class="nt">-f</span>
      
  <span class="c"># 외부 클라이언트(mypc)에서 접속 시도</span>
      
  <span class="c"># 노드의 IP와 NodePort를 변수에 지정</span>
  <span class="c">## CNODE=&lt;컨트롤플레인노드의 IP주소&gt;</span>
  <span class="c">## NODE1=&lt;노드1의 IP주소&gt;</span>
  <span class="c">## NODE2=&lt;노드2의 IP주소&gt;</span>
  <span class="c">## NODE3=&lt;노드3의 IP주소&gt;</span>
  <span class="nv">$ CNODE</span><span class="o">=</span>172.23.0.2
  <span class="nv">$ NODE1</span><span class="o">=</span>172.23.0.4
  <span class="nv">$ NODE2</span><span class="o">=</span>172.23.0.5
  <span class="nv">$ NODE3</span><span class="o">=</span>172.23.0.3
      
  <span class="c">## NodePort 를 변수에 지정</span>
  <span class="nv">$ NPORT</span><span class="o">=</span><span class="si">$(</span>kubectl get service svc-nodeport <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.spec.ports[0].nodePort}'</span><span class="si">)</span>
  <span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$NPORT</span>
  <span class="c"># =&gt; 31177</span>
      
  <span class="c"># 서비스(NodePort) 부하분산 접속 확인 : 파드가 존재하지 않는 노드로는 접속 실패!, 파드가 존재하는 노드는 접속 성공 및 클라이언트 IP 확인!</span>
  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc curl <span class="nt">-s</span> <span class="nt">--connect-timeout</span> 1 <span class="nv">$CNODE</span>:<span class="nv">$NPORT</span> | jq
  <span class="c"># =&gt; (공백)</span>
  <span class="c"># 컨트롤 플레인에는 파드가 없으므로 결과가 없습니다.</span>
      
  <span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="nv">$CNODE</span> <span class="nv">$NODE1</span> <span class="nv">$NODE2</span> <span class="nv">$NODE3</span> <span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> mypc curl <span class="nt">-s</span> <span class="nt">--connect-timeout</span> 1 <span class="nv">$i</span>:<span class="nv">$NPORT</span><span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
  <span class="c"># =&gt; &gt;&gt; node 172.23.0.2 &lt;&lt;</span>
  <span class="c">#    </span>
  <span class="c">#    &gt;&gt; node 172.23.0.4 &lt;&lt;</span>
  <span class="c">#    {</span>
  <span class="c">#      "path": "/",</span>
  <span class="c">#      "headers": {</span>
  <span class="c">#        "host": "172.23.0.4:31177",</span>
  <span class="c">#        ...</span>
  <span class="c">#      },</span>
  <span class="c">#      ...</span>
  <span class="c">#      "hostname": "172.23.0.4",</span>
  <span class="c">#      "ip": "::ffff:172.23.0.6",</span>
  <span class="c">#      ...</span>
  <span class="c">#      "os": {</span>
  <span class="c">#        "hostname": "deploy-echo-5c689d5454-24cql"</span>
  <span class="c">#      }</span>
  <span class="c">#    }</span>
  <span class="c">#    &gt;&gt; node 172.23.0.5 &lt;&lt;</span>
  <span class="c">#    {</span>
  <span class="c">#      "headers": {</span>
  <span class="c">#        "host": "172.23.0.5:31177",</span>
  <span class="c">#        ...</span>
  <span class="c">#      },</span>
      
  <span class="c">#      "hostname": "172.23.0.5",</span>
  <span class="c">#      "ip": "::ffff:172.23.0.6",</span>
  <span class="c">#      ...</span>
  <span class="c">#      "os": {</span>
  <span class="c">#        "hostname": "deploy-echo-5c689d5454-2kgfj"</span>
  <span class="c">#      }</span>
  <span class="c">#    }</span>
  <span class="c">#    &gt;&gt; node 172.23.0.3 &lt;&lt;</span>
  <span class="c">#    </span>
      
  <span class="c"># 목적지 파드가 배치되지 않은 노드는 접속이 어떻게? 왜 그런가?</span>
  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc zsh <span class="nt">-c</span> <span class="s2">"for i in {1..100}; do curl -s </span><span class="nv">$CNODE</span><span class="s2">:</span><span class="nv">$NPORT</span><span class="s2"> | grep hostname; done | sort | uniq -c | sort -nr"</span>
  <span class="c"># =&gt; </span>
  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc zsh <span class="nt">-c</span> <span class="s2">"for i in {1..100}; do curl -s </span><span class="nv">$NODE1</span><span class="s2">:</span><span class="nv">$NPORT</span><span class="s2"> | grep hostname; done | sort | uniq -c | sort -nr"</span>
  <span class="c"># =&gt;     100   "hostname": "172.23.0.4",</span>
  <span class="c">#        100     "hostname": "deploy-echo-5c689d5454-24cql"</span>
  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc zsh <span class="nt">-c</span> <span class="s2">"for i in {1..100}; do curl -s </span><span class="nv">$NODE2</span><span class="s2">:</span><span class="nv">$NPORT</span><span class="s2"> | grep hostname; done | sort | uniq -c | sort -nr"</span>
  <span class="c"># =&gt;     100   "hostname": "172.23.0.5",</span>
  <span class="c">#        100     "hostname": "deploy-echo-5c689d5454-2kgfj"</span>
  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc zsh <span class="nt">-c</span> <span class="s2">"for i in {1..100}; do curl -s </span><span class="nv">$NODE3</span><span class="s2">:</span><span class="nv">$NPORT</span><span class="s2"> | grep hostname; done | sort | uniq -c | sort -nr"</span>
  <span class="c"># =&gt; </span>
  <span class="c"># 목적지 파드가 배치되지 않은 노드는 응답이 없어 타임아웃이 됩니다. 그 이유는 externalTrafficPolicy: Local여서 노드포트로 온 패킷이, local pod로 전달하려고 하는데</span>
  <span class="c"># local pod가 없기 때문입니다.</span>
      
  <span class="c"># 아래 반복 접속 실행 해두자</span>
  <span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> mypc zsh <span class="nt">-c</span> <span class="s2">"while true; do curl -s --connect-timeout 1 </span><span class="nv">$NODE2</span><span class="s2">:</span><span class="nv">$NPORT</span><span class="s2"> | grep hostname; date '+%Y-%m-%d %H:%M:%S' ; echo ;  sleep 1; done"</span>
      
  <span class="c"># (옵션) 노드에서 Network Connection</span>
  <span class="nv">$ </span>conntrack <span class="nt">-E</span>
  <span class="nv">$ </span>conntrack <span class="nt">-L</span> <span class="nt">--any-nat</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>외부 클라이언트 → 각각 워커 노드 1,2 접속시 각각 노드의 파드로만 접속 됩니다.</p>

        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c"># 호스트에서 실행</span>
  <span class="nv">$ </span><span class="k">for </span>i <span class="k">in</span> <span class="nv">$CNODE</span> <span class="nv">$NODE1</span> <span class="nv">$NODE2</span> <span class="nv">$NODE3</span> <span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> curl <span class="nt">-s</span> <span class="nt">--connect-timeout</span> 1 <span class="nv">$i</span>:<span class="nv">$NPORT</span><span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
  <span class="c"># =&gt; &gt;&gt; node 172.23.0.2 &lt;&lt;</span>
  <span class="c">#    </span>
  <span class="c">#    &gt;&gt; node 172.23.0.4 &lt;&lt;</span>
  <span class="c">#    {</span>
  <span class="c">#      "headers": {</span>
  <span class="c">#        "host": "172.23.0.4:31177",</span>
  <span class="c">#      },</span>
  <span class="c">#      "hostname": "172.23.0.4",</span>
  <span class="c">#      "ip": "::ffff:172.23.0.1",</span>
  <span class="c">#      "os": {</span>
  <span class="c">#        "hostname": "deploy-echo-5c689d5454-24cql"</span>
  <span class="c">#      }</span>
  <span class="c">#    }</span>
  <span class="c">#    &gt;&gt; node 172.23.0.5 &lt;&lt;</span>
  <span class="c">#    {</span>
  <span class="c">#      "headers": {</span>
  <span class="c">#        "host": "172.23.0.5:31177",</span>
  <span class="c">#      },</span>
  <span class="c">#      "hostname": "172.23.0.5",</span>
  <span class="c">#      "ip": "::ffff:172.23.0.1",</span>
  <span class="c">#      "os": {</span>
  <span class="c">#        "hostname": "deploy-echo-5c689d5454-2kgfj"</span>
  <span class="c">#      }</span>
  <span class="c">#    }</span>
  <span class="c">#    &gt;&gt; node 172.23.0.3 &lt;&lt;    </span>
    
  <span class="c"># 다른 터미널에서 로그 표시</span>
  <span class="nv">$ </span>kubectl logs <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>deploy-websrv <span class="nt">-f</span> | <span class="nb">grep </span>HTTP
  ::ffff:172.23.0.1 - - <span class="o">[</span>26/Sep/2024:07:33:09 +0000] <span class="s2">"GET / HTTP/1.1"</span> 200 397 <span class="s2">"-"</span> <span class="s2">"curl/8.7.1"</span>
  ::ffff:172.23.0.1 - - <span class="o">[</span>26/Sep/2024:07:33:10 +0000] <span class="s2">"GET / HTTP/1.1"</span> 200 397 <span class="s2">"-"</span> <span class="s2">"curl/8.7.1"</span>
  ::ffff:172.23.0.1 - - <span class="o">[</span>26/Sep/2024:07:33:26 +0000] <span class="s2">"GET / HTTP/1.1"</span> 200 397 <span class="s2">"-"</span> <span class="s2">"curl/8.7.1"</span>
  ::ffff:172.23.0.1 - - <span class="o">[</span>26/Sep/2024:07:33:26 +0000] <span class="s2">"GET / HTTP/1.1"</span> 200 397 <span class="s2">"-"</span> <span class="s2">"curl/8.7.1"</span>
</code></pre></div>        </div>
        <ul>
          <li><code class="language-plaintext highlighter-rouge">kubectl logs -l app=deploy-websrv -f</code>로 확인시 외부 클라이언트인 172.23.0.1이 보존되는 것을 확인 할 수 있습니다.</li>
        </ul>
      </li>
      <li>이렇게 동작하는 이유를 iptables 룰을 통해 확인해보겠습니다.
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 컨트롤플레인 노드 - iptables 분석 &lt;&lt; 정책 확인 : 아래 정책 내용은 핵심적인 룰(rule)만 표시했습니다!</span>
<span class="c"># (예시) 파드가 배포되어 있는 노드1에서 확인했습니다</span>
    
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker bash
<span class="nt">---------------------------------------</span>
    
<span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span>
<span class="c"># $ iptables -t nat -S | grep &lt;NodePort&gt;</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span> | <span class="nb">grep </span>31177
<span class="c"># =&gt; -A KUBE-NODEPORTS -p tcp -m comment --comment "default/svc-nodeport:svc-webport" -m tcp --dport 31177 -j KUBE-EXT-VTR7MTHHNMFZ3OFS</span>
    
<span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span> | <span class="nb">grep</span> <span class="s1">'A KUBE-EXT-VTR7MTHHNMFZ3OFS'</span>
<span class="c"># =&gt; -A KUBE-EXT-VTR7MTHHNMFZ3OFS -s 10.10.0.0/16 -m comment --comment "pod traffic for default/svc-nodeport:svc-webport external destinations" -j KUBE-SVC-VTR7MTHHNMFZ3OFS</span>
<span class="c">#    -A KUBE-EXT-VTR7MTHHNMFZ3OFS -m comment --comment "masquerade LOCAL traffic for default/svc-nodeport:svc-webport external destinations" -m addrtype --src-type LOCAL -j KUBE-MARK-MASQ</span>
<span class="c">#    -A KUBE-EXT-VTR7MTHHNMFZ3OFS -m comment --comment "route LOCAL traffic for default/svc-nodeport:svc-webport external destinations" -m addrtype --src-type LOCAL -j KUBE-SVC-VTR7MTHHNMFZ3OFS</span>
<span class="c">#    -A KUBE-EXT-VTR7MTHHNMFZ3OFS -j KUBE-SVL-VTR7MTHHNMFZ3OFS</span>
    
<span class="c"># 실습 환경에서는 아래처럼 2개의 파드 중 자신의 노드에 생성된 파드 1개만 DNAT 연결됨</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span> | <span class="nb">grep</span> <span class="s1">'A KUBE-SVL-VTR7MTHHNMFZ3OFS'</span>
<span class="c"># =&gt; -A KUBE-SVL-VTR7MTHHNMFZ3OFS -m comment --comment "default/svc-nodeport:svc-webport -&gt; 10.10.4.5:8080" -j KUBE-SEP-COBCKEECYTEF2ZXK</span>
    
<span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span> | <span class="nb">grep</span> <span class="s1">'A KUBE-SEP-COBCKEECYTEF2ZXK'</span>
<span class="c"># =&gt; -A KUBE-SEP-COBCKEECYTEF2ZXK -s 10.10.4.5/32 -m comment --comment "default/svc-nodeport:svc-webport" -j KUBE-MARK-MASQ</span>
<span class="c">#    -A KUBE-SEP-COBCKEECYTEF2ZXK -p tcp -m comment --comment "default/svc-nodeport:svc-webport" -m tcp -j DNAT --to-destination 10.10.4.5:8080</span>
    
<span class="nv">$ </span><span class="nb">exit</span>
<span class="c"># ---------------------------------------</span>
</code></pre></div>        </div>
        <ul>
          <li>정책을 확인해보면 <code class="language-plaintext highlighter-rouge">externalTrafficPolicy: Local</code> 설정 전에는 MASQUERADE로 SNAT 되었지만, 설정 후에는 DNAT으로 바로 전달되는 것을 확인할 수 있습니다.</li>
          <li>SNAT 되지 않았기 때문에 클라이언트의 IP가 그대로 전달되어 파드에서 확인할 수 있습니다.</li>
        </ul>
      </li>
      <li>서비스(NodePort, externalTrafficPolicy: Local) 생성 시 iptables 규칙(KUBE-SVL-#)이 모든 노드에 추가되는지 확인해보겠습니다.
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 컨트롤 플레인에는 파드가 없으므로 결과가 없습니다.</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane iptables <span class="nt">-t</span> nat <span class="nt">-S</span> | <span class="nb">grep</span> <span class="s1">'A KUBE-SVL-VTR7MTHHNMFZ3OFS'</span>
<span class="c"># =&gt; (공백)</span>
    
<span class="c"># 각 노드에 확인해보겠습니다.</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>control-plane worker worker2 worker3<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node myk8s-</span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-<span class="nv">$i</span> iptables <span class="nt">-t</span> nat <span class="nt">-S</span> | <span class="nb">grep</span> <span class="s1">'A KUBE-SVL-VTR7MTHHNMFZ3OFS'</span><span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; &gt;&gt; node myk8s-control-plane &lt;&lt;</span>
<span class="c">#    </span>
<span class="c">#    &gt;&gt; node myk8s-worker &lt;&lt;</span>
<span class="c">#    -A KUBE-SVL-VTR7MTHHNMFZ3OFS -m comment --comment "default/svc-nodeport:svc-webport -&gt; 10.10.4.5:8080" -j KUBE-SEP-COBCKEECYTEF2ZXK</span>
<span class="c">#    </span>
<span class="c">#    &gt;&gt; node myk8s-worker2 &lt;&lt;</span>
<span class="c">#    -A KUBE-SVL-VTR7MTHHNMFZ3OFS -m comment --comment "default/svc-nodeport:svc-webport -&gt; 10.10.1.6:8080" -j KUBE-SEP-ABUS75FNO53OAK6G</span>
<span class="c">#    </span>
<span class="c">#    &gt;&gt; node myk8s-worker3 &lt;&lt;</span>
</code></pre></div>        </div>
        <ul>
          <li>파드가 있는 worker, worker2 노드에만 iptables 규칙이 추가되어 있음을 확인할 수 있습니다.</li>
        </ul>
      </li>
      <li>NodePort의 부족한 점
        <ul>
          <li>외부에서 노드의 IP와 포트로 직접 접속이 필요합니다.</li>
          <li>따라서 내부망이 외부에 공개(라우팅 가능)되어 보안에 취약합니다.
            <ul>
              <li>=&gt; <strong>LoadBalancer 서비스</strong> 타입으로 외부 공개 최소화 가능</li>
            </ul>
          </li>
          <li>클라이언트 IP 보존을 위해서, <code class="language-plaintext highlighter-rouge">externalTrafficPolicy: local</code>를 사용하면 파드가 없는 노드 IP로 NodePort 접속 시 실패하게 됩니다.
            <ul>
              <li>=&gt; <strong>LoadBalancer 서비스</strong>에서 헬스체크(Probe) 로 대응 가능</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h4 id="파드간-속도-측정">파드간 속도 측정</h4>

<ul>
  <li>이번 실습에서는 iperf3를 사용해서 파드간 속도를 측정해보겠습니다.</li>
  <li>iperf3는 네트워크 대역폭을 측정하는 도구로, 서버와 클라이언트로 나뉘어 서버는 대역폭을 제공하고 클라이언트는 대역폭을 측정합니다. TCP와 UDP, SCTP를 지원합니다.</li>
  <li>iperf3의 기본 사용법을 살펴 보겠습니다.
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># iperf3 설치 </span>
<span class="c"># macOS 인 경우</span>
<span class="nv">$ </span>brew <span class="nb">install </span>iperf3
<span class="c"># ubuntu 등 debian 계열인 경우 </span>
<span class="nv">$ </span><span class="nb">sudo </span>apt <span class="nb">install </span>iperf3 <span class="nt">-y</span>
  
<span class="c"># iperf3 테스트 1 : TCP 5201, 측정시간 10초</span>
<span class="nv">$ </span>iperf3 <span class="nt">-s</span> <span class="c"># 서버모드 실행</span>
<span class="c"># =&gt; -----------------------------------------------------------</span>
<span class="c">#    Server listening on 5201</span>
<span class="c">#    -----------------------------------------------------------</span>
<span class="c">#    Accepted connection from 127.0.0.1, port 40142</span>
<span class="c">#    [  5] local 127.0.0.1 port 5201 connected to 127.0.0.1 port 40154</span>
<span class="c">#    [ ID] Interval           Transfer     Bitrate</span>
<span class="c">#    [  5]   0.00-1.00   sec  7.11 GBytes  61.1 Gbits/sec</span>
<span class="c">#    [  5]   1.00-2.00   sec  8.03 GBytes  68.9 Gbits/sec</span>
<span class="c">#    [  5]   2.00-3.00   sec  7.53 GBytes  64.7 Gbits/sec</span>
<span class="c">#    [  5]   3.00-4.00   sec  7.73 GBytes  66.4 Gbits/sec</span>
<span class="c">#    [  5]   4.00-5.00   sec  7.64 GBytes  65.6 Gbits/sec</span>
<span class="c">#    [  5]   5.00-6.00   sec  7.89 GBytes  67.8 Gbits/sec</span>
<span class="c">#    [  5]   6.00-7.00   sec  7.95 GBytes  68.3 Gbits/sec</span>
<span class="c">#    [  5]   7.00-8.00   sec  7.78 GBytes  66.9 Gbits/sec</span>
<span class="c">#    [  5]   8.00-9.00   sec  7.91 GBytes  67.9 Gbits/sec</span>
<span class="c">#    [  5]   9.00-10.00  sec  7.64 GBytes  65.6 Gbits/sec</span>
<span class="c">#    [  5]  10.00-10.05  sec   384 MBytes  66.0 Gbits/sec</span>
<span class="c">#    - - - - - - - - - - - - - - - - - - - - - - - - -</span>
<span class="c">#    [ ID] Interval           Transfer     Bitrate</span>
<span class="c">#    [  5]   0.00-10.05  sec  77.6 GBytes  66.3 Gbits/sec                  receiver</span>
<span class="nv">$ </span>iperf3 <span class="nt">-c</span> 127.0.0.1 <span class="c"># 다른 터미널에서 클라이언트모드 실행</span>
<span class="c"># =&gt; Connecting to host 127.0.0.1, port 5201</span>
<span class="c">#    [  5] local 127.0.0.1 port 40154 connected to 127.0.0.1 port 5201</span>
<span class="c">#    [ ID] Interval           Transfer     Bitrate         Retr  Cwnd</span>
<span class="c">#    [  5]   0.00-1.00   sec  7.48 GBytes  64.2 Gbits/sec    8   2.69 MBytes</span>
<span class="c">#    ...</span>
<span class="c">#    [  5]   9.00-10.00  sec  7.72 GBytes  66.3 Gbits/sec    1   3.06 MBytes</span>
<span class="c">#    - - - - - - - - - - - - - - - - - - - - - - - - -</span>
<span class="c">#    [ ID] Interval           Transfer     Bitrate         Retr</span>
<span class="c">#    [  5]   0.00-10.00  sec  77.6 GBytes  66.6 Gbits/sec   53             sender</span>
<span class="c">#    [  5]   0.00-10.05  sec  77.6 GBytes  66.3 Gbits/sec                  receiver</span>
  
<span class="c"># iperf3 테스트 2 : TCP 80, 측정시간 5초</span>
<span class="nv">$ </span>iperf3 <span class="nt">-s</span> <span class="nt">-p</span> 80
<span class="nv">$ </span>iperf3 <span class="nt">-c</span> 127.0.0.1 <span class="nt">-p</span> 80 <span class="nt">-t</span> 5
  
<span class="c"># iperf3 테스트 3 : UDP 사용, 역방향 모드(-R)</span>
<span class="nv">$ </span>iperf3 <span class="nt">-s</span> 
<span class="nv">$ </span>iperf3 <span class="nt">-c</span> 127.0.0.1 <span class="nt">-u</span> <span class="nt">-b</span> 100G
  
<span class="c"># iperf3 테스트 4 : 역방향 모드(-R) =&gt; 서버에서 클라이언트로 전송할때 속도를 측정합니다.  </span>
<span class="nv">$ </span>iperf3 <span class="nt">-s</span> 
<span class="nv">$ </span>iperf3 <span class="nt">-c</span> 127.0.0.1 <span class="nt">-R</span>
  
<span class="c"># iperf3 테스트 5 : 쌍방향 모드(-R)</span>
<span class="nv">$ </span>iperf3 <span class="nt">-s</span> 
<span class="nv">$ </span>iperf3 <span class="nt">-c</span> 127.0.0.1 <span class="nt">--bidir</span>
  
<span class="c"># iperf3 테스트 6 : TCP 다중 스트림(30개), -P(number of parallel client streams to run)</span>
<span class="nv">$ </span>iperf3 <span class="nt">-s</span> 
<span class="nv">$ </span>iperf3 <span class="nt">-c</span> 127.0.0.1 <span class="nt">-P</span> 2 <span class="nt">-t</span> 30
</code></pre></div>    </div>
  </li>
  <li>쿠버네티스 환경에서 속도 측정 테스트해보겠습니다.
    <ul>
      <li>테스트 환경 배포
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 배포</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> https://raw.githubusercontent.com/gasida/PKOS/main/aews/k8s-iperf3.yaml
    
<span class="c"># 확인 : 서버와 클라이언트가 다른 워커노드에 배포되었는지 확인</span>
<span class="nv">$ </span>kubectl get deploy,svc,pod <span class="nt">-owide</span>
<span class="c"># =&gt; NAME                            READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS      IMAGES                    SELECTOR</span>
<span class="c">#    deployment.apps/iperf3-client   0/1     1            0           5s    iperf3-client   networkstatic/iperf3      app=iperf3-client</span>
<span class="c">#    deployment.apps/iperf3-server   0/1     1            0           5s    iperf3-server   networkstatic/iperf3      app=iperf3-server</span>
<span class="c">#    </span>
<span class="c">#    NAME                    TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)             AGE    SELECTOR</span>
<span class="c">#    service/iperf3-server   ClusterIP   10.200.1.166   &lt;none&gt;        5201/TCP,5201/UDP   5s     app=iperf3-server</span>
<span class="c">#    </span>
<span class="c">#    NAME                                 READY   STATUS              RESTARTS   AGE    IP          NODE                  NOMINATED NODE   READINESS GATES</span>
<span class="c">#    pod/iperf3-client-598b85fd6b-tq5xg   0/1     ContainerCreating   0          5s     &lt;none&gt;      myk8s-worker3         &lt;none&gt;           &lt;none&gt;</span>
<span class="c">#    pod/iperf3-server-688df6d56f-hlhrm   0/1     ContainerCreating   0          5s     &lt;none&gt;      myk8s-worker          &lt;none&gt;           &lt;none&gt;</span>
    
<span class="c"># 서버 파드 로그 확인 : 기본 5201 포트 Listen</span>
<span class="nv">$ </span>kubectl logs <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>iperf3-server <span class="nt">-f</span>
</code></pre></div>        </div>
      </li>
      <li>TCP 5201, 측정시간 5초
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 클라이언트 파드에서 아래 명령 실행</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deploy/iperf3-client <span class="nt">--</span> iperf3 <span class="nt">-c</span> iperf3-server <span class="nt">-t</span> 5
<span class="c"># =&gt; Connecting to host iperf3-server, port 5201</span>
<span class="c">#    [  5] local 10.10.2.9 port 54972 connected to 10.200.1.166 port 5201</span>
<span class="c">#    [ ID] Interval           Transfer     Bitrate         Retr  Cwnd</span>
<span class="c">#    [  5]   0.00-1.00   sec  4.68 GBytes  40.2 Gbits/sec  3333   1.07 MBytes</span>
<span class="c">#    [  5]   1.00-2.00   sec  4.87 GBytes  41.8 Gbits/sec  1293   1.09 MBytes</span>
<span class="c">#    [  5]   2.00-3.00   sec  4.86 GBytes  41.8 Gbits/sec  1020   1.11 MBytes</span>
<span class="c">#    [  5]   3.00-4.00   sec  4.85 GBytes  41.7 Gbits/sec  590   1.21 MBytes</span>
<span class="c">#    [  5]   4.00-5.00   sec  4.93 GBytes  42.4 Gbits/sec  988   1.27 MBytes</span>
<span class="c">#    - - - - - - - - - - - - - - - - - - - - - - - - -</span>
<span class="c">#    [ ID] Interval           Transfer     Bitrate         Retr</span>
<span class="c">#    [  5]   0.00-5.00   sec  24.2 GBytes  41.6 Gbits/sec  7224             sender</span>
<span class="c">#    [  5]   0.00-5.00   sec  24.2 GBytes  41.6 Gbits/sec                  receiver</span>
    
<span class="c"># 서버 파드 로그 확인 : 기본 5201 포트 Listen</span>
<span class="nv">$ </span>kubectl logs <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>iperf3-server <span class="nt">-f</span>
<span class="c"># =&gt; -----------------------------------------------------------</span>
<span class="c">#    Server listening on 5201 (test #1)</span>
<span class="c">#    -----------------------------------------------------------</span>
<span class="c">#    Accepted connection from 10.10.2.9, port 54962</span>
<span class="c">#    [  5] local 10.10.4.6 port 5201 connected to 10.10.2.9 port 54972</span>
<span class="c">#    [ ID] Interval           Transfer     Bitrate</span>
<span class="c">#    [  5]   0.00-1.00   sec  4.67 GBytes  40.1 Gbits/sec</span>
<span class="c">#    [  5]   1.00-2.00   sec  4.87 GBytes  41.8 Gbits/sec</span>
<span class="c">#    [  5]   2.00-3.00   sec  4.86 GBytes  41.8 Gbits/sec</span>
<span class="c">#    [  5]   3.00-4.00   sec  4.85 GBytes  41.7 Gbits/sec</span>
<span class="c">#    [  5]   4.00-5.00   sec  4.93 GBytes  42.4 Gbits/sec</span>
<span class="c">#    [  5]   5.00-5.00   sec   384 KBytes  41.4 Gbits/sec</span>
<span class="c">#    - - - - - - - - - - - - - - - - - - - - - - - - -</span>
<span class="c">#    [ ID] Interval           Transfer     Bitrate</span>
<span class="c">#    [  5]   0.00-5.00   sec  24.2 GBytes  41.6 Gbits/sec                  receiver</span>
</code></pre></div>        </div>
      </li>
      <li>UDP 사용, 역방향 모드(-R)
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 클라이언트 파드에서 아래 명령 실행</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deploy/iperf3-client <span class="nt">--</span> iperf3 <span class="nt">-c</span> iperf3-server <span class="nt">-u</span> <span class="nt">-b</span> 20G
<span class="c"># =&gt; Connecting to host iperf3-server, port 5201</span>
<span class="c">#    [  5] local 10.10.2.9 port 41928 connected to 10.200.1.166 port 5201</span>
<span class="c">#    [ ID] Interval           Transfer     Bitrate         Total Datagrams</span>
<span class="c">#    [  5]   0.00-1.00   sec   161 MBytes  1.35 Gbits/sec  116453</span>
<span class="c">#    [  5]   1.00-2.00   sec   187 MBytes  1.57 Gbits/sec  135745</span>
<span class="c">#    [  5]   2.00-3.00   sec   163 MBytes  1.36 Gbits/sec  117693</span>
<span class="c">#    [  5]   3.00-4.00   sec   220 MBytes  1.84 Gbits/sec  159109</span>
<span class="c">#    [  5]   4.00-5.00   sec   168 MBytes  1.41 Gbits/sec  121705</span>
<span class="c">#    [  5]   5.00-6.00   sec   183 MBytes  1.54 Gbits/sec  132730</span>
<span class="c">#    [  5]   6.00-7.00   sec   184 MBytes  1.54 Gbits/sec  133267</span>
<span class="c">#    [  5]   7.00-8.00   sec   158 MBytes  1.32 Gbits/sec  114073</span>
<span class="c">#    [  5]   8.00-9.00   sec   171 MBytes  1.44 Gbits/sec  124005</span>
<span class="c">#    [  5]   9.00-10.00  sec   160 MBytes  1.35 Gbits/sec  116175</span>
<span class="c">#    - - - - - - - - - - - - - - - - - - - - - - - - -</span>
<span class="c">#    [ ID] Interval           Transfer     Bitrate         Jitter    Lost/Total Datagrams</span>
<span class="c">#    [  5]   0.00-10.00  sec  1.71 GBytes  1.47 Gbits/sec  0.000 ms  0/1270955 (0%)  sender</span>
<span class="c">#    [  5]   0.00-10.00  sec  1.68 GBytes  1.44 Gbits/sec  0.008 ms  28438/1270955 (2.2%)  receiver</span>
    
<span class="c"># 서버 파드 로그 확인 : 기본 5201 포트 Listen</span>
<span class="nv">$ </span>kubectl logs <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>iperf3-server <span class="nt">-f</span>
<span class="c"># =&gt; -----------------------------------------------------------</span>
<span class="c">#    Server listening on 5201 (test #3)</span>
<span class="c">#    -----------------------------------------------------------</span>
<span class="c">#    Accepted connection from 10.10.2.9, port 48546</span>
<span class="c">#    [  5] local 10.10.4.6 port 5201 connected to 10.10.2.9 port 41928</span>
<span class="c">#    [ ID] Interval           Transfer     Bitrate         Jitter    Lost/Total Datagrams</span>
<span class="c">#    [  5]   0.00-1.00   sec   158 MBytes  1.33 Gbits/sec  0.011 ms  2000/116449 (1.7%)</span>
<span class="c">#    [  5]   1.00-2.00   sec   180 MBytes  1.51 Gbits/sec  0.009 ms  5401/135743 (4%)</span>
<span class="c">#    [  5]   2.00-3.00   sec   161 MBytes  1.35 Gbits/sec  0.011 ms  1241/117696 (1.1%)</span>
<span class="c">#    [  5]   3.00-4.00   sec   215 MBytes  1.81 Gbits/sec  0.009 ms  3132/159105 (2%)</span>
<span class="c">#    [  5]   4.00-5.00   sec   165 MBytes  1.39 Gbits/sec  0.007 ms  2073/121704 (1.7%)</span>
<span class="c">#    [  5]   5.00-6.00   sec   179 MBytes  1.51 Gbits/sec  0.008 ms  2758/132731 (2.1%)</span>
<span class="c">#    [  5]   6.00-7.00   sec   181 MBytes  1.52 Gbits/sec  0.009 ms  2397/133243 (1.8%)</span>
<span class="c">#    [  5]   7.00-8.00   sec   153 MBytes  1.28 Gbits/sec  0.007 ms  3612/114097 (3.2%)</span>
<span class="c">#    [  5]   8.00-9.00   sec   166 MBytes  1.39 Gbits/sec  0.009 ms  3707/124005 (3%)</span>
<span class="c">#    [  5]   9.00-10.00  sec   158 MBytes  1.32 Gbits/sec  0.009 ms  2117/116179 (1.8%)</span>
<span class="c">#    [  5]  10.00-10.00  sec  4.24 KBytes   656 Mbits/sec  0.008 ms  0/3 (0%)</span>
<span class="c">#    - - - - - - - - - - - - - - - - - - - - - - - - -</span>
<span class="c">#    [ ID] Interval           Transfer     Bitrate         Jitter    Lost/Total Datagrams</span>
<span class="c">#    [  5]   0.00-10.00  sec  1.68 GBytes  1.44 Gbits/sec  0.008 ms  28438/1270955 (2.2%)  receiver</span>
</code></pre></div>        </div>
      </li>
      <li>TCP, 쌍방향 모드(-R)
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 클라이언트 파드에서 아래 명령 실행</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deploy/iperf3-client <span class="nt">--</span> iperf3 <span class="nt">-c</span> iperf3-server <span class="nt">-t</span> 5 <span class="nt">--bidir</span>
<span class="c"># =&gt; Connecting to host iperf3-server, port 5201</span>
<span class="c">#    [  5] local 10.10.2.9 port 59852 connected to 10.200.1.166 port 5201</span>
<span class="c">#    [  7] local 10.10.2.9 port 59860 connected to 10.200.1.166 port 5201</span>
<span class="c">#    [ ID][Role] Interval           Transfer     Bitrate         Retr  Cwnd</span>
<span class="c">#    [  5][TX-C]   0.00-1.00   sec  3.85 GBytes  33.0 Gbits/sec  2249   1.55 MBytes</span>
<span class="c">#    [  7][RX-C]   0.00-1.00   sec   553 MBytes  4.64 Gbits/sec</span>
<span class="c">#    [  5][TX-C]   1.00-2.00   sec  1.77 GBytes  15.2 Gbits/sec  3105   1.07 MBytes</span>
<span class="c">#    [  7][RX-C]   1.00-2.00   sec  2.73 GBytes  23.4 Gbits/sec</span>
<span class="c">#    [  5][TX-C]   2.00-3.00   sec  1.64 GBytes  14.1 Gbits/sec  639    850 KBytes</span>
<span class="c">#    [  7][RX-C]   2.00-3.00   sec  2.93 GBytes  25.2 Gbits/sec</span>
<span class="c">#    [  5][TX-C]   3.00-4.00   sec  2.07 GBytes  17.8 Gbits/sec    0    853 KBytes</span>
<span class="c">#    [  7][RX-C]   3.00-4.00   sec  2.48 GBytes  21.3 Gbits/sec</span>
<span class="c">#    [  5][TX-C]   4.00-5.00   sec  1.22 GBytes  10.5 Gbits/sec    2    877 KBytes</span>
<span class="c">#    [  7][RX-C]   4.00-5.00   sec  3.25 GBytes  27.9 Gbits/sec</span>
<span class="c">#    - - - - - - - - - - - - - - - - - - - - - - - - -</span>
<span class="c">#    [ ID][Role] Interval           Transfer     Bitrate         Retr</span>
<span class="c">#    [  5][TX-C]   0.00-5.00   sec  10.5 GBytes  18.1 Gbits/sec  5995             sender</span>
<span class="c">#    [  5][TX-C]   0.00-5.00   sec  10.5 GBytes  18.1 Gbits/sec                  receiver</span>
<span class="c">#    [  7][RX-C]   0.00-5.00   sec  11.9 GBytes  20.5 Gbits/sec  9201             sender</span>
<span class="c">#    [  7][RX-C]   0.00-5.00   sec  11.9 GBytes  20.5 Gbits/sec                  receiver</span>
    
<span class="c"># 서버 파드 로그 확인 : 기본 5201 포트 Listen</span>
<span class="nv">$ </span>kubectl logs <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>iperf3-server <span class="nt">-f</span>
<span class="c"># =&gt; -----------------------------------------------------------</span>
<span class="c">#    Server listening on 5201 (test #2)</span>
<span class="c">#    -----------------------------------------------------------</span>
<span class="c">#    Accepted connection from 10.10.2.9, port 59836</span>
<span class="c">#    [  5] local 10.10.4.6 port 5201 connected to 10.10.2.9 port 59852</span>
<span class="c">#    [  8] local 10.10.4.6 port 5201 connected to 10.10.2.9 port 59860</span>
<span class="c">#    [ ID][Role] Interval           Transfer     Bitrate         Retr  Cwnd</span>
<span class="c">#    [  5][RX-S]   0.00-1.00   sec  3.85 GBytes  33.0 Gbits/sec</span>
<span class="c">#    [  8][TX-S]   0.00-1.00   sec   561 MBytes  4.70 Gbits/sec   59   1.02 MBytes</span>
<span class="c">#    [  5][RX-S]   1.00-2.00   sec  1.77 GBytes  15.2 Gbits/sec</span>
<span class="c">#    [  8][TX-S]   1.00-2.00   sec  2.73 GBytes  23.5 Gbits/sec  2468   1.09 MBytes</span>
<span class="c">#    [  5][RX-S]   2.00-3.00   sec  1.63 GBytes  14.0 Gbits/sec</span>
<span class="c">#    [  8][TX-S]   2.00-3.00   sec  2.92 GBytes  25.1 Gbits/sec  3327   1.10 MBytes</span>
<span class="c">#    [  5][RX-S]   3.00-4.00   sec  2.08 GBytes  17.9 Gbits/sec</span>
<span class="c">#    [  8][TX-S]   3.00-4.00   sec  2.49 GBytes  21.4 Gbits/sec  2315   1.13 MBytes</span>
<span class="c">#    [  5][RX-S]   4.00-5.00   sec  1.22 GBytes  10.5 Gbits/sec</span>
<span class="c">#    [  8][TX-S]   4.00-5.00   sec  3.25 GBytes  27.9 Gbits/sec  1032   1.16 MBytes</span>
<span class="c">#    [  5][RX-S]   5.00-5.00   sec   768 KBytes  27.6 Gbits/sec</span>
<span class="c">#    [  8][TX-S]   5.00-5.00   sec  1.25 MBytes  41.3 Gbits/sec    0   1.16 MBytes</span>
<span class="c">#    - - - - - - - - - - - - - - - - - - - - - - - - -</span>
<span class="c">#    [ ID][Role] Interval           Transfer     Bitrate         Retr</span>
<span class="c">#    [  5][RX-S]   0.00-5.00   sec  10.5 GBytes  18.1 Gbits/sec                  receiver</span>
<span class="c">#    [  8][TX-S]   0.00-5.00   sec  11.9 GBytes  20.5 Gbits/sec  9201             sender</span>
</code></pre></div>        </div>
      </li>
      <li>TCP 다중 스트림(30개), -P(number of parallel client streams to run)
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 클라이언트 파드에서 아래 명령 실행</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> deploy/iperf3-client <span class="nt">--</span> iperf3 <span class="nt">-c</span> iperf3-server <span class="nt">-t</span> 10 <span class="nt">-P</span> 2
<span class="c"># =&gt; [  5] local 10.10.2.9 port 41976 connected to 10.200.1.166 port 5201</span>
<span class="c">#    [  7] local 10.10.2.9 port 41982 connected to 10.200.1.166 port 5201</span>
<span class="c">#    [ ID] Interval           Transfer     Bitrate         Retr  Cwnd</span>
<span class="c">#    [  5]   0.00-1.00   sec  2.87 GBytes  24.7 Gbits/sec  822    570 KBytes</span>
<span class="c">#    [  7]   0.00-1.00   sec  2.88 GBytes  24.7 Gbits/sec  159    576 KBytes</span>
<span class="c">#    [SUM]   0.00-1.00   sec  5.75 GBytes  49.4 Gbits/sec  981</span>
<span class="c">#    - - - - - - - - - - - - - - - - - - - - - - - - -</span>
<span class="c">#    ...</span>
<span class="c">#    - - - - - - - - - - - - - - - - - - - - - - - - -</span>
<span class="c">#    [ ID] Interval           Transfer     Bitrate         Retr</span>
<span class="c">#    [  5]   0.00-10.00  sec  29.2 GBytes  25.1 Gbits/sec  3825             sender</span>
<span class="c">#    [  5]   0.00-10.00  sec  29.2 GBytes  25.1 Gbits/sec                  receiver</span>
<span class="c">#    [  7]   0.00-10.00  sec  29.2 GBytes  25.1 Gbits/sec  2063             sender</span>
<span class="c">#    [  7]   0.00-10.00  sec  29.2 GBytes  25.0 Gbits/sec                  receiver</span>
<span class="c">#    [SUM]   0.00-10.00  sec  58.3 GBytes  50.1 Gbits/sec  5888             sender</span>
<span class="c">#    [SUM]   0.00-10.00  sec  58.3 GBytes  50.1 Gbits/sec                  receiver</span>
    
<span class="c"># 서버 파드 로그 확인 : 기본 5201 포트 Listen</span>
<span class="nv">$ </span>kubectl logs <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>iperf3-server <span class="nt">-f</span>
<span class="c"># =&gt; -----------------------------------------------------------</span>
<span class="c">#    Server listening on 5201 (test #4)</span>
<span class="c">#    -----------------------------------------------------------</span>
<span class="c">#    Accepted connection from 10.10.2.9, port 41962</span>
<span class="c">#    [  5] local 10.10.4.6 port 5201 connected to 10.10.2.9 port 41976</span>
<span class="c">#    [  8] local 10.10.4.6 port 5201 connected to 10.10.2.9 port 41982</span>
<span class="c">#    [ ID] Interval           Transfer     Bitrate</span>
<span class="c">#    [  5]   0.00-1.00   sec  2.87 GBytes  24.6 Gbits/sec</span>
<span class="c">#    [  8]   0.00-1.00   sec  2.87 GBytes  24.7 Gbits/sec</span>
<span class="c">#    [SUM]   0.00-1.00   sec  5.74 GBytes  49.3 Gbits/sec</span>
<span class="c">#    - - - - - - - - - - - - - - - - - - - - - - - - -</span>
<span class="c">#    ...</span>
<span class="c">#    - - - - - - - - - - - - - - - - - - - - - - - - -</span>
<span class="c">#    [ ID] Interval           Transfer     Bitrate</span>
<span class="c">#    [  5]   0.00-10.00  sec  29.2 GBytes  25.1 Gbits/sec                  receiver</span>
<span class="c">#    [  8]   0.00-10.00  sec  29.2 GBytes  25.0 Gbits/sec                  receiver</span>
<span class="c">#    [SUM]   0.00-10.00  sec  58.3 GBytes  50.1 Gbits/sec                  receiver</span>
</code></pre></div>        </div>
      </li>
      <li>실습결과 <code class="language-plaintext highlighter-rouge">iperf3 -c 127.0.0.1 -t 5</code>로 측정하였을때는 호스트에서는 67.1 Gbits/sec 였던것에 반해, 쿠버네티스를 통하면 41.6 Gbits/sec로 측정됩니다.
        <ul>
          <li>쿠버네티스도 로컬호스트에서 docker로 실행되는데 kube-proxy, iptables 포워딩 등의 오버헤드로 인해 발생하는것 같습니다.</li>
        </ul>
      </li>
      <li>UDP의 경우에도 <code class="language-plaintext highlighter-rouge">iperf3 -c 127.0.0.1 -u -b 20G</code>로 측정했을때 호스트에서는 20.0 Gbits/sec가 나오는데, 쿠버네티스를 통하면 1.41 Gbits/sec로 측정됩니다.
        <ul>
          <li>UDP는 더 오버헤드가 심한데 원인을 찾아봐야 할것 같습니다.</li>
        </ul>
      </li>
      <li>이번 실습을 통해 다양한 네트워크 CNI, 설정등을 변경해가며 최적의 설정을 찾아보는 방법을 배워보았습니다.</li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="마치며">마치며</h2>

<p>실습을 할수록 점점 더 iptables과 친숙해지는것 같습니다.
눈에 익은게 많아지고는 있지만, nftables라던지 ipvs라던지, eBPF라던지 아직 갈길이 멉니다. 😅</p>

<p>새삼스레 스터디를 진행하시는 가시다님을 비롯해서 조력자 분들도 정말 대단하다는 생각이 듭니다.
그리고 내용들 및 그림들이 가시다님이 집필하신 책에서 많이 가져왔습니다.
책이 출판되면 꼭 구매해서 읽어보겠습니다! 
이제 스터디도 중반을 향해 달려가고 있습니다. 남은 날들도 스터디에서 생존할 수 있기를 바랍니다. :pray:</p>]]></content><author><name></name></author><category term="kans" /><category term="kubernetes," /><category term="network," /><category term="linux" /><summary type="html"><![CDATA[지난주에 이어 이번주에는 Kubernetes의 Service, 그 중에 ClusterIP, NodePort에 대해 알아보겠습니다.]]></summary></entry><entry><title type="html">[KANS 3기] K8S Calico CNI</title><link href="https://sweetlittlebird.github.io/posts/2024-09-22-KANS-Study-Week3/" rel="alternate" type="text/html" title="[KANS 3기] K8S Calico CNI" /><published>2024-09-22T02:00:18+09:00</published><updated>2024-09-22T02:00:18+09:00</updated><id>https://sweetlittlebird.github.io/posts/KANS%20Study%20-%20Week3</id><content type="html" xml:base="https://sweetlittlebird.github.io/posts/2024-09-22-KANS-Study-Week3/"><![CDATA[<h2 id="들어가며">들어가며</h2>

<p>지난주에 이어 이번주에는 Calico CNI와 Calico Network Mode에 대해 알아보겠습니다.
KANS 3기 3주차 스터디를 시작하겠습니다.</p>

<hr />

<h2 id="calico-cni">Calico CNI</h2>

<h3 id="calico-소개">Calico 소개</h3>

<h4 id="calico란">Calico란?</h4>

<p>Calico CNI는 Kubernetes 클러스터에서 네트워크를 관리하는 CNI(Container Network Interface) 플러그인 중 하나로
Kubernetes와 non-Kubernetes/legacy 네트워크를 연결하는 역할을 합니다.
특징으로는 L3/L4 네트워크를 제공하며, BGP 프로토콜을 사용하여 라우팅을 수행합니다.
(모드에 따라 BGP를 사용하지 않을 수도 있습니다.)</p>

<h3 id="calico-설치">Calico 설치</h3>

<ul>
  <li>컨트롤플레인에서 <code class="language-plaintext highlighter-rouge">kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.28.1/manifests/calico.yaml</code> 명령어를 실행하여
Calico CNI를 설치할 수 있습니다. 이때 실습환경에 맞추기 위해 <code class="language-plaintext highlighter-rouge">CALICO_IPV4POOL_BLOCK_SIZE</code>를 “24”로 설정해야 합니다.</li>
  <li>
    <p>해당 부분이 적용된 yaml 파일인 <a href="https://raw.githubusercontent.com/gasida/KANS/main/kans3/calico-kans.yaml">calico-kans.yaml</a>를 사용하여 설치하였습니다.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 모니터링</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s1">'kubectl get pod -A -owide'</span>
  
<span class="c"># 컨트롤플레인(k8s-m)에서 calico cni 설치 실행</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> https://raw.githubusercontent.com/projectcalico/calico/v3.28.1/manifests/calico.yaml
<span class="c"># 기본 yaml 에 4946줄 이동 후 아래 내용 추가 해둠</span>
<span class="c">##            # Block size to use for the IPv4 POOL created at startup. Block size for IPv4 should be in the range 20-32. default 24</span>
<span class="c">##            - name: CALICO_IPV4POOL_BLOCK_SIZE</span>
<span class="c">##              value: "24"</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> https://raw.githubusercontent.com/gasida/KANS/main/kans3/calico-kans.yaml
<span class="c"># =&gt; poddisruptionbudget.policy/calico-kube-controllers created</span>
<span class="c">#    serviceaccount/calico-kube-controllers created</span>
<span class="c">#    serviceaccount/calico-node created</span>
<span class="c">#    serviceaccount/calico-cni-plugin created</span>
<span class="c">#    ...</span>
  
<span class="c"># 설치 확인</span>
<span class="nv">$ </span>tree /opt/cni/bin/
<span class="c"># =&gt; /opt/cni/bin/</span>
<span class="c">#    ├── bandwidth</span>
<span class="c">#    ├── bridge</span>
<span class="c">#    ├── calico</span>
<span class="c">#    ├── calico-ipam</span>
<span class="c">#    ├── dhcp</span>
<span class="c">#    ├── dummy</span>
<span class="c">#    ...</span>
<span class="nv">$ </span><span class="nb">ls</span> <span class="nt">-l</span> /opt/cni/bin/
<span class="nv">$ </span>ip <span class="nt">-c</span> route
<span class="c"># =&gt; ...</span>
<span class="c">#    172.16.34.0/24 via 192.168.20.100 dev tunl0 proto bird onlink</span>
<span class="c">#    blackhole 172.16.116.0/24 proto bird</span>
<span class="c">#    172.16.158.0/24 via 192.168.10.101 dev tunl0 proto bird onlink</span>
<span class="c">#    172.16.184.0/24 via 192.168.10.102 dev tunl0 proto bird onlink</span>
<span class="c">#    ...</span>
<span class="nv">$ </span>ip <span class="nt">-c</span> addr
<span class="nv">$ </span>iptables <span class="nt">-t</span> filter <span class="nt">-L</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-L</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> filter <span class="nt">-L</span> | <span class="nb">wc</span> <span class="nt">-l</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-L</span> | <span class="nb">wc</span> <span class="nt">-l</span>
  
<span class="c"># calicoctl 설치</span>
<span class="nv">$ </span>curl <span class="nt">-L</span> https://github.com/projectcalico/calico/releases/download/v3.28.1/calicoctl-linux-amd64 <span class="nt">-o</span> calicoctl
<span class="nv">$ </span><span class="nb">chmod</span> +x calicoctl <span class="o">&amp;&amp;</span> <span class="nb">mv </span>calicoctl /usr/bin
<span class="nv">$ </span>calicoctl version
  
<span class="c"># CNI 설치 후 파드 상태 확인</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-A</span> <span class="nt">-o</span> wide
<span class="c"># =&gt; NAMESPACE     NAME                                       READY   STATUS    RESTARTS      AGE     IP               NODE     NOMINATED NODE   READINESS GATES</span>
<span class="c">#    kube-system   calico-kube-controllers-77d59654f4-wbzth   1/1     Running   0             4m22s   172.16.34.2      k8s-w0   &lt;none&gt;           &lt;none&gt;</span>
<span class="c">#    kube-system   calico-node-545hj                          1/1     Running   0             4m22s   192.168.20.100   k8s-w0   &lt;none&gt;           &lt;none&gt;</span>
<span class="c">#    kube-system   calico-node-p5xpt                          1/1     Running   0             4m22s   192.168.10.10    k8s-m    &lt;none&gt;           &lt;none&gt;</span>
<span class="c">#    kube-system   calico-node-rmzvb                          1/1     Running   0             4m22s   192.168.10.101   k8s-w1   &lt;none&gt;           &lt;none&gt;</span>
<span class="c">#    kube-system   calico-node-sd9x8                          1/1     Running   0             4m22s   192.168.10.102   k8s-w2   &lt;none&gt;           &lt;none&gt;</span>
<span class="c">#    ...</span>
</code></pre></div>    </div>
  </li>
  <li>설치 확인을 했을때 위와 같이 정상적으로 설치가 되었다면 Calico CNI가 정상적으로 동작하고 있는 것입니다.</li>
  <li><code class="language-plaintext highlighter-rouge">ip -c route</code> 명령어를 통해 Bird 라우팅 테이블에 Calico CNI가 적용된 것을 확인할 수 있습니다.
그 중에서 <code class="language-plaintext highlighter-rouge">blackhole</code>은 해당 명령어를 실행하는 노드를 의미합니다.</li>
  <li>실습을 위해서 metrics-server를 설치하고, <code class="language-plaintext highlighter-rouge">kubectl top node</code> 명령어를 통해 노드의 리소스 사용량을 확인해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># metrics-server 설치</span>
<span class="nv">$ </span>helm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server/
<span class="c"># =&gt; "metrics-server" has been added to your repositories</span>
<span class="nv">$ </span>helm upgrade <span class="nt">--install</span> metrics-server metrics-server/metrics-server <span class="nt">--set</span> <span class="s1">'args[0]=--kubelet-insecure-tls'</span> <span class="nt">-n</span> kube-system
<span class="c"># =&gt; Release "metrics-server" does not exist. Installing it now.</span>
<span class="c">#    ...</span>
<span class="c">#      Chart version: 3.12.1</span>
<span class="c">#      App version:   0.7.1</span>
<span class="c">#      Image tag:     registry.k8s.io/metrics-server/metrics-server:v0.7.1</span>
<span class="nv">$ </span>kubectl get all <span class="nt">-n</span> kube-system <span class="nt">-l</span> app.kubernetes.io/instance<span class="o">=</span>metrics-server
<span class="nv">$ </span>kubectl get apiservices |egrep <span class="s1">'(AVAILABLE|metrics)'</span>
<span class="c"># =&gt; NAME                                   SERVICE                      AVAILABLE                  AGE</span>
<span class="c">#    v1beta1.metrics.k8s.io                 kube-system/metrics-server   False (MissingEndpoints)   16s</span>

<span class="c"># 확인</span>
<span class="nv">$ </span>kubectl top node  <span class="c"># 노드 리소스 사용량 확인</span>
<span class="c"># =&gt; NAME     CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%</span>
<span class="c">#    k8s-m    218m         5%     1131Mi          29%</span>
<span class="c">#    k8s-w0   95m          2%     807Mi           43%</span>
<span class="c">#    k8s-w1   77m          1%     768Mi           41%</span>
<span class="c">#    k8s-w2   62m          1%     806Mi           43%</span>
<span class="nv">$ </span>kubectl top pod <span class="nt">-A</span> <span class="nt">--sort-by</span><span class="o">=</span><span class="s1">'cpu'</span>    <span class="c"># 파드 리소스 사용량을 CPU 사용량 순으로 정렬해서 확인</span>
<span class="nv">$ </span>kubectl top pod <span class="nt">-A</span> <span class="nt">--sort-by</span><span class="o">=</span><span class="s1">'memory'</span> <span class="c"># 파드 리소스 사용량을 Memory 사용량 순으로 정렬해서 확인</span>

<span class="c"># (참고) 삭제</span>
<span class="nv">$ </span>helm uninstall <span class="nt">-n</span> kube-system metrics-server
</code></pre></div></div>

<h3 id="calico-cni-구성요소">Calico CNI 구성요소</h3>

<p><img src="/assets/2024/kans-3th/w3/20240921_kans_w3_1.png" alt="img.png" class="image-center" />
<em class="image-caption">출처: 추가예정</em></p>
<ul>
  <li><strong>Calico Datastore</strong> : Calico의 구성 정보를 저장하는 데이터베이스입니다. Kubernetes API 서버(기본값) 또는 etcd에 저장됩니다.</li>
  <li><strong>Bird</strong> : 오픈소스 라우팅 데몬으로, Calico CNI에서 라우팅을 수행합니다.
Bird는 BGP 프로토콜을 이용한 라우팅 데몬으로
Calico나 Kubernetes에서만 사용되는것이 아닌 일반적인 라우팅 데몬입니다.
노드의 파드 네트워크 대역을 BGP 라우팅 프로토콜을 통해서 광고(advertise)합니다.</li>
  <li><strong>Felix</strong> : Bird를 통해 배포된 라우팅 정보를 수신하여, 노드의 파드 네트워크 대역을 호스트의 라우팅 테이블에
업데이트 하는 역할을 합니다. 또한 Iptables 등 방화벽 규칙 설정 관리를 합니다.</li>
  <li><strong>Confd</strong> : Calico 구성 정보를 관리하는 데몬으로, BGP 설정등으로 Calico 데이터 저장소에 변경이 발생하면
Bird의 설정 파일을 만들고, 변경된 설정 파일을 반영하게 합니다.</li>
  <li><strong>CNI IPAM Plugin</strong> : Calico가 제공하는 IPAM(IP Address Management) 플러그인으로
Calico CNI에서 IP 주소를 할당하는 역할을 합니다. (Flannel CNI의 경우 기본 IPAM인 host-local IPAM을 사용합니다.)</li>
  <li><strong>calico-kube-controllers</strong> : Calico의 동작을 감시 및 제어하는 컨트롤러입니다.</li>
  <li><strong>Typha</strong> : Calico의 성능을 향상시키기 위한 컴포넌트로, Calico의 데이터베이스에 대한 읽기 전용 요청을 처리합니다.
워커노드 수가 많지 않은 경우 생략해도 무방합니다.</li>
  <li><strong>calicoctl</strong> : Calico를 제어할 수 있는 CLI 인터페이스로, datastore에 접근하여 Calico의 구성 정보를 관리할 수 있습니다.</li>
</ul>

<h4 id="calico-구성요소-확인">Calico 구성요소 확인</h4>

<ul>
  <li>
    <p>설치된 구성요소를 명령들을 사용해서 살펴보겠습니다.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 버전 확인 - 링크</span>
<span class="c">## kdd 의미는 쿠버네티스 API 를 데이터저장소로 사용 : k8s API datastore(kdd)</span>
<span class="nv">$ </span>calicoctl version
<span class="c"># =&gt; Client Version:    v3.28.1</span>
<span class="c">#    ...</span>
  
<span class="c"># calico 관련 정보 확인</span>
<span class="nv">$ </span>kubectl get daemonset <span class="nt">-n</span> kube-system
<span class="c"># =&gt; NAME          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE</span>
<span class="c">#    calico-node   4         4         4       4            4           kubernetes.io/os=linux   26m</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-n</span> kube-system <span class="nt">-l</span> k8s-app<span class="o">=</span>calico-node <span class="nt">-owide</span>
<span class="c"># =&gt; NAME                READY   STATUS    RESTARTS   AGE   IP               NODE     NOMINATED NODE   READINESS GATES</span>
<span class="c">#    calico-node-545hj   1/1     Running   0          26m   192.168.20.100   k8s-w0   &lt;none&gt;           &lt;none&gt;</span>
<span class="c">#    calico-node-p5xpt   1/1     Running   0          26m   192.168.10.10    k8s-m    &lt;none&gt;           &lt;none&gt;</span>
<span class="c">#    calico-node-rmzvb   1/1     Running   0          26m   192.168.10.101   k8s-w1   &lt;none&gt;           &lt;none&gt;</span>
<span class="c">#    calico-node-sd9x8   1/1     Running   0          26m   192.168.10.102   k8s-w2   &lt;none&gt;           &lt;none&gt;</span>
<span class="c"># calico-node 는 데몬셋으로 모든 노드에 배포되어 있음을 확인할 수 있습니다.</span>
  
<span class="c"># calico-kube-controllers 정보 확인</span>
<span class="nv">$ </span>kubectl get deploy <span class="nt">-n</span> kube-system calico-kube-controllers
<span class="c"># =&gt; NAME                      READY   UP-TO-DATE   AVAILABLE   AGE</span>
<span class="c">#    calico-kube-controllers   1/1     1            1           27m</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-n</span> kube-system <span class="nt">-l</span> k8s-app<span class="o">=</span>calico-kube-controllers <span class="nt">-owide</span>
<span class="c"># =&gt; NAME                                       READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES</span>
<span class="c">#    calico-kube-controllers-77d59654f4-wbzth   1/1     Running   0          28m   172.16.34.2   k8s-w0   &lt;none&gt;           &lt;none&gt;</span>
  
<span class="c"># 칼리코 IPAM 정보 확인 : 칼리코 CNI 를 사용한 파드가 생성된 노드에 podCIDR 네트워크 대역 확인 - 링크</span>
<span class="nv">$ </span>calicoctl ipam show
<span class="c"># =&gt; +----------+---------------+-----------+------------+--------------+</span>
<span class="c">#    | GROUPING |     CIDR      | IPS TOTAL | IPS IN USE |   IPS FREE   |</span>
<span class="c">#    +----------+---------------+-----------+------------+--------------+</span>
<span class="c">#    | IP Pool  | 172.16.0.0/16 |     65536 | 8 (0%)     | 65528 (100%) |</span>
<span class="c">#    +----------+---------------+-----------+------------+--------------+</span>
  
<span class="c"># Block 는 각 노드에 할당된 podCIDR 정보</span>
<span class="nv">$ </span>calicoctl ipam show <span class="nt">--show-blocks</span>
<span class="c"># =&gt; +----------+-----------------+-----------+------------+--------------+</span>
<span class="c">#    | GROUPING |      CIDR       | IPS TOTAL | IPS IN USE |   IPS FREE   |</span>
<span class="c">#    +----------+-----------------+-----------+------------+--------------+</span>
<span class="c">#    | IP Pool  | 172.16.0.0/16   |     65536 | 8 (0%)     | 65528 (100%) |</span>
<span class="c">#    | Block    | 172.16.116.0/24 |       256 | 1 (0%)     | 255 (100%)   |</span>
<span class="c">#    | Block    | 172.16.158.0/24 |       256 | 2 (1%)     | 254 (99%)    |</span>
<span class="c">#    | Block    | 172.16.184.0/24 |       256 | 1 (0%)     | 255 (100%)   |</span>
<span class="c">#    | Block    | 172.16.34.0/24  |       256 | 4 (2%)     | 252 (98%)    |</span>
<span class="c">#    +----------+-----------------+-----------+------------+--------------+</span>
<span class="nv">$ </span>calicoctl ipam show <span class="nt">--show-borrowed</span>
<span class="nv">$ </span>calicoctl ipam show <span class="nt">--show-configuration</span>
  
<span class="c"># host-local IPAM 정보 확인 : k8s-m 노드의 podCIDR 은 host-local 대신 칼리코 IPAM 를 사용함</span>
  
<span class="c"># 워커 노드마다 할당된 dedicated subnet (podCIDR) 확인</span>
<span class="nv">$ </span>kubectl get nodes <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.items[*].spec.podCIDR}'</span> <span class="p">;</span><span class="nb">echo</span>
<span class="c"># =&gt; 172.16.0.0/24 172.16.1.0/24 172.16.2.0/24 172.16.4.0/24</span>
  
<span class="c"># 컨트롤플레인의 pod에 할당되는 podCIDR 확인</span>
<span class="nv">$ </span>kubectl get node k8s-m <span class="nt">-o</span> json | jq <span class="s1">'.spec.podCIDR'</span>
<span class="c"># =&gt; "172.16.0.0/24"</span>
  
<span class="c"># CNI Plugin 정보 확인 - 링크</span>
<span class="nv">$ </span>tree /etc/cni/net.d/
<span class="c"># =&gt; /etc/cni/net.d/</span>
<span class="c">#    ├── 10-calico.conflist</span>
<span class="c">#    └── calico-kubeconfig</span>
<span class="nv">$ </span><span class="nb">cat</span> /etc/cni/net.d/10-calico.conflist | jq
<span class="c"># =&gt; ...</span>
<span class="c">#    "datastore_type": "kubernetes", # 칼리코 데이터저장소는 쿠버네티스 API 를 사용</span>
<span class="c">#    "ipam": { </span>
<span class="c">#      "type": "calico-ipam" # IPAM 은 칼리코 자체 IPAM 을 사용</span>
<span class="c">#    },</span>
<span class="c">#    ...</span>
  
<span class="c"># calicoctl node 정보 확인 : Bird 데몬(BGP)을 통한 BGP 네이버 연결 정보(bgp peer 는 노드의 IP로 연결) - 링크</span>
<span class="nv">$ </span>calicoctl node status
<span class="c"># =&gt; Calico process is running.</span>
<span class="c">#    </span>
<span class="c">#    IPv4 BGP status</span>
<span class="c">#    +----------------+-------------------+-------+----------+-------------+</span>
<span class="c">#    |  PEER ADDRESS  |     PEER TYPE     | STATE |  SINCE   |    INFO     |</span>
<span class="c">#    +----------------+-------------------+-------+----------+-------------+</span>
<span class="c">#    | 192.168.20.100 | node-to-node mesh | up    | 14:13:02 | Established |</span>
<span class="c">#    | 192.168.10.101 | node-to-node mesh | up    | 14:13:31 | Established |</span>
<span class="c">#    | 192.168.10.102 | node-to-node mesh | up    | 14:12:44 | Established |</span>
<span class="c">#    +----------------+-------------------+-------+----------+-------------+</span>
<span class="nv">$ </span>calicoctl node checksystem
<span class="c"># =&gt; Checking kernel version...</span>
<span class="c">#       5.15.0-119-generic  					OK</span>
<span class="c">#    Checking kernel modules...</span>
<span class="c">#       xt_mark             					OK</span>
  
<span class="c"># ippool 정보 확인 : 클러스터가 사용하는 IP 대역 정보와 칼리코 모드 정보 확인</span>
<span class="nv">$ </span>calicoctl get ippool <span class="nt">-o</span> wide
<span class="c"># =&gt; NAME                  CIDR            NAT    IPIPMODE   VXLANMODE   DISABLED   DISABLEBGPEXPORT   SELECTOR</span>
<span class="c">#    default-ipv4-ippool   172.16.0.0/16   true   Always     Never       false      false              all()</span>
  
<span class="c"># 파드와 서비스 사용 네트워크 대역 정보 확인 </span>
<span class="nv">$ </span>kubectl cluster-info dump | <span class="nb">grep</span> <span class="nt">-m</span> 2 <span class="nt">-E</span> <span class="s2">"cluster-cidr|service-cluster-ip-range"</span>  
<span class="c"># =&gt; "--service-cluster-ip-range=10.200.1.0/24",</span>
<span class="c">#    "--cluster-cidr=172.16.0.0/16",</span>
                              
<span class="nv">$ </span>kubectl get cm <span class="nt">-n</span> kube-system kubeadm-config <span class="nt">-oyaml</span> | <span class="nb">grep</span> <span class="nt">-i</span> subnet
<span class="c"># =&gt; podSubnet: 172.16.0.0/16</span>
<span class="c">#    serviceSubnet: 10.200.1.0/24</span>
   
<span class="c"># calico endpoint (파드)의 정보 확인 : WORKLOAD 는 파드 이름이며, 어떤 노드에 배포되었고 IP 와 cali 인터페이스와 연결됨을 확인</span>
<span class="nv">$ </span>calicoctl get workloadEndpoint
<span class="nv">$ </span>calicoctl get workloadEndpoint <span class="nt">-A</span>
<span class="nv">$ </span>calicoctl get workloadEndpoint <span class="nt">-o</span> wide <span class="nt">-A</span>
<span class="c"># =&gt; NAMESPACE     NAME                                                            WORKLOAD                                   NODE     NETWORKS          INTERFACE         PROFILES                                                  NATS</span>
<span class="c">#    kube-system   k8s--w0-k8s-calico--kube--controllers--77d59654f4--wbzth-eth0   calico-kube-controllers-77d59654f4-wbzth   k8s-w0   172.16.34.5/32    cali544ab3155a5   kns.kube-system,ksa.kube-system.calico-kube-controllers</span>
<span class="c">#    kube-system   k8s--w0-k8s-coredns--55cb58b774--7qvtv-eth0                     coredns-55cb58b774-7qvtv                   k8s-w0   172.16.34.6/32    cali6be4c908feb   kns.kube-system,ksa.kube-system.coredns</span>
<span class="c">#    kube-system   k8s--w0-k8s-coredns--55cb58b774--8q4f6-eth0                     coredns-55cb58b774-8q4f6                   k8s-w0   172.16.34.4/32    cali23a9e6edc85   kns.kube-system,ksa.kube-system.coredns</span>
<span class="c">#    kube-system   k8s--w1-k8s-metrics--server--68cfccbdf6--wjjs2-eth0             metrics-server-68cfccbdf6-wjjs2            k8s-w1   172.16.158.2/32   cali2789c1b51d6   kns.kube-system,ksa.kube-system.metrics-server</span>
  
<span class="c"># 노드에서 컨테이너(프로세스) 확인</span>
<span class="nv">$ </span>ps axf 
<span class="c"># =&gt;    1325 ?        Sl     0:02 /usr/bin/containerd-shim-runc-v2 -namespace k8s.io -id 4b2a9892a9147b1a3b73b4864bec5270f18da7d8393b82f8863dc8a6cee8ac0c -address /run/containerd/conta</span>
<span class="c">#       1352 ?        Ss     0:00  \_ /pause</span>
<span class="c">#       1762 ?        Ss     0:00  \_ /usr/local/bin/runsvdir -P /etc/service/enabled</span>
<span class="c">#       1838 ?        Ss     0:00      \_ runsv confd</span>
<span class="c">#       1853 ?        Sl     0:00      |   \_ calico-node -confd</span>
<span class="c">#       1839 ?        Ss     0:00      \_ runsv bird</span>
<span class="c">#       2032 ?        S      0:00      |   \_ bird -R -s /var/run/calico/bird.ctl -d -c /etc/calico/confd/config/bird.cfg</span>
<span class="c">#       1840 ?        Ss     0:00      \_ runsv node-status-reporter</span>
<span class="c">#       1847 ?        Sl     0:00      |   \_ calico-node -status-reporter</span>
<span class="c">#       1841 ?        Ss     0:00      \_ runsv monitor-addresses</span>
<span class="c">#       1851 ?        Sl     0:00      |   \_ calico-node -monitor-addresses</span>
<span class="c">#       1842 ?        Ss     0:00      \_ runsv felix</span>
<span class="c">#       1849 ?        Sl     0:27      |   \_ calico-node -felix</span>
<span class="c">#       1843 ?        Ss     0:00      \_ runsv allocate-tunnel-addrs</span>
<span class="c">#       1846 ?        Sl     0:00      |   \_ calico-node -allocate-tunnel-addrs</span>
<span class="c">#       1844 ?        Ss     0:00      \_ runsv cni</span>
<span class="c">#       1848 ?        Sl     0:00      |   \_ calico-node -monitor-token</span>
<span class="c">#       1845 ?        Ss     0:00      \_ runsv bird6</span>
<span class="c">#       2033 ?        S      0:00          \_ bird6 -R -s /var/run/calico/bird6.ctl -d -c /etc/calico/confd/config/bird6.cfg</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p><strong>felix</strong> : Host의 Network Interface, Routing Table, Iptables를 관리합니다.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Calico의 Felix를 통해 설정된 iptables 규칙 설정 확인</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> filter <span class="nt">-S</span> | <span class="nb">grep </span>cali
<span class="c"># =&gt; -N cali-FORWARD</span>
<span class="c">#    -N cali-INPUT</span>
<span class="c">#    -N cali-OUTPUT</span>
<span class="c">#    -N cali-cidr-block</span>
<span class="c">#    -N cali-from-hep-forward</span>
<span class="c">#    -N cali-from-host-endpoint</span>
<span class="c">#    -N cali-from-wl-dispatch</span>
<span class="c">#    -N cali-to-hep-forward</span>
<span class="c">#    -N cali-to-host-endpoint</span>
<span class="c">#    -N cali-to-wl-dispatch</span>
<span class="c">#    -N cali-wl-to-host</span>
<span class="c">#    -A INPUT -m comment --comment "cali:Cz_u1IQiXIMmKD4c" -j cali-INPUT</span>
<span class="c">#    -A FORWARD -m comment --comment "cali:wUHhoiAYhphO9Mso" -j cali-FORWARD</span>
<span class="c">#    -A FORWARD -m comment --comment "cali:S93hcgKJrXEqnTfs" -m comment --comment "Policy explicitly accepted packet." -j ACCEPT</span>
<span class="c">#    -A FORWARD -m comment --comment "cali:mp77cMpurHhyjLrM" -j MARK --set-xmark 0x10000/0x10000</span>
<span class="c">#    -A OUTPUT -m comment --comment "cali:tVnHkvAo15HuiPy0" -j cali-OUTPUT</span>
<span class="c">#    -A cali-FORWARD -m comment --comment "cali:vjrMJCRpqwy5oRoX" -j MARK --set-xmark 0x0/0xe0000</span>
<span class="c">#    -A cali-FORWARD -m comment --comment "cali:A_sPAO0mcxbT9mOV" -j cali-from-hep-forward</span>
<span class="c">#    -A cali-FORWARD -i cali+ -m comment --comment "cali:8ZoYfO5HKXWbB3pk" -j cali-from-wl-dispatch</span>
<span class="c">#    -A cali-FORWARD -o cali+ -m comment --comment "cali:jdEuaPBe14V2hutn" -j cali-to-wl-dispatch</span>
<span class="c">#    -A cali-FORWARD -m comment --comment "cali:12bc6HljsMKsmfr-" -j cali-to-hep-forward</span>
<span class="c">#    -A cali-FORWARD -m comment --comment "cali:NOSxoaGx8OIstr1z" -j cali-cidr-block</span>
<span class="c">#    -A cali-INPUT -p ipencap -m comment --comment "cali:PajejrV4aFdkZojI" -m comment --comment "Allow IPIP packets from Calico hosts" -m set --match-set cali40all-hosts-net src -m addrtype --dst-type LOCAL -j ACCEPT</span>
<span class="c">#    -A cali-INPUT -p ipencap -m comment --comment "cali:_wjq-Yrma8Ly1Svo" -m comment --comment "Drop IPIP packets from non-Calico hosts" -j DROP</span>
<span class="c">#    -A cali-INPUT -i cali+ -m comment --comment "cali:8TZGxLWh_Eiz66wc" -g cali-wl-to-host</span>
<span class="c">#    -A cali-INPUT -m comment --comment "cali:6McIeIDvPdL6PE1T" -j ACCEPT</span>
<span class="c">#    -A cali-INPUT -m comment --comment "cali:YGPbrUms7NId8xVa" -j MARK --set-xmark 0x0/0xf0000</span>
<span class="c">#    -A cali-INPUT -m comment --comment "cali:2gmY7Bg2i0i84Wk_" -j cali-from-host-endpoint</span>
<span class="c">#    -A cali-INPUT -m comment --comment "cali:q-Vz2ZT9iGE331LL" -m comment --comment "Host endpoint policy accepted packet." -j ACCEPT</span>
<span class="c">#    -A cali-OUTPUT -m comment --comment "cali:Mq1_rAdXXH3YkrzW" -j ACCEPT</span>
<span class="c">#    -A cali-OUTPUT -o cali+ -m comment --comment "cali:69FkRTJDvD5Vu6Vl" -j RETURN</span>
<span class="c">#    -A cali-OUTPUT -p ipencap -m comment --comment "cali:AnEsmO6bDZbQntWW" -m comment --comment "Allow IPIP packets to other Calico hosts" -m set --match-set cali40all-hosts-net dst -m addrtype --src-type LOCAL -j ACCEPT</span>
<span class="c">#    -A cali-OUTPUT -m comment --comment "cali:9e9Uf3GU5tX--Lxy" -j MARK --set-xmark 0x0/0xf0000</span>
<span class="c">#    -A cali-OUTPUT -m comment --comment "cali:0f3LDz_VKuHFaA2K" -m conntrack ! --ctstate DNAT -j cali-to-host-endpoint</span>
<span class="c">#    -A cali-OUTPUT -m comment --comment "cali:OgU2f8BVEAZ_fwkq" -m comment --comment "Host endpoint policy accepted packet." -j ACCEPT</span>
<span class="c">#    -A cali-from-wl-dispatch -m comment --comment "cali:zTj6P0TIgYvgz-md" -m comment --comment "Unknown interface" -j DROP</span>
<span class="c">#    -A cali-to-wl-dispatch -m comment --comment "cali:7KNphB1nNHw80nIO" -m comment --comment "Unknown interface" -j DROP</span>
<span class="c">#    -A cali-wl-to-host -m comment --comment "cali:Ee9Sbo10IpVujdIY" -j cali-from-wl-dispatch</span>
<span class="c">#    -A cali-wl-to-host -m comment --comment "cali:nSZbcOoG1xPONxb8" -m comment --comment "Configured DefaultEndpointToHostAction" -j ACCEPT</span>
  
<span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span> | <span class="nb">grep </span>cali
<span class="c"># =&gt; -N cali-OUTPUT</span>
<span class="c">#    -N cali-POSTROUTING</span>
<span class="c">#    -N cali-PREROUTING</span>
<span class="c">#    -N cali-fip-dnat</span>
<span class="c">#    -N cali-fip-snat</span>
<span class="c">#    -N cali-nat-outgoing</span>
<span class="c">#    -A PREROUTING -m comment --comment "cali:6gwbT8clXdHdC1b1" -j cali-PREROUTING</span>
<span class="c">#    -A OUTPUT -m comment --comment "cali:tVnHkvAo15HuiPy0" -j cali-OUTPUT</span>
<span class="c">#    -A POSTROUTING -m comment --comment "cali:O3lYWMrLQYEMJtB5" -j cali-POSTROUTING</span>
<span class="c">#    -A cali-OUTPUT -m comment --comment "cali:GBTAv2p5CwevEyJm" -j cali-fip-dnat</span>
<span class="c">#    -A cali-POSTROUTING -m comment --comment "cali:Z-c7XtVd2Bq7s_hA" -j cali-fip-snat</span>
<span class="c">#    -A cali-POSTROUTING -m comment --comment "cali:nYKhEzDlr11Jccal" -j cali-nat-outgoing</span>
<span class="c">#    -A cali-POSTROUTING -o tunl0 -m comment --comment "cali:SXWvdsbh4Mw7wOln" -m addrtype ! --src-type LOCAL --limit-iface-out -m addrtype --src-type LOCAL -j MASQUERADE --random-fully</span>
<span class="c">#    -A cali-PREROUTING -m comment --comment "cali:r6XmIziWUJsdOK6Z" -j cali-fip-dnat</span>
<span class="c">#    -A cali-nat-outgoing -m comment --comment "cali:flqWnvo8yq4ULQLa" -m set --match-set cali40masq-ipam-pools src -m set ! --match-set cali40all-ipam-pools dst -j MASQUERADE --random-fully</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p><strong>bird</strong> : BGP 라우팅 데몬으로, Calico CNI에서 라우팅을 수행합니다.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl get pod <span class="nt">-n</span> kube-system <span class="nt">-l</span> k8s-app<span class="o">=</span>calico-node <span class="nt">-o</span> wide
<span class="c"># =&gt; NAME                READY   STATUS    RESTARTS      AGE   IP               NODE     NOMINATED NODE   READINESS GATES</span>
<span class="c">#    calico-node-p5xpt   1/1     Running   1 (42m ago)   24h   192.168.10.10    k8s-m    &lt;none&gt;           &lt;none&gt;</span>
<span class="c">#    calico-node-545hj   1/1     Running   1 (42m ago)   24h   192.168.20.100   k8s-w0   &lt;none&gt;           &lt;none&gt;</span>
<span class="c">#    calico-node-rmzvb   1/1     Running   1 (42m ago)   24h   192.168.10.101   k8s-w1   &lt;none&gt;           &lt;none&gt;</span>
<span class="c">#    calico-node-sd9x8   1/1     Running   1 (42m ago)   24h   192.168.10.102   k8s-w2   &lt;none&gt;           &lt;none&gt;</span>
  
<span class="c"># Bird 라우팅 테이블 확인</span>
<span class="nv">$ </span>kubectl <span class="nt">-n</span> kube-system <span class="nb">exec</span> <span class="nt">-it</span> calico-node-545hj <span class="nt">--</span> birdcl show route
<span class="c"># =&gt; BIRD v0.3.3+birdv1.6.8 ready.</span>
<span class="c">#    0.0.0.0/0          via 10.0.2.2 on enp0s3 [kernel1 14:13:00] * (10)</span>
<span class="c">#    10.0.2.2/32        dev enp0s3 [kernel1 14:13:00] * (10)</span>
<span class="c">#    10.0.2.3/32        dev enp0s3 [kernel1 14:13:00] * (10)</span>
<span class="c">#    10.0.2.0/24        dev enp0s3 [direct1 14:12:59] * (240)</span>
<span class="c">#    172.16.184.0/24    via 192.168.20.254 on enp0s8 [Mesh_192_168_10_102 14:13:01 from 192.168.10.102] * (100/?) [i]</span>
<span class="c">#    192.168.10.0/24    via 192.168.20.254 on enp0s8 [kernel1 14:13:00] * (10)</span>
<span class="c">#    172.16.158.0/24    via 192.168.20.254 on enp0s8 [Mesh_192_168_10_101 14:13:30 from 192.168.10.101] * (100/?) [i]</span>
<span class="c">#    192.168.20.0/24    dev enp0s8 [direct1 14:12:59] * (240)</span>
<span class="c">#    172.16.116.0/24    via 192.168.20.254 on enp0s8 [Mesh_192_168_10_10 14:13:01 from 192.168.10.10] * (100/?) [i]</span>
<span class="c">#    172.16.34.0/24     blackhole [static1 14:12:59] * (200)</span>
<span class="c">#    172.16.34.0/32     dev tunl0 [direct1 14:12:59] * (240)</span>
<span class="c">#    172.16.34.6/32     dev cali6be4c908feb [kernel1 14:13:09] * (10)</span>
<span class="c">#    172.16.34.5/32     dev cali544ab3155a5 [kernel1 14:13:08] * (10)</span>
<span class="c">#    172.16.34.4/32     dev cali23a9e6edc85 [kernel1 14:13:08] * (10)</span>
  
<span class="nv">$ </span>kubectl <span class="nt">-n</span> kube-system <span class="nb">exec</span> <span class="nt">-it</span> calico-node-545hj <span class="nt">--</span> birdcl show protocol
<span class="c"># =&gt; BIRD v0.3.3+birdv1.6.8 ready.</span>
<span class="c">#    name     proto    table    state  since       info</span>
<span class="c">#    static1  Static   master   up     14:13:00</span>
<span class="c">#    kernel1  Kernel   master   up     14:13:00</span>
<span class="c">#    device1  Device   master   up     14:13:00</span>
<span class="c">#    direct1  Direct   master   up     14:13:00</span>
<span class="c">#    Mesh_192_168_10_10 BGP      master   up     14:13:02    Established</span>
<span class="c">#    Mesh_192_168_10_101 BGP      master   up     14:13:31    Established</span>
<span class="c">#    Mesh_192_168_10_102 BGP      master   up     14:13:02    Established</span>
  
<span class="nv">$ </span>kubectl <span class="nt">-n</span> kube-system <span class="nb">exec</span> <span class="nt">-it</span> calico-node-545hj <span class="nt">--</span> birdcl show status
<span class="c"># =&gt; BIRD v0.3.3+birdv1.6.8 ready.</span>
<span class="c">#    BIRD v0.3.3+birdv1.6.8</span>
<span class="c">#    Router ID is 192.168.20.100</span>
<span class="c">#    Current server time is 2024-09-19 14:57:39</span>
<span class="c">#    Last reboot on 2024-09-19 14:13:00</span>
<span class="c">#    Last reconfiguration on 2024-09-19 14:13:00</span>
<span class="c">#    Daemon is up and running</span>
</code></pre></div>    </div>
  </li>
</ul>

<h4 id="노드간의-bgp-전달과정-확인">노드간의 BGP 전달과정 확인</h4>

<ul>
  <li>Calico CNI에서는 BGP 프로토콜을 사용하여 노드간의 라우팅 정보를 교환합니다.
해당 역할을 BIRD가 수행하는데 그림으로 살펴보면 아래와 같습니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w3/20240921_kans_w3_3.png" alt="img.png" class="image-center" />
<em class="image-caption">Bird와 Felix의 역할 모식도 (출처: 추가예정)</em></p>

<ul>
  <li>노드간의 BGP 전달을 패킷을 캡쳐해서 확인해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 워커 노드를 종료한 상태에서 아래의 작업을 시작합니다.</span>

<span class="c"># 노드에서 패킷 캡쳐</span>
<span class="nv">$ </span>vagrant ssh k8s-m

<span class="c"># BGP는 TCP 179 포트를 사용하므로 해당 포트로 패킷을 캡쳐합니다.</span>
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> enp0s8 tcp port 179 <span class="nt">-w</span> bgp.cap

<span class="c"># 종료된 워커 노드를 시작합니다.</span>
<span class="c"># 워커 노드가 시작되면 BGP 연결이 재설정되고 패킷이 전송됩니다.</span>
</code></pre></div></div>

<ul>
  <li>
    <p>캡쳐된 패킷이 bgp.cap 파일로 저장되었으면, Wireshark를 사용하여 패킷을 확인합니다.</p>

    <p><img src="/assets/2024/kans-3th/w3/20240921_kans_w3_4.png" alt="20240921_kans_w3_4.png" /></p>
  </li>
  <li>
    <p>워커 노드에 접속해서 노드 ip와 ipip tunneling ip를 확인합니다.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 워커노드 k8s-w2 접속</span>
<span class="nv">$ </span>vagrant ssh k8s-w2
  
<span class="nv">$ </span>ip addr
<span class="c"># =&gt; ...</span>
<span class="c">#    3: enp0s8: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000</span>
<span class="c">#        link/ether 08:00:27:af:2e:7a brd ff:ff:ff:ff:ff:ff</span>
<span class="c">#        inet &lt;span style="color: red;"&gt;192.168.10.102&lt;/span&gt;/24 brd 192.168.10.255 scope global enp0s8  &lt;span style="color: red;"&gt;# node ip&lt;/span&gt;   </span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#    4: tunl0@NONE: &lt;NOARP,UP,LOWER_UP&gt; mtu 1480 qdisc noqueue state UNKNOWN group default qlen 1000</span>
<span class="c">#        link/ipip 0.0.0.0 brd 0.0.0.0                                  </span>
<span class="c">#        inet &lt;span style="color: red;"&gt;172.16.184.0&lt;/span&gt;/32 scope global tunl0                        &lt;span style="color: red;"&gt;# ipip tunneling ip&lt;/span&gt;</span>
<span class="c">#           valid_lft forever preferred_lft forever </span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>컨트롤 플레인 노드에 접속해서 라우팅 테이블을 확인해보겠습니다.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 컨트롤 플레인 노드 k8s-m 접속</span>
<span class="nv">$ </span>vagrant ssh k8s-m
  
<span class="c"># 라우팅 테이블 확인</span>
<span class="nv">$ </span>ip route show 172.16.184.0/24
<span class="c"># =&gt; &lt;span style="color: red;"&gt;172.16.184.0&lt;/span&gt;/24 via &lt;span style="color: red;"&gt;192.168.10.102&lt;/span&gt; dev tunl0 proto bird onlink</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>위와 같이 BGP를 통해 노드간 라우팅 정보를 교환하고, 라우팅 테이블을 업데이트하는 것을 확인할 수 있었습니다.</p>
  </li>
</ul>

<h3 id="calico-통신-흐름-확인">Calico 통신 흐름 확인</h3>

<ul>
  <li>Calico CNI를 사용한 통신을 확인해보겠습니다.</li>
</ul>

<h4 id="동일-노드에서-파드pod-간-통신">동일 노드에서 파드(Pod) 간 통신</h4>

<ul>
  <li>동일 노드에서 파드간 통신은 기본적으로 직접 통신합니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w3/20240921_kans_w3_5.png" alt="img.png" class="w-80 image-center" /></p>

<ul>
  <li>iptables FORWARD Rule 정책에서 파드간 포워딩 정책을 허용합니다.</li>
  <li>calice# 인터페이스에 proxy arp 설정을 통해 게이트웨이의 MAC 주소를 파드가 전달 받아 사용합니다.</li>
  <li>동일 노드 내의 파드 간 통신에서는 tunnel interface를 사용하지 않습니다.</li>
</ul>

<h5 id="파드-배포-전-기본-상태-확인">파드 배포 전 기본 상태 확인</h5>

<ul>
  <li>파드 배포 전에는 아래와 같이 파드가 없는 상태입니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w3/20240921_kans_w3_6.png" alt="img.png" class="w-70 image-center" />
<em class="image-caption">파드 배포 전 상태 (출처: 추가예정)</em></p>

<ul>
  <li>
    <p>파드 생성 전 노드(k8s-w1) Shell 에서 기본 정보 확인</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 네트워크 인터페이스 정보 확인 : 터널(ipip) 인터페이스가 존재</span>
<span class="nv">$ </span>ip <span class="nt">-c</span> <span class="nt">-d</span> addr show tunl0
<span class="c"># =&gt; 4: tunl0@NONE: &lt;NOARP,UP,LOWER_UP&gt; mtu 1480 qdisc noqueue state UNKNOWN group default qlen 1000</span>
<span class="c">#        link/ipip 0.0.0.0 brd 0.0.0.0 promiscuity 0 minmtu 0 maxmtu 0</span>
<span class="c">#        &lt;span style="color: red;"&gt;ipip&lt;/span&gt; any remote any local any ttl inherit nopmtudisc numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535</span>
<span class="c">#        inet &lt;span style="color: red;"&gt;172.16.158.0/32&lt;/span&gt; scope global tunl0</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
  
<span class="c"># 네트워크 네임스페이스 확인</span>
<span class="nv">$ </span>lsns <span class="nt">-t</span> net
<span class="c"># =&gt;         NS TYPE NPROCS   PID USER     NETNSID NSFS                                                COMMAND</span>
<span class="c">#    4026531840 net     145     1 root  unassigned                                                     /sbin/i</span>
<span class="c">#    4026532204 net       2  2009 65535          0 /run/netns/cni-6923c332-7c35-669a-8787-232597fa3fa8 /pause</span>
  
<span class="c"># 네트워크 라우팅 경로 정보 확인</span>
<span class="c"># 이중 bird 는 bird 데몬이 BGP 라우팅 프로토콜에 의해 파드 네트워크 대역을 전달받거나 전달하는 경로 → 각각 노드의 파드 대역입니다</span>
<span class="nv">$ </span>ip <span class="nt">-c</span> route | <span class="nb">grep </span>bird
<span class="c"># =&gt; blackhole 172.16.158.0/24 proto bird</span>
<span class="c"># =&gt; 172.16.34.0/24 via 192.168.20.100 dev tunl0 proto bird onlink</span>
<span class="c"># =&gt; 172.16.116.0/24 via 192.168.10.10 dev tunl0 proto bird onlink</span>
<span class="c"># =&gt; 172.16.184.0/24 via 192.168.10.102 dev tunl0 proto bird onlink</span>
  
<span class="c"># 아래 tunl0 Iface 에 목적지 네트워크 대역은 ipip 인캡슐레이션에 의해서 각 노드에 전달됩니다 → 각각 노드의 파드 대역입니다</span>
<span class="nv">$ </span>route <span class="nt">-n</span>
<span class="c"># =&gt; Kernel IP routing table</span>
<span class="c">#    Destination     Gateway         Genmask         Flags Metric Ref    Use Iface</span>
<span class="c">#    172.16.34.0     192.168.20.100  255.255.255.0   UG    0      0        0 tunl0</span>
<span class="c">#    172.16.116.0    192.168.10.10   255.255.255.0   UG    0      0        0 tunl0</span>
<span class="c">#    172.16.158.0    0.0.0.0         255.255.255.0   U     0      0        0 *</span>
<span class="c">#    172.16.184.0    192.168.10.102  255.255.255.0   UG    0      0        0 tunl0</span>
<span class="c">#    ...</span>
  
<span class="c"># (옵션) iptables rule 갯수 확인</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> filter <span class="nt">-S</span> | <span class="nb">grep </span>cali | <span class="nb">wc</span> <span class="nt">-l</span>
<span class="c"># =&gt; 69</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span> | <span class="nb">grep </span>cali | <span class="nb">wc</span> <span class="nt">-l</span>
<span class="c"># =&gt; 15</span>
</code></pre></div>    </div>
  </li>
</ul>

<h5 id="파드-배포-후-상태-확인">파드 배포 후 상태 확인</h5>

<ul>
  <li>노드(k8s-w1)에 파드가 배포되면 아래와 같이 파드가 생성되고, 통신이 가능한 상태입니다.</li>
  <li>
    <p>node1-pod2.yaml 파일 작성</p>

    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># node1-pod2.yaml </span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">pod1</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">nodeName</span><span class="pi">:</span> <span class="s">k8s-w1</span>  
  <span class="na">containers</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">pod1</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">nicolaka/netshoot</span>  
    <span class="na">command</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">tail"</span><span class="pi">]</span>
    <span class="na">args</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">-f"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">/dev/null"</span><span class="pi">]</span>
  <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">pod2</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">nodeName</span><span class="pi">:</span> <span class="s">k8s-w1</span>
  <span class="na">containers</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">pod2</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">nicolaka/netshoot</span>
    <span class="na">command</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">tail"</span><span class="pi">]</span>
    <span class="na">args</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">-f"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">/dev/null"</span><span class="pi">]</span>
  <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
</code></pre></div>    </div>
  </li>
  <li>파드 생성
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl apply <span class="nt">-f</span> node1-pod2.yaml
<span class="c"># =&gt; pod/pod1 created</span>
<span class="c">#    pod/pod2 created</span>
</code></pre></div>    </div>
  </li>
  <li>컨트롤 플레인에서 파드 생성 후 확인</li>
</ul>

<p><img src="/assets/2024/kans-3th/w3/20240921_kans_w3_7.png" alt="img.png" class="w-80 image-center" /></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>vagrant ssh k8s-m 

<span class="c"># [터미널1] k8s-m 모니터링</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> calicoctl get workloadEndpoint

<span class="c"># 생성된 파드 정보 확인</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-o</span> wide
<span class="c"># =&gt; NAME   READY   STATUS    RESTARTS        AGE   IP             NODE     NOMINATED NODE   READINESS GATES</span>
<span class="c">#    pod1   1/1     Running   1 (6m56s ago)   16h   172.16.158.9   k8s-w1   &lt;none&gt;           &lt;none&gt;</span>
<span class="c">#    pod2   1/1     Running   1 (6m56s ago)   16h   172.16.158.8   k8s-w1   &lt;none&gt;           &lt;none&gt;</span>

<span class="c"># calicoctl 이용한 endpoint 확인</span>
<span class="nv">$ </span>calicoctl get workloadendpoints
<span class="c"># =&gt; WORKLOAD   NODE     NETWORKS          INTERFACE</span>
<span class="c">#    pod1       k8s-w1   172.16.158.9/32   calice0906292e2</span>
<span class="c">#    pod2       k8s-w1   172.16.158.8/32   calibd2348b4f67</span>
</code></pre></div></div>

<ul>
  <li>
    <p>워커노드에서 파드 생성 후 확인</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>vagrant ssh k8s-w1 
  
<span class="c"># 네트워크 인터페이스 정보 확인 : calice#~ 2개 추가됨!, 각각 net ns(네임스페이스) 0, 1로 호스트와 구별됨</span>
<span class="nv">$ </span>ip <span class="nt">-c</span> <span class="nb">link</span>
<span class="c"># =&gt; ...</span>
<span class="c">#    4: tunl0@NONE: &lt;NOARP,UP,LOWER_UP&gt; mtu 1480 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000</span>
<span class="c">#        link/ipip 0.0.0.0 brd 0.0.0.0</span>
<span class="c">#    &lt;span style="color: #393;"&gt;7: cali2789c1b51d6@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1480 qdisc noqueue state UP mode DEFAULT group default qlen 1000&lt;/span&gt;</span>
<span class="c">#        &lt;span style="color: #393;"&gt;link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netns cni-a67773eb-f809-7ce1-72f5-e16e93cbe4c4&lt;/span&gt;</span>
<span class="c">#    &lt;span style="color: #393;"&gt;8: calibd2348b4f67@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1480 qdisc noqueue state UP mode DEFAULT group default qlen 1000&lt;/span&gt;</span>
<span class="c">#        &lt;span style="color: #393;"&gt;link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netns cni-511c24a7-0a0a-0c9a-29c2-345cfcc0dd21&lt;/span&gt;</span>
<span class="c">#    &lt;span style="color: #393;"&gt;9: calice0906292e2@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1480 qdisc noqueue state UP mode DEFAULT group default qlen 1000&lt;/span&gt;</span>
<span class="c">#        &lt;span style="color: #393;"&gt;link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netns cni-47754704-48d2-04a9-28fe-e8bff1be17ba&lt;/span&gt;</span>
  
<span class="c"># 네트워크 네임스페이스 확인 : 아래 2개 pause(infra 컨테이너)가 각각 파드별로 생성됨 - 위 link-netnsid 0, link-netnsid 1 매칭됨</span>
<span class="nv">$ </span>lsns <span class="nt">-t</span> net
<span class="c"># =&gt;         NS TYPE NPROCS   PID USER     NETNSID NSFS                                                COMMAND</span>
<span class="c">#    4026531840 net     146     1 root  unassigned                                                     /sbin/init</span>
<span class="c">#    4026532205 net       2  2092 65535          0 /run/netns/cni-a67773eb-f809-7ce1-72f5-e16e93cbe4c4 /pause</span>
<span class="c">#    4026532282 net       2  2317 65535          1 /run/netns/cni-511c24a7-0a0a-0c9a-29c2-345cfcc0dd21 /pause</span>
<span class="c">#    4026532343 net       2  2431 65535          2 /run/netns/cni-47754704-48d2-04a9-28fe-e8bff1be17ba /pause</span>
  
<span class="c"># 파드의 IP/32bit 호스트 라우팅 대역이 라우팅 테이블에 추가됨</span>
<span class="nv">$ </span>ip <span class="nt">-c</span> route
<span class="c"># =&gt; ...</span>
<span class="c">#    blackhole 172.16.158.0/24 proto bird</span>
<span class="c">#    &lt;span style="color: #393;"&gt;172.16.158.7 dev cali2789c1b51d6 scope link&lt;/span&gt;</span>
<span class="c">#    &lt;span style="color: #393;"&gt;172.16.158.8 dev calibd2348b4f67 scope link&lt;/span&gt;</span>
<span class="c">#    &lt;span style="color: #393;"&gt;172.16.158.9 dev calice0906292e2 scope link&lt;/span&gt;</span>
<span class="c">#    ...</span>
  
<span class="c"># (옵션) iptables rule 갯수 확인</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> filter <span class="nt">-S</span> | <span class="nb">grep </span>cali | <span class="nb">wc</span> <span class="nt">-l</span>
<span class="c"># =&gt; 121</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span> | <span class="nb">grep </span>cali | <span class="nb">wc</span> <span class="nt">-l</span>
<span class="c"># =&gt; 15</span>
</code></pre></div>    </div>
  </li>
</ul>

<h5 id="파드간-통신-실행-및-확인">파드간 통신 실행 및 확인</h5>

<ul>
  <li>
    <p>파드간 통신 실행 이해를 위한 준비
<img src="/assets/2024/kans-3th/w3/20240921_kans_w3_8.png" alt="img.png" class="w-80 image-center" /></p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># iptables 필터 테이블에 FORWARD 리스트 중 cali-FORWARD 룰 정보를 필터링해서 watch 로 확인</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> <span class="nt">-n</span> 1 <span class="s2">"iptables -v --numeric --table filter --list FORWARD | egrep '(cali-FORWARD|pkts)'"</span> 
  
<span class="c"># (컨트롤플레인) 파드 연결된 veth 를 변수를 확인</span>
<span class="nv">$ VETH1</span><span class="o">=</span><span class="si">$(</span>calicoctl get workloadEndpoint | <span class="nb">grep </span>pod1 | <span class="nb">awk</span> <span class="s1">'{print $4}'</span><span class="si">)</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$VETH1</span>
<span class="c"># =&gt; calice0906292e2</span>
<span class="nv">$ VETH2</span><span class="o">=</span><span class="si">$(</span>calicoctl get workloadEndpoint | <span class="nb">grep </span>pod2 | <span class="nb">awk</span> <span class="s1">'{print $4}'</span><span class="si">)</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$VETH2</span>
<span class="c"># =&gt; calibd2348b4f67</span>
  
<span class="c"># (워커노드1) 위에서 확인한 파드 연결된 veth 를 변수에 지정</span>
<span class="nv">$ VETH1</span><span class="o">=</span>calice0906292e2
<span class="nv">$ VETH2</span><span class="o">=</span>calibd2348b4f67
  
<span class="c"># 노드1 calice# 인터페이스의 proxy arp 설정 확인</span>
<span class="c"># cat /proc/sys/net/ipv4/conf/&lt;자신의 pod1에 연결된 calice# 이름&gt;/proxy_arp</span>
<span class="nv">$ </span><span class="nb">cat</span> /proc/sys/net/ipv4/conf/<span class="nv">$VETH1</span>/proxy_arp
<span class="c"># =&gt; 1</span>
<span class="nv">$ </span><span class="nb">cat</span> /proc/sys/net/ipv4/conf/<span class="nv">$VETH2</span>/proxy_arp
<span class="c"># =&gt; 1</span>
  
<span class="c"># 파드1 혹은 파드2에 veth 로 연결된 호스트 네트워크 인터페이스 calice# 중 1개 선택해서 tcpdump</span>
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> <span class="nv">$VETH1</span> <span class="nt">-nn</span>
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> <span class="nv">$VETH2</span> <span class="nt">-nn</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>파드1 -&gt; 파드2 ping 통신을 확인해 보겠습니다.</p>
  </li>
</ul>

<p><img src="/assets/2024/kans-3th/w3/20240921_kans_w3_9.png" alt="img.png" class="w-80 image-center" /></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 파드1 Shell 에서 실행 : 정상 통신!</span>
<span class="nv">$ </span>kubectl <span class="nb">exec </span>pod1 <span class="nt">-it</span> <span class="nt">--</span> zsh
<span class="c"># --------------------</span>
<span class="c"># ping -c 10 &lt;파드2 IP&gt;</span>
<span class="nv">$ </span>ping <span class="nt">-c</span> 10 172.16.158.8

<span class="c"># 게이트웨이 169.254.1.1 의 MAC 주소를 ARP 에 의해서 학습되었습니다.</span>
<span class="nv">$ </span>ip <span class="nt">-c</span> <span class="nt">-s</span> neigh
<span class="c"># =&gt; 10.0.2.15 dev eth0 lladdr ee:ee:ee:ee:ee:ee  used 675/735/675probes 0 STALE</span>
<span class="c">#    &lt;span style="color: #900;"&gt;169.254.1.1&lt;/span&gt; dev eth0 lladdr &lt;span style="color: #900;"&gt;ee:ee:ee:ee:ee:ee&lt;/span&gt;  ref 1 used 3/3/3probes 1 REACHABLE</span>

<span class="c"># 노드에서 확인</span>
<span class="c"># iptables 에 기본 FORWARD 는 DROP 이지만, 아래 cali-FORWARD Rule에 의해 허용되며, pkts 카운트가 증가합니다.</span>
<span class="nv">$ </span>iptables <span class="nt">-v</span> <span class="nt">--numeric</span> <span class="nt">--table</span> filter <span class="nt">--list</span> FORWARD | egrep <span class="s1">'(cali-FORWARD|pkts)'</span>
<span class="c"># =&gt; pkts bytes target     prot opt in     out     source               destination</span>
<span class="c">#    29375 6505K cali-FORWARD  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* cali:wUHhoiAYhphO9Mso */</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s2">"iptables -v --numeric --table filter --list FORWARD | egrep '(cali-FORWARD|pkts)'"</span>

<span class="c"># 파드1에서 게이트웨이의 IP인 169.254.1.1 의 MAC 주소를 알기 위해서 ARP Request 를 보낸다</span>
<span class="c"># 이때 veth 연결된 calice#~ 에 proxy arp 설정이 되어 있고, 자신의 mac 주소(ee:ee:ee:ee:ee:ee)를 알려주고, 이후 정상 통신됨</span>
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> <span class="nv">$VETH1</span> <span class="nt">-nn</span>
<span class="c"># =&gt; tcpdump: verbose output suppressed, use -v[v]... for full protocol decode</span>
<span class="c">#    listening on calice0906292e2, link-type EN10MB (Ethernet), snapshot length 262144 bytes</span>
<span class="c">#    11:27:01.638238 IP 172.16.158.9 &gt; 172.16.158.8: ICMP echo request, id 134, seq 1, length 64</span>
<span class="c">#    11:27:01.638430 IP 172.16.158.8 &gt; 172.16.158.9: ICMP echo reply, id 134, seq 1, length 64</span>
<span class="c">#    ...</span>
<span class="c">#    11:27:06.675426 &lt;span style="color: #900;"&gt;ARP, Request who-has 169.254.1.1&lt;/span&gt; tell 172.16.158.9, length 28</span>
<span class="c">#    11:27:06.675498 &lt;span style="color: #900;"&gt;ARP, Reply 169.254.1.1 is-at ee:ee:ee:ee:ee:ee&lt;/span&gt;, length 28</span>

<span class="nv">$ </span>tcpdump <span class="nt">-i</span> <span class="nv">$VETH2</span> <span class="nt">-nn</span>
<span class="c"># =&gt; tcpdump: verbose output suppressed, use -v[v]... for full protocol decode</span>
<span class="c">#    listening on calibd2348b4f67, link-type EN10MB (Ethernet), snapshot length 262144 bytes</span>
<span class="c">#    11:27:01.638389 IP 172.16.158.9 &gt; 172.16.158.8: ICMP echo request, id 134, seq 1, length 64</span>
<span class="c">#    11:27:01.638409 IP 172.16.158.8 &gt; 172.16.158.9: ICMP echo reply, id 134, seq 1, length 64</span>
<span class="c">#    ...</span>
<span class="c">#    11:27:16.151004 &lt;span style="color: #900;"&gt;ARP, Request who-has 169.254.1.1&lt;/span&gt; tell 172.16.158.8, length 28</span>
<span class="c">#    11:27:16.151780 &lt;span style="color: #900;"&gt;ARP, Reply 169.254.1.1 is-at ee:ee:ee:ee:ee:ee&lt;/span&gt;, length 28</span>

<span class="c"># 호스트에서 calice0906292e2 의 MAC 주소 다시 확인</span>
<span class="nv">$ </span>ip <span class="nt">-c</span> <span class="nt">-d</span> <span class="nb">link</span>
<span class="c"># =&gt; ... </span>
<span class="c">#    9: &lt;span style="color: #900;"&gt;calice0906292e2&lt;/span&gt;@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1480 qdisc noqueue state UP mode DEFAULT group default qlen 1000</span>
<span class="c">#        link/ether &lt;span style="color: #900;"&gt;ee:ee:ee:ee:ee:ee&lt;/span&gt; brd ff:ff:ff:ff:ff:ff link-netns cni-47754704-48d2-04a9-28fe-e8bff1be17ba promiscuity 1 minmtu 68 maxmtu 65535</span>
<span class="c">#        veth addrgenmode eui64 numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535</span>
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w3/20240921_kans_w3_10.png" alt="img.png" class="image-center mb-0" />
<em class="image-caption">파드간 통신 확인 실습</em></p>

<ul>
  <li>실습결과 calice# 인터페이스에 proxy arp 설정을 통해 게이트웨이의 MAC 주소를 파드가 전달 받아 사용하는 것을 확인할 수 있었습니다.</li>
  <li>169.254.1.1은 Calico CNI에서 사용하는 게이트웨이 주소로, 같은 노드의 파드끼리 통신시 사용되는 것을 확인하였습니다.</li>
</ul>

<h4 id="파드---외부인터넷-통신">파드 -&gt; 외부(인터넷) 통신</h4>

<ul>
  <li>이번에는 파드에서 외부(인터넷)로 통신하는 과정을 확인해보겠습니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w3/20240921_kans_w3_11.png" alt="img.png" class="w-80 image-center" /></p>

<ul>
  <li>calico는 기본 설정에 <code class="language-plaintext highlighter-rouge">natOutgoing: true</code>여서 파드에서 외부로 통신할 때는 노드의 IP 주소로 MASQUERADE(Source NAT)을 수행하여 외부로 통신합니다.</li>
  <li>calice# 인터페이스에 proxy arp 설정을 통해 게이트웨이의 MAC 주소를 파드가 전달 받아 사용합니다.</li>
  <li>외부로 통신할 때는 tunnel interface를 사용하지 않습니다.</li>
</ul>

<h5 id="파드-배포-전-기본-상태-확인-1">파드 배포 전 기본 상태 확인</h5>

<ul>
  <li>calico 설정 및 노드의 iptables 설정을 확인하겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 마스터 노드에서 확인 : natOutgoing 의 기본값은 true 이다</span>
<span class="nv">$ </span>calicoctl get ippool <span class="nt">-o</span> wide
<span class="c"># =&gt; NAME                  CIDR            NAT    IPIPMODE   VXLANMODE   DISABLED   DISABLEBGPEXPORT   SELECTOR</span>
<span class="c">#    default-ipv4-ippool   172.16.0.0/16   true   Always     Never       false      false              all()</span>

<span class="c"># 노드에서 확인 : 노드에서 외부로 통신 시 MASQUERADE 동작 Rule 확인</span>
<span class="nv">$ </span>iptables <span class="nt">-n</span> <span class="nt">-t</span> nat <span class="nt">--list</span> cali-nat-outgoing
<span class="c"># =&gt; Chain cali-nat-outgoing (1 references)</span>
<span class="c">#    target     prot opt source               destination</span>
<span class="c">#    MASQUERADE  all  --  0.0.0.0/0            0.0.0.0/0            /* cali:flqWnvo8yq4ULQLa */ match-set cali40masq-ipam-pools src ! match-set cali40all-ipam-pools dst random-fully</span>

<span class="nv">$ </span>ipset list
<span class="nv">$ </span>ipset list cali40masq-ipam-pools
<span class="c"># =&gt; Name: cali40masq-ipam-pools</span>
<span class="c">#    Type: hash:net</span>
<span class="c">#    Revision: 7</span>
<span class="c">#    Header: family inet hashsize 1024 maxelem 1048576 bucketsize 12 initval 0xe6c37fa0</span>
<span class="c">#    Size in memory: 504</span>
<span class="c">#    References: 1</span>
<span class="c">#    Number of entries: 1</span>
<span class="c">#    Members:</span>
<span class="c">#    172.16.0.0/16</span>
</code></pre></div></div>

<h5 id="파드-배포-및-외부-통신-확인">파드 배포 및 외부 통신 확인</h5>

<ul>
  <li>
    <p>node1-pod1.yaml 파일 작성</p>

    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">pod1</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">nodeName</span><span class="pi">:</span> <span class="s">k8s-w1</span>
  <span class="na">containers</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">pod1</span>
      <span class="na">image</span><span class="pi">:</span> <span class="s">nicolaka/netshoot</span>
      <span class="na">command</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">tail"</span><span class="pi">]</span>
      <span class="na">args</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">-f"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">/dev/null"</span><span class="pi">]</span>
  <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>파드 생성 후 확인</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 파드 생성</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> node1-pod1.yaml
  
<span class="c"># 생성된 파드 확인</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-o</span> wide
<span class="c"># =&gt; NAME   READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES</span>
<span class="c">#    pod1   1/1     Running   0          25s   172.16.158.10   k8s-w1   &lt;none&gt;           &lt;none&gt;</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>파드간 통신 실행 이해를 위한 준비</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 노드에서 실행</span>
<span class="c"># iptables NAT MASQUERADE 모니터링 : pkts 증가 확인</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s1">'iptables -n -v -t nat --list cali-nat-outgoing'</span>
  
<span class="c"># (컨트롤플레인 노드) 파드 연결된 veth 를 변수를 확인</span>
<span class="nv">$ VETH1</span><span class="o">=</span><span class="si">$(</span>calicoctl get workloadEndpoint | <span class="nb">grep </span>pod1 | <span class="nb">awk</span> <span class="s1">'{print $4}'</span><span class="si">)</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$VETH1</span>
<span class="c"># =&gt; calice0906292e2</span>
  
<span class="c"># (워커노드1) 위에서 확인한 파드 연결된 veth 를 변수에 지정</span>
<span class="nv">$ VETH1</span><span class="o">=</span>calice0906292e2
  
<span class="c"># 패킷 덤프 실행</span>
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> any <span class="nt">-nn</span> icmp
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> <span class="nv">$VETH1</span> <span class="nt">-nn</span> icmp
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> tunl0 <span class="nt">-nn</span> icmp
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> ens5 <span class="nt">-nn</span> icmp    <span class="c"># [실습환경 A Type]</span>
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> enp0s8 <span class="nt">-nn</span> icmp  <span class="c"># [실습환경 B Type]</span>
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> enp0s3 <span class="nt">-nn</span> icmp  <span class="c"># [실습환경 B Type]</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>외부 통신 실행 및 확인</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 파드에서 외부 정상 통신 확인</span>
<span class="nv">$ </span>kubectl <span class="nb">exec </span>pod1 <span class="nt">-it</span> <span class="nt">--</span> zsh
<span class="nt">----------------------------</span>
  
<span class="c"># 혹은 통신 확인 </span>
<span class="nv">$ </span>ping <span class="nt">-c</span> 10 8.8.8.8
  
<span class="c"># The right way to check the weather - 링크</span>
<span class="nv">$ </span>curl wttr.in/seoul
<span class="nv">$ </span>curl <span class="s1">'wttr.in/seoul?format=3'</span>
<span class="nv">$ </span>curl <span class="s1">'wttr.in/busan?format=3'</span>
<span class="nv">$ </span>curl <span class="nt">-s</span> <span class="s1">'wttr.in/{London,Busan}'</span>
<span class="nv">$ </span>curl v3.wttr.in/Seoul.sxl
<span class="nv">$ </span>curl wttr.in/Moon
<span class="nv">$ </span>curl wttr.in/:help
  
<span class="c"># 패킷 덤프 내용 확인</span>
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> calice0906292e2 <span class="nt">-nn</span> icmp
<span class="c"># =&gt; tcpdump: verbose output suppressed, use -v[v]... for full protocol decode</span>
<span class="c">#    listening on calice0906292e2, link-type EN10MB (Ethernet), snapshot length 262144 bytes</span>
<span class="c">#    12:23:43.792218 IP 172.16.158.10 &gt; 8.8.8.8: ICMP echo request, id 124, seq 1, length 64</span>
<span class="c">#    12:23:43.831040 IP 8.8.8.8 &gt; 172.16.158.10: ICMP echo reply, id 124, seq 1, length 64</span>
<span class="c">#    12:23:44.797713 IP 172.16.158.10 &gt; 8.8.8.8: ICMP echo request, id 124, seq 2, length 64</span>
<span class="c">#    12:23:44.836854 IP 8.8.8.8 &gt; 172.16.158.10: ICMP echo reply, id 124, seq 2, length 64</span>
<span class="c">#    ...</span>
  
<span class="c"># 아래 10.0.2.15는 VM의 1번 네트워크 인터페이스의 IP이며, 출발지 IP가 변경되어서 외부로 나감</span>
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> enp0s3 <span class="nt">-nn</span> icmp
<span class="c"># =&gt; tcpdump: verbose output suppressed, use -v[v]... for full protocol decode</span>
<span class="c">#    listening on enp0s3, link-type EN10MB (Ethernet), snapshot length 262144 bytes</span>
<span class="c">#    12:23:43.792299 IP 10.0.2.15 &gt; 8.8.8.8: ICMP echo request, id 25687, seq 1, length 64</span>
<span class="c">#    12:23:43.830978 IP 8.8.8.8 &gt; 10.0.2.15: ICMP echo reply, id 25687, seq 1, length 64</span>
<span class="c">#    12:23:44.797877 IP 10.0.2.15 &gt; 8.8.8.8: ICMP echo request, id 25687, seq 2, length 64</span>
<span class="c">#    12:23:44.836812 IP 8.8.8.8 &gt; 10.0.2.15: ICMP echo reply, id 25687, seq 2, length 64</span>
  
<span class="c"># nat MASQUERADE rule 카운트(pkts)가 증가!</span>
<span class="c">## 출발지 매칭은 cali40masq-ipam-pools 을 사용</span>
<span class="c"># watch -d 'iptables -n -v -t nat --list cali-nat-outgoing'</span>
<span class="nv">$ </span>iptables <span class="nt">-n</span> <span class="nt">-v</span> <span class="nt">-t</span> nat <span class="nt">--list</span> cali-nat-outgoing
<span class="c"># =&gt; Chain cali-nat-outgoing (1 references)</span>
<span class="c">#     pkts bytes target     prot opt in     out     source               destination</span>
<span class="c">#        9   636 MASQUERADE  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* cali:flqWnvo8yq4ULQLa */ match-set cali40masq-ipam-pools src ! match-set cali40all-ipa</span>
<span class="c">#    m-pools dst random-fully</span>
  
<span class="c"># IPSET 으로 의 cali40masq-ipam-pools IP 대역 정보 확인 : 172.16.0.0/16 대역임을 확인</span>
<span class="nv">$ </span>ipset list cali40masq-ipam-pools
<span class="c"># =&gt; Name: cali40masq-ipam-pools</span>
<span class="c">#    Type: hash:net</span>
<span class="c">#    Revision: 7</span>
<span class="c">#    Header: family inet hashsize 1024 maxelem 1048576 bucketsize 12 initval 0x59a55159</span>
<span class="c">#    Size in memory: 504</span>
<span class="c">#    References: 1</span>
<span class="c">#    Number of entries: 1</span>
<span class="c">#    Members:</span>
<span class="c">#    172.16.0.0/16</span>
</code></pre></div>    </div>

    <p><img src="/assets/2024/kans-3th/w3/20240921_kans_w3_12.png" alt="20240921_kans_w3_12.png" /></p>
  </li>
  <li>이렇게 외부와의 통신시 터널링(tunl0)은 통하지 않고 calice# 인터페이스와 enp0s3 인터페이스를 통해 외부로 통신하는 것을 확인할 수 있었습니다.</li>
  <li>enp0s8 인터페이스는 virtualbox상의 host only 인터페이스여서 패킷이 전달되지 않습니다.</li>
</ul>

<h4 id="다른-노드의-파드--파드간-통신">다른 노드의 파드 &lt;=&gt; 파드간 통신</h4>

<ul>
  <li>이번에는 서로 다른 노드의 파드간의 통신을 알아보겠습니다.</li>
  <li>다른 노드의 파드와의 통신은 <strong>IPIP</strong>(IP in IP) 터널링을 통해 통신합니다.</li>
  <li>각 노드에서 파드 네트워크 대역은 Bird에 의해서 BGP로 전파되며, Felix에 의해 노드의 라우팅 테이블에 자동으로 추가/삭제 됩니다.</li>
  <li>다른 노드간의 통신은 tunl0 인터페이스를 통해 IP 헤더에 IPIP 헤더를 추가하여, 상대방 노드로 도착 후 tunl0 인터페이스에서 IPIP 헤더를 제거하고, 최종적으로 상대방 파드로 전달됩니다.
<img src="/assets/2024/kans-3th/w3/20240921_kans_w3_14.png" alt="img.png" class="w-80 image-center" />
<em class="image-caption">IPIP 동작방식 <a href="https://en.wikipedia.org/wiki/IP_in_IP">출처</a></em></li>
  <li>그림으로 나타내면 아래와 같습니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w3/20240921_kans_w3_13.png" alt="img.png" /></p>

<h5 id="파드-배포-전-기본-상태-확인-2">파드 배포 전 기본 상태 확인</h5>

<ul>
  <li>노드에서 Bird 라우팅 정보와 Felix 라우팅 테이블 정보를 확인합니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 컨트롤플레인</span>
<span class="nv">$ </span>route | <span class="nb">head</span> <span class="nt">-2</span> <span class="p">;</span> route <span class="nt">-n</span> | <span class="nb">grep </span>tunl0
<span class="c"># =&gt; Kernel IP routing table</span>
<span class="c">#    Destination     Gateway         Genmask         Flags Metric Ref    Use Iface</span>
<span class="c">#    172.16.34.0     192.168.20.100  255.255.255.0   UG    0      0        0 tunl0</span>
<span class="c">#    172.16.158.0    192.168.10.101  255.255.255.0   UG    0      0        0 tunl0</span>
<span class="c">#    172.16.184.0    192.168.10.102  255.255.255.0   UG    0      0        0 tunl0</span>

<span class="c"># 노드1</span>
<span class="nv">$ </span>route | <span class="nb">head</span> <span class="nt">-2</span> <span class="p">;</span> route <span class="nt">-n</span> | <span class="nb">grep </span>tunl0
<span class="c"># =&gt; Kernel IP routing table</span>
<span class="c">#    Destination     Gateway         Genmask         Flags Metric Ref    Use Iface</span>
<span class="c">#    172.16.34.0     192.168.20.100  255.255.255.0   UG    0      0        0 tunl0</span>
<span class="c">#    172.16.116.0    192.168.10.10   255.255.255.0   UG    0      0        0 tunl0</span>
<span class="c">#    172.16.184.0    192.168.10.102  255.255.255.0   UG    0      0        0 tunl0</span>
</code></pre></div></div>

<ul>
  <li>노드의 tunl0 정보 확인해 보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 노드1</span>
<span class="nv">$ </span>ifconfig enp0s3
<span class="c"># =&gt; enp0s3: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  &lt;span style="color: red;"&gt;mtu 1500&lt;/span&gt;</span>
<span class="c">#             inet 10.0.2.15  netmask 255.255.255.0  broadcast 10.0.2.255</span>

<span class="nv">$ </span>ifconfig tunl0
<span class="c"># =&gt; tunl0: flags=193&lt;UP,RUNNING,NOARP&gt;  &lt;span style="color: red;"&gt;mtu 1480&lt;/span&gt;</span>
<span class="c">#            inet 172.16.158.0  netmask 255.255.255.255</span>
<span class="c">#            tunnel   txqueuelen 1000  (IPIP Tunnel)</span>
<span class="c">#            RX packets 11581  bytes 917302 (917.3 KB)</span>
<span class="c">#            RX errors 0  dropped 0  overruns 0  frame 0</span>
<span class="c">#            TX packets 12036  bytes 2774296 (2.7 MB)</span>
<span class="c">#            TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span>

<span class="c"># 노드2</span>
<span class="nv">$ </span>ifconfig tunl0
<span class="c"># =&gt; tunl0: flags=193&lt;UP,RUNNING,NOARP&gt;  &lt;span style="color: red;"&gt;mtu 1480&lt;/span&gt;</span>
<span class="c">#            inet 172.16.184.0  netmask 255.255.255.255</span>
<span class="c">#            tunnel   txqueuelen 1000  (IPIP Tunnel)</span>
<span class="c">#            RX packets 0  bytes 0 (0.0 B)</span>
<span class="c">#            RX errors 0  dropped 0  overruns 0  frame 0</span>
<span class="c">#            TX packets 0  bytes 0 (0.0 B)</span>
<span class="c">#            TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span>
</code></pre></div></div>

<ul>
  <li>터널 인터페이스인 tunl0에 IP가 할당되어있고, MTU는 1480으로 설정되어 있습니다. enp0s3 인터페이스는 mtu가 1500인데 tunl0 인터페이스는 mtu가 1480으로 설정된 이유는, IPIP의 20바이트 헤더를 추가하기 위해서입니다.</li>
</ul>

<h5 id="파드-배포">파드 배포</h5>

<ul>
  <li>테스트를 위해 노드1과 노드2에 각각 파드 1개씩 생성하겠습니다.</li>
  <li>node2-pod2.yaml 생성
    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># node2-pod2.yaml</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">pod1</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">nodeName</span><span class="pi">:</span> <span class="s">k8s-w1</span>
  <span class="na">containers</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">pod1</span>
      <span class="na">image</span><span class="pi">:</span> <span class="s">nicolaka/netshoot</span>
      <span class="na">command</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">tail"</span><span class="pi">]</span>
      <span class="na">args</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">-f"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">/dev/null"</span><span class="pi">]</span>
  <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">pod2</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">nodeName</span><span class="pi">:</span> <span class="s">k8s-w2</span>
  <span class="na">containers</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">pod2</span>
      <span class="na">image</span><span class="pi">:</span> <span class="s">nicolaka/netshoot</span>
      <span class="na">command</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">tail"</span><span class="pi">]</span>
      <span class="na">args</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">-f"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">/dev/null"</span><span class="pi">]</span>
  <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>파드 생성 후 확인</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl apply <span class="nt">-f</span> node2-pod2.yaml
<span class="c"># =&gt; pod/pod1 created</span>
<span class="c">#    pod/pod2 created</span>
  
<span class="c"># calicoctl 이용한 endpoint 확인</span>
<span class="nv">$ </span>calicoctl get workloadendpoints
<span class="c"># =&gt; WORKLOAD   NODE     NETWORKS           INTERFACE</span>
<span class="c">#    pod1       k8s-w1   172.16.158.11/32   calice0906292e2</span>
<span class="c">#    pod2       k8s-w2   172.16.184.16/32   calibd2348b4f67</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>pod1이 node1에, pod2가 node2에 생성되었음을 확인할 수 있습니다. 파드 생성 후 상태는 아래와 같습니다.</p>

    <p><img src="/assets/2024/kans-3th/w3/20240921_kans_w3_15.png" alt="img.png" /></p>
  </li>
</ul>

<h5 id="파드간-통신-실행-및-확인-1">파드간 통신 실행 및 확인</h5>

<ul>
  <li>Calico CNI는 다른 노드의 파드간 통신을 위해 IPIP 터널링을 사용합니다.</li>
  <li>IPIP를 사용하려면 <strong>클라우드 서비스에서 IPIP 터널링을 허용해야</strong> 합니다. 현재 <strong>Azure는 IPIP 패킷을 허용하지 않고</strong> 있으며, 이러한 경우 <strong>Flannel을 사용하거나</strong>, <strong>Calico의 VXLAN을 사용</strong>할 수 있습니다.</li>
  <li>다음 그림과 같이 tunl0와 eth0 인터페이스에 tcpdump를 사용하여 패킷을 캡쳐해보겠습니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w3/20240921_kans_w3_16.png" alt="img.png" /></p>

<ul>
  <li>노드1과 노드2에서 각각 아래와 같이 실행합니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># tunl0 인터페이스 TX/RX 패킷 카운트 모니터링 실행</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s1">'ifconfig tunl0 | head -2 ; ifconfig tunl0 | grep bytes'</span>

<span class="c"># 패킷 덤프 tunl0</span>
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> tunl0 <span class="nt">-nn</span>

<span class="c"># 패킷 덤프 : IP 헤더의 상위 프로토콜을 IPIP(4)인 패킷만 필터링</span>
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> enp0s8 <span class="nt">-nn</span> ip proto 4
</code></pre></div></div>

<ul>
  <li>파드1에서 파드2로 ping 통신을 실행해보겠습니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w3/20240921_kans_w3_17.png" alt="img.png" /></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 마스터 노드에서 pod1 Shell 접속</span>
<span class="nv">$ </span>kubectl <span class="nb">exec </span>pod1 <span class="nt">-it</span> <span class="nt">--</span> zsh

<span class="c"># pod1 에서 pod2 로 핑 통신 : 정상 통신!</span>
<span class="c"># ping -c 10 &lt;pod2 IP&gt;</span>
<span class="nv">$ </span>ping <span class="nt">-c</span> 10 172.16.184.16
<span class="c"># =&gt; PING 172.16.184.16 (172.16.184.16) 56(84) bytes of data.</span>
<span class="c">#    64 bytes from 172.16.184.16: icmp_seq=1 ttl=62 time=1.42 ms</span>
<span class="c">#    64 bytes from 172.16.184.16: icmp_seq=2 ttl=62 time=2.32 ms</span>
<span class="c">#    ...</span>

<span class="c"># tunl0 인터페이스 TX/RX 패킷 카운트 모니터링 확인 : TX/RX 패킷 카운트가 각각 10개로 증가했다</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s1">'ifconfig tunl0 | head -2 ; ifconfig tunl0 | grep bytes'</span>
<span class="c"># =&gt; tunl0: flags=193&lt;UP,RUNNING,NOARP&gt;  mtu 1480</span>
<span class="c">#            inet 172.16.184.0  netmask 255.255.255.255</span>
<span class="c">#            RX packets 10  bytes 840 (840.0 B)</span>
<span class="c">#            TX packets 10  bytes 840 (840.0 B)</span>

<span class="c"># 패킷 덤프 : tunl0 - 터널 인터페이스에 파드간 IP 패킷 정보 확인!</span>
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> tunl0 <span class="nt">-nn</span>
<span class="c"># =&gt; tcpdump: verbose output suppressed, use -v or -vv for full protocol decode</span>
<span class="c">#    listening on tunl0, link-type RAW (Raw IP), capture size 262144 bytes</span>
<span class="c">#    15:42:06.220413 IP 172.16.158.11 &gt; 172.16.184.16: ICMP echo request, id 259, seq 1, length 64</span>
<span class="c">#    15:42:06.220720 IP 172.16.184.16 &gt; 172.16.158.11: ICMP echo reply, id 259, seq 1, length 64</span>
<span class="c">#    15:42:07.250941 IP 172.16.158.11 &gt; 172.16.184.16: ICMP echo request, id 259, seq 2, length 64</span>
<span class="c">#    15:42:07.252035 IP 172.16.184.16 &gt; 172.16.158.11: ICMP echo reply, id 259, seq 2, length 64</span>
<span class="c">#    ...</span>
...

<span class="c"># 패킷 덤프 : eth0(enp#~) - IP Outer 헤더 안쪽에 IP 헤더 1개가 더 있음을 알 수 있습니다.</span>
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> enp0s8 <span class="nt">-nn</span> proto 4 
<span class="c"># =&gt; 15:42:06.220427 &lt;span style="color: red;"&gt;IP&lt;/span&gt; 192.168.10.101 &gt; 192.168.10.102: &lt;span style="color: red;"&gt;IP&lt;/span&gt; 172.16.158.11 &gt; 172.16.184.16: &lt;span style="color: red;"&gt;ICMP echo&lt;/span&gt; request, id 259, seq 1, length 64</span>
<span class="c">#    15:42:06.220720 &lt;span style="color: red;"&gt;IP&lt;/span&gt; 192.168.10.102 &gt; 192.168.10.101: &lt;span style="color: red;"&gt;IP&lt;/span&gt; 172.16.184.16 &gt; 172.16.158.11: &lt;span style="color: red;"&gt;ICMP echo&lt;/span&gt; reply, id 259, seq 1, length 64</span>
<span class="c">#    15:42:07.250959 &lt;span style="color: red;"&gt;IP&lt;/span&gt; 192.168.10.101 &gt; 192.168.10.102: &lt;span style="color: red;"&gt;IP&lt;/span&gt; 172.16.158.11 &gt; 172.16.184.16: &lt;span style="color: red;"&gt;ICMP echo&lt;/span&gt; request, id 259, seq 2, length 64</span>
<span class="c">#    15:42:07.252035 &lt;span style="color: red;"&gt;IP&lt;/span&gt; 192.168.10.102 &gt; 192.168.10.101: &lt;span style="color: red;"&gt;IP&lt;/span&gt; 172.16.184.16 &gt; 172.16.158.11: &lt;span style="color: red;"&gt;ICMP echo&lt;/span&gt; reply, id 259, seq 2, length 64</span>
</code></pre></div></div>

<ul>
  <li>실습 결과 tunl0 인터페이스를 통한 패킷이 IP 헤더에 IPIP 헤더가 추가되어 노드의 enp0s8 인터페이스로 전달되는 것을 확인할 수 있었습니다.</li>
  <li>enp0s8 인터페이스에는 파드의 IP인 172.16.x.x 대역의 IP 패킷이 노드의 IP 대역인 192.168.x.x 대역으로 감싸져서 전달되는 것을 확인할 수 있었습니다.</li>
  <li>캡쳐된 패킷을 wireshark로 확인해보겠습니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w3/20240921_kans_w3_18.png" alt="img.png" /></p>

<ul>
  <li>실제 ICMP 프로토콜의 상위에 파드의 IP 프로토콜이 있고, 그 위에 노드의 IP 프로토콜이 있는 것을 확인할 수 있습니다.</li>
</ul>

<hr />

<h3 id="calico-네트워크-모드">Calico 네트워크 모드</h3>

<ul>
  <li>Calico는 다양한 네트워크 모드를 지원합니다. 네트워크 모드는 Calico의 CNI 설정을 통해 설정할 수 있습니다.</li>
  <li>
    <p>Calico의 네트워크 모드는 다음과 같습니다.</p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">네트워크 모드</th>
          <th style="text-align: left">설명</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center">IPIP</td>
          <td style="text-align: left">IPIP 터널링을 사용하여 노드 간 통신을 위한 네트워크 모드입니다.</td>
        </tr>
        <tr>
          <td style="text-align: center">Direct</td>
          <td style="text-align: left">호스트의 물리 네트워크 인터페이스를 사용하여 노드 간 통신을 위한 네트워크 모드입니다.</td>
        </tr>
        <tr>
          <td style="text-align: center">VXLAN</td>
          <td style="text-align: left">Flannel에서 사용했던 VXLAN을 사용하여 노드 간 통신을 위한 네트워크 모드입니다.</td>
        </tr>
        <tr>
          <td style="text-align: center">Pod 패킷 암호화</td>
          <td style="text-align: left">WireGuard를 사용하여 노드 간 통신을 암호화 하기 위한 네트워크 모드입니다.</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>성능은 Direct &gt; IPIP &gt; VXLAN 순으로 높지만, IPIP는 클라우드 서비스에서 IPIP 터널링을 허용해야 하고,
Direct는 물리 네트워크를 통해 직접 통신하므로 파드의 라우팅이나 방화벽 정책을 적용하는것이 까다로운 단점이 있습니다.</li>
  <li>Pod 패킷 암호화는 WireGuard를 사용하여 노드 간 통신을 암호화하는 방법으로, 가장 안전하지만 암호화에 따른 
부하와 지연이 발생하게 됩니다.</li>
  <li>이러한 특성들을 잘 이해하여 적절한 네트워크 모드를 선택하여 사용해야 합니다.</li>
  <li>각각의 mode에 대해 알아보겠습니다.</li>
</ul>

<h4 id="ipip-모드">IPIP 모드</h4>

<ul>
  <li>IPIP 모드는 노드 간 통신을 위한 네트워크 모드로, IPIP 터널링을 사용하여 노드 간 통신을 합니다.</li>
  <li>앞선 실습들이 모두 IPIP 모드로 진행되었습니다. Calico의 기본 네트워크 모드이며, 적절한 속도와 좋은 사용성을 제공합니다.</li>
  <li>단점으로는 클라우드 서비스에서 IPIP 터널링을 허용해야 하며, IPIP 터널링으로 인한 오버헤드가 발생할 수 있습니다.</li>
</ul>

<h5 id="ipip-모드-설정">IPIP 모드 설정</h5>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">projectcalico.org/v3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">IPPool</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">default-ipv4-ippool</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">allowedUses</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">Workload</span>
  <span class="pi">-</span> <span class="s">Tunnel</span>
  <span class="na">blockSize</span><span class="pi">:</span> <span class="m">24</span>
  <span class="na">cidr</span><span class="pi">:</span> <span class="s">172.16.0.0/16</span>
  <span class="na">natOutgoing</span><span class="pi">:</span> <span class="kc">true</span>
  <span class="na">ipipMode</span><span class="pi">:</span> <span class="s">Always</span>  <span class="c1"># IPIP 모드 설정</span>
  <span class="na">vxlanMode</span><span class="pi">:</span> <span class="s">Never</span>
</code></pre></div></div>

<h5 id="통신-흐름">통신 흐름</h5>

<p><img src="/assets/2024/kans-3th/w3/20240921_kans_w3_19.png" alt="img.png" class="w-80 image-center" /></p>

<ul>
  <li>다른 노드와의 파드간의 통신은 tunl0 인터페이스를 통해 IP 헤더에 IP 헤더를 감싸서(IP-in-IP) 상대 노드에 도달후 IPIP 헤더를 제거하고 최종적으로 상대방 파드로 전달됩니다.</li>
  <li>다른 노드의 파드 대역은 BGP로 전파되며, Felix에 의해 노드의 라우팅 테이블에 자동으로 추가/삭제 됩니다.</li>
</ul>

<h4 id="direct-모드">Direct 모드</h4>

<ul>
  <li>Direct 모드는 노드 간 통신을 위한 네트워크 모드로, 호스트의 물리 네트워크 인터페이스를 사용하여 노드 간 통신을 합니다.</li>
  <li>클라우드 사업자 네트워크에서는 NIC에 매칭되지 않은 IP 패킷이 차단되기 때문에, NIC에서 Source/Destination Check 기능을 해제해야 합니다. <a href="https://docs.aws.amazon.com/ko_kr/vpc/latest/userguide/VPC_NAT_Instance.html#EIP_Disable_SrcDestCheck">링크</a>
    <ul>
      <li>AWS에서는 다음의 AWS CLI 명령으로 NIC의 Source/Destination Check 기능을 해제할 수 있습니다.
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>aws ec2 modify-instance-attribute <span class="nt">--instance-id</span> &lt;INSTANCE_ID&gt; <span class="nt">--source-dest-check</span> <span class="s2">"{</span><span class="se">\"</span><span class="s2">Value</span><span class="se">\"</span><span class="s2">: false}"</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>Virtual box에서는 NIC의 promiscuous mode를 사용해야 합니다.</li>
</ul>

<h5 id="direct-모드-설정">Direct 모드 설정</h5>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">projectcalico.org/v3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">IPPool</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">default-ipv4-ippool</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">allowedUses</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">Workload</span>
  <span class="pi">-</span> <span class="s">Tunnel</span>
  <span class="na">blockSize</span><span class="pi">:</span> <span class="m">24</span>
  <span class="na">cidr</span><span class="pi">:</span> <span class="s">172.16.0.0/16</span>
  <span class="na">natOutgoing</span><span class="pi">:</span> <span class="kc">true</span>
  <span class="na">ipipMode</span><span class="pi">:</span> <span class="s">Never</span>   <span class="c1"># IPIP 모드 사용하지 않음</span>
  <span class="na">vxlanMode</span><span class="pi">:</span> <span class="s">Never</span>
</code></pre></div></div>

<h5 id="통신-흐름-1">통신 흐름</h5>

<p><img src="/assets/2024/kans-3th/w3/20240921_kans_w3_20.png" alt="img.png" class="w-80 image-center" /></p>

<h5 id="cross-subnet-모드">Cross Subnet 모드</h5>

<ul>
  <li>Direct 모드는 Cross Subnet 모드로 사용할 수 있습니다.</li>
  <li>
    <p>Cross Subnet 모드는 노드간의 네트워크 대역이 다를때 사용할 수 있으며, <strong>노드간 같은 네트워크 대역이면 Direct 모드</strong>로 동작하고,
<strong>다른 네트워크 대역이면 IPIP/VXLAN 모드</strong>로 동작합니다.</p>
  </li>
  <li>
    <p>IPIP를 이용한 Cross Subnet 모드 설정</p>

    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">projectcalico.org/v3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">IPPool</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">default-ipv4-ippool</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">allowedUses</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">Workload</span>
  <span class="pi">-</span> <span class="s">Tunnel</span>
  <span class="na">blockSize</span><span class="pi">:</span> <span class="m">24</span>
  <span class="na">cidr</span><span class="pi">:</span> <span class="s">172.16.0.0/16</span>
  <span class="na">natOutgoing</span><span class="pi">:</span> <span class="kc">true</span>
  <span class="na">ipipMode</span><span class="pi">:</span> <span class="s">CrossSubnet</span>   <span class="c1"># IPIP 모드를 Subnet이 다를때만 사용</span>
  <span class="na">vxlanMode</span><span class="pi">:</span> <span class="s">Never</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>VXLAN를 이용한 Cross Subnet 모드 설정</p>

    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">projectcalico.org/v3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">IPPool</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">default-ipv4-ippool</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">allowedUses</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">Workload</span>
  <span class="pi">-</span> <span class="s">Tunnel</span>
  <span class="na">blockSize</span><span class="pi">:</span> <span class="m">24</span>
  <span class="na">cidr</span><span class="pi">:</span> <span class="s">172.16.0.0/16</span>
  <span class="na">natOutgoing</span><span class="pi">:</span> <span class="kc">true</span>
  <span class="na">ipipMode</span><span class="pi">:</span> <span class="s">Never</span>   
  <span class="na">vxlanMode</span><span class="pi">:</span> <span class="s">CrossSubnet</span>      <span class="c1"># VXLAN 모드를 Subnet이 다를때만 사용</span>
</code></pre></div>    </div>
  </li>
</ul>

<h4 id="vxlan-모드">VXLAN 모드</h4>

<ul>
  <li>VXLAN 모드는 Flannel에서 사용했던 VXLAN을 사용하여 노드 간 통신을 위한 네트워크 모드입니다.</li>
  <li>IPIP 모드와 비슷하지만, VXLAN은 IPIP보다 속도가 느리지만, 클라우드 서비스에서 IPIP 터널링을 허용하지 않을 때 사용할 수 있습니다.</li>
</ul>

<h5 id="vxlan-모드-설정">VXLAN 모드 설정</h5>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">projectcalico.org/v3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">IPPool</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">default-ipv4-ippool</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">allowedUses</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">Workload</span>
  <span class="pi">-</span> <span class="s">Tunnel</span>
  <span class="na">blockSize</span><span class="pi">:</span> <span class="m">24</span>
  <span class="na">cidr</span><span class="pi">:</span> <span class="s">172.16.0.0/16</span>
  <span class="na">natOutgoing</span><span class="pi">:</span> <span class="kc">true</span>
  <span class="na">ipipMode</span><span class="pi">:</span> <span class="s">Never</span>  
  <span class="na">vxlanMode</span><span class="pi">:</span> <span class="s">Always</span> <span class="c1"># VXLAN 모드 설정</span>
</code></pre></div></div>

<h5 id="통신-흐름-2">통신 흐름</h5>

<p><img src="/assets/2024/kans-3th/w3/20240921_kans_w3_21.png" alt="img.png" class="w-80 image-center" /></p>

<ul>
  <li>VXLAN은 vxlan 인터페이스를 통해 L2 패킷을 UDP - VXLAN으로 감싸서 상대측 노드로 전달 후 vxlan 인터페이스에서 VXLAN에서 실제 L2 프레임을 추출하여 최종적으로 상대방 파드로 전달됩니다.</li>
  <li>이때 BGP 는 미사용되며, VXLAN L3 라우팅을 통해서 동작합니다.</li>
</ul>

<h4 id="파드-패킷-암호화-네트워크-레벨">파드 패킷 암호화 (네트워크 레벨)</h4>

<ul>
  <li>WireGuard를 사용하여 노드 간 통신을 암호화하는 방법으로, 가장 안전하지만 암호화에 따른 부하와 지연이 발생하게 됩니다.</li>
  <li>최근 대두되고 있는 제로 트러스트 네트워크 환경에서 사용할 수 있습니다.</li>
  <li>사용되는 WireGuard는 IPSec이나 OpenVPN과 같은 VPN 프로토콜이며, 기존의 VPN들 보다 빠르고 간단하며, 적은 리소스를 사용합니다.</li>
</ul>

<h5 id="wireguard-모드-설정">WireGuard 모드 설정</h5>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">projectcalico.org/v3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">FelixConfiguration</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">default</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">bpfConnectTimeLoadBalancing</span><span class="pi">:</span> <span class="s">TCP</span>
  <span class="na">bpfHostNetworkedNATWithoutCTLB</span><span class="pi">:</span> <span class="s">Enabled</span>
  <span class="na">bpfLogLevel</span><span class="pi">:</span> <span class="s2">"</span><span class="s">"</span>
  <span class="na">floatingIPs</span><span class="pi">:</span> <span class="s">Disabled</span>
  <span class="na">logSeverityScreen</span><span class="pi">:</span> <span class="s">Info</span>
  <span class="na">reportingInterval</span><span class="pi">:</span> <span class="s">0s</span>
  <span class="na">wireguardEnabled</span><span class="pi">:</span> <span class="kc">true</span>    <span class="c1"># wireguard 사용 설정</span>
</code></pre></div></div>

<h5 id="통신-흐름-3">통신 흐름</h5>

<p><img src="/assets/2024/kans-3th/w3/20240921_kans_w3_22.png" alt="img_1.png" class="w-80 image-center" /></p>

<ul>
  <li>원본 패킷이 wireg 인터페이스를 통해 WireGuard로 암호화되어 상대방 노드로 전달되며, 상대방 노드에서는 wireg 인터페이스를 통해 복호화하여 최종적으로 상대방 파드로 전달됩니다.</li>
</ul>

<hr />

<h2 id="마치며">마치며</h2>

<p>지금까지 Calico의 기본적인 구성과 동작 방식에 대해 알아보았습니다.
Flannel에 비해 많은 기능을 제공하고, 다양한 네트워크를 통해 좀 더 안전하고 빠른 네트워크 환경을 제공하는것 같습니다.
IP-in-IP 개념은 처음 보는 내용이라 신선하였습니다. 하지만 IPIP의 경우 클라우드 환경에 따라 사용하기 어려울 수도 있고,
Direct 모드는 promiscuous mode로 인해 불필요한 오버헤드와 보안 이슈가 발생할 수 있을 것 같습니다.
VXLAN을 사용하자니 Flannel에 비해 우위가 크게 없어 보이고, 제로트러스트가 필요한 환경에서 파드 패킷 암호화를 
사용하는 것은 좋아보입니다.</p>

<p>기존에 많이들 쓴다고 해서 Calico를 사용했었는데 이렇게 심오한 세계가 있다는 것을 알게 되어서 기쁩니다.
앞으로 배우게될 Cilium CNI가 더 기대가 됩니다.</p>]]></content><author><name></name></author><category term="kans" /><category term="kubernetes," /><category term="network," /><category term="linux" /><summary type="html"><![CDATA[지난주에 이어 이번주에는 Calico CNI와 Calico Network Mode에 대해 알아보겠습니다.]]></summary></entry><entry><title type="html">[KANS 3기] K8S Flannel CNI &amp;amp; PAUSE</title><link href="https://sweetlittlebird.github.io/posts/2024-09-07-KANS-Study-Week2/" rel="alternate" type="text/html" title="[KANS 3기] K8S Flannel CNI &amp;amp; PAUSE" /><published>2024-09-07T17:40:18+09:00</published><updated>2024-09-07T17:40:18+09:00</updated><id>https://sweetlittlebird.github.io/posts/KANS%20Study%20-%20Week2</id><content type="html" xml:base="https://sweetlittlebird.github.io/posts/2024-09-07-KANS-Study-Week2/"><![CDATA[<h2 id="들어가며">들어가며</h2>

<p>지난주에 이어 이번주에는 쿠버네티스에대해 간략하게 알아보고 KIND, PAUSE 컨테이너와 Flannel CNI에 대해 알아보겠습니다.
KANS 3기 2주차 스터디를 시작하겠습니다.</p>

<h2 id="쿠버네티스-소개">쿠버네티스 소개</h2>

<ul>
  <li>쿠버네티스는 구글에서 오픈소스로 공개한 컨테이너화된 애플리케이션을 자동으로 배포, 스케일링 및 관리하는 오픈소스 플랫폼입니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w2/20240907_kans_w2_1.gif" alt="20240907_kans_w2_1.gif" class="image-center" />
<em class="image-caption">출처: <a href="https://blog.naver.com/love_tolty/222167051615">https://blog.naver.com/love_tolty/222167051615</a></em></p>

<ul>
  <li>
    <p>쿠버네티스는 위의 그림과 같이 다양한 컴포넌트로 이루어져 있습니다. 각 요소를 살펴보면 아래와 같습니다.</p>
  </li>
  <li><strong>Control Plane(마스터 노드)</strong> : 마스터는 단일 서버 혹은 고가용성을 위한 클러스터 마스터로 구축
    <ul>
      <li>kube-<strong>apiserver</strong> : <strong>모든 요청</strong>을 받아 드리는 <strong>API 서버</strong></li>
      <li>etcd : 클러스터내 모든 메타 정보를 저장하는 key/value DB 서비스</li>
      <li>kube-scheduler : 컨테이너를 워커 노드에 배치하는 스케줄러</li>
      <li>kube-controller-manager : 현재 상태와 바라는 상태를 지속적으로 확인하며 특정 이벤트에 따라 특정 동작을 수행하는 컨트롤러 - <a href="https://kubernetes.io/docs/concepts/architecture/controller/">링크</a></li>
      <li>cloud-controller-manager : (AWS, GCP, Azure 등 클라우드 플랫폼에 특화된 리소스를 제어하는 클라우드 컨트롤러 - <a href="https://kubernetes.io/docs/concepts/architecture/cloud-controller/">링크</a></li>
    </ul>
  </li>
  <li><strong>Worker Node(워커 노드)</strong> - <a href="https://kubernetes.io/docs/concepts/architecture/nodes/">링크</a>
    <ul>
      <li>kubelet : 마스터의 명령에 따라 컨테이너의 라이프 사이클을 관리하는 노드 관리자</li>
      <li>kube-proxy : 컨테이너의 네트워킹을 책임지는 프록시, 네트워크 규칙을 유지 관리</li>
      <li>Container Runtime : 실제 컨테이너를 실행하는 컨테이너 실행 환경, (ContainerD, CRI-O, …) - <a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/">링크</a></li>
    </ul>
  </li>
  <li><strong>Addon(애드온)</strong>
    <ul>
      <li>CNI : Container Network Interface 는 k8s 네트워크 환경을 구성해줍니다.
        <ul>
          <li>예) Flannel, Calico, Weave Net, Cilium, …</li>
        </ul>
      </li>
      <li>DNS : 클러스터 내부 DNS 서비스를 제공합니다.
        <ul>
          <li>예) CoreDNS, Kube-DNS, …</li>
        </ul>
      </li>
      <li>기타 : 모니터링, 대시보드, 로깅 등등</li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="kind-소개-및-설치">kind 소개 및 설치</h2>

<p><img src="/assets/2024/kans-3th/w2/20240907_kans_w2_2.png" alt="img.png" class="w-30 image-center" /></p>

<p>kind는 <strong>K</strong>ubernetes <strong>IN</strong> <strong>D</strong>ocker의 약자로, 로컬 환경에서 쿠버네티스 클러스터를 쉽게 구성할 수 있도록 도와주는 도구입니다. 이름에서 알 수 있듯이 Kubernetes를 Docker 안에서 DIND(Docker in Docker) 방식으로 구동시켜주는 도구입니다.
minikube나 k3s 등과 달리 <strong>Docker만 설치되어 있으면 손쉽게 쿠버네티스 클러스터를 구성</strong>할 수 있습니다.</p>

<ul>
  <li>kind의 구조를 그림으로 표현하면 아래와 같습니다.
<img src="/assets/2024/kans-3th/w2/20240907_kans_w2_3.png" alt="img.png" class="image-center" />
<em class="image-caption">출처 : <a href="[https://kind.sigs.k8s.io/docs/design/initial/">https://kind.sigs.k8s.io/docs/design/initial/</a></em></li>
</ul>

<h3 id="설치">설치</h3>

<p>제가 사용중인 macOS를 기준으로 작성하였습니다. macOS에서 테라폼을 설치하려면 Homebrew를 이용하여 설치할 수 있습니다.
(홈브루 설치 방법은 <a href="https://whalec.io/homebrew-설치-및-사용-방법">https://whalec.io/homebrew-설치-및-사용-방법</a> 를 참고하세요.)</p>

<ul>
  <li>kind 설치 및 필수 툴 설치</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Install Kind</span>
<span class="nv">$ </span>brew <span class="nb">install </span>kind
<span class="nv">$ </span>kind <span class="nt">--version</span>
<span class="c"># =&gt; kind version 0.24.0</span>

<span class="c"># Install kubectl</span>
<span class="nv">$ </span>brew <span class="nb">install </span>kubernetes-cli
<span class="nv">$ </span>kubectl version <span class="nt">--client</span><span class="o">=</span><span class="nb">true</span>
<span class="c"># =&gt; Client Version: v1.31.0</span>
<span class="c">#    Kustomize Version: v5.4.2</span>

<span class="c"># Install Helm</span>
<span class="nv">$ </span>brew <span class="nb">install </span>helm
<span class="nv">$ </span>helm version
<span class="c"># =&gt; version.BuildInfo{Version:&amp;quot;v3.15.4&amp;quot;, GitCommit:&amp;quot;fa9efb07d9d8debbb4306d72af76a383895aa8c4&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;, GoVersion:&amp;quot;go1.22.6&amp;quot;}</span>

<span class="c"># Install Wireshark : 캡처된 패킷 확인</span>
<span class="nv">$ </span>brew <span class="nb">install</span> <span class="nt">--cask</span> wireshark

<span class="c"># (선택) kubectl 출력 시 하이라이트 처리</span>
<span class="nv">$ </span>brew <span class="nb">install </span>kubecolor
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"alias kubectl=kubecolor"</span> <span class="o">&gt;&gt;</span> ~/.zshrc
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"compdef kubecolor=kubectl"</span> <span class="o">&gt;&gt;</span> ~/.zshrc
</code></pre></div></div>

<h3 id="1-node-클러스터-구성-테스트">1-Node 클러스터 구성 테스트</h3>

<ul>
  <li>간단한 클러스터를 만들고 테스트 해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 클러스터 배포 전 확인</span>
<span class="nv">$ </span>docker ps

<span class="c"># Create a cluster with kind</span>
<span class="nv">$ </span>kind create cluster
<span class="c"># =&gt; Creating cluster &amp;quot;kind&amp;quot; ...</span>
<span class="c">#    &lt;span style="color:green;"&gt;✓&lt;/span&gt; Ensuring node image (kindest/node:v1.31.0) 🖼</span>
<span class="c">#    &lt;span style="color:green;"&gt;✓&lt;/span&gt; Preparing nodes 📦 </span>
<span class="c">#    &lt;span style="color:green;"&gt;✓&lt;/span&gt; Writing configuration 📜</span>
<span class="c">#    &lt;span style="color:green;"&gt;✓&lt;/span&gt; Starting control-plane 🕹️</span>
<span class="c">#    &lt;span style="color:green;"&gt;✓&lt;/span&gt; Installing CNI 🔌</span>
<span class="c">#    &lt;span style="color:green;"&gt;✓&lt;/span&gt; Installing StorageClass 💾</span>
<span class="c">#    Set kubectl context to &amp;quot;kind-kind&amp;quot;</span>
<span class="c">#    You can now use your cluster with:</span>
<span class="c">#    </span>
<span class="c">#    kubectl cluster-info --context kind-kind </span>
<span class="c">#    </span>
<span class="c">#    Not sure what to do next? 😅  Check out https://kind.sigs.k8s.io/docs/user/quick-start/</span>

<span class="c"># 클러스터 배포 확인</span>
<span class="nv">$ </span>kind get clusters
<span class="nv">$ </span>kind get nodes
<span class="nv">$ </span>kubectl cluster-info

<span class="c"># 노드 정보 확인</span>
<span class="nv">$ </span>kubectl get node <span class="nt">-o</span> wide

<span class="c"># 파드 정보 확인</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-A</span>
<span class="nv">$ </span>kubectl get componentstatuses

<span class="c"># 컨트롤플레인 (컨테이너) 노드 1대가 실행</span>
<span class="nv">$ </span>docker ps
<span class="nv">$ </span>docker images

<span class="c"># kube config 파일 확인</span>
<span class="nv">$ </span><span class="nb">cat</span> ~/.kube/config

<span class="c"># nginx 파드 배포 및 확인 : 컨트롤플레인 노드인데 파드가 배포 될까요?</span>
<span class="nv">$ </span>kubectl run nginx <span class="nt">--image</span><span class="o">=</span>nginx:alpine
<span class="nv">$ </span>kubectl get pod <span class="nt">-owide</span>

<span class="c"># 노드에 Taints 정보 확인</span>
<span class="nv">$ </span>kubectl describe node | <span class="nb">grep </span>Taints
<span class="c"># =&gt; Taints:             &lt;none&gt;</span>

<span class="c"># 클러스터 삭제</span>
<span class="nv">$ </span>kind delete cluster

<span class="c"># kube config 삭제 확인</span>
<span class="nv">$ </span><span class="nb">cat</span> ~/.kube/config
</code></pre></div></div>

<ul>
  <li>테스트 중에 노드가 커트롤 플레인 하나 뿐인데도 파드가 배포되는것을 확인할 수 있습니다.</li>
  <li><code class="language-plaintext highlighter-rouge">kubectl describe node | grep Taints</code>를 통해 확인해본 결과 <code class="language-plaintext highlighter-rouge">=&gt; Taints: &lt;none&gt;</code>로 컨트롤 플레인에 보통 걸려있는 taint가 없어서 파드가 배포되는 것을 확인할 수 있습니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w2/20240907_kans_w2_4.png" alt="img.png" /></p>

<h3 id="2-node-클러스터-구성-테스트">2-Node 클러스터 구성 테스트</h3>

<ul>
  <li>이번에는 좀 더 나아가서 control-plane과 worker의 2개의 노드로 구성된 KIND 클러스터를 만들고, 클러스터 구성을 확인해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 클러스터 배포 전 확인</span>
<span class="nv">$ </span>docker ps
<span class="c"># =&gt; CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES</span>

<span class="c"># kind 는 별도 도커 네트워크 생성 후 사용 : 기본값 172.18.0.0/16</span>
<span class="nv">$ </span>docker network <span class="nb">ls</span>
<span class="c"># =&gt; NETWORK ID     NAME                     DRIVER    SCOPE</span>
<span class="c">#    ...</span>
<span class="c">#    3bbcc6aa8f38   kind                     bridge    local</span>
<span class="c">#    ...</span>

<span class="nv">$ </span>docker inspect kind | jq
<span class="c"># =&gt; [</span>
<span class="c">#      {</span>
<span class="c">#        &amp;quot;Name&amp;quot;: &amp;quot;kind&amp;quot;,</span>
<span class="c">#        &amp;quot;Driver&amp;quot;: &amp;quot;bridge&amp;quot;,</span>
<span class="c">#        ...</span>
<span class="c">#        &amp;quot;IPAM&amp;quot;: {</span>
<span class="c">#          &amp;quot;Driver&amp;quot;: &amp;quot;default&amp;quot;,</span>
<span class="c">#          &amp;quot;Options&amp;quot;: {},</span>
<span class="c">#          &amp;quot;Config&amp;quot;: [</span>
<span class="c">#            {</span>
<span class="c">#              &amp;quot;Subnet&amp;quot;: &amp;quot;172.20.0.0/16&amp;quot;,</span>
<span class="c">#              &amp;quot;Gateway&amp;quot;: &amp;quot;172.20.0.1&amp;quot;</span>
<span class="c">#            }</span>
<span class="c">#          ]</span>
<span class="c">#        },</span>
<span class="c">#       ...</span>
<span class="c">#      }</span>
<span class="c">#    ]</span>

<span class="c"># KIND로 control-plane, worker라는 2개의 노드를 가진 클러스터 만들기</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span> <span class="no">EOT</span><span class="sh"> &gt; kind-2node.yaml 
# two node (one workers) cluster config
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
- role: worker
</span><span class="no">EOT
</span><span class="nv">$ </span>kind create cluster <span class="nt">--config</span> kind-2node.yaml <span class="nt">--name</span> myk8s

<span class="c"># 확인</span>
<span class="nv">$ </span>kind get nodes <span class="nt">--name</span> myk8s
<span class="c"># =&gt; myk8s-worker</span>
<span class="c">#    myk8s-control-plane</span>

<span class="c"># k8s api 주소 확인</span>
<span class="nv">$ </span>kubectl cluster-info
<span class="c"># =&gt; &lt;span style="color:green;"&gt;Kubernetes control plane&lt;/span&gt; is running at &lt;span style="color:olive;"&gt;https://127.0.0.1:58638&lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:green;"&gt;CoreDNS&lt;/span&gt; is running at &lt;span style="color:olive;"&gt;https://127.0.0.1:58638/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy&lt;/span&gt;</span>
<span class="c">#    </span>
<span class="c">#    To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.</span>

<span class="c"># 호스트에서 접속 테스트 </span>
<span class="nv">$ </span>curl <span class="nt">-k</span> https://localhost:58638
<span class="c"># =&gt; {</span>
<span class="c">#      &amp;quot;kind&amp;quot;: &amp;quot;Status&amp;quot;,</span>
<span class="c">#      &amp;quot;apiVersion&amp;quot;: &amp;quot;v1&amp;quot;,</span>
<span class="c">#      &amp;quot;metadata&amp;quot;: {},</span>
<span class="c">#      &amp;quot;status&amp;quot;: &amp;quot;Failure&amp;quot;,</span>
<span class="c">#      &amp;quot;message&amp;quot;: &amp;quot;forbidden: User \&amp;quot;system:anonymous\&amp;quot; cannot get path \&amp;quot;/\&amp;quot;&amp;quot;,</span>
<span class="c">#      &amp;quot;reason&amp;quot;: &amp;quot;Forbidden&amp;quot;,</span>
<span class="c">#      &amp;quot;details&amp;quot;: {},</span>
<span class="c">#      &amp;quot;code&amp;quot;: 403</span>
<span class="c">#    } # 호스트에서 접속이 됩니다! 왜 그럴까요?</span>

<span class="nv">$ </span>docker ps <span class="c"># 포트 포워딩 정보 확인</span>
<span class="c"># =&gt; CONTAINER ID   IMAGE                  COMMAND                  CREATED         STATUS         PORTS                                  NAMES</span>
<span class="c">#    6228f280b992   kindest/node:v1.31.0   &amp;quot;/usr/local/bin/entr…&amp;quot;   3 minutes ago   Up 3 minutes   0.0.0.0:30000-30001-&amp;gt;30000-30001/tcp   myk8s-worker</span>
<span class="c">#    219113c36204   kindest/node:v1.31.0   &amp;quot;/usr/local/bin/entr…&amp;quot;   3 minutes ago   Up 3 minutes   127.0.0.1:58638-&amp;gt;6443/tcp              myk8s-control-plane</span>

<span class="c"># 도커에서 127.0.0.1:58638-&amp;gt;6443/tcp 로 포트포워딩을 하기 때문인것을 확인할 수 있습니다. </span>

<span class="c"># apiserver 프로세스 확인</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane ss <span class="nt">-tnlp</span> | <span class="nb">grep </span>6443
<span class="c"># =&gt; LISTEN 0      4096               *:6443             *:*    users:((&amp;quot;kube-apiserver&amp;quot;,pid=584,fd=3)) </span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-n</span> kube-system <span class="nt">-l</span> <span class="nv">component</span><span class="o">=</span>kube-apiserver <span class="nt">-owide</span> <span class="c"># 파드 IP 확인</span>
<span class="c"># =&gt; NAME                                 READY   STATUS    RESTARTS   AGE     IP           NODE                  NOMINATED NODE   READINESS GATES</span>
<span class="c">#    kube-apiserver-myk8s-control-plane   1/1     Running   0          5m39s   172.20.0.2   myk8s-control-plane   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c"># kube-apiserver 파드 상세 정보 확인</span>
<span class="nv">$ </span>kubectl describe  pod <span class="nt">-n</span> kube-system <span class="nt">-l</span> <span class="nv">component</span><span class="o">=</span>kube-apiserver

<span class="c"># health check 주소 접속 확인</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane curl <span class="nt">-k</span> https://localhost:6443/livez <span class="p">;</span><span class="nb">echo</span>
<span class="c"># =&gt; ok</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane curl <span class="nt">-k</span> https://localhost:6443/readyz <span class="p">;</span><span class="nb">echo</span>
<span class="c"># =&gt; ok</span>

<span class="c"># 노드 정보 확인 : CRI 는 containerd 사용</span>
<span class="nv">$ </span>kubectl get node <span class="nt">-o</span> wide
<span class="c"># =&gt; NAME                  STATUS   ROLES           AGE     VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                         KERNEL-VERSION     CONTAINER-RUNTIME</span>
<span class="c">#    myk8s-control-plane   Ready    control-plane   7m15s   v1.31.0   172.20.0.2    &amp;lt;none&amp;gt;        Debian GNU/Linux 12 (bookworm)   5.10.76-linuxkit   containerd://1.7.18</span>
<span class="c">#    myk8s-worker          Ready    &amp;lt;none&amp;gt;          7m1s    v1.31.0   172.20.0.3    &amp;lt;none&amp;gt;        Debian GNU/Linux 12 (bookworm)   5.10.76-linuxkit   containerd://1.7.18</span>

<span class="c"># 파드 정보 확인 : CNI 는 kindnet 사용</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-A</span> <span class="nt">-owide</span>
<span class="c"># =&gt; NAMESPACE            NAME                                          READY   STATUS    RESTARTS   AGE   IP           NODE                  NOMINATED NODE   READINESS GATES</span>
<span class="c">#    kube-system          coredns-6f6b679f8f-9gxnw                      1/1     Running   0          10m   10.244.0.4   myk8s-control-plane   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    kube-system          etcd-myk8s-control-plane                      1/1     Running   0          10m   172.20.0.2   myk8s-control-plane   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    kube-system          kindnet-b5brb                                 1/1     Running   0          10m   172.20.0.3   myk8s-worker          &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    kube-system          kube-apiserver-myk8s-control-plane            1/1     Running   0          10m   172.20.0.2   myk8s-control-plane   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    kube-system          kube-controller-manager-myk8s-control-plane   1/1     Running   0          10m   172.20.0.2   myk8s-control-plane   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    kube-system          kube-proxy-tbh7b                              1/1     Running   0          10m   172.20.0.3   myk8s-worker          &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    kube-system          kube-scheduler-myk8s-control-plane            1/1     Running   0          10m   172.20.0.2   myk8s-control-plane   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    local-path-storage   local-path-provisioner-57c5987fd4-nfcv8       1/1     Running   0          10m   10.244.0.2   myk8s-control-plane   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    ...</span>

<span class="c"># 네임스페이스 확인 </span>
<span class="nv">$ </span>kubectl get namespaces
<span class="c"># =&gt; NAME                 STATUS   AGE</span>
<span class="c">#    default              Active   11m</span>
<span class="c">#    kube-node-lease      Active   11m</span>
<span class="c">#    kube-public          Active   11m</span>
<span class="c">#    kube-system          Active   11m</span>
<span class="c">#    local-path-storage   Active   10m</span>

<span class="c"># 디버그용 내용 출력에 ~/.kube/config 권한 인증 로드</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-v6</span>
<span class="c"># =&gt; I0907 21:08:08.698472   40997 loader.go:395] Config loaded from file:  /Users/psyche/.kube/config</span>
<span class="c">#    I0907 21:08:08.709347   40997 round_trippers.go:553] GET https://127.0.0.1:58638/api/v1/namespaces/default/pods?limit=500 200 OK in 8 milliseconds</span>
<span class="c">#    No resources found in default namespace.</span>

<span class="c"># kube config 파일 확인</span>
<span class="nv">$ </span><span class="nb">cat</span> ~/.kube/config

<span class="c"># local-path 라는 StorageClass 가 설치, local-path 는 노드의 로컬 저장소를 활용함</span>
<span class="c"># 로컬 호스트의 path 를 지정할 필요 없이 local-path provisioner 이 볼륨을 관리</span>
<span class="nv">$ </span>kubectl get sc
<span class="c"># =&gt; NAME                 PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE</span>
<span class="c">#    standard (default)   rancher.io/local-path   Delete          WaitForFirstConsumer   false                  12m</span>
<span class="nv">$ </span>kubectl get deploy <span class="nt">-n</span> local-path-storage
</code></pre></div></div>

<h3 id="쿠버네티스-관련-정보-조사">쿠버네티스 관련 정보 조사</h3>

<p>이번에는 KIND 내부의 쿠버네티스 관련 정보를 살펴보겠습니다.
원활한 테스트를 위해 필요한 툴을 설치하고, KIND 내부의 쿠버네티스 관련 정보를 살펴보겠습니다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 툴 설치</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane sh <span class="nt">-c</span> <span class="s1">'apt update &amp;&amp; apt install tree jq psmisc lsof wget bridge-utils tcpdump htop git nano -y'</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker sh <span class="nt">-c</span> <span class="s1">'apt update &amp;&amp; apt install tree jq psmisc lsof wget bridge-utils tcpdump htop git nano -y'</span>

<span class="c"># static pod manifest 위치 찾기</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane <span class="nb">grep </span>staticPodPath /var/lib/kubelet/config.yaml
<span class="c"># =&gt; staticPodPath: /etc/kubernetes/manifests</span>

<span class="c"># static pod 정보 확인 : kubectl 및 control plane 에서 관리되지 않고 kubelet 을 통해 지정한 컨테이너를 배포</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane tree /etc/kubernetes/manifests/
<span class="c"># =&gt; /etc/kubernetes/manifests/</span>
<span class="c">#    |-- etcd.yaml</span>
<span class="c">#    |-- kube-apiserver.yaml</span>
<span class="c">#    |-- kube-controller-manager.yaml</span>
<span class="c">#    `-- kube-scheduler.yaml</span>

<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker tree /etc/kubernetes/manifests/
<span class="c"># =&gt; /etc/kubernetes/manifests/</span>

<span class="c"># 워커 노드(컨테이너) bash 진입</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker bash
<span class="c"># ---------------------------------</span>
<span class="nv">$ </span><span class="nb">whoami</span>
<span class="c"># =&gt; root</span>

<span class="c"># kubelet 상태 확인</span>
<span class="nv">$ </span>systemctl status kubelet
<span class="c"># =&gt; ● kubelet.service - kubelet: The Kubernetes Node Agent</span>
<span class="c">#         Loaded: loaded (/etc/systemd/system/kubelet.service; enabled; preset: enabled)</span>
<span class="c">#        Drop-In: /etc/systemd/system/kubelet.service.d</span>
<span class="c">#                 └─10-kubeadm.conf, 11-kind.conf</span>
<span class="c">#         Active: active (running) since Sat 2024-09-07 11:56:44 UTC; 48min ago</span>
<span class="c">#           Docs: http://kubernetes.io/docs/</span>
<span class="c">#       Main PID: 236 (kubelet)</span>
<span class="c">#          Tasks: 15 (limit: 2254)</span>
<span class="c">#         Memory: 32.9M</span>
<span class="c">#            CPU: 1min 7.074s</span>
<span class="c">#         CGroup: /kubelet.slice/kubelet.service</span>
<span class="c">#                 └─236 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/conf&gt;</span>
<span class="c">#    ...</span>

<span class="c"># 컨테이너 확인</span>
<span class="nv">$ </span>docker ps
<span class="c"># =&gt; bash: docker: command not found</span>
<span class="nv">$ </span>crictl ps
<span class="c"># =&gt; CONTAINER           IMAGE               CREATED             STATE               NAME                ATTEMPT             POD ID              POD</span>
<span class="c">#    dd7ff0509e335       6a23fa8fd2b78       49 minutes ago      Running             kindnet-cni         0                   99f981aced025       kindnet-b5brb</span>
<span class="c">#    41b224e66bc3c       c573e1357a14e       49 minutes ago      Running             kube-proxy          0                   8880634c13ba5       kube-proxy-tbh7b</span>
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">docker ps</code>했을때는 <code class="language-plaintext highlighter-rouge">command not found</code>가 나오고 <code class="language-plaintext highlighter-rouge">crictl ps</code>로 했을때 컨테이너 정보가 나오는 것을 보면, KIND는 이름과는 다르게 Docker 대신 CRI(Container Runtime Interface)를 사용하고 있기 때문에 <code class="language-plaintext highlighter-rouge">docker</code> 명령어 대신 <code class="language-plaintext highlighter-rouge">crictl</code> 명령어를 사용해야 합니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># kube-proxy 확인</span>
<span class="nv">$ </span>pstree
<span class="c"># =&gt; systemd-+-containerd---15*[{containerd}]</span>
<span class="c">#            |-containerd-shim-+-kube-proxy---8*[{kube-proxy}]</span>
<span class="c">#            |                 |-pause</span>
<span class="c">#            |                 `-12*[{containerd-shim}]</span>
<span class="c">#            |-containerd-shim-+-kindnetd---11*[{kindnetd}]</span>
<span class="c">#            |                 |-pause</span>
<span class="c">#            |                 `-12*[{containerd-shim}]</span>
<span class="c">#            |-kubelet---14*[{kubelet}]</span>
<span class="c">#            `-systemd-journal</span>
<span class="nv">$ </span>pstree <span class="nt">-p</span>
<span class="c"># kube-proxy 프로세스 정보</span>
<span class="nv">$ </span>ps afxuwww |grep proxy 
<span class="c"># =&gt; root         387  0.0  1.1 1290144 24112 ?       Ssl  11:56   0:02  \_ /usr/local/bin/kube-proxy --config=/var/lib/kube-proxy/config.conf --hostname-override=myk8s-worker</span>
<span class="c"># 방화벽 설정 확인</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> filter <span class="nt">-S</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> mangle <span class="nt">-S</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> raw <span class="nt">-S</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> security <span class="nt">-S</span>

<span class="c"># tcp listen 포트 정보 확인</span>
<span class="nv">$ </span>ss <span class="nt">-tnlp</span>

<span class="c"># 빠져나오기</span>
<span class="nv">$ </span><span class="nb">exit</span>
<span class="c"># ---------------------------------</span>
</code></pre></div></div>

<h3 id="파드-생성-및-확인">파드 생성 및 확인</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 파드 생성</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl create -f -
apiVersion: v1
kind: Pod
metadata:
  name: netpod
spec:
  containers:
  - name: netshoot-pod
    image: nicolaka/netshoot
    command: ["tail"]
    args: ["-f", "/dev/null"]
  terminationGracePeriodSeconds: 0
---
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx-pod
    image: nginx:alpine
  terminationGracePeriodSeconds: 0
</span><span class="no">EOF
</span><span class="c"># =&gt; pod/netpod created</span>
<span class="c">#    pod/nginx created</span>

<span class="c"># 파드 확인</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-owide</span>
<span class="c"># =&gt; NAME     READY   STATUS    RESTARTS   AGE   IP           NODE           NOMINATED NODE   READINESS GATES</span>
<span class="c">#    netpod   1/1     Running   0          52s   10.244.1.3   myk8s-worker   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    nginx    1/1     Running   0          52s   10.244.1.2   myk8s-worker   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>

<span class="c"># netpod 파드에서 nginx 웹 접속</span>
<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> netpod <span class="nt">--</span> curl <span class="nt">-s</span> <span class="si">$(</span>kubectl get pod nginx <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.status.podIP<span class="o">}</span><span class="si">)</span> | <span class="nb">grep</span> <span class="nt">-o</span> <span class="s2">"&lt;title&gt;.*&lt;/title&gt;"</span>
<span class="c"># =&gt; &lt;title&gt;Welcome to nginx!&lt;/title&gt;</span>
</code></pre></div></div>

<h3 id="컨트롤-플레인-컨테이너-정보-확인">컨트롤 플레인 컨테이너 정보 확인</h3>

<p>이번에는 컨트롤 플레인 컨테이너의 정보를 확인해보겠습니다. kind는 Docker IN Docker 방식으로 컨트롤 플레인을 구성하기 때문에
컨트롤 플레인에서 정보를 확인할 때와 호스트에서 docker 정보를 확인할때와 차이가 있습니다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 도커 컨테이너 확인</span>
<span class="nv">$ </span>docker ps
<span class="c"># =&gt; CONTAINER ID   IMAGE                  COMMAND                  CREATED       STATUS       PORTS                                  NAMES</span>
<span class="c">#    6228f280b992   kindest/node:v1.31.0   &amp;quot;/usr/local/bin/entr…&amp;quot;   2 hours ago   Up 2 hours   0.0.0.0:30000-30001-&amp;gt;30000-30001/tcp   myk8s-worker</span>
<span class="c">#    219113c36204   kindest/node:v1.31.0   &amp;quot;/usr/local/bin/entr…&amp;quot;   2 hours ago   Up 2 hours   127.0.0.1:58638-&amp;gt;6443/tcp              myk8s-control-plane</span>

<span class="nv">$ </span>docker inspect myk8s-control-plane | jq
...
      <span class="s2">"Entrypoint"</span>: <span class="o">[</span>
        <span class="s2">"/usr/local/bin/entrypoint"</span>,
        <span class="s2">"/sbin/init"</span>
      <span class="o">]</span>,
...

<span class="c"># 컨트롤플레인 컨테이너 bash 접속 후 확인</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane bash
<span class="nt">-------------------------------------------</span>
<span class="c"># CPU 정보 확인</span>
<span class="nv">$ </span><span class="nb">arch</span>
<span class="c"># =&gt; aarch64      # intel 호환 CPU인 경우 x86_64가 표시됩니다.</span>

<span class="c"># 기본 사용자 확인</span>
<span class="nv">$ </span><span class="nb">whoami</span>
<span class="c"># =&gt; root</span>

<span class="c"># 네트워크 정보 확인</span>
<span class="nv">$ </span>ip <span class="nt">-br</span> <span class="nt">-c</span> <span class="nt">-4</span> addr
<span class="c"># =&gt; lo               UNKNOWN        127.0.0.1/8</span>
<span class="c">#    vethfbd4a037@if4 UP             10.244.0.1/32</span>
<span class="c">#    veth50a51781@if4 UP             10.244.0.1/32</span>
<span class="c">#    veth1822edcc@if4 UP             10.244.0.1/32</span>
<span class="c">#    eth0@if17        UP             172.20.0.2/16</span>
<span class="nv">$ </span>ip <span class="nt">-c</span> route
<span class="c"># =&gt; 10.244.0.2 dev vethfbd4a037 scope host</span>
<span class="c">#    10.244.0.3 dev veth50a51781 scope host</span>
<span class="c">#    10.244.0.4 dev veth1822edcc scope host</span>
<span class="c">#    10.244.1.0/24 via 172.20.0.3 dev eth0</span>
<span class="c">#    172.20.0.0/16 dev eth0 proto kernel scope link src 172.20.0.2</span>
<span class="nv">$ </span><span class="nb">cat</span> /etc/resolv.conf
<span class="c"># =&gt; nameserver 192.168.65.2</span>
<span class="c">#    options ndots:0</span>

<span class="c"># Entrypoint 정보 확인</span>
<span class="nv">$ </span><span class="nb">cat</span> /usr/local/bin/entrypoint

<span class="c"># 프로세스 확인 : PID 1 은 /sbin/init</span>
<span class="nv">$ </span>ps <span class="nt">-ef</span>
<span class="c"># =&gt; UID          PID    PPID  C STIME TTY          TIME CMD</span>
<span class="c">#    root           1       0  0 11:56 ?        00:00:01 /sbin/init</span>
<span class="c">#    ...</span>

<span class="c"># kind는 docker 안에서 docker를 운영하기 위해 OS를 흉내내기 위해 자체적으로 systemd를 사용하기 때문에</span>
<span class="c"># 위와 같이 PID 1이 /sbin/init 가 되고, systemctl 명령도 사용할 수 있습니다.</span>
 
<span class="c"># 컨테이터 런타임 정보 확인</span>
<span class="nv">$ </span>systemctl status containerd

<span class="c"># DinD 컨테이너 확인 : crictl 사용</span>
<span class="nv">$ </span>crictl version
<span class="c"># =&gt; Version:  0.1.0</span>
<span class="c">#    RuntimeName:  containerd</span>
<span class="c">#    RuntimeVersion:  v1.7.18</span>
<span class="c">#    RuntimeApiVersion:  v1</span>
<span class="nv">$ </span>crictl info
<span class="nv">$ </span>crictl ps <span class="nt">-o</span> json | jq <span class="nt">-r</span> <span class="s1">'.containers[] | {NAME: .metadata.name, POD: .labels["io.kubernetes.pod.name"]}'</span>
<span class="nv">$ </span>crictl ps
<span class="c"># =&gt; CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD</span>
<span class="c">#    075e7a9f5f7a3       2437cf7621777       2 hours ago         Running             coredns                   0                   5e7c18501fc65       coredns-6f6b679f8f-9gxnw</span>
<span class="c">#    6f61738127a55       2437cf7621777       2 hours ago         Running             coredns                   0                   3b69c8d195b5d       coredns-6f6b679f8f-fk27q</span>
<span class="c">#    ...</span>

<span class="c"># 파드 이미지 확인</span>
<span class="nv">$ </span>crictl images
<span class="c"># =&gt; IMAGE                                           TAG                  IMAGE ID            SIZE</span>
<span class="c">#    docker.io/library/nginx                         alpine               9d6767b714bf1       20.2MB</span>
<span class="c">#    docker.io/nicolaka/netshoot                     latest               eead9e442471d       178MB</span>
<span class="c">#    ...</span>

<span class="c"># kubectl 확인</span>
<span class="nv">$ </span>kubectl get node <span class="nt">-v6</span>
<span class="nv">$ </span><span class="nb">cat</span> /etc/kubernetes/admin.conf

<span class="nv">$ </span><span class="nb">exit</span>
<span class="nt">-------------------------------------------</span>

<span class="c"># 도커 컨테이너 확인 : 다시 한번 자신의 호스트PC에서 도커 컨테이너 확인, DinD 컨테이너가 호스트에서 보이는지 확인</span>
<span class="nv">$ </span>docker ps
<span class="c"># =&gt; CONTAINER ID   IMAGE                  COMMAND                  CREATED       STATUS       PORTS                                  NAMES</span>
<span class="c">#    6228f280b992   kindest/node:v1.31.0   "/usr/local/bin/entr…"   2 hours ago   Up 2 hours   0.0.0.0:30000-30001-&gt;30000-30001/tcp   myk8s-worker</span>
<span class="c">#    219113c36204   kindest/node:v1.31.0   "/usr/local/bin/entr…"   2 hours ago   Up 2 hours   127.0.0.1:58638-&gt;6443/tcp              myk8s-control-plane</span>
<span class="nv">$ </span>docker port myk8s-control-plane

<span class="c"># kubectl 확인 : k8s api 호출 주소 확인</span>
<span class="nv">$ </span>kubectl get node <span class="nt">-v6</span> 
</code></pre></div></div>

<ul>
  <li>KIND의 컨트롤 플레인에서 <code class="language-plaintext highlighter-rouge">crictl ps</code>로 컨테이너를 확인할때와 호스트에서 <code class="language-plaintext highlighter-rouge">docker ps</code>로 확인할때의 차이가 나는것을 확인할 수 있습니다.</li>
  <li>
    <p>이것은 앞에서도 docker 컨테이너 안에서 docker (정확히는 containerd)를 별도로 사용하기 때문에 발생하는 현상입니다. 이것이 DIND(Docker IN Docker)입니다.
—만약 같은 컨테이너가 나온다면 Docker OUT Docker로 동작하기 때문일것입니다—</p>
  </li>
  <li>클러스터를 삭제하겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 클러스터 삭제</span>
<span class="nv">$ </span>kind delete cluster <span class="nt">--name</span> myk8s
<span class="c"># =&gt; Deleting cluster "myk8s" ...</span>
<span class="c">#    Deleted nodes: ["myk8s-worker" "myk8s-control-plane"]</span>
<span class="nv">$ </span>docker ps
<span class="c"># =&gt; CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES</span>
</code></pre></div></div>

<h3 id="multi-node-클러스터-with-kube-ops-view--mapping-ports">Multi-Node 클러스터 with kube-ops-view &amp; mapping ports</h3>

<p>이번에는 KIND로 Multi-Node 클러스터를 구성하고, kube-ops-view를 설치하여 클러스터 정보를 시각화하고, 포트 매핑을 통해 호스트에서 접속할 수 있도록 설정해보겠습니다.</p>

<ul>
  <li>클러스터 구성 및 노드 정보 확인</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># '컨트롤플레인, 워커 노드 1대' 클러스터 배포 : 파드에 접속하기 위한 포트 맵핑 설정</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOT</span><span class="sh">&gt; kind-2node.yaml
# two node (one workers) cluster config
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
- role: worker
  extraPortMappings:
  - containerPort: 31000
    hostPort: 31000
    listenAddress: "0.0.0.0" # Optional, defaults to "0.0.0.0"
    protocol: tcp # Optional, defaults to tcp
  - containerPort: 31001
    hostPort: 31001
</span><span class="no">EOT

</span><span class="nv">$ CLUSTERNAME</span><span class="o">=</span>myk8s
<span class="nv">$ </span>kind create cluster <span class="nt">--config</span> kind-2node.yaml <span class="nt">--name</span> <span class="nv">$CLUSTERNAME</span>
<span class="c"># =&gt; Creating cluster "myk8s" ...</span>
<span class="c">#     ✓ Ensuring node image (kindest/node:v1.31.0) 🖼</span>
<span class="c">#     ✓ Preparing nodes 📦 📦</span>
<span class="c">#     ✓ Writing configuration 📜</span>
<span class="c">#     ✓ Starting control-plane 🕹️</span>
<span class="c">#     ✓ Installing CNI 🔌</span>
<span class="c">#     ✓ Installing StorageClass 💾</span>
<span class="c">#     ✓ Joining worker nodes 🚜</span>
<span class="c">#    Set kubectl context to "kind-myk8s"</span>
<span class="c">#    You can now use your cluster with:</span>
<span class="c">#    </span>
<span class="c">#    kubectl cluster-info --context kind-myk8s</span>
<span class="c">#    </span>
<span class="c">#    Have a nice day! 👋</span>

<span class="c"># 배포 확인</span>
<span class="nv">$ </span>kind get clusters
<span class="c"># =&gt; myk8s</span>
<span class="nv">$ </span>kind get nodes <span class="nt">--name</span> <span class="nv">$CLUSTERNAME</span>
<span class="c"># =&gt; myk8s-control-plane</span>
<span class="c">#    myk8s-worker</span>

<span class="c"># 노드 확인</span>
<span class="nv">$ </span>kubectl get nodes <span class="nt">-o</span> wide
<span class="c"># =&gt; NAME                  STATUS   ROLES           AGE     VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                         KERNEL-VERSION     CONTAINER-RUNTIME</span>
<span class="c">#    myk8s-control-plane   Ready    control-plane   2m12s   v1.31.0   172.20.0.2    &amp;lt;none&amp;gt;        Debian GNU/Linux 12 (bookworm)   5.10.76-linuxkit   containerd://1.7.18</span>
<span class="c">#    myk8s-worker          Ready    &amp;lt;none&amp;gt;          119s    v1.31.0   172.20.0.3    &amp;lt;none&amp;gt;        Debian GNU/Linux 12 (bookworm)   5.10.76-linuxkit   containerd://1.7.18</span>

<span class="c"># 노드에 Taints 정보 확인</span>
<span class="nv">$ </span>kubectl describe node <span class="nv">$CLUSTERNAME</span><span class="nt">-control-plane</span> | <span class="nb">grep </span>Taints
<span class="c"># =&gt; Taints:             node-role.kubernetes.io/control-plane:NoSchedule</span>

<span class="nv">$ </span>kubectl describe node <span class="nv">$CLUSTERNAME</span><span class="nt">-worker</span> | <span class="nb">grep </span>Taints
<span class="c"># =&gt; Taints:             &amp;lt;none&amp;gt;</span>

<span class="c"># control-plane 노드에는 taints가 걸려있어서 스케쥴링이 되지 않고 </span>
<span class="c"># worker 노드에는 taints가 없어서 스케쥴링이 될 것을 예상할 수 있습니다.</span>

<span class="c"># 컨테이너 확인 : 컨테이너 갯수, 컨테이너 이름 확인</span>
<span class="c"># kind yaml 에 포트 맵핑 정보 처럼, 자신의 PC 호스트에 31000 포트 접속 시, 워커노드(실제로는 컨테이너)에 TCP 31000 포트로 연결</span>
<span class="c"># 즉, 워커노드에 NodePort TCP 31000 설정 시 자신의 PC 호스트에서 접속 가능!</span>
<span class="nv">$ </span>docker ps
<span class="c"># =&gt; CONTAINER ID   IMAGE                  COMMAND                  CREATED         STATUS         PORTS                                  NAMES</span>
<span class="c">#    7724a7ff92bb   kindest/node:v1.31.0   &amp;quot;/usr/local/bin/entr…&amp;quot;   6 minutes ago   Up 6 minutes   127.0.0.1:58498-&amp;gt;6443/tcp              myk8s-control-plane</span>
<span class="c">#    5b7fa2f98703   kindest/node:v1.31.0   &amp;quot;/usr/local/bin/entr…&amp;quot;   6 minutes ago   Up 6 minutes   0.0.0.0:31000-31001-&amp;gt;31000-31001/tcp   myk8s-worker</span>
<span class="nv">$ </span>docker port <span class="nv">$CLUSTERNAME</span><span class="nt">-worker</span>
<span class="c"># =&gt; 31000/tcp -&amp;gt; 0.0.0.0:31000</span>
<span class="c">#    31001/tcp -&amp;gt; 0.0.0.0:31001</span>

<span class="c"># 각 노드들의 정보 확인을 docker를 통해서 확인해 볼 수도 있습니다.</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> <span class="nv">$CLUSTERNAME</span><span class="nt">-control-plane</span> ip <span class="nt">-br</span> <span class="nt">-c</span> <span class="nt">-4</span> addr
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> <span class="nv">$CLUSTERNAME</span><span class="nt">-worker</span>  ip <span class="nt">-br</span> <span class="nt">-c</span> <span class="nt">-4</span> addr
</code></pre></div></div>

<ul>
  <li>
    <p>이번에 KIND를 통해 만든 클러스터는 호스트에서 31000, 31001 포트로 접속시 워커노드(컨테이너)의 31000, 31001 포트로 
연결되도록 설정되는데, 그 이유는 KIND 클러스터를 생성할때 아래와 같이 포트를 열것을 지정했기 때문입니다.</p>

    <div class="language-yml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">...</span>
<span class="pi">-</span> <span class="na">role</span><span class="pi">:</span> <span class="s">worker</span>
  <span class="na">extraPortMappings</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">31000</span>
    <span class="na">hostPort</span><span class="pi">:</span> <span class="m">31000</span>
    <span class="na">listenAddress</span><span class="pi">:</span> <span class="s2">"</span><span class="s">0.0.0.0"</span> <span class="c1"># Optional, defaults to "0.0.0.0"</span>
    <span class="na">protocol</span><span class="pi">:</span> <span class="s">tcp</span> <span class="c1"># Optional, defaults to tcp</span>
  <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">31001</span>
    <span class="na">hostPort</span><span class="pi">:</span> <span class="m">31001</span>
<span class="nn">...</span>
</code></pre></div>    </div>
    <p>추가적인 포트 매핑이 필요한 경우 위와 같이 <code class="language-plaintext highlighter-rouge">extraPortMappings</code>에 추가하여 포트를 매핑할 수 있습니다.</p>
  </li>
  <li>
    <p>Kube-ops-view 설치 : Node port 31000</p>
  </li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># kube-ops-view</span>
<span class="c"># helm show values geek-cookbook/kube-ops-view</span>
<span class="nv">$ </span>helm repo add geek-cookbook https://geek-cookbook.github.io/charts/
<span class="nv">$ </span>helm <span class="nb">install </span>kube-ops-view geek-cookbook/kube-ops-view <span class="nt">--version</span> 1.2.2 <span class="nt">--set</span> service.main.type<span class="o">=</span>NodePort,service.main.ports.http.nodePort<span class="o">=</span>31000 <span class="nt">--set</span> env.TZ<span class="o">=</span><span class="s2">"Asia/Seoul"</span> <span class="nt">--namespace</span> kube-system

<span class="c"># 설치 확인</span>
<span class="nv">$ </span>kubectl get deploy,pod,svc,ep <span class="nt">-n</span> kube-system <span class="nt">-l</span> app.kubernetes.io/instance<span class="o">=</span>kube-ops-view
<span class="c"># =&gt; NAME                            READY   UP-TO-DATE   AVAILABLE   AGE</span>
<span class="c">#    deployment.apps/kube-ops-view   0/1     1            0           18s</span>
<span class="c">#    </span>
<span class="c">#    NAME                                 READY   STATUS              RESTARTS   AGE</span>
<span class="c">#    pod/kube-ops-view-657dbc6cd8-tmkl5   0/1     ContainerCreating   0          18s</span>
<span class="c">#    </span>
<span class="c">#    NAME                    TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE</span>
<span class="c">#    service/kube-ops-view   NodePort   10.96.212.51   &amp;lt;none&amp;gt;        8080:31000/TCP   18s</span>
<span class="c">#    </span>
<span class="c">#    NAME                      ENDPOINTS   AGE</span>
<span class="c">#    endpoints/kube-ops-view   &amp;lt;none&amp;gt;      18s</span>

<span class="c"># kube-ops-view 접속 URL 확인 (1.5 , 2 배율)</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"KUBE-OPS-VIEW URL = http://localhost:31000/#scale=1.5"</span>
<span class="c"># =&gt; KUBE-OPS-VIEW URL = http://localhost:31000/#scale=1.5</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"KUBE-OPS-VIEW URL = http://localhost:31000/#scale=2"</span>
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w2/20240907_kans_w2_5.png" alt="img.png" /></p>

<p>클러스터 구성시 노드의 31000포트를 호스트의 31000에 매핑시켜서 호스트에서 위와 같이 열 수 있습니다.</p>

<ul>
  <li>nginx 설치 : NodePort 31001</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 디플로이먼트와 서비스 배포</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl create -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deploy-websrv
spec:
  replicas: 2
  selector:
    matchLabels:
      app: deploy-websrv
  template:
    metadata:
      labels:
        app: deploy-websrv
    spec:
      terminationGracePeriodSeconds: 0
      containers:
      - name: deploy-websrv
        image: nginx:alpine
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: deploy-websrv
spec:
  ports:
    - name: svc-webport
      port: 80
      targetPort: 80
      nodePort: 31001
  selector:
    app: deploy-websrv
  type: NodePort
</span><span class="no">EOF

</span><span class="c"># 확인</span>
<span class="nv">$ </span>docker ps
<span class="c"># =&gt; CONTAINER ID   IMAGE                  COMMAND                  CREATED          STATUS          PORTS                                  NAMES</span>
<span class="c">#    5b7fa2f98703   kindest/node:v1.31.0   &amp;quot;/usr/local/bin/entr…&amp;quot;   15 minutes ago   Up 15 minutes   0.0.0.0:31000-31001-&amp;gt;31000-31001/tcp   myk8s-worker</span>
<span class="c">#    ...</span>

<span class="nv">$ </span>kubectl get deploy,svc,ep deploy-websrv
<span class="c"># =&gt; ...</span>
<span class="c">#    NAME                    TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE</span>
<span class="c">#    service/deploy-websrv   NodePort   10.96.115.233   &amp;lt;none&amp;gt;        80:31001/TCP   47s</span>
<span class="c">#    ...</span>

<span class="c"># 자신의 PC에 호스트 포트 31001 접속 시 쿠버네티스 서비스에 접속 확인</span>
<span class="nv">$ </span>open http://localhost:31001
<span class="nv">$ </span>curl <span class="nt">-s</span> localhost:31001 | <span class="nb">grep</span> <span class="nt">-o</span> <span class="s2">"&lt;title&gt;.*&lt;/title&gt;"</span>
<span class="c"># =&gt; &lt;title&gt;Welcome to nginx!&lt;/title&gt;</span>

<span class="c"># 디플로이먼트와 서비스 삭제</span>
<span class="nv">$ </span>kubectl delete deploy,svc deploy-websrv
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w2/20240907_kans_w2_6.png" alt="img.png" class="image-center" />
<em class="image-caption">31001 포트로 접속시 nginx 페이지가 나오는 것을 확인할 수 있습니다.</em></p>

<p><img src="/assets/2024/kans-3th/w2/20240907_kans_w2_7.png" alt="img_1.png" />
<em class="image-caption">kube-ops-view에서 deploy된 정보를 확인할 수 있습니다.</em></p>

<hr />

<h2 id="파드--pause-컨테이너">파드 &amp; PAUSE 컨테이너</h2>

<p><strong>파드(pod)</strong>는 쿠버네티스에서 <strong>배포하는 최소 단위</strong>이며, 파드 내부에는 <strong>여러 컨테이너가 포함될 수</strong> 있습니다. 
파드 내부에는 PAUSE 컨테이너가 존재하며, <strong>PAUSE 컨테이너</strong>는 <strong>Network/IPC/UTS 네임스페이스</strong>를 <strong>생성</strong>하고 <strong>유지</strong>/<strong>공유</strong>하는 역할울 합니다. 
네임스페이스와 네트워크 등에 대해서는 1주차에서 학습한 내용이 많이 도움되었습니다. <a href="https://sweetlittlebird.github.io/posts/2024-08-27-KANS-Study-Week1/">1주차 링크</a></p>

<h3 id="k8s-cri-container-runtime-interface">K8S CRI (Container Runtime Interface)</h3>

<p>먼저 파드와 PAUSE 컨테이너에 대해 알아보기전에 앞선 실습에서 보았던 CRI(Container Runtime Interface)에 대해 알아보겠습니다.
쿠버네티스는 컨테이너를 관리하기 위해 <strong>CRI(Container Runtime Interface)</strong>를 사용합니다.</p>

<p>CRI의 탄생 배경은 먼저 Docker에서 부터 찾아볼 수 있습니다.
Docker가 대성공하고 컨테이너 기술이 확산되면서, 쿠버네티스도 Docker를 기본 컨테이너 런타임으로 사용했습니다.
하지만 Docker Inc라는 회사에 종속되는것을 우려하여, 표준화된 인터페이스를 만들어서 다양한 컨테이너 런타임을 지원하고자 했습니다.
그래서 CRI가 탄생하게 되었습니다. CRI라는 표준 인터페이스만 지키면 어떤 컨테이너 런타임이라도 쿠버네티스에서 사용할 수 있게 되었습니다.
이 과정에서 아쉬운건 Docker는 CRI를 지원하지 않았기 때문에, Docker를 사용하는 경우에는 Docker shim이라는 프록시를 사용해야 했었습니다.</p>

<h3 id="파드-pod">파드 (Pod)</h3>

<p>컨테이너 애플리케이션의 기본 단위를 파드(Pod)라고 부르며, 파드는 1개 이상의 컨테이너로 구성된 컨테이너의 집합입니다.</p>

<p><img src="/assets/2024/kans-3th/w2/20240907_kans_w2_8.png" alt="img.png" /></p>

<ul>
  <li>Pod는 1개 이상의 컨테이너를 가질 수 있습니다.</li>
  <li>Pod내에 실행되는 컨테이너들은 동일한 노드에 할당되며 동일한 생명 주기(Life-cycle)를 갖습니다.</li>
  <li>Pod는 노드 IP 와 별개로 클러스터 내에서 접근 가능한 IP를 할당 받으며, 다른 노드에 위치한 Pod 도 <strong>CNI를 통해</strong> NAT 없이 Pod IP로 접근 가능합니다.</li>
  <li>Pod내에 있는 컨테이너들은 서로 IP를 공유합니다. 같은 Pod내의 컨테이너끼리는 localhost 통해 서로 접근가능 합니다.
    <ul>
      <li><strong>pause</strong> 컨테이너가 network ns 를 만들어 주고, 내부의 컨테이너들은 해당 net ns 를 공유하기 때문에 IP를 공유하게 됩니다.</li>
    </ul>
  </li>
  <li>Pod 안의 컨테이너들은 동일한 볼륨과 연결이 가능하여 파일 시스템을 기반으로 서로 파일을 주고받을 수 있습니다.</li>
  <li>Pod는 리소스 제약이 있는 격리된 환경의 애플리케이션 컨테이너 그룹으로 구성됩니다.</li>
  <li>포드를 시작하기 전에 kubelet은 RuntimeService.RunPodSandbox를 호출하여 환경을 만듭니다.</li>
  <li>Kubelet은 RPC를 통해 컨테이너의 수명 주기를 관리하고, 컨테이너 수명 주기 후크와 활성/준비 확인을 실행하며, Pod의 재시작 정책을 준수합니다</li>
</ul>

<h3 id="pause-컨테이너">PAUSE 컨테이너</h3>

<ul>
  <li>쿠버네티스에서 <strong>pause</strong> 컨테이너는 포드의 모든 컨테이너에 대한 “<strong>부모 컨테이너</strong>” 역할을 합니다. - <a href="https://sklar.rocks/what-is-a-pod-sandbox/">Link</a></li>
  <li><strong>pause</strong> 컨테이너에는 두 가지 핵심 책임이 있습니다.
    <ol>
      <li>파드에서 Linux <strong>네임스페이스 공유의 기반</strong> 역할을 합니다. (Network, IPC, UTS 네임스페이스)</li>
      <li>PID(프로세스 ID) 네임스페이스 공유가 활성화되면 각 포드에 대한 <strong>PID 1 역할</strong>을 하며 <strong>좀비 프로세스를 거둡</strong>니다.</li>
    </ol>
  </li>
  <li>pause의 핵심 소스코드는 <a href="https://github.com/kubernetes/kubernetes/blob/master/build/pause/linux/pause.c">여기</a>에서 확인할 수 있습니다. 
매우 짧지만 중요한 코드입니다. 상세하게 코드분석한 분이 있어서 자세히 알고 싶으신 분은 다음 링크를 참고하세요.
<a href="https://mateon.tistory.com/127">한글 링크</a>
<a href="https://www.ianlewis.org/en/almighty-pause-container">영문 링크</a></li>
</ul>

<h4 id="pause-컨테이너-실습">Pause 컨테이너 실습</h4>

<ul>
  <li>Pause 컨테이너의 동작에 대해 실습하기 위한 환경을 구성해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># '컨트롤플레인, 워커 노드 1대' 클러스터 배포 : 파드에 접속하기 위한 포트 맵핑 설정</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOT</span><span class="sh">&gt; kind-2node.yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
- role: worker
  extraPortMappings:
  - containerPort: 30000
    hostPort: 30000
  - containerPort: 30001
    hostPort: 30001
</span><span class="no">EOT
</span><span class="nv">$ </span>kind create cluster <span class="nt">--config</span> kind-2node.yaml <span class="nt">--name</span> myk8s

<span class="c"># 툴 설치</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane sh <span class="nt">-c</span> <span class="s1">'apt update &amp;&amp; apt install tree jq psmisc lsof wget bridge-utils tcpdump htop git nano -y'</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker        sh <span class="nt">-c</span> <span class="s1">'apt update &amp;&amp; apt install tree jq psmisc lsof wget bridge-utils tcpdump htop -y'</span>

<span class="c"># 확인</span>
<span class="nv">$ </span>kubectl get nodes <span class="nt">-o</span> wide
<span class="nv">$ </span>docker ps
<span class="nv">$ </span>docker port myk8s-worker
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane ip <span class="nt">-br</span> <span class="nt">-c</span> <span class="nt">-4</span> addr
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker  ip <span class="nt">-br</span> <span class="nt">-c</span> <span class="nt">-4</span> addr

<span class="c"># kube-ops-view</span>
<span class="nv">$ </span>helm repo add geek-cookbook https://geek-cookbook.github.io/charts/
<span class="nv">$ </span>helm <span class="nb">install </span>kube-ops-view geek-cookbook/kube-ops-view <span class="nt">--version</span> 1.2.2 <span class="nt">--set</span> service.main.type<span class="o">=</span>NodePort,service.main.ports.http.nodePort<span class="o">=</span>30000 <span class="nt">--set</span> env.TZ<span class="o">=</span><span class="s2">"Asia/Seoul"</span> <span class="nt">--namespace</span> kube-system

<span class="c"># 설치 확인</span>
<span class="nv">$ </span>kubectl get deploy,pod,svc,ep <span class="nt">-n</span> kube-system <span class="nt">-l</span> app.kubernetes.io/instance<span class="o">=</span>kube-ops-view

<span class="c"># kube-ops-view 접속 URL 확인 (1.5 , 2 배율)</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"KUBE-OPS-VIEW URL = http://localhost:30000/#scale=1.5"</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"KUBE-OPS-VIEW URL = http://localhost:30000/#scale=2"</span>
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w2/20240907_kans_w2_9.png" alt="img.png" /></p>

<ul>
  <li>worker 노드에 진입 후 네임스페이스 격리를 확인해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># [터미널1] myk8s-worker bash 진입 후 실행 및 확인</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker bash
<span class="nt">----------------------------------</span>
<span class="nv">$ </span>systemctl list-unit-files | <span class="nb">grep</span> <span class="s1">'enabled         enabled'</span>
<span class="c"># =&gt; containerd.service                                                                    enabled         enabled</span>
<span class="c">#    kubelet.service                                                                       enabled         enabled</span>
<span class="c">#    ...</span>

<span class="c"># 확인 : kubelet에 --container-runtime-endpoint=unix:///run/containerd/containerd.sock</span>
<span class="nv">$ </span>pstree <span class="nt">-aln</span>
<span class="c"># =&gt; systemd</span>
<span class="c">#      |-systemd-journal</span>
<span class="c">#      |-containerd</span>
<span class="c">#      |   `-15*[{containerd}]</span>
<span class="c">#      |-kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///run/containerd/containerd.sock --node-ip=172.20.0.3 --node-labels= --pod-infra-container-image=registry.k8s.io/pause:3.10 --provider-id=kind://docker/myk8s/myk8s-worker --runtime-cgroups=/system.slice/containerd.service</span>
<span class="c">#      |   `-13*[{kubelet}]</span>
<span class="c">#      |-containerd-shim -namespace k8s.io -id 3368a087d8af3e241201257993e178d6a7d8ea23d3148cdf2ec5392f9db49832 -address /run/containerd/containerd.sock</span>
<span class="c">#      |   |-11*[{containerd-shim}]</span>
<span class="c">#      |   |-pause</span>
<span class="c">#      |   `-kindnetd</span>
<span class="c">#      |       `-11*[{kindnetd}]</span>
<span class="c">#      |-containerd-shim -namespace k8s.io -id e983e9fcff0162dec6128e014ae9092454fe0fd748ba237320aec17fa85fb17b -address /run/containerd/containerd.sock</span>
<span class="c">#      |   |-11*[{containerd-shim}]</span>
<span class="c">#      |   |-pause</span>
<span class="c">#      |   `-kube-proxy --config=/var/lib/kube-proxy/config.conf --hostname-override=myk8s-worker</span>
<span class="c">#      |       `-8*[{kube-proxy}]</span>
<span class="c">#      `-containerd-shim -namespace k8s.io -id 9cdabafac520e373ff0efdbbbe8b87bdfd7a0d02bd897863b609eefc4ae21b36 -address /run/containerd/containerd.sock</span>
<span class="c">#          |-12*[{containerd-shim}]</span>
<span class="c">#          |-pause</span>
<span class="c">#          `-python3 /usr/local/bin/python3 -m kube_ops_view</span>
<span class="c">#              `-2*[{python3}]</span>
          
<span class="c"># 확인 : 파드내에 pause 컨테이너와 kube_ops_view 컨테이너, 네임스페이스 정보</span>
<span class="nv">$ </span>pstree <span class="nt">-aclnpsS</span>
<span class="c"># =&gt; ...</span>
<span class="c">#      `-containerd-shim,1089 -namespace k8s.io -id 9cdabafac520e373ff0efdbbbe8b87bdfd7a0d02bd897863b609eefc4ae21b36 -address /run/containerd/contai</span>
<span class="c">#    nerd.sock</span>
<span class="c">#          |-{containerd-shim},1090</span>
<span class="c">#          ...</span>
<span class="c">#          |-pause,1110,ipc,mnt,net,pid,uts</span>
<span class="c">#          |-python3,1173,cgroup,ipc,mnt,net,pid,uts /usr/local/bin/python3 -m kube_ops_view</span>
<span class="c">#          ...</span>
<span class="c">#    ...</span>
      
<span class="c"># 네임스페이스 확인 : lsns - List system namespaces</span>
<span class="nv">$ </span>lsns <span class="nt">-p</span> 1
<span class="nv">$ </span>lsns <span class="nt">-p</span> <span class="nv">$$</span>
<span class="c"># =&gt;         NS TYPE   NPROCS PID USER COMMAND</span>
<span class="c">#    4026531834 time       15   1 root /sbin/init</span>
<span class="c">#    4026531837 user       15   1 root /sbin/init</span>
<span class="c">#    4026532329 mnt         9   1 root /sbin/init</span>
<span class="c">#    4026532330 uts        13   1 root /sbin/init</span>
<span class="c">#    4026532338 ipc         9   1 root /sbin/init</span>
<span class="c">#    4026532339 pid         9   1 root /sbin/init</span>
<span class="c">#    4026532341 net        13   1 root /sbin/init</span>
<span class="c">#    4026532417 cgroup     13   1 root /sbin/init</span>

<span class="c"># 파드의 pause 컨테이너는 노드의 NS와 다른 5개의 NS를 가짐 : mnt/pid 는 pasue 자신만 사용, net/uts/ipc는 app 컨테이너를 위해서 먼저 생성해둠</span>
<span class="nv">$ </span>lsns <span class="nt">-p</span> 1797  <span class="c"># kube_ops_view 파드의 pause 컨테이너</span>
<span class="c"># =&gt;         NS TYPE   NPROCS   PID USER  COMMAND</span>
<span class="c">#    4026531834 time       15     1 root  /sbin/init</span>
<span class="c">#    4026531837 user       15     1 root  /sbin/init</span>
<span class="c">#    4026532417 cgroup     13     1 root  /sbin/init </span>
<span class="c">#    4026532805 net         2  1110 65535 /pause      # Node NS와 다름  </span>
<span class="c">#    4026532883 mnt         1  1110 65535 /pause      # Node NS와 다름</span>
<span class="c">#    4026532884 uts         2  1110 65535 /pause      # Node NS와 다름</span>
<span class="c">#    4026532885 ipc         2  1110 65535 /pause      # Node NS와 다름</span>
<span class="c">#    4026532886 pid         1  1110 65535 /pause      # Node NS와 다름</span>

<span class="c"># app 컨테이너(kube_ops_view)는 호스트NS와 다른 6개의 NS를 가짐 : mnt/pid/cgroup 는 자신만 사용, net/uts/ipc는 pause 컨테이너가 생성한 것을 공유 사용함</span>
<span class="nv">$ </span>pgrep <span class="nt">-f</span> kube_ops_view
<span class="c"># =&gt; 1173</span>
<span class="nv">$ </span>lsns <span class="nt">-p</span> <span class="si">$(</span>pgrep <span class="nt">-f</span> kube_ops_view<span class="si">)</span>
<span class="c"># =&gt;         NS TYPE   NPROCS   PID USER  COMMAND</span>
<span class="c">#    4026531834 time       15     1 root  /sbin/init</span>
<span class="c">#    4026531837 user       15     1 root  /sbin/init</span>
<span class="c">#    4026532805 net         2  1110 65535 /pause      # pause 컨테이너와 공유</span>
<span class="c">#    4026532884 uts         2  1110 65535 /pause      # pause 컨테이너와 공유</span>
<span class="c">#    4026532885 ipc         2  1110 65535 /pause      # pause 컨테이너와 공유</span>
<span class="c">#    4026532887 mnt         1  1173 1000  /usr/bin/qemu-x86_64 /usr/local/bin/python3 -m kube_ops_view  # 자신만 사용</span>
<span class="c">#    4026532888 pid         1  1173 1000  /usr/bin/qemu-x86_64 /usr/local/bin/python3 -m kube_ops_view  # 자신만 사용</span>
<span class="c">#    4026532889 cgroup      1  1173 1000  /usr/bin/qemu-x86_64 /usr/local/bin/python3 -m kube_ops_view  # 자신만 사용</span>
</code></pre></div></div>

<ul>
  <li>위와 같이 파드 내부의 pause 컨테이너와 kube_ops_view 컨테이너는 net, uts, ipc 네임스페이스를 공유하고, mnt, pid, cgroup 네임스페이스는 각각의 컨테이너가 사용하는 것을 확인할 수 있습니다.</li>
  <li>이렇게 pause 컨테이너가 파드 내부의 컨테이너들이 공유하는 네임스페이스를 생성하고 유지하는 역할을 하는것을 확인 할 수 있었습니다.</li>
  <li>마지막으로 DIND를 위한 containerd.sock 과 cgroup2fs, sys, proc 등의 정보를 확인해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># containerd.sock 정보 확인 (docker.sock과 비슷한 역할)</span>
<span class="nv">$ </span><span class="nb">ls</span> <span class="nt">-l</span> /run/containerd/containerd.sock
<span class="c"># =&gt; srw-rw---- 1 root root 0 Sep  7 15:20 /run/containerd/containerd.sock</span>

<span class="c"># 특정 소켓 파일을 사용하는 프로세스 확인</span>
<span class="nv">$ </span>lsof /run/containerd/containerd.sock
<span class="c"># =&gt; COMMAND   PID USER   FD   TYPE             DEVICE SIZE/OFF   NODE NAME</span>
<span class="c">#    container 106 root    9u  unix 0x0000000000000000      0t0 726698 /run/containerd/containerd.sock type=STREAM (LISTEN)</span>
<span class="c">#    container 106 root   11u  unix 0x0000000000000000      0t0 734691 /run/containerd/containerd.sock type=STREAM (CONNECTED)</span>
<span class="c">#    container 106 root   12u  unix 0x0000000000000000      0t0 738482 /run/containerd/containerd.sock type=STREAM (CONNECTED)</span>
<span class="c">#    container 106 root   14u  unix 0x0000000000000000      0t0 736712 /run/containerd/containerd.sock type=STREAM (CONNECTED)</span>

<span class="c"># /sys 디렉터리 확인</span>
<span class="nv">$ </span>findmnt <span class="nt">-A</span>
<span class="c"># =&gt; TARGET                                                  SOURCE                 FSTYPE    OPTIONS</span>
<span class="c">#    /                                                       overlay                overlay   rw,relatime,lowerdir=/var/lib/docker/overlay2/l/5UMBJJ</span>
<span class="c">#    ...</span>
<span class="c">#    |-/sys                                                  sysfs                  sysfs     ro,nosuid,nodev,noexec,relatime</span>
<span class="c">#    | |-/sys/kernel/tracing                                 tracefs                tracefs   rw,nosuid,nodev,noexec,relatime</span>
<span class="c">#    | |-/sys/kernel/debug                                   debugfs                debugfs   rw,nosuid,nodev,noexec,relatime</span>
<span class="c">#    | |-/sys/fs/fuse/connections                            fusectl                fusectl   rw,nosuid,nodev,noexec,relatime</span>
<span class="c">#    | |-/sys/kernel/config                                  configfs               configfs  rw,nosuid,nodev,noexec,relatime</span>
<span class="c">#    | `-/sys/fs/cgroup                                      cgroup                 cgroup2   rw,nosuid,nodev,noexec,relatime</span>
<span class="c">#    ...</span>

<span class="c"># cgroup 정보 확인</span>
<span class="nv">$ </span>findmnt <span class="nt">-t</span> cgroup2
<span class="c"># =&gt; TARGET         SOURCE FSTYPE  OPTIONS</span>
<span class="c">#    /sys/fs/cgroup cgroup cgroup2 rw,nosuid,nodev,noexec,relatime</span>
<span class="nv">$ </span><span class="nb">grep </span>cgroup /proc/filesystems
<span class="c"># =&gt; nodev	cgroup</span>
<span class="c">#    nodev	cgroup2</span>
<span class="nv">$ </span><span class="nb">stat</span> <span class="nt">-fc</span> %T /sys/fs/cgroup/
<span class="c"># =&gt; cgroup2fs</span>

<span class="nv">$ </span><span class="nb">exit</span>
<span class="nt">----------------------------------</span>
</code></pre></div></div>

<ul>
  <li>신규파드를 배포하고 확인해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># [터미널2] kubectl 명령 실행 및 확인</span>

<span class="c"># Pod 생성 : YAML 파일에 컨테이너가 사용할 포트(TCP 80)을 설정</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: myweb
spec:
  containers:
  - image: nginx:alpine
    name: myweb-container
    ports:
    - containerPort: 80
      protocol: TCP
  terminationGracePeriodSeconds: 0
</span><span class="no">EOF

</span><span class="c"># Pod 정보 확인 : pause 컨테이너 정보가 보이는지 확인</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-o</span> wide
<span class="c"># =&gt; NAME    READY   STATUS    RESTARTS   AGE   IP           NODE           NOMINATED NODE   READINESS GATES</span>
<span class="c">#    myweb   1/1     Running   0          15s   10.244.1.3   myk8s-worker   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="nv">$ </span>kubectl describe pod myweb | <span class="nb">grep</span> <span class="nt">-i</span> pause
<span class="nv">$ </span>kubectl get pod myweb <span class="nt">-o</span> json | <span class="nb">grep</span> <span class="nt">-i</span> pause

<span class="nt">---</span>

<span class="c"># [터미널1] myk8s-worker bash 진입 후 실행 및 확인</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker bash
<span class="nt">----------------------------------</span>
<span class="nv">$ </span>crictl ps
<span class="nv">$ </span>pstree <span class="nt">-aln</span>
<span class="nv">$ </span>pstree <span class="nt">-aclnpsS</span> <span class="c"># 파드내에 pause 컨테이너와 app 컨테이너, 네임스페이스 정보</span>
<span class="c"># =&gt;   `-containerd-shim,1673 -namespace k8s.io -id 482ba93ecf76d46938d60e58f363e63e681d8a779439f270ff9c8417ad25a641 -address /run/containerd/containerd.sock</span>
<span class="c">#          |-{containerd-shim},1674</span>
<span class="c">#          ...</span>
<span class="c">#          |-pause,1693,ipc,mnt,net,pid,uts</span>
<span class="c">#          |-nginx,1754,cgroup,ipc,mnt,net,pid,uts</span>

<span class="c"># 네임스페이스 확인 : lsns - List system namespaces</span>
<span class="nv">$ </span>lsns <span class="nt">-p</span> 1
<span class="nv">$ </span>lsns <span class="nt">-p</span> <span class="nv">$$</span>
<span class="c"># =&gt;         NS TYPE   NPROCS PID USER COMMAND</span>
<span class="c">#    4026531834 time       25   1 root /sbin/init</span>
<span class="c">#    4026531837 user       25   1 root /sbin/init</span>
<span class="c">#    4026532329 mnt        10   1 root /sbin/init</span>
<span class="c">#    4026532330 uts        14   1 root /sbin/init</span>
<span class="c">#    4026532338 ipc        10   1 root /sbin/init</span>
<span class="c">#    4026532339 pid        10   1 root /sbin/init</span>
<span class="c">#    4026532341 net        14   1 root /sbin/init</span>
<span class="c">#    4026532417 cgroup     15   1 root /sbin/init</span>
<span class="nv">$ </span>lsns <span class="nt">-p</span> 1693 
<span class="c"># =&gt;         NS TYPE   NPROCS   PID USER  COMMAND</span>
<span class="c">#    4026531834 time       25     1 root  /sbin/init</span>
<span class="c">#    4026531837 user       25     1 root  /sbin/init</span>
<span class="c">#    4026532417 cgroup     15     1 root  /sbin/init</span>
<span class="c">#    4026532891 net         9  1693 65535 /pause</span>
<span class="c">#    4026532969 mnt         1  1693 65535 /pause</span>
<span class="c">#    4026532970 uts         9  1693 65535 /pause</span>
<span class="c">#    4026532971 ipc         9  1693 65535 /pause</span>
<span class="c">#    4026532972 pid         1  1693 65535 /pause</span>
<span class="nv">$ </span>lsns <span class="nt">-p</span> <span class="si">$(</span>pgrep <span class="nt">-n</span> nginx<span class="si">)</span> <span class="c"># app 컨테이너(nginx)</span>
<span class="c"># =&gt;         NS TYPE   NPROCS   PID USER  COMMAND</span>
<span class="c">#    4026531834 time       25     1 root  /sbin/init</span>
<span class="c">#    4026531837 user       25     1 root  /sbin/init</span>
<span class="c">#    4026532891 net         9  1693 65535 /pause</span>
<span class="c">#    4026532970 uts         9  1693 65535 /pause</span>
<span class="c">#    4026532971 ipc         9  1693 65535 /pause</span>
<span class="c">#    4026532973 mnt         8  1754 root  nginx: master process nginx -g daemon off;</span>
<span class="c">#    4026532974 pid         8  1754 root  nginx: master process nginx -g daemon off;</span>
<span class="c">#    4026532975 cgroup      8  1754 root  nginx: master process nginx -g daemon off;</span>
<span class="nt">----------------------------------</span>
<span class="c"># [터미널2] kubectl 명령 실행 및 확인</span>
<span class="nv">$ </span>kubectl delete pod myweb
</code></pre></div></div>

<ul>
  <li>위와 같이 <code class="language-plaintext highlighter-rouge">kubectl</code>과 같은 high-level에서는 <code class="language-plaintext highlighter-rouge">pause</code> 컨테이너가 보이지 않지만, <code class="language-plaintext highlighter-rouge">ps</code>나 <code class="language-plaintext highlighter-rouge">lsns</code>와 같은 OS에 접근 가능한 명령으로는 
<code class="language-plaintext highlighter-rouge">pause</code> 컨테이너가 확인이 되었습니다.</li>
  <li>이는 pause 컨테이너는 공기와 같이 항상 파드에 존재하기 때문에 굳이 보여줘서 화면만 복잡하게 할 뿐 보여줄 필요가 없다 판단한것 같습니다.</li>
  <li>PAUSE Container가 ns를 공유하는 특성을 이용하여,
애플리케이션과 런타임 의존성만 포함하는 최소화된 이미지기반 컨테이너인 Distroless Container를 대상으로
Ephemeral Containers와 같은 형태로 디버깅에 활용할 수 있습니다. <a href="https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/">Link</a></li>
</ul>

<h2 id="cni-container-network-interface">CNI (Container Network Interface)</h2>

<ul>
  <li>쿠버네티스는 CNI(Container Network Interface)를 사용하여 네트워크를 관리합니다. 
CNI는 컨테이너 런타임과 네트워크 플러그인을 연결하는 인터페이스로 네트워크 인터페이스를 생성하고, IP 주소를 할당하며,
네트워크 정책을 적용하는 역할을 수행합니다.</li>
  <li>CNI의 주요 기능
    <ul>
      <li><strong>네트워크 인터페이스 생성</strong>: CNI 플러그인은 컨테이너에 네트워크 인터페이스(예: 가상 이더넷 장치)를 생성하고 컨테이너 네임스페이스에 이를 연결합니다.</li>
      <li><strong>IP 주소 할당</strong>: 플러그인은 네트워크 인터페이스에 IP 주소를 할당하고, 필요에 따라 IP 주소 관리를 처리합니다.</li>
      <li><strong>네트워크 정책 적용</strong>: 네트워크 정책을 통해 트래픽을 제어할 수 있으며, CNI 플러그인은 이를 구현하는 데 사용됩니다.</li>
      <li><strong>다양한 네트워크 모드 지원</strong>: 다양한 네트워크 토폴로지와 요구 사항을 지원하기 위해 여러 네트워크 모드(예: 브리지, VLAN, 오버레이 네트워크 등)를 지원합니다.</li>
    </ul>
  </li>
  <li>CNI의 작동 방식
    <ul>
      <li>CNI는 기본적으로 플러그인 기반 구조를 따릅니다. 오케스트레이션 도구는 특정 이벤트가 발생할 때 CNI 플러그인을 호출하여 필요한 네트워크 설정을 수행합니다. 각 CNI 플러그인은 JSON 형식의 구성 파일로 정의되며, 이를 통해 플러그인의 동작을 제어할 수 있습니다.</li>
    </ul>
  </li>
  <li>CNI 플러그인의 종류 : CNI 플러그인은 다양한 종류가 있으며, 각기 다른 네트워킹 요구 사항을 충족시킵니다. 대표적인 CNI 플러그인은 다음과 같습니다.
    <ul>
      <li><strong>Flannel</strong>: 간단하고 사용하기 쉬운 오버레이 네트워크 플러그인입니다.</li>
      <li><strong>Calico</strong>: 네트워크 정책과 보안을 강조하는 플러그인으로, 네트워크 격리 및 정책 적용에 강점이 있습니다.</li>
      <li><strong>Weave</strong>: 자동 메쉬 네트워크와 서비스 디스커버리를 제공하는 플러그인입니다.</li>
      <li><strong>Cilium</strong>: 고성능 BPF 기반 네트워킹 및 보안을 제공하는 플러그인입니다.</li>
      <li><strong>Multus</strong>: 여러 CNI 플러그인을 사용하여 컨테이너에 여러 네트워크 인터페이스를 지원하는 플러그인입니다.</li>
    </ul>
  </li>
</ul>

<h3 id="4가지-요구사항과-4가지-문제">4가지 요구사항과 4가지 문제</h3>

<p>쿠버네티스의 네트워크 모델은 4가지 요구사항을 만족해아하며 4가지 문제를 해결해야 합니다.</p>

<ul>
  <li>4가지 요구사항
    <ol>
      <li>파드와 파드 간 통신 시 NAT(Network Address Translation) 없이 통신이 가능해야 합니다.</li>
      <li>노드의 에이전트(예) kubelet, 시스템 데몬)는 Pod와 통신이 가능해야 합니다.</li>
      <li>호스트 네트워크를 사용하는 파드는 NAT 없이 파드와 통신이 가능해야 합니다.</li>
      <li>서비스 클러스터 IP 대역과 파드가 사용하는 IP 대역은 중복되지 않아야 합니다.</li>
    </ol>
  </li>
  <li>해결해야 하는 문제
    <ol>
      <li>파드 내 컨테이너는 Loopback을 통한 통신을 할 수 있도록 해야 합니다.</li>
      <li>파드 간 통신을 할 수 있어야 합니다.</li>
      <li>클러스터 내부에서 Service를 통한 통신을 할 수 있어야 합니다.</li>
      <li>클러스터 외부에서 Service를 통한 통신을 할 수 있어야 합니다.</li>
    </ol>
  </li>
</ul>

<p>위와 같은 요구사항과 문제를 해결하고 원활한 네트워크 통신을 위해 CNI(Container Network Interface)를 정의했습니다.
CNI 플러그인들은 이러한 요구사항들을 기반으로 만들어졌습니다.</p>

<p><img src="/assets/2024/kans-3th/w2/20240907_kans_w2_10.png" alt="img.png" class="image-center" />
<em class="image-caption">CNI 플러그인 동작 (출처: 추가예정)</em></p>

<p>Kubelet을 통해 파드가 신규 생성될 때 네트워크 관련 설정 추가 필요합니다. 
CNI 플러그인은 전달되는 설정 정의서를 보고 실제 파드가 통신하기 위한 네트워크 설정들을 실행하게 됩니다.
또한 CNI 플러그인은 IPAM(IP Address Management), 
즉 IP 할당 관리를 수행해야 하며, 파드 간 통신을 위한 라우팅 설정을 처리해야 합니다.</p>

<h2 id="flannel">Flannel</h2>

<ul>
  <li>Flannel은 쿠버네티스의 네트워크 요구사항을 충족하는 가장 간단하고 사용하기 쉬운 오버레이 네트워크 플러그인입니다.</li>
  <li>Flannel은 가상 네트워크를 생성하여 파드 간 통신을 가능하게 하며, VXLAN, UDP, Host-GW 등의 백엔드를 지원합니다.
하지만 VXLAN 사용이 권장됩니다.</li>
  <li>VXLAN은 Virtual eXtensible Local Area Network의 약자로, 물리적인 네트워크 환경에서  논리적인 가상의 네트워크 환경을 만들어 주는 것으로, UDP 8472  포트를 통해 노드 간 터널링 기법으로 통신하는 기술입니다. </li>
</ul>

<p><img src="/assets/2024/kans-3th/w2/20240907_kans_w2_11.png" alt="img.png" class="image-center" />
<em class="image-caption">Flannel 구조 (출처: 추가예정)</em></p>

<ul>
  <li>위의 그림과 같이 파드의 eth0 네트워크 인터페이스는 호스트 네임스페이스의 veth 인터페이스와 연결되고,
veth는 cni0와 연결됩니다.</li>
  <li>이를 통해 같은 노드에서 통신시 cni0 브릿지를 통해서 통신하고, 다른 노드와 통신시 VXLAN을 통해 통신합니다.</li>
  <li>VXLAN으로 가는 과정은 cni0 브릿지를 통해 flannel.1 인터페이스로 가고, flannel.1은 호스트의 eth0을 통해 다른 노드에 전송을 합니다.
이때 <strong>flannel.1은 VTEP(Vxlan Tunnel End Point)</strong>라고 하며 패킷을 감싸서 목표 node의 IP로 전송하면, 목표 node에서 감싼 패킷을 풀어서 
해당 파드의 IP로 다시 보내는 역할을 수행합니다.</li>
  <li>각 노드마다 파드에 할당할 수 있는 IP 네트워크 대역이 있고, flannel을 통하여 ETCD나 Kubernetes API에 전달되어, 모든 노드는 해당 정보를 자신의 라우팅 테이블에 업데이트합니다.
이를 통해 각각 다른 노드의 파드끼리도 내부 IP 주소를 통해 통신이 가능하게 됩니다.</li>
</ul>

<h3 id="kind-와-flannel-설치">Kind 와 Flannel 설치</h3>

<ul>
  <li>Kind 클러스터에 Flannel을 설치해보겠습니다. kind는 기본 CNI로 kindnet을 사용하는데 실습을 위해 kindnet을 끄고 클러스터를 구축하겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">&gt; kind-cni.yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  labels:
    mynode: control-plane
  extraPortMappings:
  - containerPort: 30000
    hostPort: 30000
  - containerPort: 30001
    hostPort: 30001
  - containerPort: 30002
    hostPort: 30002
  kubeadmConfigPatches:
  - |
    kind: ClusterConfiguration
    controllerManager:
      extraArgs:
        bind-address: 0.0.0.0
    etcd:
      local:
        extraArgs:
          listen-metrics-urls: http://0.0.0.0:2381
    scheduler:
      extraArgs:
        bind-address: 0.0.0.0
  - |
    kind: KubeProxyConfiguration
    metricsBindAddress: 0.0.0.0
- role: worker
  labels:
    mynode: worker
- role: worker
  labels:
    mynode: worker2
networking:
  disableDefaultCNI: true
</span><span class="no">EOF
</span><span class="nv">$ </span>kind create cluster <span class="nt">--config</span> kind-cni.yaml <span class="nt">--name</span> myk8s <span class="nt">--image</span> kindest/node:v1.30.4

<span class="c"># 배포 확인</span>
<span class="nv">$ </span>kind get clusters
<span class="nv">$ </span>kind get nodes <span class="nt">--name</span> myk8s
<span class="nv">$ </span>kubectl cluster-info

<span class="c"># 네트워크 확인</span>
<span class="nv">$ </span>kubectl cluster-info dump | <span class="nb">grep</span> <span class="nt">-m</span> 2 <span class="nt">-E</span> <span class="s2">"cluster-cidr|service-cluster-ip-range"</span>

<span class="c"># 노드 확인 : CRI</span>
<span class="nv">$ </span>kubectl get nodes <span class="nt">-o</span> wide

<span class="c"># 노드 라벨 확인</span>
<span class="nv">$ </span>kubectl get nodes myk8s-control-plane <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.metadata.labels<span class="o">}</span> | jq
...
<span class="s2">"mynode"</span>: <span class="s2">"control-plane"</span>,
...

<span class="nv">$ </span>kubectl get nodes myk8s-worker <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.metadata.labels<span class="o">}</span> | jq
<span class="nv">$ </span>kubectl get nodes myk8s-worker2 <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">={</span>.metadata.labels<span class="o">}</span> | jq

<span class="c"># 컨테이너 확인 : 컨테이너 갯수, 컨테이너 이름 확인</span>
<span class="nv">$ </span>docker ps
<span class="nv">$ </span>docker port myk8s-control-plane
<span class="nv">$ </span>docker port myk8s-worker
<span class="nv">$ </span>docker port myk8s-worker2

<span class="c"># 컨테이너 내부 정보 확인</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane ip <span class="nt">-br</span> <span class="nt">-c</span> <span class="nt">-4</span> addr
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker  ip <span class="nt">-br</span> <span class="nt">-c</span> <span class="nt">-4</span> addr
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker2  ip <span class="nt">-br</span> <span class="nt">-c</span> <span class="nt">-4</span> addr

<span class="c">#</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane sh <span class="nt">-c</span> <span class="s1">'apt update &amp;&amp; apt install tree jq psmisc lsof wget bridge-utils tcpdump iputils-ping htop git nano -y'</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker  sh <span class="nt">-c</span> <span class="s1">'apt update &amp;&amp; apt install tree jq psmisc lsof wget bridge-utils tcpdump iputils-ping -y'</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker2 sh <span class="nt">-c</span> <span class="s1">'apt update &amp;&amp; apt install tree jq psmisc lsof wget bridge-utils tcpdump iputils-ping -y'</span>
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w2/20240907_kans_w2_12.png" alt="img.png" /></p>

<ul>
  <li>다음 명령을 통해 bridge 실행파일을 생성해서 각 인스턴스마다 배포해야합니다. 먼저 다음과 같이 bridge 파일을 생헝 후 로컬에 복사하겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane bash
<span class="nt">---------------------------------------</span>
<span class="c"># 빌드환경 구성</span>
<span class="nv">$ </span>apt update <span class="o">&amp;&amp;</span> apt <span class="nb">install </span>golang git <span class="nt">-y</span>
<span class="nv">$ </span>git clone https://github.com/containernetworking/plugins
<span class="nv">$ </span><span class="nb">cd </span>plugins
<span class="nv">$ </span><span class="nb">chmod</span> +x build_linux.sh

<span class="c"># 빌드</span>
<span class="nv">$ </span>./build_linux.sh

<span class="c"># 파일 권한 확인 755</span>
<span class="nv">$ </span><span class="nb">ls</span> <span class="nt">-l</span> bin
<span class="c"># =&gt; -rwxr-xr-x 1 root root  4471145 Sep  7 16:53 bridge</span>
<span class="c">#    ...</span>

<span class="nv">$ </span><span class="nb">exit</span>
<span class="nt">---------------------------------------</span>

<span class="c"># 자신의 PC에 복사 : -a 권한 보존하여 복사(755)</span>
<span class="nv">$ </span>docker <span class="nb">cp</span> <span class="nt">-a</span> myk8s-control-plane:/plugins/bin/bridge <span class="nb">.</span>
<span class="nv">$ </span><span class="nb">ls</span> <span class="nt">-l</span> bridge
<span class="c"># =&gt; .rwxr-xr-x root staff 4.3 MB Sun Sep  0 00:00:00 2024 bridge</span>
</code></pre></div></div>

<ul>
  <li>이제 Flannel을 설치하겠습니다. 현재 기본 CNI 인 kindnet 없이 설치했기 때문에 node가 not ready 상태여서 스케쥴링이 안 되고 있습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>watch <span class="nt">-d</span> kubectl get pod <span class="nt">-A</span> <span class="nt">-owide</span>

<span class="c">#</span>
<span class="nv">$ </span>kubectl describe pod <span class="nt">-n</span> kube-system <span class="nt">-l</span> k8s-app<span class="o">=</span>kube-dns | <span class="nb">grep </span>Events: <span class="nt">-A</span> 6
<span class="c"># =&gt; Events:</span>
<span class="c">#      Type     Reason            Age                  From               Message</span>
<span class="c">#      ----     ------            ----                 ----               -------</span>
<span class="c">#      Warning  FailedScheduling  19m                  default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.</span>

<span class="c"># 기본 CNI 인 kindnet 없이 설치했기 때문에 node가 not ready 상태여서 스케쥴링이 안 되고 있습니다.</span>

<span class="c"># Flannel cni 설치</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml
<span class="c"># =&gt; namespace/kube-flannel created</span>
<span class="c">#    clusterrole.rbac.authorization.k8s.io/flannel created</span>
<span class="c">#    clusterrolebinding.rbac.authorization.k8s.io/flannel created</span>
<span class="c">#    serviceaccount/flannel created</span>
<span class="c">#    configmap/kube-flannel-cfg created</span>
<span class="c">#    daemonset.apps/kube-flannel-ds created</span>

<span class="c"># namespace 에 pod-security.kubernetes.io/enforce=privileged Label 확인 </span>
<span class="nv">$ </span>kubectl get ns <span class="nt">--show-labels</span>
<span class="c"># =&gt; ...</span>
<span class="c">#    kube-flannel         Active   60s   k8s-app=flannel,kubernetes.io/metadata.name=kube-flannel,pod-security.kubernetes.io/enforce=privileged</span>
<span class="nv">$ </span>kubectl get ds,pod,cm <span class="nt">-n</span> kube-flannel <span class="nt">-owide</span>
<span class="c"># =&gt; NAME                             DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE    CONTAINERS     IMAGES                              SELECTOR</span>
<span class="c">#    daemonset.apps/kube-flannel-ds   3         3         3       3            3           &amp;lt;none&amp;gt;          100s   kube-flannel   docker.io/flannel/flannel:v0.25.6   app=flannel</span>
<span class="c">#    </span>
<span class="c">#    NAME                        READY   STATUS    RESTARTS   AGE    IP           NODE                  NOMINATED NODE   READINESS GATES</span>
<span class="c">#    pod/kube-flannel-ds-2l9p6   1/1     Running   0          100s   172.20.0.2   myk8s-worker2         &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    pod/kube-flannel-ds-67ftf   1/1     Running   0          100s   172.20.0.3   myk8s-worker          &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    pod/kube-flannel-ds-87wv8   1/1     Running   0          100s   172.20.0.4   myk8s-control-plane   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    </span>
<span class="c">#    NAME                         DATA   AGE</span>
<span class="c">#    configmap/kube-flannel-cfg   2      100s</span>
<span class="c">#    configmap/kube-root-ca.crt   1      100s</span>

<span class="c"># kube-flannel-ds가 daemonset으로 노드마다 실행중인것을 확인할 수 있습니다.</span>

<span class="nv">$ </span>kubectl describe cm <span class="nt">-n</span> kube-flannel kube-flannel-cfg

<span class="nv">$ </span>kubectl describe ds <span class="nt">-n</span> kube-flannel kube-flannel-ds

<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> ds/kube-flannel-ds <span class="nt">-n</span> kube-flannel <span class="nt">-c</span> kube-flannel <span class="nt">--</span> <span class="nb">ls</span> <span class="nt">-l</span> /etc/kube-flannel


<span class="c"># failed to find plugin "bridge" in path [/opt/cni/bin]</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-A</span> <span class="nt">-owide</span>
<span class="nv">$ </span>kubectl describe pod <span class="nt">-n</span> kube-system <span class="nt">-l</span> k8s-app<span class="o">=</span>kube-dns
<span class="c"># =&gt; Warning  FailedCreatePodSandBox  2m16s                   kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox "b0023dea7d730b58cfea0163318641ff1d000d368f8ad3b552c53040b371388c": plugin type="flannel" failed (add): failed to delegate add: failed to find plugin "bridge" in path [/opt/cni/bin]</span>
<span class="c">#    Warning  FailedCreatePodSandBox  2m15s                   kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox "0454cbf9ae3c434f163865f6cd261ef2fa298a2de107471600195ebffa0b44cc": plugin type="flannel" failed (add): failed to delegate add: failed to find plugin "bridge" in path [/opt/cni/bin]</span>
</code></pre></div></div>

<ul>
  <li>현재 /opt/cni/bin/bridge 파일이 없어서 오류가 발생하고 있습니다. 이를 해결하기 위해 bridge 파일을 복사하겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># bridge 파일 복사</span>
<span class="nv">$ </span>docker <span class="nb">cp </span>bridge myk8s-control-plane:/opt/cni/bin/bridge
<span class="nv">$ </span>docker <span class="nb">cp </span>bridge myk8s-worker:/opt/cni/bin/bridge
<span class="nv">$ </span>docker <span class="nb">cp </span>bridge myk8s-worker2:/opt/cni/bin/bridge

<span class="c"># 권한 부여</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane  <span class="nb">chmod </span>755 /opt/cni/bin/bridge
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker         <span class="nb">chmod </span>755 /opt/cni/bin/bridge
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker2        <span class="nb">chmod </span>755 /opt/cni/bin/bridge
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane  <span class="nb">chown </span>root:root /opt/cni/bin/bridge
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker         <span class="nb">chown </span>root:root /opt/cni/bin/bridge
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker2        <span class="nb">chown </span>root:root /opt/cni/bin/bridge

<span class="c"># bridge 파일이 잘 복사되었는지 확인합니다.</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane  <span class="nb">ls</span> <span class="nt">-l</span> /opt/cni/bin/
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker  <span class="nb">ls</span> <span class="nt">-l</span> /opt/cni/bin/
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker2 <span class="nb">ls</span> <span class="nt">-l</span> /opt/cni/bin/
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>myk8s-control-plane myk8s-worker myk8s-worker2<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> <span class="nv">$i</span> <span class="nb">ls</span> /opt/cni/bin/<span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done
</span>bridge	flannel  host-local  loopback  portmap	ptp

<span class="c">#</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-A</span> <span class="nt">-owide</span>
<span class="c"># =&gt; NAMESPACE            NAME                                          READY   STATUS    RESTARTS   AGE     IP           NODE                  NOMINATED NODE   READINESS GATES</span>
<span class="c">#    ...</span>
<span class="c">#    kube-system          coredns-7db6d8ff4d-hjmjf                      1/1     Running   0          29m     10.244.1.4   myk8s-worker2         &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    kube-system          coredns-7db6d8ff4d-lfmzj                      1/1     Running   0          29m     10.244.1.3   myk8s-worker2         &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;</span>
<span class="c">#    ...</span>
</code></pre></div></div>

<h3 id="flannel-설치-확인">Flannel 설치 확인</h3>

<ul>
  <li>Flannel 설치 후 coredns가 정상적으로 배포되었습니다. 이제 Flannel이 정상적으로 설치되었는지 확인해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#</span>
<span class="nv">$ </span>kubectl get ds,pod,cm <span class="nt">-n</span> kube-flannel <span class="nt">-owide</span>
<span class="c"># =&gt; &lt;span style="font-weight:bold;"&gt;NAME                             DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE   CONTAINERS     IMAGES                              SELECTOR&lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:gray;"&gt;daemonset.apps/kube-flannel-ds&lt;/span&gt;   &lt;span style="color:teal;"&gt;3&lt;/span&gt;         &lt;span style="color:gray;"&gt;3&lt;/span&gt;         &lt;span style="color:teal;"&gt;3&lt;/span&gt;       &lt;span style="color:gray;"&gt;3&lt;/span&gt;            &lt;span style="color:teal;"&gt;3&lt;/span&gt;           &lt;span style="color:gray;"&gt;&amp;lt;none&amp;gt;&lt;/span&gt;          &lt;span style="color:teal;"&gt;13m&lt;/span&gt;   &lt;span style="color:gray;"&gt;kube-flannel&lt;/span&gt;   &lt;span style="color:teal;"&gt;docker.io/flannel/flannel:v0.25.6&lt;/span&gt;   &lt;span style="color:gray;"&gt;app=flannel&lt;/span&gt;</span>
<span class="c">#    </span>
<span class="c">#    &lt;span style="font-weight:bold;"&gt;NAME                        READY   STATUS    RESTARTS   AGE   IP           NODE                  NOMINATED NODE   READINESS GATES&lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:gray;"&gt;pod/kube-flannel-ds-2l9p6&lt;/span&gt;   &lt;span style="color:teal;"&gt;1/1&lt;/span&gt;     &lt;span style="color:green;"&gt;Running&lt;/span&gt;   &lt;span style="color:teal;"&gt;0&lt;/span&gt;          &lt;span style="color:gray;"&gt;13m&lt;/span&gt;   &lt;span style="color:teal;"&gt;172.20.0.2&lt;/span&gt;   &lt;span style="color:gray;"&gt;myk8s-worker2&lt;/span&gt;         &lt;span style="color:teal;"&gt;&amp;lt;none&amp;gt;&lt;/span&gt;           &lt;span style="color:gray;"&gt;&amp;lt;none&amp;gt;&lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:gray;"&gt;pod/kube-flannel-ds-67ftf&lt;/span&gt;   &lt;span style="color:teal;"&gt;1/1&lt;/span&gt;     &lt;span style="color:green;"&gt;Running&lt;/span&gt;   &lt;span style="color:teal;"&gt;0&lt;/span&gt;          &lt;span style="color:gray;"&gt;13m&lt;/span&gt;   &lt;span style="color:teal;"&gt;172.20.0.3&lt;/span&gt;   &lt;span style="color:gray;"&gt;myk8s-worker&lt;/span&gt;          &lt;span style="color:teal;"&gt;&amp;lt;none&amp;gt;&lt;/span&gt;           &lt;span style="color:gray;"&gt;&amp;lt;none&amp;gt;&lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:gray;"&gt;pod/kube-flannel-ds-87wv8&lt;/span&gt;   &lt;span style="color:teal;"&gt;1/1&lt;/span&gt;     &lt;span style="color:green;"&gt;Running&lt;/span&gt;   &lt;span style="color:teal;"&gt;0&lt;/span&gt;          &lt;span style="color:gray;"&gt;13m&lt;/span&gt;   &lt;span style="color:teal;"&gt;172.20.0.4&lt;/span&gt;   &lt;span style="color:gray;"&gt;myk8s-control-plane&lt;/span&gt;   &lt;span style="color:teal;"&gt;&amp;lt;none&amp;gt;&lt;/span&gt;           &lt;span style="color:gray;"&gt;&amp;lt;none&amp;gt;&lt;/span&gt;</span>
<span class="c">#    </span>
<span class="c">#    &lt;span style="font-weight:bold;"&gt;NAME                         DATA   AGE&lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:gray;"&gt;configmap/kube-flannel-cfg&lt;/span&gt;   &lt;span style="color:teal;"&gt;2&lt;/span&gt;      &lt;span style="color:gray;"&gt;13m&lt;/span&gt;</span>
<span class="c">#    &lt;span style="color:gray;"&gt;configmap/kube-root-ca.crt&lt;/span&gt;   &lt;span style="color:teal;"&gt;1&lt;/span&gt;      &lt;span style="color:gray;"&gt;13m&lt;/span&gt;</span>

<span class="nv">$ </span>kubectl describe cm <span class="nt">-n</span> kube-flannel kube-flannel-cfg

<span class="c"># iptables 정보 확인</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>filter nat mangle raw <span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; IPTables Type : </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane  iptables <span class="nt">-t</span> <span class="nv">$i</span> <span class="nt">-S</span> <span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>filter nat mangle raw <span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; IPTables Type : </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker  iptables <span class="nt">-t</span> <span class="nv">$i</span> <span class="nt">-S</span> <span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>filter nat mangle raw <span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; IPTables Type : </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker2 iptables <span class="nt">-t</span> <span class="nv">$i</span> <span class="nt">-S</span> <span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>

<span class="c"># flannel 정보 확인 : 대역, MTU</span>
<span class="nv">$ </span><span class="k">for </span>i <span class="k">in </span>myk8s-control-plane myk8s-worker myk8s-worker2<span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"&gt;&gt; node </span><span class="nv">$i</span><span class="s2"> &lt;&lt;"</span><span class="p">;</span> docker <span class="nb">exec</span> <span class="nt">-it</span> <span class="nv">$i</span> <span class="nb">cat</span> /run/flannel/subnet.env <span class="p">;</span> <span class="nb">echo</span><span class="p">;</span> <span class="k">done</span>
<span class="c"># =&gt; &gt;&gt; node myk8s-control-plane &lt;&lt;</span>
<span class="c">#    FLANNEL_NETWORK=10.244.0.0/16</span>
<span class="c">#    FLANNEL_SUBNET=10.244.0.1/24</span>
<span class="c">#    FLANNEL_MTU=1450</span>
<span class="c">#    FLANNEL_IPMASQ=true</span>
<span class="c">#    ...</span>

<span class="c"># 노드마다 할당된 dedicated subnet (podCIDR) 확인</span>
<span class="nv">$ </span>kubectl get nodes <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.items[*].spec.podCIDR}'</span> <span class="p">;</span><span class="nb">echo</span>
<span class="c"># =&gt; 10.244.0.0/24 10.244.2.0/24 10.244.1.0/24</span>

<span class="c"># 노드 정보 중 flannel 관련 정보 확인 : VXLAN 모드 정보와, VTEP 정보(노드 IP, VtepMac) 를 확인</span>
<span class="nv">$ </span>kubectl describe node | <span class="nb">grep</span> <span class="nt">-A3</span> Annotations
<span class="c"># =&gt; Annotations:        flannel.alpha.coreos.com/backend-data: {&amp;quot;VNI&amp;quot;:1,&amp;quot;VtepMAC&amp;quot;:&amp;quot;6e:0e:72:1e:72:1d&amp;quot;}</span>
<span class="c">#                        flannel.alpha.coreos.com/backend-type: vxlan</span>
<span class="c">#                        flannel.alpha.coreos.com/kube-subnet-manager: true</span>
<span class="c">#                        flannel.alpha.coreos.com/public-ip: 172.20.0.4</span>
<span class="c">#    --</span>
<span class="c">#    Annotations:        flannel.alpha.coreos.com/backend-data: {&amp;quot;VNI&amp;quot;:1,&amp;quot;VtepMAC&amp;quot;:&amp;quot;ca:de:a3:b8:65:d8&amp;quot;}</span>
<span class="c">#                        flannel.alpha.coreos.com/backend-type: vxlan</span>
<span class="c">#                        flannel.alpha.coreos.com/kube-subnet-manager: true</span>
<span class="c">#                        flannel.alpha.coreos.com/public-ip: 172.20.0.3</span>
<span class="c">#    --</span>
<span class="c">#    Annotations:        flannel.alpha.coreos.com/backend-data: {&amp;quot;VNI&amp;quot;:1,&amp;quot;VtepMAC&amp;quot;:&amp;quot;1e:1e:b1:15:9e:d3&amp;quot;}</span>
<span class="c">#                        flannel.alpha.coreos.com/backend-type: vxlan</span>
<span class="c">#                        flannel.alpha.coreos.com/kube-subnet-manager: true</span>
<span class="c">#                        flannel.alpha.coreos.com/public-ip: 172.20.0.2</span>

<span class="c"># 각 노드(?) 마다 bash 진입 후 아래 기본 정보 확인 : 먼저 worker 부터 bash 진입 후 확인하자</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker        bash
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker2       bash
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-control-plane bash
<span class="nt">----------------------------------------</span>
<span class="c"># 호스트 네트워크 NS와 flannel, kube-proxy 컨테이너의 네트워크 NS 비교 =&gt; 모두 동일한 NS를 가집니다. </span>
<span class="nv">$ </span>lsns <span class="nt">-p</span> 1
<span class="c"># =&gt;         NS TYPE   NPROCS PID USER COMMAND</span>
<span class="c">#    ...</span>
<span class="c">#    4026532344 net        12   1 root /sbin/init</span>
<span class="nv">$ </span>lsns <span class="nt">-p</span> <span class="si">$(</span>pgrep flanneld<span class="si">)</span>
<span class="c"># =&gt;         NS TYPE   NPROCS   PID USER  COMMAND</span>
<span class="c">#    ...</span>
<span class="c">#    4026532344 net        12     1 root  /sbin/init</span>
<span class="nv">$ </span>lsns <span class="nt">-p</span> <span class="si">$(</span>pgrep kube-proxy<span class="si">)</span>
<span class="c"># =&gt;         NS TYPE   NPROCS   PID USER  COMMAND</span>
<span class="c">#    ...</span>
<span class="c">#    4026532344 net        12     1 root  /sbin/init</span>

<span class="c"># 기본 네트워크 정보 확인</span>
<span class="nv">$ </span>ip <span class="nt">-c</span> <span class="nt">-br</span> addr
<span class="c"># =&gt; lo               UNKNOWN        127.0.0.1/8 ::1/128</span>
<span class="c">#    flannel.1        UNKNOWN        10.244.2.0/32</span>
<span class="c">#    eth0@if37        UP             172.20.0.3/16 fc00:f853:ccd:e793::3/64 fe80::42:acff:fe14:3/64</span>
<span class="nv">$ </span>ip <span class="nt">-c</span> <span class="nb">link</span> | <span class="nb">grep</span> <span class="nt">-E</span> <span class="s1">'flannel|cni|veth'</span> <span class="nt">-A1</span>
<span class="c"># =&gt; 4: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default</span>
<span class="c">#       link/ether ca:de:a3:b8:65:d8 brd ff:ff:ff:ff:ff:ff</span>
<span class="nv">$ </span>ip <span class="nt">-c</span> addr
<span class="nv">$ </span>ip <span class="nt">-c</span> <span class="nt">-d</span> addr show cni0     <span class="c"># 네트워크 네임스페이스 격리 파드가 1개 이상 배치 시 확인됨</span>
<span class="c"># =&gt; (공백)</span>

<span class="c"># 현재 네트워크 네임스페이스에 격리된 파드가 없어서 cni0 인터페이스가 없습니다.</span>

<span class="nv">$ </span>ip <span class="nt">-c</span> <span class="nt">-d</span> addr show flannel.1
<span class="c"># =&gt; 4: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN group default</span>
<span class="c">#        link/ether ca:de:a3:b8:65:d8 brd ff:ff:ff:ff:ff:ff promiscuity 0 minmtu 68 maxmtu 65535</span>
<span class="c">#        vxlan id 1 local 172.20.0.3 dev eth0 srcport 0 0 dstport 8472 nolearning ttl auto ageing 300 udpcsum noudp6zerocsumtx noudp6zerocsumrx numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535</span>
<span class="c">#        inet 10.244.2.0/32 scope global flannel.1</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
    
<span class="nv">$ </span>brctl show
<span class="c"># =&gt; (공백)</span>

<span class="c"># 라우팅 정보 확인 : 다른 노드의 파드 대역(podCIDR)의 라우팅 정보가 업데이트되어 있음을 확인		</span>
<span class="nv">$ </span>ip <span class="nt">-c</span> route
<span class="c"># =&gt; default via 172.20.0.1 dev eth0</span>
<span class="c">#    10.244.0.0/24 via 10.244.0.0 dev flannel.1 onlink</span>
<span class="c">#    10.244.1.0/24 via 10.244.1.0 dev flannel.1 onlink</span>
<span class="c">#    172.20.0.0/16 dev eth0 proto kernel scope link src 172.20.0.3</span>

<span class="c"># flannel.1 인터페이스를 통한 ARP 테이블 정보 확인 : 다른 노드의 flannel.1 IP와 MAC 정보를 확인</span>
<span class="nv">$ </span>ip <span class="nt">-c</span> neigh show dev flannel.1
<span class="c"># =&gt; 10.244.1.0 lladdr 1e:1e:b1:15:9e:d3 PERMANENT</span>
<span class="c">#    10.244.0.0 lladdr 6e:0e:72:1e:72:1d PERMANENT</span>

<span class="c"># 브리지 fdb 정보에서 해당 MAC 주소와 통신 시 각 노드의 enp0s8 </span>
<span class="nv">$ </span>bridge fdb show dev flannel.1
<span class="c"># =&gt; 6e:0e:72:1e:72:1d dst 172.20.0.4 self permanent</span>
<span class="c">#    1e:1e:b1:15:9e:d3 dst 172.20.0.2 self permanent</span>

<span class="c"># 다른 노드의 flannel.1 인터페이스로 ping 통신 : VXLAN 오버레이를 통해서 통신</span>
<span class="nv">$ </span>ping <span class="nt">-c</span> 1 10.244.0.0
<span class="c"># =&gt; ...</span>
<span class="c">#    1 packets transmitted, 1 received, 0% packet loss, time 0ms</span>
<span class="nv">$ </span>ping <span class="nt">-c</span> 1 10.244.1.0
<span class="c"># =&gt; ...</span>
<span class="c">#    1 packets transmitted, 1 received, 0% packet loss, time 0ms</span>
<span class="nv">$ </span>ping <span class="nt">-c</span> 1 10.244.2.0
<span class="c"># =&gt; ...</span>
<span class="c">#    1 packets transmitted, 1 received, 0% packet loss, time 0ms</span>

<span class="c"># 다른 노드와 VXLAN을 통해서 잘 통신 됩니다.</span>

<span class="c"># iptables 필터 테이블 정보 확인 : 파드의 10.244.0.0/16 대역 끼리는 모든 노드에서 전달이 가능</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> filter <span class="nt">-S</span> | <span class="nb">grep </span>10.244.0.0
<span class="c"># =&gt; -A FLANNEL-FWD -s 10.244.0.0/16 -m comment --comment "flanneld forward" -j ACCEPT</span>
<span class="c">#    -A FLANNEL-FWD -d 10.244.0.0/16 -m comment --comment "flanneld forward" -j ACCEPT</span>

<span class="c"># iptables NAT 테이블 정보 확인 : 10.244.0.0/16 대역 끼리 통신은 마스커레이딩 없이 통신을 하며,</span>
<span class="c"># 10.244.0.0/16 대역에서 동일 대역(10.244.0.0/16)과 멀티캐스트 대역(224.0.0.0/4) 를 제외한 나머지 (외부) 통신 시에는 마스커레이딩을 수행</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span> | <span class="nb">grep</span> <span class="s1">'flanneld masq'</span> | <span class="nb">grep</span> <span class="nt">-v</span> <span class="s1">'! -s'</span>
<span class="c"># =&gt; -A POSTROUTING -m comment --comment "flanneld masq" -j FLANNEL-POSTRTG</span>
<span class="c">#    -A FLANNEL-POSTRTG -m mark --mark 0x4000/0x4000 -m comment --comment "flanneld masq" -j RETURN</span>
<span class="c">#    -A FLANNEL-POSTRTG -s 10.244.2.0/24 -d 10.244.0.0/16 -m comment --comment "flanneld masq" -j RETURN</span>
<span class="c">#    -A FLANNEL-POSTRTG -s 10.244.0.0/16 -d 10.244.2.0/24 -m comment --comment "flanneld masq" -j RETURN</span>
<span class="c">#    -A FLANNEL-POSTRTG -s 10.244.0.0/16 ! -d 224.0.0.0/4 -m comment --comment "flanneld masq" -j MASQUERADE --random-fully</span>

<span class="nt">----------------------------------------</span>
</code></pre></div></div>

<ul>
  <li>파드 2개를 생성해서 CNI 네트워크 브리지의 정보를  확인해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># [터미널1,2] 워커 노드1,2 - 모니터링</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker  bash
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker2 bash
<span class="nt">-----------------------------</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s2">"ip link | egrep 'cni|veth' ;echo; brctl show cni0"</span>
<span class="nt">-----------------------------</span>

<span class="c"># [터미널3] cat &amp; here document 명령 조합으로 즉석(?) 리소스 생성</span>
<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl create -f -
apiVersion: v1
kind: Pod
metadata:
  name: pod-1
  labels:
    app: pod
spec:
  nodeSelector:
    kubernetes.io/hostname: myk8s-worker
  containers:
  - name: netshoot-pod
    image: nicolaka/netshoot
    command: ["tail"]
    args: ["-f", "/dev/null"]
  terminationGracePeriodSeconds: 0
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-2
  labels:
    app: pod
spec:
  nodeSelector:
    kubernetes.io/hostname: myk8s-worker2
  containers:
  - name: netshoot-pod
    image: nicolaka/netshoot
    command: ["tail"]
    args: ["-f", "/dev/null"]
  terminationGracePeriodSeconds: 0
</span><span class="no">EOF

</span><span class="c"># 파드 확인 : IP 확인</span>
<span class="nv">$ </span>kubectl get pod <span class="nt">-o</span> wide
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w2/20240907_kans_w2_13.png" alt="img.png" /></p>

<ul>
  <li>CNI 가 없었는데 파드 패포후 CNI 네트워크 브리지가 생성되는 것을 확인할 수 있습니다. 정보를 확인해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker  bash
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker2 bash
<span class="nt">-----------------------------</span>
<span class="c"># 브리지 정보 확인</span>
<span class="nv">$ </span>brctl show cni0

<span class="c"># 브리지 연결 링크(veth) 확인</span>
<span class="nv">$ </span>bridge <span class="nb">link</span>

<span class="c"># 브리지 VLAN 정보 확인</span>
<span class="nv">$ </span>bridge vlan

<span class="c"># cbr(custom bridge) 정보 : kubenet CNI의 bridge - 링크</span>
<span class="nv">$ </span>tree /var/lib/cni/networks/cbr0

<span class="c"># 네트워크 관련 정보들 확인</span>
<span class="nv">$ </span>ip <span class="nt">-c</span> addr | <span class="nb">grep </span>veth <span class="nt">-A3</span>
<span class="nt">-----------------------------</span>
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w2/20240907_kans_w2_14.png" alt="img.png" /></p>

<h3 id="통신-흐름-이해">통신 흐름 이해</h3>

<ul>
  <li>Flannel 기반으로 네트워크를 구축할 때는 다음과 같이 세가지의 통신 시나리오가 생길 수 있습니다.
    <ol>
      <li>동일 노드에서 파드간 통신하는 경우
  <img src="/assets/2024/kans-3th/w2/20240907_kans_w2_15.png" alt="img.png" class="image-center" /></li>
      <li>파드에서 외부와 통신하는 경우
  <img src="/assets/2024/kans-3th/w2/20240907_kans_w2_16.png" alt="img_1.png" class="image-center" /></li>
      <li>서로다른 노드에서 파드간 통신하는 경우
  <img src="/assets/2024/kans-3th/w2/20240907_kans_w2_17.png" alt="img_2.png" class="image-center" /></li>
    </ol>
  </li>
</ul>

<p>파드 간 통신, 서로다른 노드의 파드간 통신, 외부 통신 여부에 대해 살펴보고, 통신이 일어나는 상황의 패킷을 캡처하면서 Flannel network에 대해 이해해 보았습니다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> pod-1 <span class="nt">--</span> zsh
<span class="nt">-----------------------------</span>
<span class="nv">$ </span>ip <span class="nt">-c</span> addr show eth0

<span class="c"># GW IP는 어떤 인터페이스인가? =&gt; cni0</span>
<span class="nv">$ </span>ip <span class="nt">-c</span> route
<span class="nv">$ </span>route <span class="nt">-n</span>
<span class="nv">$ </span>ping <span class="nt">-c</span> 1 &lt;GW IP&gt;
<span class="nv">$ </span>ping <span class="nt">-c</span> 1 &lt;pod-2 IP&gt;  <span class="c"># 다른 노드에 배포된 파드 통신 확인</span>
<span class="nv">$ </span>ping <span class="nt">-c</span> 1 8.8.8.8     <span class="c"># 외부 인터넷 IP   접속 확인</span>
<span class="nv">$ </span>curl <span class="nt">-s</span> wttr.in/Seoul <span class="c"># 외부 인터넷 도메인 접속 확인</span>
<span class="nv">$ </span>ip <span class="nt">-c</span> neigh
<span class="nv">$ </span><span class="nb">exit</span>
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w2/20240907_kans_w2_18.png" alt="img.png" /></p>

<ul>
  <li>서로다른 노드간 파드의 통신, 외부와의 통신 등이 모두 잘 통신 되는것을 확인할 수 있습니다.</li>
  <li>이번에는 각 노드의 cni0에서 패킷 캡쳐를 진행해 보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># [터미널1,2] 워커 노드1,2</span>
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker  bash
<span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> myk8s-worker2 bash
<span class="nt">-----------------------------</span>
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> cni0 <span class="nt">-nn</span> icmp
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> flannel.1 <span class="nt">-nn</span> icmp
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> eth0 <span class="nt">-nn</span> icmp
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> eth0 <span class="nt">-nn</span> udp port 8472 <span class="nt">-w</span> /root/vxlan.pcap 
<span class="c"># CTRL+C 취소 후 확인 : ls -l /root/vxlan.pcap</span>

<span class="nv">$ </span>conntrack <span class="nt">-L</span> | <span class="nb">grep</span> <span class="nt">-i</span> icmp
<span class="nt">-----------------------------</span>

<span class="c"># [터미널3]</span>
<span class="nv">$ </span>docker <span class="nb">cp </span>myk8s-worker:/root/vxlan.pcap <span class="nb">.</span>
<span class="nv">$ </span>wireshark vxlan.pcap
</code></pre></div></div>

<ul>
  <li>Pod-1 =&gt; Pod-2 (cni0 관점)
    <ul>
      <li>브릿지를 통해 각각 오가는 패킷이 잘 보입니다.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/2024/kans-3th/w2/20240907_kans_w2_19.png" alt="img.png" /></p>

<ul>
  <li>Pod-1 =&gt; 외부 (cni0 관점)
    <ul>
      <li>같은 노드에서는 패킷이 외부로 나갔다 오는것이 잘 보입니다.</li>
      <li>하지만 다른 노드 (worker2)에서는 외부와 오가는 패킷이 보이지 않습니다.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/2024/kans-3th/w2/20240907_kans_w2_20.png" alt="img.png" /></p>

<ul>
  <li>Pod-1 =&gt; Pod-2 (flannel.1 관점)
    <ul>
      <li>flannel.1을 통해 각각 오가는 패킷이 잘 보입니다.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/2024/kans-3th/w2/20240907_kans_w2_21.png" alt="img.png" /></p>

<ul>
  <li>Pod-1 -&gt; 외부 (flannel.1 관점)
    <ul>
      <li>앞선 그림에서 보았듯 외부와 통신시 VTEP인 flannel.1을 거치지 않고 cni0에서 호스트의 eth0로 바로 나가기 때문에 패킷이 캡쳐되지 않습니다.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/2024/kans-3th/w2/20240907_kans_w2_22.png" alt="img.png" /></p>

<ul>
  <li>Pod 1 -&gt; Pod 2 (eth0 관점)
    <ul>
      <li>eth0에서 봤을때는 캡쳐가 되지 않습니다. 이유는 flannel.1을 통해 tcp 패킷이 캡슐화 되어 udp 8472로 eth0를 통해 전달되기 때문에
<code class="language-plaintext highlighter-rouge">-nn icmp</code> 옵션으로 icmp 패킷 (ping)만 캡쳐할때는 보이지 않습니다.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/2024/kans-3th/w2/20240907_kans_w2_23.png" alt="img.png" /></p>

<ul>
  <li>Pod 1 -&gt; Pod 2 (eth0 관점, udp port 8472 덤프)
    <ul>
      <li>udp 패킷을 캡쳐하여 wireshark로 확인해보겠습니다.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/2024/kans-3th/w2/20240907_kans_w2_25.png" alt="img.png" /></p>

<ul>
  <li>udp 8472 포트를 통해 icmp (ping)이 오가는 것을 확인할 수 있습니다. (8472는 VXLAN 포트가 아니기 때문에 옵션에서 VXLAN을 지정해야 보입니다.)</li>
</ul>

<p><img src="/assets/2024/kans-3th/w2/20240907_kans_w2_26.png" alt="img_1.png" /></p>

<ul>
  <li>Pod 1 -&gt; 8.8.8.8(외부) (eth0 관점)
    <ul>
      <li>eth0에서 외부와 통신하는 패킷이 같은 노드에서는 보이고, 다른 노드에서는 안 보입니다.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/2024/kans-3th/w2/20240907_kans_w2_24.png" alt="img.png" /></p>

<hr />

<h2 id="마치며">마치며</h2>

<p>이번주에도 많은 내용들을 스터디해보았습니다. 
KIND를 알게되어서 참 좋았던것 같습니다. 
기존에 사내 교육을 위해서 kubernetes를 설치하려면 갖은 어려움이 있었는데
docker만 있으면 간단하게 설치가 가능하다는 것이 참 좋은것 같습니다.</p>

<p>막연하게 알고 있었던 노드간의 파드의 통신에 대해서 알게되었고,
쿠버네티스의 운영중 네트워크 장애에 대해 1주차 스터디와 이번 스터디를 통해
자신감이 조금 생겼습니다.
남은 스터디도 열심히해서 쿠버네티스의 네트워크에 대해 조금 아는 사람이 되어보겠습니다.</p>]]></content><author><name></name></author><category term="kans" /><category term="kubernetes," /><category term="network," /><category term="linux" /><summary type="html"><![CDATA[지난주에 이어 이번주에는 쿠버네티스에대해 간략하게 알아보고 KIND, PAUSE 컨테이너와 Flannel CNI에 대해 알아보겠습니다.]]></summary></entry><entry><title type="html">[KANS 3기] 컨테이너 격리</title><link href="https://sweetlittlebird.github.io/posts/2024-08-27-KANS-Study-Week1/" rel="alternate" type="text/html" title="[KANS 3기] 컨테이너 격리" /><published>2024-08-27T22:50:18+09:00</published><updated>2024-08-27T22:50:18+09:00</updated><id>https://sweetlittlebird.github.io/posts/KANS%20Study%20-%20Week1</id><content type="html" xml:base="https://sweetlittlebird.github.io/posts/2024-08-27-KANS-Study-Week1/"><![CDATA[<h2 id="들어가며">들어가며</h2>

<p>지난 테라폼 스터디에 이어 이번 주 부터 KANS 스터디를 시작하게 되었습니다!
KANS는 <strong>K</strong>ubernetes <strong>A</strong>dvanced <strong>N</strong>etworking <strong>S</strong>tudy의 줄임말로 쿠버네티스 네트워킹에 대한 심도있게 공부하는 스터디입니다.
이번 스터디도 과제할 걱정도 :sweat: 되지만 재미있을것 같아 기대됩니다. :smile:</p>

<p>첫 주 스터디도 컨테이너 격리와 리눅스 네트워크에 대해 많은것을 배웠고 이 자리에 정리해보려고 합니다. 
이번 스터디도 다들 완주하기를 기도하며 스터디 정리를 시작해 보겠습니다.</p>

<h2 id="도커-소개">도커 소개</h2>

<h3 id="도커란-무엇인가">도커란 무엇인가?</h3>

<ul>
  <li>
    <p>도커(Docker)는 컨테이너(Container)라고 불리는 가상실행 환경을 제공하고, 
그 가상환경에서 유용한 어플리케이션을 실행할 수 있게 해주는 오픈소스 플랫폼입니다.</p>
  </li>
  <li>
    <p>컨테이너라는 이름의 기원
컨테이너라는 이름은 배에 화물을 실을때 사용하는 그 컨테이너에서 왔습니다.
<img src="/assets/2024/kans-3th/w1/20240831_kans_w1_1.png" alt="컨테이너선" />
과거에 컨테이너가 발명되기 이전에는 짐의 부피와 모양이 제각각이라서, 화물을 적재하기도 어렵고 
파도가 쳐서 배가 흔들릴때 짐이 이리 저리 움직여서 파손되는 경우가 많았습니다.</p>

    <p>이 문제를 해결하기 위해 Malcom McLean이라는 분이 발명한것이 직육면체의 바로 컨테이너입니다.<br />
<img src="/assets/2024/kans-3th/w1/20240831_kans_w1_2.png" alt="Shipping Container" />
직육면체이기 때문에 적재가 쉽고, 파도가 치더라도 안정적으로 화물을 운반할 수 있었습니다. 
또한 크고 작은 물건도 컨테이너 안에 넣어서 운반할 수 있어서 화물의 종류에 상관없이 효율적으로 운반할 수 있었습니다.</p>

    <p>이 개념을 컴퓨팅에 도입한것이 컨테이너입니다. 
기존에는 각 리눅스 버전마다, glibc냐 musl이냐, debian 기반이냐 redhat 기반이냐 등등 
프로그램을 배포할때 환경을 맞춰야 하는것이 많았습니다. 그 뿐만아니라 각종 라이브러리들도 설치해야 하고
심지어 프로그램 마다 필요한 라이브러리 버전이 다를때도 있었습니다.</p>

    <p>이러한 문제를 해결하기위해 도커라는 컨테이너를 이용한 가상화 기술이 등장하게 되었습니다.
도커 컨테이너 이미지에는 프로그램 실행에 필요한 모든것이 포함되어 있기 때문에
마치 컨테이너에 화물을 싣듯이 프로그램을 배포할 수 있게 되었습니다. 
도커의 로고가 컨테이너를 싣고 있는 배를 형상화한것도 이러한 의미에서 나온것입니다.</p>

    <p><img src="/assets/2024/kans-3th/w1/20240831_kans_w1_3.png" alt="Docker Logo" class="image-center" /></p>
  </li>
  <li>
    <p>컨테이너 이외에도 가상 머신(Virtual Machine)이라는 기술이 있습니다. 
가상 머신은 하이퍼바이저(Hypervisor)를 이용하여 호스트 OS 위에 게스트 OS를 올리는 방식으로 가상화를 구현합니다.
가상 머신은 게스트 OS를 올리기 때문에 무겁습니다. 
반면 컨테이너는 호스트 OS의 커널을 공유하기 때문에 가볍습니다.
<img src="/assets/2024/kans-3th/w1/20240831_kans_w1_4.png" alt="img.png" /></p>
  </li>
</ul>

<h3 id="컨테이너와-가상-머신">컨테이너와 가상 머신</h3>

<p><img src="/assets/2024/kans-3th/w1/20240831_kans_w1_5.png" alt="가상머신과 컨테이너 비교" /></p>

<ul>
  <li>가상머신은 호스트 OS 위에 하이퍼바이저를 두고 하드웨어 일부(또는 전부)를 가상화하고, 그 위에 게스트 OS를 올립니다. 즉, <strong>하드웨어 레벨의 가상화</strong>를 지원합니다.</li>
  <li>컨테이너는 하드웨어 가상화와 게스트 OS 없이, 호스트의 리눅스 커널을 공유하여 바로 프로세스를 실행합니다. 단, 각종 라이브러리와 사용자 환경(User Land)는 컨테이너 단위로 패키징되어 <strong>OS 레벨의 가상화</strong>를 지원한다 할 수 있습니다.</li>
  <li>따라서 컨테이너는 가상머신보다 가볍고 빠르며, 낮은 격리(Weak Isolation) 수준을 가집니다.</li>
  <li>가상머신은 게스트 OS를 올리기 때문에 무겁고 느리지만, 높은 격리(Strong Isolation) 수준을 가집니다.
    <ul>
      <li>낮은 격리 수준을 보완하기 위해 리눅스의 pivot-root, namespace, cgroup 등의 기능들을 활용함으로써 프로세스 단위의 격리 환경과 리소스 제어를 제공합니다.
<img src="/assets/2024/kans-3th/w1/20240831_kans_w1_6.png" alt="img.png" class="image-center" /></li>
    </ul>
  </li>
</ul>

<h3 id="도커-아키텍쳐">도커 아키텍쳐</h3>

<p><img src="/assets/2024/kans-3th/w1/20240831_kans_w1_7.png" alt="도커 아키텍쳐" />
<a href="https://docs.docker.com/get-started/overview/#docker-architecture">https://docs.docker.com/get-started/overview/#docker-architecture</a></p>

<hr />

<h2 id="도커-기본-사용">도커 기본 사용</h2>

<h3 id="도커-설치-및-확인">도커 설치 및 확인</h3>

<ul>
  <li>도커 설치
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 방법1. debian 계열 리눅스에서 패키지 매니저로 설치</span>
<span class="nv">$ </span><span class="nb">sudo </span>apt-get update
<span class="nv">$ </span><span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> docker.io
  
<span class="c"># 방법2. 공식 사이트에서 설치</span>
<span class="nv">$ </span><span class="nb">sudo</span> <span class="nt">-i</span>
<span class="nv">$ </span>curl <span class="nt">-fsSL</span> https://get.docker.com | sh
</code></pre></div>    </div>
  </li>
  <li>기본정보 확인
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 도커 정보 확인 : Client 와 Server , Storage Driver(overlay2), Cgroup Version(2), Default Runtime(runc)</span>
<span class="nv">$ </span><span class="nb">sudo </span>docker info
<span class="c">#    Client:</span>
<span class="c">#     Context:    default</span>
<span class="c">#     Debug Mode: false</span>
<span class="c">#    </span>
<span class="c">#    Server:</span>
<span class="c">#     Containers: 0</span>
<span class="c">#     ...</span>
<span class="c">#     Server Version: 20.10.25+dfsg1</span>
<span class="c">#     Storage Driver: overlay2</span>
<span class="c">#     ...</span>
<span class="c">#     Cgroup Driver: systemd</span>
<span class="c">#     Cgroup Version: 2</span>
<span class="c">#     ...</span>
<span class="c">#     containerd version: 1.6.24~ds1-2</span>
<span class="c">#     runc version: 1.1.12+ds1-5</span>
<span class="c">#     ...</span>
  
<span class="nv">$ </span><span class="nb">sudo </span>docker version
<span class="c"># =&gt; Client:</span>
<span class="c">#     Version:           20.10.25+dfsg1</span>
<span class="c">#     API version:       1.41</span>
<span class="c">#     Go version:        go1.22.3</span>
<span class="c">#     Git commit:        b82b9f3</span>
<span class="c">#     Built:             Tue May  7 10:33:18 2024</span>
<span class="c">#     OS/Arch:           linux/amd64</span>
<span class="c">#     Context:           default</span>
<span class="c">#     Experimental:      true</span>
<span class="c">#    </span>
<span class="c">#    Server:</span>
<span class="c">#     Engine:</span>
<span class="c">#      Version:          20.10.25+dfsg1</span>
<span class="c">#      API version:      1.41 (minimum version 1.12)</span>
<span class="c">#      Go version:       go1.22.3</span>
<span class="c">#      Git commit:       5df983c</span>
<span class="c">#      Built:            Tue May  7 10:33:18 2024</span>
<span class="c">#      OS/Arch:          linux/amd64</span>
<span class="c">#      Experimental:     false</span>
<span class="c">#    ...  </span>
  
<span class="c"># 도커 서비스 상태 확인</span>
<span class="nv">$ </span><span class="nb">sudo </span>systemctl status docker <span class="nt">-l</span> <span class="nt">--no-pager</span>
  
<span class="c"># 모든 서비스의 상태 표시</span>
<span class="nv">$ </span>systemctl list-units <span class="nt">--type</span><span class="o">=</span>service
  
<span class="c"># 도커 루트 디렉터리 확인 : Docker Root Dir(/var/lib/docker)</span>
<span class="nv">$ </span><span class="nb">sudo </span>tree <span class="nt">-L</span> 3 /var/lib/docker
<span class="c"># =&gt; /var/lib/docker</span>
<span class="c">#    |-- buildkit</span>
<span class="c">#    |   ...</span>
<span class="c">#    |-- containers</span>
<span class="c">#    |-- image</span>
<span class="c">#    |   `-- overlay2</span>
<span class="c">#    |       |-- distribution</span>
<span class="c">#    |       |-- imagedb</span>
<span class="c">#    |       |-- layerdb</span>
<span class="c">#    |       `-- repositories.json</span>
<span class="c">#    |-- network</span>
<span class="c">#    |   `-- files</span>
<span class="c">#    |       `-- local-kv.db</span>
<span class="c">#    |-- overlay2</span>
<span class="c">#    |   ...</span>
<span class="c">#    `-- volumes</span>
<span class="c">#        |-- backingFsBlockDev</span>
<span class="c">#        `-- metadata.db</span>
<span class="c">#    </span>
<span class="c">#    24 directories, 8 files</span>
</code></pre></div>    </div>
  </li>
  <li>네트워크 정보 확인
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 프로세스 확인 - 셸변수</span>
<span class="nv">$ </span>ps <span class="nt">-ef</span>      <span class="c"># 프로세스 목록 보기</span>
<span class="nv">$ </span>pstree <span class="nt">-p</span>   <span class="c"># 프로세스 트리로 보기</span>
  
<span class="nv">$ </span><span class="nb">df</span> <span class="nt">-hT</span>    <span class="c"># 디스크 사용량 확인</span>

<span class="c"># 네트워크 정보 확인. 도커에서 사용하는 docker0 네트워크가 추가되어있고 현재 DOWN 상태입니다.</span>
<span class="c"># 컨테이너가 있으면 UP 상태로 변경됩니다.  </span>
<span class="nv">$ </span>ip <span class="nt">-br</span> <span class="nt">-c</span> addr
<span class="c"># =&gt; &lt;span style="color:teal;"&gt;lo               &lt;/span&gt;UNKNOWN        &lt;span style="color:purple;"&gt;127.0.0.1&lt;/span&gt;/8 &lt;span style="color:blue;"&gt;::1&lt;/span&gt;/128 </span>
<span class="c">#    &lt;span style="color:teal;"&gt;eth0             &lt;/span&gt;&lt;span style="color:green;"&gt;UP             &lt;/span&gt;&lt;span style="color:purple;"&gt;10.10.10.109&lt;/span&gt;/24 &lt;span style="color:purple;"&gt;10.10.10.51&lt;/span&gt;/24 &lt;span style="color:blue;"&gt;fe80::a70d:8639:be6:671e&lt;/span&gt;/64</span>
<span class="c">#    &lt;span style="color:teal;"&gt;docker0          &lt;/span&gt;&lt;span style="color:red;"&gt;DOWN           &lt;/span&gt;&lt;span style="color:purple;"&gt;172.17.0.1&lt;/span&gt;/16 &lt;span style="color:blue;"&gt;fe80::42:57ff:fe56:997c&lt;/span&gt;/64</span>
<span class="nv">$ </span>ip <span class="nt">-c</span> addr
<span class="nv">$ </span>ip <span class="nt">-c</span> <span class="nb">link</span>
<span class="nv">$ </span>ip <span class="nt">-br</span> <span class="nt">-c</span> <span class="nb">link</span>
<span class="nv">$ </span>ip <span class="nt">-c</span> route
  
<span class="c"># 이더넷 브릿지 정보 확인</span>
<span class="nv">$ </span>brctl show
<span class="c"># =&gt; bridge name	bridge id		STP enabled	interfaces</span>
<span class="c">#    docker0		8000.02425756997c	no</span>
  
<span class="c"># iptables 정책 확인</span>
<span class="c"># FORWARD 정책이 DROP으로 설정되어 있고, </span>
<span class="c"># docker0에서 docker0 혹은 외부로 전달되는 패킷은 허용되어 있습니다.</span>
<span class="nv">$ </span><span class="nb">sudo </span>iptables <span class="nt">-t</span> filter <span class="nt">-S</span>
<span class="c"># =&gt; -P INPUT ACCEPT</span>
<span class="c">#    &lt;span style="color: red;"&gt;-P FORWARD DROP&lt;/span&gt;</span>
<span class="c">#    -P OUTPUT ACCEPT</span>
<span class="c">#    -N DOCKER</span>
<span class="c">#    -N DOCKER-ISOLATION-STAGE-1</span>
<span class="c">#    -N DOCKER-ISOLATION-STAGE-2</span>
<span class="c">#    -N DOCKER-USER</span>
<span class="c">#    -A FORWARD -j DOCKER-USER</span>
<span class="c">#    -A FORWARD -j DOCKER-ISOLATION-STAGE-1</span>
<span class="c">#    -A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT</span>
<span class="c">#    -A FORWARD -o docker0 -j DOCKER</span>
<span class="c">#    &lt;span style="color: red;"&gt;-A FORWARD -i docker0 ! -o docker0 -j ACCEPT&lt;/span&gt;</span>
<span class="c">#    &lt;span style="color: red;"&gt;-A FORWARD -i docker0 -o docker0 -j ACCEPT&lt;/span&gt;</span>
<span class="c">#    -A DOCKER-ISOLATION-STAGE-1 -i docker0 ! -o docker0 -j DOCKER-ISOLATION-STAGE-2</span>
<span class="c">#    -A DOCKER-ISOLATION-STAGE-1 -j RETURN</span>
<span class="c">#    -A DOCKER-ISOLATION-STAGE-2 -o docker0 -j DROP</span>
<span class="c">#    -A DOCKER-ISOLATION-STAGE-2 -j RETURN</span>
<span class="c">#    -A DOCKER-USER -j RETURN</span>

<span class="c"># NAT POSTROUTING에 172.17.0.0/16에서 외부로 전달시 MASQUERADE (SNAT) 정책이 설정되어 있습니다.</span>
<span class="nv">$ </span><span class="nb">sudo </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span>
<span class="c"># =&gt; -P PREROUTING ACCEPT</span>
<span class="c">#    -P INPUT ACCEPT</span>
<span class="c">#    -P OUTPUT ACCEPT</span>
<span class="c">#    -P POSTROUTING ACCEPT</span>
<span class="c">#    -N DOCKER</span>
<span class="c">#    -A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER</span>
<span class="c">#    -A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER</span>
<span class="c">#    &lt;span style="color: red;"&gt;-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE&lt;/span&gt;</span>
<span class="c">#    -A DOCKER -i docker0 -j RETURN</span>
</code></pre></div>    </div>
  </li>
</ul>

<h3 id="도커를-비-root-유저로-관리하기">도커를 비 root 유저로 관리하기</h3>

<p>도커는 기본적으로 root로 관리할 수 있습니다. 
root가 아닌 유저로 docker 명령을 실행하면 다음과 같은 에러가 발생합니다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">whoami</span>
<span class="c"># =&gt; kali</span>

<span class="nv">$ </span>docker info
<span class="c"># =&gt; ...</span>
<span class="c">#    Server:</span>
<span class="c">#    ERROR: permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get &amp;quot;http://%2Fvar%2Frun%2Fdocker.sock/v1.24/info&amp;quot;: dial unix /var/run/docker.sock: connect: permission denied</span>
</code></pre></div></div>

<p>하지만, 다음의 방법 처럼 현재 사용자를 docker 그룹에 추가하면, root가 아닌 일반 유저로도 관리할 수 있습니다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">whoami</span> 
<span class="c"># =&gt; kali</span>

<span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$USER</span>
<span class="c"># =&gt; kali</span>

<span class="c"># 도커 그룹 추가</span>
<span class="nv">$ </span><span class="nb">sudo </span>usermod <span class="nt">-aG</span> docker <span class="nv">$USER</span>

<span class="c"># 그룹 확인</span>
<span class="nv">$ </span><span class="nb">groups</span> 
<span class="c"># =&gt; adm ... kaboxer</span>

<span class="c"># 로그아웃</span>
<span class="nb">exit</span> 

<span class="c"># ssh 재접속 후 확인</span>
<span class="nv">$ </span><span class="nb">groups</span> 
<span class="c"># =&gt; adm ... kaboxer docker</span>

<span class="nv">$ </span>docker info
<span class="c"># =&gt; Client:</span>
<span class="c">#     Context:    default</span>
<span class="c">#     Debug Mode: false</span>
<span class="c">#    </span>
<span class="c">#    Server:</span>
<span class="c">#     Containers: 0</span>
<span class="c">#     ...</span>
<span class="c">#     Cgroup Version: 2</span>
<span class="c">#     ...</span>
<span class="c">#     Default Runtime: runc</span>
<span class="c">#     Init Binary: docker-init</span>
<span class="c">#     containerd version: 1.6.24~ds1-2</span>
<span class="c">#     runc version: 1.1.12+ds1-5</span>
<span class="c">#     ...    </span>

<span class="c"># 컨테이너 확인</span>
<span class="nv">$ </span>docker run <span class="nt">--rm</span> hello-world
<span class="c"># =&gt; Hello from Docker!</span>
<span class="c">#    This message shows that your installation appears to be working correctly.</span>
<span class="c">#    ...</span>

<span class="c"># 실행중인 도커 컨테이너 확인</span>
<span class="nv">$ </span>docker ps
<span class="c"># 전체 도커 컨테이너 확인</span>
<span class="nv">$ </span>docker ps <span class="nt">-a</span>
<span class="c"># 이미지 목록 확인</span>
<span class="nv">$ </span>docker images
<span class="c"># =&gt; REPOSITORY    TAG       IMAGE ID       CREATED         SIZE</span>
<span class="c">#    hello-world   latest    d2c94e258dcb   16 months ago   13.3kB</span>

<span class="c"># 도커 컨테이너 삭제</span>
<span class="nv">$ </span>docker ps <span class="nt">-aq</span>
<span class="nv">$ </span>docker <span class="nb">rm</span> <span class="nt">-f</span> <span class="si">$(</span>docker ps <span class="nt">-aq</span><span class="si">)</span>
<span class="nv">$ </span>docker ps <span class="nt">-a</span>
</code></pre></div></div>

<h3 id="컨테이너가-host의-docker-socket-file-공유로-도커-실행">컨테이너가 host의 docker socket file 공유로 도커 실행</h3>

<ul>
  <li>
    <p>도커 컨테이너를 GUI로 관리할 수 있는 툴인 <a href="https://www.portainer.io/">portainer</a>처럼 도커 컨테이너가 호스트의 도커 소켓 파일을 공유하여 도커를 관리하는데 사용 할 수 있습니다.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 도커 컨테이너 실행</span>
<span class="nv">$ </span>docker run <span class="nt">-d</span> <span class="nt">-p</span> 9000:9000 <span class="nt">-v</span> /var/run/docker.sock:/var/run/docker.sock portainer/portainer-ce
<span class="nv">$ </span>docker ps
<span class="c"># =&gt; CONTAINER ID   IMAGE                    COMMAND        CREATED         STATUS         PORTS                                                           NAMES</span>
<span class="c">#    1495728fd014   portainer/portainer-ce   &amp;quot;/portainer&amp;quot;   2 minutes ago   Up 2 minutes   8000/tcp, 9443/tcp, 0.0.0.0:9000-&amp;gt;9000/tcp, :::9000-&amp;gt;9000/tcp   wizardly_ride</span>
  
</code></pre></div>    </div>

    <p><code class="language-plaintext highlighter-rouge">-v</code> 옵션으로 호스트의 도커 소켓 파일을 컨테이너의 도커 소켓 파일로 공유하면 아래와 같이 도커 컨테이너에서 호스트의 도커를 관리할 수 있습니다.</p>

    <p><img src="/assets/2024/kans-3th/w1/20240831_kans_w1_8.png" alt="소켓 공유를 통해 portainer 사용" /></p>
  </li>
  <li>
    <p>또한 Jenkins 같은 CI/CD 툴을 사용할때도 도커 소켓 파일을 공유하여 도커 기반 워커를 사용할 수도 있습니다.</p>
  </li>
</ul>

<h3 id="cpu-아키텍쳐">CPU 아키텍쳐</h3>

<ul>
  <li>도커 허브에 등록된 이미지들은 CPU 아키텍쳐별로 이미지를 제공하는데, <strong>호스트의 CPU 아키텍쳐와 다른 이미지는 동작할 수 없습니다.</strong></li>
  <li>아래와 같이 docker hub에서는 지원 CPU 아키텍쳐별로 필터링하는 기능을 제공하니, 특정 아키텍쳐의 이미지가 필요한 경우 사용할 수 있습니다.
<img src="/assets/2024/kans-3th/w1/20240831_kans_w1_9.png" alt="img.png" /></li>
  <li>또한 도커이미지 페이지의 Tags 탭에서 태그의 지원하는 아키텍쳐를 확인할 수 있습니다.
<img src="../../../assets/2024/kans-3th/w1/20240831_kans_w1_10.png" alt="20240831_kans_w1_10.png" /></li>
  <li>현재 리눅스의 CPU 아키텍쳐를 확인 해보겠습니다.
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>lscpu
<span class="c"># =&gt; Architecture:                       x86_64</span>
<span class="c">#    CPU op-mode(s):                     32-bit, 64-bit</span>
<span class="c">#    ...</span>
</code></pre></div>    </div>
    <p>사용중인 CPU 아키텍쳐는 x86_64 입니다.</p>
  </li>
  <li>현재 CPU 아키텍쳐와는 다른 아키텍쳐의 이미지를 설치해서 실패하는것을 확인해 보겠습니다.
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># arm64 실행 실패</span>
<span class="nv">$ </span>docker run <span class="nt">--rm</span> <span class="nt">-it</span> arm64v8/ubuntu bash
<span class="c"># =&gt; WARNING: The requested image's platform (linux/arm64/v8) does not match the detected host platform (linux/amd64) and no specific platform was requested</span>
<span class="c">#    exec /usr/bin/bash: exec format error</span>
  
<span class="c"># riscv64 실행 실패</span>
<span class="nv">$ </span>docker run <span class="nt">--rm</span> <span class="nt">-it</span> riscv64/ubuntu bash
<span class="c"># =&gt; WARNING: The requested image's platform (linux/riscv64) does not match the detected host platform (linux/amd64) and no specific platform was requested</span>
<span class="c">#    exec /usr/bin/bash: exec format error</span>
</code></pre></div>    </div>
  </li>
</ul>

<hr />

<h2 id="컨테이너-격리">컨테이너 격리</h2>

<ul>
  <li>docker는 리눅스의 프로세스 격리 기술을 활용하는데, 프로세스 격리 기술은 chroot에서 부터 cgroup, namespace 등을 거쳐 발전하고 있습니다.</li>
  <li>주요 격리 기술들을 실습해보며 이해해보겠습니다.
<img src="/assets/2024/kans-3th/w1/20240831_kans_w1_11.png" alt="img.png" />
<a href="https://speakerdeck.com/kakao/ige-dwaeyo-dokeo-eobsi-keonteineo-mandeulgi?slide=200">https://speakerdeck.com/kakao/ige-dwaeyo-dokeo-eobsi-keonteineo-mandeulgi?slide=200</a></li>
</ul>

<h3 id="chroot">chroot</h3>

<ul>
  <li>chroot는 리눅스의 프로세스 격리 기술 중 하나로, 프로세스가 접근할 수 있는 파일 시스템의 루트 디렉터리를 변경하는 기술입니다.</li>
  <li>1979년에 처음 등장했으며, 한계가 뚜렷하지만 다양한 목적으로 현재도 현역으로 사용되고 있습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 관리자 전환</span>
<span class="nv">$ </span><span class="nb">sudo</span> <span class="nt">-i</span>
<span class="nv">$ </span><span class="nb">whoami</span>
<span class="c"># =&gt; root</span>

<span class="nv">$ </span><span class="nb">cd</span> /tmp
<span class="nv">$ </span><span class="nb">mkdir </span>myroot

<span class="c"># chroot 실행 (chroot [새 루트] [명령])</span>
<span class="nv">$ </span><span class="nb">chroot </span>myroot /bin/bash
<span class="c"># =&gt; chroot: failed to run command ‘/bin/bash’: No such file or directory</span>
</code></pre></div></div>

<ul>
  <li>/tmp/myroot 로 chroot하려니 bash가 없어서 실행이 되지 않습니다. bash를 복사해 넣어보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># bash를 실행하는데 필요한 라이브러리를 확인하겠습니다.</span>
<span class="nv">$ </span>ldd /bin/bash
<span class="c"># =&gt; linux-vdso.so.1 (0x00007fffecfa8000)</span>
<span class="c">#    libtinfo.so.6 =&gt; /lib/x86_64-linux-gnu/libtinfo.so.6 (0x00007fbfe6a4f000)</span>
<span class="c">#    libc.so.6 =&gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007fbfe686a000)</span>
<span class="c">#    /lib64/ld-linux-x86-64.so.2 (0x00007fbfe6be0000)</span>

<span class="nv">$ </span><span class="nb">mkdir</span> <span class="nt">-p</span> myroot/bin
<span class="nv">$ </span><span class="nb">cp</span> /bin/bash myroot/bin
<span class="nv">$ </span><span class="nb">mkdir</span> <span class="nt">-p</span> myroot/<span class="o">{</span>lib64,lib/x86_64-linux-gnu<span class="o">}</span>
<span class="nv">$ </span><span class="nb">cp</span> /lib/x86_64-linux-gnu/libtinfo.so.6 myroot/lib/x86_64-linux-gnu
<span class="nv">$ </span><span class="nb">cp</span> /lib/x86_64-linux-gnu/libc.so.6 myroot/lib/x86_64-linux-gnu
<span class="nv">$ </span><span class="nb">cp</span> /lib64/ld-linux-x86-64.so.2 myroot/lib64
<span class="nv">$ </span>tree myroot
<span class="c"># =&gt; myroot</span>
<span class="c">#    |-- bin</span>
<span class="c">#    |   `-- bash</span>
<span class="c">#    |-- lib</span>
<span class="c">#    |   `-- x86_64-linux-gnu</span>
<span class="c">#    |       |-- libc.so.6</span>
<span class="c">#    |       `-- libtinfo.so.6</span>
<span class="c">#    `-- lib64</span>
<span class="c">#        `-- ld-linux-x86-64.so.2</span>
<span class="c">#    </span>
<span class="c">#    5 directories, 4 files</span>

<span class="nv">$ </span><span class="nb">chroot </span>myroot /bin/bash
<span class="c"># =&gt; bash-5.2# </span>
<span class="c"># bash와 bash에 필요한 라이브러리를 넣어주니 chroot로 실행할 수 있게 되었습니다.</span>
<span class="c"># ls를 실행해보겠습니다.</span>
<span class="nv">$ </span><span class="nb">ls</span>
<span class="c"># =&gt; bash: ls: command not found</span>
<span class="c"># ls가 없어서 실행이 되지 않습니다. ls를 넣기위해 chroot에서 나오겠습니다.</span>
<span class="nv">$ </span><span class="nb">exit</span>

<span class="c"># ls 위치 확인</span>
<span class="nv">$ </span>whereis <span class="nb">ls</span>
<span class="c"># =&gt; ls: /usr/bin/ls /usr/share/man/man1/ls.1.gz</span>
<span class="nv">$ </span>ldd /usr/bin/ls
<span class="c"># ldd로 확인된 라이브러리를 포함해 ls를 myroot에 넣어보겠습니다.</span>
<span class="nv">$ </span><span class="nb">cp</span> /usr/bin/ls myroot/bin
<span class="nv">$ </span><span class="nb">cp</span> /lib/x86_64-linux-gnu/libselinux.so.1 myroot/lib/x86_64-linux-gnu
<span class="nv">$ </span><span class="nb">cp</span> /lib/x86_64-linux-gnu/libpcre2-8.so.0 myroot/lib/x86_64-linux-gnu

<span class="nv">$ </span><span class="nb">chroot </span>myroot /bin/bash
<span class="c"># ls시 /tmp/myroot에 있는 파일들을 확인할 수 있습니다.</span>
<span class="nv">$ </span><span class="nb">ls</span>
<span class="c"># =&gt; bin  lib  lib64</span>
<span class="c"># 현재 디렉터리 확인시 / 로 되어있습니다. 이 처럼 chroot로 인해 루트 디렉터리가 변경되었습니다. </span>
<span class="nv">$ </span><span class="nb">pwd</span>
<span class="c"># =&gt; /</span>
<span class="nv">$ </span><span class="nb">cd</span> ../../..
<span class="nv">$ </span><span class="nb">ls</span>
<span class="c"># =&gt; bin lib lib64</span>

<span class="c"># chroot를 종료 합니다.</span>
<span class="nv">$ </span><span class="nb">exit</span>
</code></pre></div></div>

<ul>
  <li>이 작업을 반복하면 거의 모든 프로그램을 chroot로 실행할 수 있습니다. 하지만 /proc, /dev 등의 가상 디렉터리는 다음의 방법으로 넣어주어야 합니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 다음 동작은 chroot 밖의 호스트에서 실행해야 합니다.</span>
<span class="c"># mount 할 디렉터리 만들어주기</span>
<span class="nv">$ </span><span class="nb">mkdir</span> <span class="nt">-p</span> myroot/<span class="o">{</span>proc,dev<span class="o">}</span>

<span class="c"># /proc, /dev 마운트</span>
<span class="nv">$ </span>mount <span class="nt">-t</span> proc none myroot/proc
<span class="nv">$ </span>mount <span class="nt">-o</span> <span class="nb">bind</span> /dev myroot/dev

<span class="c"># /proc 확인을 위해 ps도 chroot 환경에 넣어보겠습니다.</span>
<span class="nv">$ </span><span class="nb">cp</span> /usr/bin/ps myroot/bin
<span class="nv">$ </span><span class="nb">cp</span> /lib/x86_64-linux-gnu/<span class="o">{</span>libproc2.so.0,libc.so.6,libsystemd.so.0,libcap.so.2,libgcrypt.so.20,liblz4.so.1,liblzma.so.5,libzstd.so.1,libgpg-error.so.0<span class="o">}</span> myroot/lib/x86_64-linux-gnu/ 
<span class="nv">$ </span><span class="nb">cp</span> /lib64/ld-linux-x86-64.so.2 myroot/lib64/ 

<span class="nv">$ </span><span class="nb">chroot </span>myroot /bin/bash
<span class="nv">$ </span><span class="nb">ls</span> /proc
<span class="nv">$ </span>ps
<span class="c"># =&gt;    PID TTY          TIME CMD</span>
<span class="c">#    729517 ?        00:00:00 sudo</span>
<span class="c">#    741301 ?        00:00:00 bash</span>
<span class="c">#    741310 ?        00:00:00 ps</span>

<span class="c"># chroot 종료</span>
<span class="nv">$ </span><span class="nb">exit</span>

<span class="c"># 마운트 해제</span>
<span class="nv">$ </span>mount <span class="nt">-t</span> proc
<span class="c"># =&gt; proc on /proc type proc (rw,nosuid,nodev,noexec,relatime)</span>
<span class="c">#    none on /tmp/myroot/proc type proc (rw,relatime)</span>
<span class="nv">$ </span>umount myroot/proc
<span class="nv">$ </span>umount myroot/dev
<span class="nv">$ </span>mount <span class="nt">-t</span> proc
<span class="c"># =&gt; proc on /proc type proc (rw,nosuid,nodev,noexec,relatime)</span>
</code></pre></div></div>

<ul>
  <li>도커 컨테이너 이미지를 추출하여 chroot로 실행해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">mkdir </span>nginx-root

<span class="c"># nginx 컨테이너 이미지에서 파일들을 추출하여 nginx-root에 넣어줍니다.</span>
<span class="nv">$ </span>docker <span class="nb">export</span> <span class="si">$(</span>docker create nginx<span class="si">)</span> | <span class="nb">tar</span> <span class="nt">-C</span> nginx-root <span class="nt">-xvf</span> -
<span class="nv">$ </span>docker images

<span class="nv">$ </span>tree <span class="nt">-L</span> 2 nginx-root

<span class="c"># chroot로 nginx-root를 루트 디렉터리로 변경합니다.</span>
<span class="nv">$ </span><span class="nb">chroot </span>nginx-root /bin/bash
<span class="c"># nginx를 실행해봅니다.</span>
<span class="nv">$ </span>nginx <span class="nt">-g</span> <span class="s1">'daemon off;'</span>

<span class="c"># [터미널2] 터미널을 하나더 열고 nginx 동작 여부를 확인합니다.</span>
<span class="nv">$ </span>ps <span class="nt">-f</span> <span class="nt">-C</span> nginx
<span class="nv">$ </span>curl localhost
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w1/20240831_kans_w1_12.png" alt="img.png" /></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 생성된 docker 컨테이너를 확인합니다. docker create nginx로 인해 컨테이너가 생겨져있습니다.</span>
<span class="nv">$ </span>docker ps <span class="nt">-a</span>
<span class="c"># =&gt; CONTAINER ID   IMAGE     COMMAND                  CREATED              STATUS    PORTS     NAMES</span>
<span class="c">#    0b506af00006   nginx     "/docker-entrypoint.…"   About a minute ago   Created             gifted_rosalind</span>

<span class="c"># 사용하지 않는 도커이미지를 지워줍니다.</span>
<span class="nv">$ </span>docker <span class="nb">rm </span>0b5
</code></pre></div></div>

<ul>
  <li>아쉽게도 chroot는 탈옥이 가능하다고 합니다. 다음 코드를 컴파일하여 탈옥을 시도해보겠습니다.</li>
</ul>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;sys/stat.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;unistd.h&gt;</span><span class="cp">
</span>
<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
  <span class="n">mkdir</span><span class="p">(</span><span class="s">".out"</span><span class="p">,</span> <span class="mo">0755</span><span class="p">);</span>
  <span class="n">chroot</span><span class="p">(</span><span class="s">".out"</span><span class="p">);</span>
  <span class="n">chdir</span><span class="p">(</span><span class="s">"../../../../../"</span><span class="p">);</span>
  <span class="n">chroot</span><span class="p">(</span><span class="s">"."</span><span class="p">);</span>

  <span class="k">return</span> <span class="n">execl</span><span class="p">(</span><span class="s">"/bin/sh"</span><span class="p">,</span> <span class="s">"-i"</span><span class="p">,</span> <span class="nb">NULL</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 컴파일</span>
<span class="nv">$ </span>gcc <span class="nt">-o</span> myroot/escape_chroot escape_chroot.c
<span class="nv">$ </span>file myroot/escape_chroot
<span class="c"># =&gt; myroot/escape_chroot: ELF 64-bit LSB pie executable, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, BuildID[sha1]=5a40e26463d1015f870c7f1b9db9be159727c250, for GNU/Linux 3.2.0, not stripped</span>

<span class="c"># chroot 실행</span>
<span class="nv">$ </span><span class="nb">chroot </span>myroot /bin/bash
<span class="nv">$ </span><span class="nb">ls</span>
<span class="nv">$ </span><span class="nb">cd</span> ../../
<span class="nv">$ </span><span class="nb">cd</span> ../../
<span class="nv">$ </span><span class="nb">ls</span>
<span class="c"># 일반적인 방법으로는 myroot에서 벗어날 수 없었습니다.</span>

<span class="c"># escape_chroot 실행해서 탈옥해보겠습니다.</span>
<span class="nv">$ </span>./escape_chroot
<span class="nv">$ </span><span class="nb">ls</span> /
<span class="c"># 탈옥이 잘 되었습니다.</span>

<span class="c"># 종료</span>
<span class="nv">$ </span><span class="nb">exit</span>
<span class="nv">$ </span><span class="nb">exit</span>
</code></pre></div></div>

<h3 id="마운트-네임스페이스--pivot_root">마운트 네임스페이스 + pivot_root</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">pivot_root</code>는 루트 파일 시스템을 변경하는 시스템 콜로, 루트 디렉터리를 변경하는 chroot와 달리 루트 파일 시스템을 별도의 디렉터리로 이동시킬 수 있습니다.</li>
  <li>아래의 그림에서 처럼 /tmp/new_root가 있고 /tmp/new_root/put_old 디렉터리가 있는 경우, <code class="language-plaintext highlighter-rouge">pivot_root /tmp/new_root /tmp_new_root/put_old</code>를 하면 
/tmp/new_root가 루트 디렉터리로 변경되고, 원래의 루트 /는 /tmp/new_root/put_old로 이동됩니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w1/20240831_kans_w1_13.png" alt="img.png" />
<a href="https://speakerdeck.com/kakao/ige-dwaeyo-dokeo-eobsi-keonteineo-mandeulgi?slide=80">https://speakerdeck.com/kakao/ige-dwaeyo-dokeo-eobsi-keonteineo-mandeulgi?slide=80</a></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">pivot_root</code>를 사용하려면 <code class="language-plaintext highlighter-rouge">unshare</code> 명령을 통해 마운트 네임스페이스를 만들어야 합니다. 마운트 네임스페이스는 리눅스 커널에서 제공하는 기능으로, 프로세스가 마운트 정보를 독립적으로 가질 수 있게 해줍니다.</li>
  <li>또한 다음과 같은 제약사항이 적용 됩니다.
    <ul>
      <li>new_root와 put_old가 디렉터리여야 한다.</li>
      <li>new_root와 put_old가 현재 루트와 같은 마운트 상에 있어선 안 된다.</li>
      <li>put_old가 new_root와 같거나 그 아래에 있어야 한다. 즉, put_old가 가리키는 경로명 앞에 “/..”를 0개 이상 붙여서 new_root와 같은 디렉터리가 나와야 한다.</li>
      <li>new_root가 마운트 지점의 경로여야 하되, “/”일 수 없다. 마운트 지점이 아닌 경우에는 그 경로를 스스로에게 바인드 마운트 해서 마운트 지점으로 바꿀 수 있다.</li>
      <li>new_root의 부모 마운트 및 현재 작업 디렉터리의 부모 마운트의 전파 유형이 MS_SHARED여선 안 된다. 마찬가지로 put_old가 기존 마운트 지점인 경우 그 전파 유형이 MS_SHARED여선 안 된다. 이 제약은 pivot_root()로 인해 다른 마운트 네임스페이스로 어떤 변화도 전파되지 않게 한다.</li>
      <li>현재 루트 디렉터리가 마운트 지점이어야 한다.</li>
    </ul>
  </li>
  <li>실습을 통해 마운트 네임스페이스와 pivot_root를 알아보겠습니다.</li>
</ul>

<h4 id="실습">실습</h4>

<ul>
  <li>먼저 pivot_root로 root 디렉터리로 만들 /tmp/new_root를 만들어보겠습니다.</li>
  <li>위의 제약사항 중 new_root와 put_old가 현재 루트와 같은 마운트 상에 있어서는 안 되기 때문에 new_root를 tmpfs 로 마운트 하겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">mkdir</span> /tmp/new_root
<span class="c"># 마운트</span>
<span class="nv">$ </span>mount <span class="nt">-t</span> tmpfs tmpfs /tmp/new_root 
<span class="c"># 기존 루트를 이동시킬 /tmp/new_root/put_old 디렉터리를 만들기</span>
<span class="nv">$ </span><span class="nb">mkdir</span> /tmp/new_root/put_old
<span class="c"># /bin, /lib, /lib64 등 chroot 실습때 사용했던 /tmp/myroot를 /tmp/new_root 로 복사해서 재사용합니다.</span>
<span class="nv">$ </span><span class="nb">cp</span> <span class="nt">-rv</span> /tmp/myroot/<span class="k">*</span> /tmp/new_root 

<span class="nv">$ </span>mount <span class="nt">-t</span> proc proc /tmp/new_root/proc

<span class="c"># unshare 해서 마운트 네임스페이스를 만들어줍니다.</span>
<span class="nv">$ </span>unshare <span class="nt">--mount</span> /bin/bash 

<span class="nv">$ </span><span class="nb">cd</span> /tmp/new_root

<span class="c"># pivot_root를 실행</span>
<span class="nv">$ </span>pivot_root <span class="nb">.</span> put_old

<span class="c"># 새로운 루트로 이동되었습니다.</span>

<span class="c"># 새 루트 확인</span>
<span class="nv">$ </span><span class="nb">ls</span> / 
<span class="c"># =&gt; bin  dev  escape_chroot  lib  lib64  proc  put_old</span>

<span class="c"># 기존 루트 확인</span>
<span class="nv">$ </span><span class="nb">ls</span> /put_old
<span class="c"># =&gt; bin   dev  home        lib32  lost+found   mnt     proc  run   srv	  sys       usr  vmlinuz</span>
<span class="c">#    boot  etc  initrd.img  lib	   lib64        media   opt   root  sbin  swapfile  tmp  var  </span>
</code></pre></div></div>

<ul>
  <li>새로운 루트로 이동되었지만, 기존 루트에 있는 파일들을 삭제하거나 이동하지 않았기 때문에 /put_old로 기존 루트에 있는 파일들을 확인할 수 있습니다.</li>
  <li>하지만 umount를 사용하면 /put_old와 기존 루트의 연결을 끊어서 기존 루트를 숨길 수 있습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>umount <span class="nt">-l</span> /put_old

<span class="nv">$ </span><span class="nb">ls</span> /put_old
<span class="c"># =&gt; (공백)</span>
</code></pre></div></div>

<ul>
  <li>escape_root를 통해 탈옥을 시도해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 탈옥 시도</span>
<span class="nv">$ </span>/escape_chroot
<span class="nv">$ </span><span class="nb">ls</span> /
<span class="c"># =&gt; bin  dev  escape_chroot  lib  lib64  proc  put_old</span>
<span class="nv">$ </span><span class="nb">cd</span> ../../..
<span class="nv">$ </span>/escape_chroot
<span class="nv">$ </span><span class="nb">ls</span> /
<span class="c"># =&gt; bin  dev  escape_chroot  lib  lib64  proc  put_old</span>
</code></pre></div></div>

<ul>
  <li>chroot와 달리 pivot_root는 탈옥이 불가능하고 훨씬 안전한것 같습니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w1/20240831_kans_w1_14.png" alt="img.png" /></p>

<h3 id="네임스페이스-namespace">네임스페이스 (namespace)</h3>

<ul>
  <li>여기에서의 네임스페이스는 쿠버네티스 등의 네임스페이스와는 다른,
리눅스 커널에서 제공하는 프로세스 격리 기술로, 프로세스가 각종 자원을 격리하여 사용할 수 있게 해줍니다.</li>
  <li>주요 네임스페이스의 유형은 아래와 같습니다.
    <ul>
      <li>Mount Namespace (2002년 도입)
        <ul>
          <li>pivot_root 예제에서 처럼 마운트 정보를 격리합니다.</li>
          <li>즉, 서로 다른 네임스페이스가 독립적으로 파일 시스템을 마운트 할 수 있습니다.</li>
        </ul>
      </li>
      <li>UTS Namespace (2006년 도입)
        <ul>
          <li>호스트 이름과 NIS 도메인 이름을 격리합니다. 각 네임스페이스는 자체 호스트 이름과 NIS 도메인 이름을 가질 수 있고,
이를 통해 호스트 이름을 변경하더라도 다른 네임스페이스에 영향을 주지 않습니다.</li>
        </ul>
      </li>
      <li>IPC Namespace (2006년 도입)
        <ul>
          <li>POSIX 메시지 큐, 세마포어, 공유 메모리 같은 IPC 리소스를 격리합니다.</li>
          <li>이를 통해 서로 다른 네임스페이스는 독립적으로 System V IPC 객체와 POSIX 메시지 큐를 사용할 수 있습니다.</li>
        </ul>
      </li>
      <li>PID Namespace (2008년 도입)
        <ul>
          <li>프로세스 ID를 격리합니다. 각 네임스페이스는 자체 PID를 가질 수 있으며 자체적인 PID 1을 가질 수 있습니다.</li>
          <li>프로세스 ID가 1인것은 시스템 시작시에 최초로 실행된 것이며 이를 init 프로세스라고 합니다. 이 프로세스가 종료되면 
시스템이 종료되거나 다시 부팅됩니다. 도커 컨테이너 실행시 실행되는 프로그램이 PID가 1이고, 해당 프로그램이 종료되며 
컨테이너도 종료되는게 이때문입니다.</li>
        </ul>
      </li>
      <li>Network Namespace (2009년 도입)
        <ul>
          <li>네트워크 인터페이스, IP 주소, 라우팅 테이블, 방화벽 규칙 등 네트워크 리소스를 격리합니다.</li>
          <li>각 네임스페이스는 자체 네트워크 인터페이스, IP 주소, 라우팅 테이블, 방화벽 규칙을 가질 수 있습니다.</li>
        </ul>
      </li>
      <li>USER Namespace (2012년 도입)
        <ul>
          <li>사용자 ID와 그룹 ID를 격리합니다. 각 네임스페이스는 자체 사용자 ID와 그룹 ID를 가질 수 있습니다.</li>
          <li>이를 통해 root 권한을 가진 사용자도 일반 사용자로 격리하여 사용할 수 있고, 일반 사용자도 root 인것 처럼 보이게 할 수 있습니다.</li>
          <li>실행 중인 도커컨테이너에서는 ps로 확인시 root로 실행 중인데, 호스트에서 ps로 확인시 일반 사용자로 실행 중인것 처럼 보이는것도 이것 때문입니다.</li>
        </ul>
      </li>
      <li>CGROUP Namespace (2016년 도입)
        <ul>
          <li>CGROUP은 프로세스의 그룹으로 CPU, 메모리, 디스크 I/O, 네트워크 등의 자원을 제한하거나 할당할 수 있습니다.</li>
          <li>CGROUP Namespace는 CPU, 메모리 등의 자원을 제한하거나 할당할 수 있는 CGROUP을 격리하는 기능입니다.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="cgroup-를-이용한-자원관리">cgroup 를 이용한 자원관리</h3>

<ul>
  <li>cgroups는 control groups의 줄일말로 리눅스 커널에서 제공하는 자원 제한 및 할당 기능으로, CPU, 메모리, 디스크 I/O, 네트워크 등의 자원을 제한하거나 할당할 수 있습니다.</li>
  <li>프로세스는 실행중인 프로그램의 인스턴스를 의미하며, OS에서는 프로세스를 관리하기 위해 프로세스 ID(PID)를 사용합니다.</li>
  <li>cgroups는 프로세스를 그룹으로 묶어서 자원을 제한하거나 할당할 수 있습니다.</li>
  <li>cgroups는 /sys/fs/cgroup 디렉터리에 마운트되어 있으며, cgroup v1과 cgroup v2가 있습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># cgroup 버전 확인</span>
<span class="nv">$ </span>mount | <span class="nb">grep </span>cgroup
<span class="c"># =&gt; cgroup2 on /sys/fs/cgroup type cgroup2 (rw,nosuid,nodev,noexec,relatime,nsdelegate,memory_recursiveprot)</span>
</code></pre></div></div>

<ul>
  <li>현재 테스트 시스템에는 cgroup v2가 사용되고 있는것을 확인할 수 있습니다.</li>
  <li>cgroup v2는 v1에 비해 자원 계층구조의 가시성이 향상 되었고, memoryQoS 라는 기능이 추가되어 컨테이너에서 OOM(Out Of Memory)이
발생가능성을 줄였습니다. 최신 리눅스 배포판은 보통 cgroup v2를 사용하고 있어서 cgroup v2로 실습을 진행하겠습니다.</li>
  <li>cgroup의 계층 구조는 /sys/fs/cgroup 에서 확인할 수 있습니다.</li>
  <li>/proc는 보았지만 /sys는 눈에 익지 않습니다. 리눅스 커널 3.x 버전에서 생긴것으로 USER SPACE 쪽은 /proc에 KERNEL SPACE 쪽 정보는 /sys에 들어간다고 합니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w1/20240831_kans_w1_23.png" alt="img.png" class="image-center" />
<em class="image-caption">출처 : <a href="https://blog.naver.com/yu3papa/223562337709">https://blog.naver.com/yu3papa/223562337709</a></em></p>

<ul>
  <li>실습을 통해 cgroup의 정보를 확인해 보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>mount <span class="nt">-t</span> cgroup
<span class="nv">$ </span>mount <span class="nt">-t</span> cgroup2
<span class="c"># =&gt; cgroup2 on /sys/fs/cgroup type cgroup2 (rw,nosuid,nodev,noexec,relatime,nsdelegate,memory_recursiveprot)</span>

<span class="nv">$ </span>findmnt <span class="nt">-t</span> cgroup2
<span class="c"># =&gt; TARGET         SOURCE  FSTYPE  OPTIONS</span>
<span class="c">#    /sys/fs/cgroup cgroup2 cgroup2 rw,nosuid,nodev,noexec,relatime,nsdelegate,memory_recursiveprot</span>

<span class="c"># cgroupv1 만 지원 시, cgroup2 출력되지 않음</span>
<span class="nv">$ </span><span class="nb">grep </span>cgroup /proc/filesystems
<span class="c"># =&gt; nodev   cgroup</span>
<span class="c">#    nodev   cgroup2</span>

<span class="nv">$ </span><span class="nb">stat</span> <span class="nt">-fc</span> %T /sys/fs/cgroup/
<span class="c"># =&gt; cgroup2fs</span>

<span class="c"># 터미널2</span>
<span class="nv">$ </span><span class="nb">sleep </span>100000

<span class="c"># 터미널1</span>
<span class="c"># /proc 에 cgroup 정보 확인</span>
<span class="nv">$ </span><span class="nb">cat</span> /proc/cgroups
<span class="nv">$ </span><span class="nb">cat</span> /proc/<span class="si">$(</span>pgrep <span class="nb">sleep</span><span class="si">)</span>/cgroup
<span class="c"># =&gt; 0::/user.slice/user-1000.slice/session-713.scope</span>

<span class="nv">$ </span>tree /proc/<span class="si">$(</span>pgrep <span class="nb">sleep</span><span class="si">)</span> <span class="nt">-L</span> 2
<span class="c"># =&gt; ...</span>
<span class="c">#    |-- &lt;span style="font-weight:bold;color:blue;"&gt;ns&lt;/span&gt;</span>
<span class="c">#    |   |-- &lt;span style="font-weight:bold;color:teal;"&gt;cgroup&lt;/span&gt; -&amp;gt; cgroup:[4026531835]</span>
<span class="c">#    |   |-- &lt;span style="font-weight:bold;color:teal;"&gt;ipc&lt;/span&gt; -&amp;gt; ipc:[4026531839]</span>
<span class="c">#    |   |-- &lt;span style="font-weight:bold;color:teal;"&gt;mnt&lt;/span&gt; -&amp;gt; mnt:[4026531841]</span>
<span class="c">#    |   |-- &lt;span style="font-weight:bold;color:teal;"&gt;net&lt;/span&gt; -&amp;gt; net:[4026531840]</span>
<span class="c">#    ...</span>

<span class="c"># cgroup 목록 확인</span>
<span class="nv">$ </span><span class="nb">ls</span> /sys/fs/cgroup
<span class="nv">$ </span><span class="nb">cat</span> /sys/fs/cgroup/cgroup.controllers
<span class="c"># =&gt; cpuset cpu io memory hugetlb pids rdma misc</span>
<span class="nv">$ </span>tree /sys/fs/cgroup/ <span class="nt">-L</span> 1
<span class="nv">$ </span>tree /sys/fs/cgroup/ <span class="nt">-L</span> 2
<span class="nv">$ </span>tree /sys/fs/cgroup/user.slice <span class="nt">-L</span> 1
<span class="nv">$ </span>tree /sys/fs/cgroup/user.slice/user-1000.slice <span class="nt">-L</span> 1
</code></pre></div></div>

<ul>
  <li>이번에는 cgroup을 이용하여 자원을 제한하는 실습을 진행해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 터미널 2개를 열어서 root 로 실습 하겠습니다.</span>
<span class="nv">$ </span><span class="nb">sudo</span> <span class="nt">-i</span>
<span class="nv">$ </span><span class="nb">whoami</span>
<span class="c"># =&gt; root</span>

<span class="c"># 툴 설치</span>
<span class="nv">$ </span>apt <span class="nb">install</span> <span class="nt">-y</span> cgroup-tools stress htop

<span class="c"># 터미널2</span>
<span class="c"># CPU 사용률 확인을 위해 htop을 실행합니다.</span>
<span class="nv">$ </span>htop

<span class="c"># 터미널1에서 실습 진행</span>

<span class="c"># 1개 CPU 코어에 부하 발생을 위해 stress를 실행합니다.</span>
<span class="nv">$ </span>stress <span class="nt">--cpu</span> 1
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w1/20240831_kans_w1_25.png" alt="img.png" /></p>

<ul>
  <li>CPU 0만 100% 사용중인것을 확인할 수 있습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cd</span> /sys/fs/cgroup
<span class="nv">$ </span><span class="nb">mkdir </span>test_cgroup_parent <span class="o">&amp;&amp;</span> <span class="nb">cd </span>test_cgroup_parent
<span class="nv">$ </span>tree

<span class="c"># 제어가능한 항목 확인</span>
<span class="nv">$ </span><span class="nb">cat </span>cgroup.controllers
<span class="c"># =&gt; cpuset cpu io memory hugetlb pids rdma misc</span>

<span class="c"># cpu를 subtree이 추가하여 컨트롤 할 수 있도록 설정 : +/-(추가/삭제) </span>
<span class="nv">$ </span><span class="nb">cat </span>cgroup.subtree_control
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"+cpu"</span> <span class="o">&gt;&gt;</span> /sys/fs/cgroup/test_cgroup_parent/cgroup.subtree_control

<span class="c"># cpu.max 제한 설정 : 첫 번쨰 값은 허용된 시간(마이크로초) 두 번째 값은 총 기간 길이 &gt; 1/10 실행 설정</span>
<span class="nv">$ </span><span class="nb">echo </span>100000 1000000 <span class="o">&gt;</span> /sys/fs/cgroup/test_cgroup_parent/cpu.max

<span class="c"># test용 자식 디렉토리를 생성하고, pid를 추가하여 제한을 걸어</span>
<span class="nv">$ </span><span class="nb">mkdir </span>test_cgroup_child <span class="o">&amp;&amp;</span> <span class="nb">cd </span>test_cgroup_child
<span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$$</span> <span class="o">&gt;</span> /sys/fs/cgroup/test_cgroup_parent/test_cgroup_child/cgroup.procs
<span class="nv">$ </span><span class="nb">cat</span> /sys/fs/cgroup/test_cgroup_parent/test_cgroup_child/cgroup.procs
<span class="c"># =&gt; 1947587</span>
<span class="c">#    2194781</span>
<span class="nv">$ </span><span class="nb">cat</span> /proc/<span class="nv">$$</span>/cgroup
<span class="c"># =&gt; 0::/test_cgroup_parent/test_cgroup_child</span>

<span class="c"># 부하 발생 확인 : 터미널2에 htop 확인</span>
<span class="nv">$ </span>stress <span class="nt">--cpu</span> 1
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w1/20240831_kans_w1_26.png" alt="img.png" /></p>

<ul>
  <li>cpu.max 제한 설정에서 설정한 대로 (100000/1000000 =&gt; 10%) CPU 사용량이 10%로 제한된것을 확인할 수 있습니다.</li>
  <li>값 수정을 해서 100%로 변경해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 값 수정</span>
<span class="nv">$ </span><span class="nb">echo </span>1000000 1000000 <span class="o">&gt;</span> /sys/fs/cgroup/test_cgroup_parent/cpu.max

<span class="c"># 부하 발생 확인 : 터미널2에 htop 확인</span>
<span class="nv">$ </span>stress <span class="nt">--cpu</span> 1
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w1/20240831_kans_w1_27.png" alt="img.png" /></p>

<ul>
  <li>테스트에 사용한 cgroup 을 삭제하고 실습을 마무리하겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">exit</span>
<span class="nv">$ </span><span class="nb">sudo</span> <span class="nt">-i</span>
<span class="nv">$ </span><span class="nb">rmdir</span> /sys/fs/cgroup/test_cgroup_parent/test_cgroup_child
<span class="nv">$ </span><span class="nb">rmdir</span> /sys/fs/cgroup/test_cgroup_parent
</code></pre></div></div>

<ul>
  <li>이상과 같이 cgroup을 사용하여 cpu 자원을 제한하는것을 실습해 보았습니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w1/20240831_kans_w1_24.png" alt="img.png" class="image-center" /></p>

<hr />

<h2 id="컨테이너-네트워크--iptables">컨테이너 네트워크 &amp; Iptables</h2>

<ul>
  <li>도커는 호스트와 컨테이너간, 컨테이너 간의 네트워크를 앞에서 살펴본 네트워크 네임스페이스를 통해 격리합니다.</li>
  <li>또한 iptables를 통해 네트워크 패킷을 제어하고, 컨테이너 간의 통신을 제어합니다.</li>
  <li>실습을 통해 네트워크 네임스페이스를 통한 격리와 iptables의 사용법에 대해 알아보겠습니다.</li>
</ul>

<h3 id="red--blue-네트워크-네임스페이스-간-통신">Red &lt;=&gt; Blue 네트워크 네임스페이스 간 통신</h3>

<p><img src="/assets/2024/kans-3th/w1/20240831_kans_w1_17.png" alt="img.png" />
<a href="https://www.slideshare.net/slideshow/make-container-withoutdocker6overlaynetwork1/248297122">출처 : 도커없이 컨테이너 만들기</a></p>

<ul>
  <li>먼저 터미널 3개를 열고 모두 관리자로 로그인 하겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo</span> <span class="nt">-i</span>
<span class="nv">$ </span><span class="nb">whoami</span>
<span class="c"># =&gt; root</span>
</code></pre></div></div>

<ul>
  <li>veth (Virtual Ethernet)를 사용하여 Red와 Blue 네트워크 네임스페이스를 만듭니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>ip <span class="nb">link </span>add veth0 <span class="nb">type </span>veth peer name veth1

<span class="c"># veth 생성 확인 (상태 DOWN)</span>
<span class="nv">$ </span>ip <span class="nb">link</span>
<span class="c"># =&gt; 22: &lt;span style="color:teal;"&gt;veth1@veth0: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST,M-DOWN&amp;gt; mtu 1500 qdisc noop state &lt;span style="color:red;"&gt;DOWN &lt;/span&gt;mode DEFAULT group default qlen 1000</span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;9e:74:34:5c:70:ef&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt;</span>
<span class="c">#    23: &lt;span style="color:teal;"&gt;veth0@veth1: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST,M-DOWN&amp;gt; mtu 1500 qdisc noop state &lt;span style="color:red;"&gt;DOWN &lt;/span&gt;mode DEFAULT group default qlen 1000</span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;72:c0:05:36:cd:1b&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt;</span>
<span class="nv">$ </span>ip addr | <span class="nb">grep </span>veth
<span class="c"># =&gt; 22: &lt;span style="color:teal;"&gt;veth1@veth0: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST,M-DOWN&amp;gt; mtu 1500 qdisc noop state &lt;span style="color:red;"&gt;DOWN &lt;/span&gt;group default qlen 1000</span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;9e:74:34:5c:70:ef&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt;</span>
<span class="c">#    23: &lt;span style="color:teal;"&gt;veth0@veth1: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST,M-DOWN&amp;gt; mtu 1500 qdisc noop state &lt;span style="color:red;"&gt;DOWN &lt;/span&gt;group default qlen 1000</span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;72:c0:05:36:cd:1b&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt;</span>

<span class="c"># 네트워크 네임스페이스 생성</span>
<span class="nv">$ </span>ip netns add RED
<span class="nv">$ </span>ip netns add BLUE

<span class="c"># 네트워크 네임스페이스 확인</span>
<span class="nv">$ </span>ip netns
<span class="c"># =&gt; RED</span>
<span class="c">#    BLUE</span>

<span class="c"># veth0와 veth1을 각각 RED와 BLUE 네트워크 네임스페이스로 이동시킵니다.</span>
<span class="nv">$ </span>ip <span class="nb">link set </span>veth0 netns RED  
<span class="nv">$ </span>ip <span class="nb">link set </span>veth1 netns BLUE

<span class="c"># 네트워크 네임스페이스 확인. id 라는것이 추가되었습니다.</span>
<span class="nv">$ </span>ip netns list
<span class="c"># =&gt; RED (id: 0)</span>
<span class="c">#    BLUE (id: 1)</span>

<span class="c"># ip 링크를 확인하면 veth0와 veth1이 각각 RED와 BLUE 네트워크 네임스페이스로 이동되어 기본 명령에서는 보이지 않습니다.</span>
<span class="nv">$ </span>ip <span class="nb">link</span> | <span class="nb">grep</span> <span class="s2">"veth."</span>
<span class="c"># =&gt; (공백)</span>

<span class="c"># ip netns exec [네임스페이스명] [명령] 으로 네트워크 네임스페이스에서 명령을 실행할 수 있습니다.</span>
<span class="nv">$ </span>ip netns <span class="nb">exec </span>RED ip <span class="nb">link</span>
<span class="c"># =&gt; 1: &lt;span style="color:teal;"&gt;lo: &lt;/span&gt;&amp;lt;LOOPBACK&amp;gt; mtu 65536 qdisc noop state &lt;span style="color:red;"&gt;DOWN &lt;/span&gt;mode DEFAULT group default qlen 1000</span>
<span class="c">#        link/loopback &lt;span style="color:olive;"&gt;00:00:00:00:00:00&lt;/span&gt; brd &lt;span style="color:olive;"&gt;00:00:00:00:00:00&lt;/span&gt;</span>
<span class="c">#    23: &lt;span style="color:teal;"&gt;veth0@if22: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST&amp;gt; mtu 1500 qdisc noop state &lt;span style="color:red;"&gt;DOWN &lt;/span&gt;mode DEFAULT group default qlen 1000</span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;72:c0:05:36:cd:1b&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt; link-netns BLUE</span>
<span class="nv">$ </span>ip netns <span class="nb">exec </span>BLUE ip <span class="nb">link</span>
<span class="c"># =&gt; 1: &lt;span style="color:teal;"&gt;lo: &lt;/span&gt;&amp;lt;LOOPBACK&amp;gt; mtu 65536 qdisc noop state &lt;span style="color:red;"&gt;DOWN &lt;/span&gt;mode DEFAULT group default qlen 1000</span>
<span class="c">#        link/loopback &lt;span style="color:olive;"&gt;00:00:00:00:00:00&lt;/span&gt; brd &lt;span style="color:olive;"&gt;00:00:00:00:00:00&lt;/span&gt;</span>
<span class="c">#    22: &lt;span style="color:teal;"&gt;veth1@if23: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST&amp;gt; mtu 1500 qdisc noop state &lt;span style="color:red;"&gt;DOWN &lt;/span&gt;mode DEFAULT group default qlen 1000</span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;9e:74:34:5c:70:ef&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt; link-netns RED</span>

<span class="c"># veth0과 veth1을 활성화 (UP) 시키겠습니다.</span>
<span class="nv">$ </span>ip netns <span class="nb">exec </span>RED ip <span class="nb">link set </span>veth0 up
<span class="c"># veth0의 IP 확인</span>
<span class="nv">$ </span>ip netns <span class="nb">exec </span>RED ip addr
<span class="c"># =&gt; ...</span>
<span class="c">#    23: &lt;span style="color:teal;"&gt;veth0@if22: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state &lt;span style="color:green;"&gt;UP &lt;/span&gt;group default qlen 1000</span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;72:c0:05:36:cd:1b&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt; link-netns BLUE</span>
<span class="c">#        inet6 &lt;span style="color:blue;"&gt;fe80::70c0:5ff:fe36:cd1b&lt;/span&gt;/64 scope link proto kernel_ll </span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="nv">$ </span>ip netns <span class="nb">exec </span>BLUE ip <span class="nb">link set </span>veth1 up
<span class="nv">$ </span>ip netns <span class="nb">exec </span>BLUE ip addr
<span class="c"># =&gt; ...</span>
<span class="c">#    22: &lt;span style="color:teal;"&gt;veth1@if23: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state &lt;span style="color:green;"&gt;UP &lt;/span&gt;group default qlen 1000</span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;9e:74:34:5c:70:ef&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt; link-netns RED</span>
<span class="c">#        inet6 &lt;span style="color:blue;"&gt;fe80::9c74:34ff:fe5c:70ef&lt;/span&gt;/64 scope link proto kernel_ll </span>
<span class="c">#           valid_lft forever preferred_lft forever</span>

<span class="c"># UP 상태로 되었으나 IP가 없습니다. IP를 할당해보겠습니다.</span>
<span class="nv">$ </span>ip netns <span class="nb">exec </span>RED ip addr add 11.11.11.2/24 dev veth0
<span class="nv">$ </span>ip netns <span class="nb">exec </span>BLUE ip addr add 11.11.11.3/24 dev veth1

<span class="c"># IP 를 확인해보겠습니다.</span>
<span class="nv">$ </span>ip netns <span class="nb">exec </span>RED ip addr
<span class="c"># =&gt; ...</span>
<span class="c">#    23: &lt;span style="color:teal;"&gt;veth0@if22: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state &lt;span style="color:green;"&gt;UP &lt;/span&gt;group default qlen 1000</span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;72:c0:05:36:cd:1b&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt; link-netns BLUE</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;11.11.11.2&lt;/span&gt;/24 scope global veth0</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#        inet6 &lt;span style="color:blue;"&gt;fe80::70c0:5ff:fe36:cd1b&lt;/span&gt;/64 scope link proto kernel_ll </span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="nv">$ </span>ip netns <span class="nb">exec </span>BLUE ip addr
<span class="c"># =&gt; ...</span>
<span class="c">#    22: &lt;span style="color:teal;"&gt;veth1@if23: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state &lt;span style="color:green;"&gt;UP &lt;/span&gt;group default qlen 1000</span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;9e:74:34:5c:70:ef&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt; link-netns RED</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;11.11.11.3&lt;/span&gt;/24 scope global veth1</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="c">#        inet6 &lt;span style="color:blue;"&gt;fe80::9c74:34ff:fe5c:70ef&lt;/span&gt;/64 scope link proto kernel_ll </span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
</code></pre></div></div>

<ul>
  <li>이제 Red와 Blue 네트워크 네임스페이스 간의 통신을 테스트 해보겠습니다.</li>
  <li><code class="language-plaintext highlighter-rouge">nsenter</code> 명령을 사용하여 네트워크에 attach 하고, <code class="language-plaintext highlighter-rouge">tcpdump</code>와 <code class="language-plaintext highlighter-rouge">ping</code>을 사용하여 통신을 확인합니다.</li>
  <li><code class="language-plaintext highlighter-rouge">tcpdump</code>는 네트워크 패킷을 캡처하는 명령어로, 패킷을 캡처하여 확인할 수 있고, <code class="language-plaintext highlighter-rouge">ping</code>은 네트워크 상태를 확인하는 명령어입니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>tree /var/run/netns
<span class="c"># =&gt; &lt;span style="font-weight:bold;color:blue;"&gt;/var/run/netns&lt;/span&gt;</span>
<span class="c">#    |-- BLUE</span>
<span class="c">#    `-- RED</span>
<span class="c">#    </span>
<span class="c">#    1 directory, 2 files</span>

<span class="c"># 터미널 1 (RED 11.11.11.2)</span>
<span class="c"># 네트워크 네임스페이스에 attach. </span>
<span class="c"># 이때 --net 옵션을 사용해 앞에서 확인한 /var/run/netns/RED를 사용해 네트워크 네임스페이스에 attach 합니다.</span>
<span class="nv">$ </span>nsenter <span class="nt">--net</span><span class="o">=</span>/var/run/netns/RED
<span class="c"># 이웃하는 IP/ARP 정보 확인</span>
<span class="nv">$ </span>ip neigh
<span class="c"># =&gt; (공백)</span>
<span class="c"># 라우팅 정보, iptables 정보</span>
<span class="nv">$ </span>ip route
<span class="c"># =&gt; &lt;span style="color:purple;"&gt;11.11.11.0/24 &lt;/span&gt;dev &lt;span style="color:teal;"&gt;veth0 &lt;/span&gt;proto kernel scope link src &lt;span style="color:purple;"&gt;11.11.11.2 &lt;/span&gt;</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> filter <span class="nt">-S</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span> 

<span class="c"># 터미널 2 (호스트)</span>
<span class="c"># 네트워크 네임스페이스 상태 확인</span>
<span class="nv">$ </span>lsns <span class="nt">-t</span> net
<span class="c"># =&gt;         NS TYPE NPROCS     PID USER     NETNSID NSFS            COMMAND</span>
<span class="c">#    ...</span>
<span class="c">#    4026532444 net       1 1940569 root           0 /run/netns/RED  -zsh</span>
<span class="c">#    4026532527 net       0         root             /run/netns/BLUE</span>
<span class="c"># 네트워크 정보 확인</span>
<span class="nv">$ </span>ip addr 
<span class="nv">$ </span>ip neigh
<span class="nv">$ </span>ip route
<span class="nv">$ </span>iptables <span class="nt">-t</span> filter <span class="nt">-S</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span> 

<span class="c"># 터미널 3 (BLUE 11.11.11.3)</span>
<span class="nv">$ </span>nsenter <span class="nt">--net</span><span class="o">=</span>/var/run/netns/BLUE
<span class="nv">$ </span>ip neigh
<span class="nv">$ </span>ip route
<span class="nv">$ </span>iptables <span class="nt">-t</span> filter <span class="nt">-S</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-S</span> 

<span class="c"># ping 통신 확인</span>

<span class="c"># 터미널3 (BLUE)</span>
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> veth1
<span class="nv">$ </span>ip <span class="nt">-c</span> neigh
<span class="nv">$ </span><span class="nb">exit</span>

<span class="c"># 터미널1 (RED)</span>
<span class="nv">$ </span>ping 11.11.11.3 <span class="nt">-c</span> 1 
<span class="nv">$ </span>ip <span class="nt">-c</span> neigh
<span class="nv">$ </span><span class="nb">exit</span>

<span class="c"># 네임스페이스 삭제</span>
<span class="nv">$ </span>ip netns del RED
<span class="nv">$ </span>ip netns del BLUE 
</code></pre></div></div>

<p><img src="/assets/2024/kans-3th/w1/20240831_kans_w1_15.png" alt="img.png" /></p>

<ul>
  <li>위의 캡쳐와 같이 통신이 되어서 tcpdump에 ARP, ICMP 패킷이 잡히는것을 확인할 수 있습니다.</li>
  <li>또한 <code class="language-plaintext highlighter-rouge">ip neigh</code> 명령을 확인했을때 ARP 테이블에 상대방의 IP와 MAC 주소가 등록되어 있는것을 확인할 수 있습니다.</li>
</ul>

<h3 id="red---bridge-br0---blue-네트워크-네임스페이스-간-통신">Red &lt;- Bridge (br0) -&gt; Blue 네트워크 네임스페이스 간 통신</h3>

<p><img src="/assets/2024/kans-3th/w1/20240831_kans_w1_16.png" alt="img.png" />
<a href="https://www.slideshare.net/slideshow/make-container-withoutdocker6overlaynetwork1/248297122">출처 : 도커없이 컨테이너 만들기</a></p>

<ul>
  <li>이전 실습에서는 Red와 Blue를 연결하여 peer 네트워크로 구성하였는데, 
이번에는 각각 독립적인 네트워크로 구성하여 Bridge를 사용하여 Red와 Blue 네트워크 네임스페이스 간의 통신을 확인해보겠습니다.</li>
  <li>왜 Bridge를 두는가 하면, peer 네트워크를 구성할 경우 구성원들 간의 통신을 위해서는 모든 노드가 서로서로 연결되어야 하기 때문입니다.
<img src="/assets/2024/kans-3th/w1/20240831_kans_w1_18.png" alt="img.png" class="image-center" /></li>
  <li>Bridge를 두면 각 노드는 Bridge와만 연결되어 있으면 통신이 가능하므로 효율적입니다.</li>
  <li>실습을 통해 아래의 그림과 같이 격리된 네트워크 네임스페이스를 만들고 브릿지를 통해 통신해보겠습니다.
<img src="/assets/2024/kans-3th/w1/20240831_kans_w1_19.png" alt="img.png" /></li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 터미널 3개를 root 로 엽니다.</span>
<span class="nv">$ </span><span class="nb">sudo</span> <span class="nt">-i</span>
<span class="nv">$ </span><span class="nb">whoami</span>
<span class="c"># =&gt; root</span>

<span class="c"># 네트워크 네임스페이스 및 veth 생성</span>
<span class="nv">$ </span>ip netns add RED
<span class="nv">$ </span>ip <span class="nb">link </span>add reth0 <span class="nb">type </span>veth peer name reth1
<span class="nv">$ </span>ip <span class="nb">link set </span>reth0 netns RED
<span class="nv">$ </span>ip netns add BLUE
<span class="nv">$ </span>ip <span class="nb">link </span>add beth0 <span class="nb">type </span>veth peer name beth1
<span class="nv">$ </span>ip <span class="nb">link set </span>beth0 netns BLUE

<span class="c"># 확인</span>
<span class="nv">$ </span>ip netns list
<span class="nv">$ </span>ip <span class="nb">link</span>
<span class="c"># =&gt; ...</span>
<span class="c">#    26: &lt;span style="color:teal;"&gt;reth1@if27: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST&amp;gt; mtu 1500 qdisc noop state &lt;span style="color:red;"&gt;DOWN &lt;/span&gt;mode DEFAULT group default qlen 1000</span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;9a:1f:bf:6f:fe:64&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt; link-netns RED</span>
<span class="c">#    28: &lt;span style="color:teal;"&gt;beth1@if29: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST&amp;gt; mtu 1500 qdisc noop state &lt;span style="color:red;"&gt;DOWN &lt;/span&gt;mode DEFAULT group default qlen 1000</span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;7e:31:cf:5f:00:8f&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt; link-netns BLUE</span>
<span class="nv">$ </span>ip netns <span class="nb">exec </span>RED ip addr
<span class="c"># =&gt; ...</span>
<span class="c">#    27: &lt;span style="color:teal;"&gt;reth0@if26: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST&amp;gt; mtu 1500 qdisc noop state &lt;span style="color:red;"&gt;DOWN &lt;/span&gt;group default qlen 1000</span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;ea:7f:a0:1f:00:3d&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt; link-netnsid 0</span>
<span class="nv">$ </span>ip netns <span class="nb">exec </span>BLUE ip addr
<span class="c"># =&gt; ...</span>
<span class="c">#    29: &lt;span style="color:teal;"&gt;beth0@if28: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST&amp;gt; mtu 1500 qdisc noop state &lt;span style="color:red;"&gt;DOWN &lt;/span&gt;group default qlen 1000</span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;66:23:89:a6:f7:70&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt; link-netnsid 0</span>

<span class="c"># 브릿지 정보 확인 </span>
<span class="nv">$ </span>brctl show
<span class="c"># =&gt; bridge name	bridge id		STP enabled	interfaces</span>
<span class="c">#    docker0		8000.02425756997c	no		</span>

<span class="c"># br0 브릿지 생성</span>
<span class="nv">$ </span>ip <span class="nb">link </span>add br0 <span class="nb">type </span>bridge

<span class="c"># br0 브릿지 정보 확인</span>
<span class="nv">$ </span>brctl show br0
<span class="c"># =&gt; bridge name	bridge id		STP enabled	interfaces</span>
<span class="c">#    br0		8000.000000000000	no		</span>
<span class="nv">$ </span>brctl showmacs br0
<span class="nv">$ </span>brctl showstp br0

<span class="c"># reth1과 beth1을 br0 브릿지에 연결</span>
<span class="nv">$ </span>ip <span class="nb">link set </span>reth1 master br0
<span class="nv">$ </span>ip <span class="nb">link set </span>beth1 master br0
<span class="nv">$ </span>brctl show br0
<span class="c"># =&gt; bridge name     bridge id               STP enabled     interfaces</span>
<span class="c">#    br0             8000.7e31cf5f008f       no              beth1</span>
<span class="c">#                                                            reth1</span>
<span class="nv">$ </span>brctl showmacs br0
<span class="c"># =&gt; port no mac addr                is local?       ageing timer</span>
<span class="c">#      2     7e:31:cf:5f:00:8f       yes                0.00</span>
<span class="c">#      2     7e:31:cf:5f:00:8f       yes                0.00</span>
<span class="c">#      1     9a:1f:bf:6f:fe:64       yes                0.00</span>
<span class="c">#      1     9a:1f:bf:6f:fe:64       yes                0.00</span>
<span class="nv">$ </span>ip <span class="nt">-br</span> <span class="nb">link</span>
<span class="c"># =&gt; ... </span>
<span class="c">#    &lt;span style="color:teal;"&gt;reth1@if27       &lt;/span&gt;&lt;span style="color:red;"&gt;DOWN           &lt;/span&gt;&lt;span style="color:olive;"&gt;9a:1f:bf:6f:fe:64 &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST&amp;gt; </span>
<span class="c">#    &lt;span style="color:teal;"&gt;beth1@if29       &lt;/span&gt;&lt;span style="color:red;"&gt;DOWN           &lt;/span&gt;&lt;span style="color:olive;"&gt;7e:31:cf:5f:00:8f &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST&amp;gt; </span>
<span class="c">#    &lt;span style="color:teal;"&gt;br0              &lt;/span&gt;&lt;span style="color:red;"&gt;DOWN           &lt;/span&gt;&lt;span style="color:olive;"&gt;7e:31:cf:5f:00:8f &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST&amp;gt; </span>

<span class="c"># reth0과 beth0에 IP 설정 및 활성화(UP) 시키고, reth1, beth1, br0를 활성화(UP) 합니다.</span>
<span class="nv">$ </span>ip netns <span class="nb">exec </span>RED  ip addr add 11.11.11.2/24 dev reth0
<span class="nv">$ </span>ip netns <span class="nb">exec </span>BLUE ip addr add 11.11.11.3/24 dev beth0
<span class="nv">$ </span>ip netns <span class="nb">exec </span>RED  ip <span class="nb">link set </span>reth0 up
<span class="nv">$ </span>ip <span class="nb">link set </span>reth1 up
<span class="nv">$ </span>ip netns <span class="nb">exec </span>BLUE ip <span class="nb">link set </span>beth0 up
<span class="nv">$ </span>ip <span class="nb">link set </span>beth1 up
<span class="nv">$ </span>ip <span class="nb">link set </span>br0 up
<span class="nv">$ </span>ip <span class="nt">-br</span> addr
<span class="c"># =&gt; ... </span>
<span class="c">#    &lt;span style="color:teal;"&gt;reth1@if27       &lt;/span&gt;&lt;span style="color:green;"&gt;UP             &lt;/span&gt;&lt;span style="color:blue;"&gt;fe80::981f:bfff:fe6f:fe64&lt;/span&gt;/64 </span>
<span class="c">#    &lt;span style="color:teal;"&gt;beth1@if29       &lt;/span&gt;&lt;span style="color:green;"&gt;UP             &lt;/span&gt;&lt;span style="color:blue;"&gt;fe80::7c31:cfff:fe5f:8f&lt;/span&gt;/64 </span>
<span class="c">#    &lt;span style="color:teal;"&gt;br0              &lt;/span&gt;&lt;span style="color:green;"&gt;UP             &lt;/span&gt;&lt;span style="color:blue;"&gt;fe80::7c31:cfff:fe5f:8f&lt;/span&gt;/64 </span>
<span class="nv">$ </span>ip netns <span class="nb">exec </span>RED ip <span class="nt">-br</span> addr
<span class="c"># =&gt; ...</span>
<span class="c">#    &lt;span style="color:teal;"&gt;reth0@if26       &lt;/span&gt;&lt;span style="color:green;"&gt;UP             &lt;/span&gt;&lt;span style="color:purple;"&gt;11.11.11.2&lt;/span&gt;/24 &lt;span style="color:blue;"&gt;fe80::e87f:a0ff:fe1f:3d&lt;/span&gt;/64 </span>
<span class="nv">$ </span>ip netns <span class="nb">exec </span>BLUE ip <span class="nt">-br</span> addr
<span class="c"># =&gt; ...</span>
<span class="c">#    &lt;span style="color:teal;"&gt;beth0@if28       &lt;/span&gt;&lt;span style="color:green;"&gt;UP             &lt;/span&gt;&lt;span style="color:purple;"&gt;11.11.11.3&lt;/span&gt;/24 &lt;span style="color:blue;"&gt;fe80::6423:89ff:fea6:f770&lt;/span&gt;/64 </span>

<span class="c"># 터미널1 (RED 11.11.11.2)</span>
<span class="nv">$ </span>nsenter <span class="nt">--net</span><span class="o">=</span>/var/run/netns/RED
<span class="nv">$ </span>ip <span class="nt">-c</span> a<span class="p">;</span><span class="nb">echo</span><span class="p">;</span> ip <span class="nt">-c</span> route<span class="p">;</span><span class="nb">echo</span><span class="p">;</span> ip <span class="nt">-c</span> neigh
<span class="c"># 현재 네트워크 네임스페이스 확인</span>
<span class="nv">$ </span>ip netns identify <span class="nv">$$</span>
<span class="c"># =&gt; RED</span>

<span class="c"># 터미널2 (호스트)</span>
<span class="nv">$ </span>brctl showmacs br0
<span class="nv">$ </span>bridge fdb show
<span class="nv">$ </span>bridge fdb show dev br0

<span class="nv">$ </span>iptables <span class="nt">-t</span> filter <span class="nt">-S</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> filter <span class="nt">-L</span> <span class="nt">-n</span> <span class="nt">-v</span>

<span class="c"># 터미널3 (BLUE 11.11.11.3)</span>
<span class="nv">$ </span>nsenter <span class="nt">--net</span><span class="o">=</span>/var/run/netns/BLUE
<span class="nv">$ </span>ip <span class="nt">-c</span> a<span class="p">;</span><span class="nb">echo</span><span class="p">;</span> ip <span class="nt">-c</span> route<span class="p">;</span><span class="nb">echo</span><span class="p">;</span> ip <span class="nt">-c</span> neigh
<span class="c"># 현재 네트워크 네임스페이스 확인</span>
<span class="nv">$ </span>ip netns identify <span class="nv">$$</span>
<span class="c"># =&gt; BLUE</span>

<span class="c"># 터미널2 (호스트)</span>
<span class="c"># ping 통신 전 사전 설정</span>
<span class="c">## iptables 정보 확인</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> filter <span class="nt">-S</span> | <span class="nb">grep</span> <span class="s1">'\-P'</span>
<span class="c"># =&gt; -P INPUT ACCEPT</span>
<span class="c">#    -P FORWARD DROP</span>
<span class="c">#    -P OUTPUT ACCEPT</span>
<span class="nv">$ </span>iptables <span class="nt">-nvL</span> <span class="nt">-t</span> filter

<span class="c"># 호스트에서 패킷 라우팅 설정 확인 - 0(off), 1(on)</span>
<span class="nv">$ </span><span class="nb">cat</span> /proc/sys/net/ipv4/ip_forward
<span class="c"># =&gt; 1</span>
<span class="c"># 위의 결과가 0인 경우 아래의 명령을 실행</span>
<span class="c"># echo 1 &gt; /proc/sys/net/ipv4/ip_forward</span>

<span class="c"># ping 통신 확인</span>
<span class="c"># 터미널2 (호스트)</span>
<span class="nv">$ </span>tcpdump <span class="nt">-l</span> <span class="nt">-i</span> br0
<span class="c"># =&gt; tcpdump: verbose output suppressed, use -v[v]... for full protocol decode</span>
<span class="c">#    listening on br0, link-type EN10MB (Ethernet), snapshot length 262144 bytes</span>
<span class="c">#    (터미널1에서 ping 실행시)</span>
<span class="c">#    08:40:00.413198 IP 11.11.11.2 &gt; 11.11.11.3: ICMP echo request, id 41028, seq 1, length 64</span>
<span class="c">#    08:40:05.455528 ARP, Request who-has 11.11.11.3 tell 11.11.11.2, length 28</span>
<span class="c">#    08:40:05.455556 ARP, Reply 11.11.11.3 is-at 66:23:89:a6:f7:70 (oui Unknown), length 28</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s1">'iptables -v --numeric --table filter --list FORWARD'</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s1">'iptables -v --numeric --table filter --list FORWARD;echo;iptables -v --numeric --table filter --list DOCKER-USER;echo;iptables -v --numeric --table filter --list DOCKER-ISOLATION-STAGE-1'</span>

<span class="c"># 터미널3 (BLUE 11.11.11.3)</span>
<span class="nv">$ </span>tcpdump <span class="nt">-l</span> <span class="nt">-i</span> beth0

<span class="c"># 터미널1 (RED 11.11.11.2)</span>
<span class="nv">$ </span>ping 11.11.11.3 <span class="nt">-c</span> 1
<span class="c"># =&gt; 실패</span>
</code></pre></div></div>

<p>위의 캡쳐와 같이 브릿지에서는 패킷이 잡히지만, 브릿지를 통해 Blue로 패킷이 전달되지 않는것을 확인할 수 있습니다.
그렇다면 왜 패킷이 전달되지 않을까요? 그것은 <code class="language-plaintext highlighter-rouge">iptables -t filter -S | grep '\-P'</code> 명령을 통해 확인했을때 FORWARD 체인이 DROP으로 설정되어 있기 때문입니다.
패킷이 브릿지를 통해 전달되려면 FORWARD 체인을 통해야 하는데 DROP이면 패킷이 전달되지 않습니다.</p>

<p><img src="/assets/2024/kans-3th/w1/20240831_kans_w1_20.png" alt="img.png" class="image-center" />
<em class="image-caption">Iptables 처리 흐름도 (<a href="https://natnat1.medium.com/iptables-b9ce0602253f">https://natnat1.medium.com/iptables-b9ce0602253f</a>)</em></p>

<ul>
  <li>위의 그림과 같이 iptables는 패킷이 들어오면 PREROUTING 체인을 통해 패킷을 처리하고, FORWARD 체인을 통해 패킷을 전달합니다.</li>
</ul>

<p><img src="/assets/2024/kans-3th/w1/20240831_kans_w1_21.png" alt="img.png" /></p>

<ul>
  <li>br0 입장에서 살펴보면 위와 같습니다. 그렇다면 11.11.11.2 &lt;=&gt; 11.11.11.3으로 FORWARD를 허용하면 되는데 방법을 살펴보면 아래와 같습니다.
    <ul>
      <li>출발지 11.11.11.2와 11.11.11.3 허용</li>
      <li>도착지 11.11.11.0/24 대역 출발지 허용</li>
      <li>FORWARD 기본 정책을 ACCEPT로 변경</li>
      <li>등등 기타 어떤 방법으로든 11.11.11.2와 11.11.11.3이 FORWARD 체인을 통해 패킷이 전달되도록 설정하면 됩니다.</li>
    </ul>
  </li>
  <li>실습을 통해 iptables를 통해 패킷이 전달되도록 설정해보겠습니다.</li>
  <li>방법1. 11.11.11.2와 11.11.11.3 허용하기
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 터미널2 (호스트)</span>
<span class="c"># 출발지 11.11.11.2 허용하기</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> filter <span class="nt">-A</span> FORWARD <span class="nt">-s</span> 11.11.11.2/32 <span class="nt">-j</span> ACCEPT
<span class="nv">$ </span>iptables <span class="nt">-t</span> filter <span class="nt">-A</span> FORWARD <span class="nt">-s</span> 11.11.11.3/32 <span class="nt">-j</span> ACCEPT
<span class="nv">$ </span>tcpdump <span class="nt">-l</span> <span class="nt">-i</span> br0
  
<span class="c"># 터미널3 (BLUE 11.11.11.3)</span>
<span class="nv">$ </span>tcpdump <span class="nt">-l</span> <span class="nt">-i</span> beth0
<span class="c"># =&gt; tcpdump: verbose output suppressed, use -v[v]... for full protocol decode</span>
<span class="c">#    listening on beth0, link-type EN10MB (Ethernet), snapshot length 262144 bytes</span>
<span class="c">#    10:39:20.225225 IP 11.11.11.2 &gt; 11.11.11.3: ICMP echo request, id 33335, seq 1, length 64</span>
<span class="c">#    10:39:20.225233 IP 11.11.11.3 &gt; 11.11.11.2: ICMP echo reply, id 33335, seq 1, length 64</span>
  
<span class="c"># 터미널1 (RED 11.11.11.2)</span>
<span class="nv">$ </span>ping 11.11.11.3
<span class="c"># =&gt; PING 11.11.11.3 (11.11.11.3) 56(84) bytes of data.</span>
<span class="c">#    64 bytes from 11.11.11.3: icmp_seq=1 ttl=64 time=0.055 ms</span>
<span class="c">#    </span>
<span class="c">#    --- 11.11.11.3 ping statistics ---</span>
<span class="c">#    1 packets transmitted, 1 received, 0% packet loss, time 0ms</span>
<span class="c">#    rtt min/avg/max/mdev = 0.055/0.055/0.055/0.000 ms</span>
  
<span class="c"># 터미널2 (호스트)</span>
<span class="c"># 허용 룰 제거</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> filter <span class="nt">-D</span> FORWARD <span class="nt">-s</span> 11.11.11.2/32 <span class="nt">-j</span> ACCEPT
<span class="nv">$ </span>iptables <span class="nt">-t</span> filter <span class="nt">-D</span> FORWARD <span class="nt">-s</span> 11.11.11.3/32 <span class="nt">-j</span> ACCEPT
</code></pre></div>    </div>

    <p><img src="/assets/2024/kans-3th/w1/20240831_kans_w1_22.png" alt="img.png" /></p>
  </li>
  <li>방법2. 도착지 11.11.11.0/24 대역 허용하기
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 터미널2 (호스트)</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> filter <span class="nt">-A</span> FORWARD <span class="nt">-d</span> 11.11.11.0/24 <span class="nt">-j</span> ACCEPT
<span class="c"># 테스트 후 허용 룰 제거</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> filter <span class="nt">-D</span> FORWARD <span class="nt">-d</span> 11.11.11.0/24 <span class="nt">-j</span> ACCEPT
</code></pre></div>    </div>
  </li>
  <li>방법3. FORWARD 기본 정책을 ACCEPT로 변경하기
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 터미널2 (호스트)</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> filter <span class="nt">-P</span> FORWARD ACCEPT
<span class="c"># 테스트 후 허용 룰 제거</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> filter <span class="nt">-P</span> FORWARD DROP
</code></pre></div>    </div>
  </li>
</ul>

<h3 id="호스트--redblue-네트워크-네임스페이스로-접근하기">호스트 &lt;=&gt; RED/BLUE 네트워크 네임스페이스로 접근하기</h3>

<ul>
  <li>“Red &lt;- Bridge (br0) -&gt; Blue 네트워크 네임스페이스 간 통신”을 실습한 환경에 이어서 실습해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 터미널1 (RED 11.11.11.2)</span>
<span class="nv">$ </span>nsenter <span class="nt">--net</span><span class="o">=</span>/var/run/netns/RED
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> any

<span class="c"># 터미널3를 호스트 네트워크로 변경합니다.</span>
<span class="nv">$ </span><span class="nb">exit</span>
<span class="nv">$ </span>ip netns identify <span class="nv">$$</span>
<span class="c"># =&gt; (공백)</span>
<span class="nv">$ </span>tcpdump <span class="nt">-i</span> br0 <span class="nt">-n</span>

<span class="c"># 터미널2 (호스트)</span>
<span class="nv">$ </span>ping <span class="nt">-c</span> 1 11.11.11.2
<span class="c"># =&gt; 1 packets transmitted, 0 received, 100% packet loss, time 0ms</span>
<span class="c"># (ping 이 실패합니다.)</span>
</code></pre></div></div>

<ul>
  <li>호스트에서 RED (11.11.11.2) 로 ping시 패킷이 전달되지 않는것을 확인할 수 있습니다.</li>
  <li>그 이유는 RED 네트워크로 접근하기 위해서는 br0를 거쳐서 접근해야하는데, br0는 ip가 없기 때문에 패킷이 전달되지 않습니다.</li>
  <li>br0에 ip를 할당하고, RED와 BLUE 네트워크 네임스페이스로 접근해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 터미널2 (호스트)</span>
<span class="nv">$ </span>ip addr add 11.11.11.1/24 dev br0
<span class="nv">$ </span>ip addr
<span class="c"># =&gt; ...</span>
<span class="c">#    30: &lt;span style="color:teal;"&gt;br0: &lt;/span&gt;&amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state &lt;span style="color:green;"&gt;UP &lt;/span&gt;group default qlen 1000</span>
<span class="c">#        link/ether &lt;span style="color:olive;"&gt;7e:31:cf:5f:00:8f&lt;/span&gt; brd &lt;span style="color:olive;"&gt;ff:ff:ff:ff:ff:ff&lt;/span&gt;</span>
<span class="c">#        inet &lt;span style="color:purple;"&gt;11.11.11.1&lt;/span&gt;/24 scope global br0</span>
<span class="c">#           valid_lft forever preferred_lft forever</span>
<span class="nv">$ </span>ping 11.11.11.2 <span class="nt">-c</span> 1
<span class="c"># =&gt; PING 11.11.11.2 (11.11.11.2) 56(84) bytes of data.</span>
<span class="c">#    64 bytes from 11.11.11.2: icmp_seq=1 ttl=64 time=0.044 ms</span>
<span class="c">#    </span>
<span class="c">#    --- 11.11.11.2 ping statistics ---</span>
<span class="c">#    1 packets transmitted, 1 received, 0% packet loss, time 0ms</span>
<span class="c">#    rtt min/avg/max/mdev = 0.044/0.044/0.044/0.000 ms</span>
<span class="nv">$ </span>ping 11.11.11.3 <span class="nt">-c</span> 1
<span class="c"># =&gt; PING 11.11.11.3 (11.11.11.3) 56(84) bytes of data.</span>
<span class="c">#    64 bytes from 11.11.11.3: icmp_seq=1 ttl=64 time=0.052 ms</span>
<span class="c">#    </span>
<span class="c">#    --- 11.11.11.3 ping statistics ---</span>
<span class="c">#    1 packets transmitted, 1 received, 0% packet loss, time 0ms</span>
<span class="c">#    rtt min/avg/max/mdev = 0.052/0.052/0.052/0.000 ms</span>
</code></pre></div></div>

<ul>
  <li>이번에는 RED에서 호스트로 ping이 되는것을 확인해 보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 터미널1 (RED 11.11.11.2)</span>
<span class="c"># br0 에 ping 테스트</span>
<span class="nv">$ </span>ping 11.11.11.1 <span class="nt">-c</span> 1
<span class="c"># =&gt; PING 11.11.11.1 (11.11.11.1) 56(84) bytes of data.</span>
<span class="c">#    64 bytes from 11.11.11.1: icmp_seq=1 ttl=64 time=0.041 ms</span>
<span class="c">#    </span>
<span class="c">#    --- 11.11.11.1 ping statistics ---</span>
<span class="c">#    1 packets transmitted, 1 received, 0% packet loss, time 0ms</span>
<span class="c">#    rtt min/avg/max/mdev = 0.041/0.041/0.041/0.000 ms</span>

<span class="c"># 호스트로 ping 테스트</span>
<span class="nv">$ </span>ping 10.10.10.51 <span class="nt">-c</span> 1
<span class="c"># =&gt; ping: connect: Network is unreachable</span>
</code></pre></div></div>

<ul>
  <li>br0에는 ping 이 성공하는데 호스트로는 Network is unreachable 에러가 발생하는것을 확인할 수 있습니다.</li>
  <li>그 이유는 11.11.11.0/24에서 호스트 네트워크인 10.10.10.0/24로 패킷을 라우팅하는 정보가 없기 때문입니다.</li>
  <li>RED나 BLUE에서 호스트로 패킷을 전달하기 위해서는 br0를 통해야 하는데, RED와 BLUE에 기본 게이트웨이를 br0로 설정하여 테스트해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 터미널1 (RED 11.11.11.2)</span>
<span class="nv">$ </span>ip route add default via 11.11.11.1
<span class="nv">$ </span>ip route
<span class="c"># =&gt; default via 11.11.11.1 dev reth0</span>
<span class="c">#    11.11.11.0/24 dev reth0 proto kernel scope link src 11.11.11.2</span>
<span class="nv">$ </span>ping 10.10.10.51 <span class="nt">-c</span> 1
<span class="c"># =&gt; PING 10.10.10.51 (10.10.10.51) 56(84) bytes of data.</span>
<span class="c">#    64 bytes from 10.10.10.51: icmp_seq=1 ttl=64 time=0.041 ms</span>
<span class="c">#    </span>
<span class="c">#    --- 10.10.10.51 ping statistics ---</span>
<span class="c">#    1 packets transmitted, 1 received, 0% packet loss, time 0ms</span>
<span class="nv">$ </span><span class="nb">exit</span>

<span class="c"># BLUE에서도 동일하게 테스트해보겠습니다</span>

<span class="c"># 터미널1 (BLUE 11.11.11.3)</span>
<span class="nv">$ </span>nsenter <span class="nt">--net</span><span class="o">=</span>/var/run/netns/BLUE
<span class="nv">$ </span>ip netns identify <span class="nv">$$</span>
<span class="c"># =&gt; BLUE</span>
<span class="nv">$ </span>ping 10.10.10.51 <span class="nt">-c</span> 1
<span class="c"># =&gt; ping: connect: Network is unreachable</span>
<span class="nv">$ </span>ip route add default via 11.11.11.1
<span class="nv">$ </span>ip route
<span class="c"># =&gt; default via 11.11.11.1 dev beth0</span>
<span class="c">#    11.11.11.0/24 dev beth0 proto kernel scope link src 11.11.11.3</span>
<span class="nv">$ </span>ping 10.10.10.51 <span class="nt">-c</span> 1
<span class="c"># =&gt; PING 10.10.10.51 (10.10.10.51) 56(84) bytes of data.</span>
<span class="c">#    64 bytes from 10.10.10.51: icmp_seq=1 ttl=64 time=0.049 ms</span>
<span class="c">#    </span>
<span class="c">#    --- 10.10.10.51 ping statistics ---</span>
<span class="c">#    1 packets transmitted, 1 received, 0% packet loss, time 0ms</span>
<span class="c">#    rtt min/avg/max/mdev = 0.049/0.049/0.049/0.000 ms</span>
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">ip route add default via 11.11.11.1</code> 로 기본 게이트웨이를 br0로 설정하고, 호스트로 ping이 되는것을 확인할 수 있습니다.</li>
</ul>

<h3 id="redblue에서-외부-인터넷-통신">RED/BLUE에서 외부 인터넷 통신</h3>

<ul>
  <li>이번에는 RED와 BLUE 네트워크 네임스페이스에서 외부 인터넷으로 통신하는 방법을 실습해보겠습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 터미널1 (RED 11.11.11.2)</span>
<span class="nv">$ </span>nsenter <span class="nt">--net</span><span class="o">=</span>/var/run/netns/RED
<span class="nv">$ </span>ping 8.8.8.8 <span class="nt">-c</span> 1
<span class="c"># =&gt; PING 8.8.8.8 (8.8.8.8) 56(84) bytes of data.</span>
<span class="c">#    </span>
<span class="c">#    --- 8.8.8.8 ping statistics ---</span>
<span class="c">#    1 packets transmitted, 0 received, 100% packet loss, time 0ms</span>
</code></pre></div></div>

<ul>
  <li>RED에서 외부 인터넷으로 ping이 되지 않는것을 확인할 수 있습니다.</li>
  <li>RED/BLUE와 같이 호스트 아래의 내부 네트워크에서 외부 인터넷으로 패킷을 전달하기 위해서는
호스트의 IP로 패킷을 전달하고, 응답을 호스트 IP로 받아서 내부 네트워크(RED/BLUE)로 전달해야하는데, 
이러한 과정을 SNAT (Source Network Address Translation) 또는 MASQUERADE라고 합니다.</li>
  <li>nat 테이블의 POSTROUTING 체인에 MASQUERADE 룰을 추가하면 SNAT이 적용되어서 외부 인터넷으로 패킷을 전달할 수 있습니다.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 터미널2 (호스트)</span>
<span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-A</span> POSTROUTING <span class="nt">-s</span> 11.11.11.0/24 <span class="nt">-j</span> MASQUERADE
<span class="c"># SNAT 통계 모니터링</span>
<span class="nv">$ </span>watch <span class="nt">-d</span> <span class="s1">'iptables -v --numeric --table nat --list POSTROUTING'</span>
<span class="nv">$ </span>iptables <span class="nt">-nvL</span> <span class="nt">-t</span> nat
<span class="nv">$ </span>conntrack <span class="nt">-L</span> <span class="nt">--src-nat</span>
<span class="c"># =&gt; icmp     1 29 src=11.11.11.2 dst=8.8.8.8 type=8 code=0 id=62779 src=8.8.8.8 dst=10.10.10.109 type=0 code=0 id=62779 mark=0 use=1</span>
<span class="c">#    conntrack v1.4.8 (conntrack-tools): 1 flow entries have been shown.</span>

<span class="c"># 터미널1 (RED 11.11.11.2)</span>
<span class="nv">$ </span>ping 8.8.8.8 <span class="nt">-c</span> 1
<span class="c"># =&gt; PING 8.8.8.8 (8.8.8.8) 56(84) bytes of data.</span>
<span class="c">#    64 bytes from 8.8.8.8: icmp_seq=1 ttl=113 time=26.3 ms</span>
<span class="c">#    </span>
<span class="c">#    --- 8.8.8.8 ping statistics ---</span>
<span class="c">#    1 packets transmitted, 1 received, 0% packet loss, time 0ms</span>
<span class="c">#    rtt min/avg/max/mdev = 26.277/26.277/26.277/0.000 ms</span>
<span class="nv">$ </span><span class="nb">exit</span>

<span class="c"># 터미널1 (BLUE 11.11.11.3)</span>
<span class="nv">$ </span>nsenter <span class="nt">--net</span><span class="o">=</span>/var/run/netns/BLUE
<span class="nv">$ </span>ip route add default via 11.11.11.1
<span class="nv">$ </span>ping 8.8.8.8 <span class="nt">-c</span> 1
<span class="nv">$ </span><span class="nb">exit</span>

<span class="c"># 삭제</span>
<span class="nv">$ </span>ip netns delete RED
<span class="nv">$ </span>ip netns delete BLUE
<span class="nv">$ </span>ip <span class="nb">link </span>delete br0

<span class="nv">$ </span>iptables <span class="nt">-t</span> nat <span class="nt">-D</span> POSTROUTING <span class="nt">-s</span> 11.11.11.0/24 <span class="nt">-j</span> MASQUERADE
</code></pre></div></div>

<ul>
  <li>SNAT 추가한 이후 ping이 잘 되는것을 확인할 수 있었습니다.</li>
</ul>

<hr />

<h2 id="마치며">마치며</h2>

<p>첫주부터 이론과 실습할것이 굉장히 많았습니다. 테라폼 스터디가 순한맛으로 보일 정도입니다. 😅
하지만 그동안 막연하게 알고 있었던 도커 컨테이너의 격리 원리와 리눅스 네트워크와 iptables에 
대해 더 깊게 이해할 수 있어서 좋았습니다.</p>

<p><del>개인적으로 *BSD를 좋아하는데 이 정도면 FreeBSD에서도 BSD만의 docker 같은 에코시스템 구축이 가능할것 같은데
왜 못하고 있는지 의문입니다. 비슷하게 돌릴 수 있는 다양한 시도들은 많은데 흐지부지 되는 이유는 대체 무엇인지..</del></p>

<p>항상 무언가를 배우는것은 즐겁습니다. 다음 스터디도 기대됩니다! :smile:</p>]]></content><author><name></name></author><category term="kans" /><category term="kubernetes," /><category term="network," /><category term="linux" /><summary type="html"><![CDATA[지난 테라폼 스터디에 이어 이번 주 부터 KANS 스터디를 시작하게 되었습니다! KANS는 Kubernetes Advanced Networking Study의 줄임말로 쿠버네티스 네트워킹에 대한 심도있게 공부하는 스터디입니다. 이번 스터디도 과제할 걱정도 되지만 재미있을것 같아 기대됩니다.]]></summary></entry></feed>